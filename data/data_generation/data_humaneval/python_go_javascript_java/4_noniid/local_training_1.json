[{"instruction": "def _construct_regex(cls, fmt):\n        \"\"\"Given a format string, construct the regex with class attributes.\"\"\"\n", "input": "", "output": "        return re.compile(fmt.format(**vars(cls)), flags=re.U)"}, {"instruction": "def triangle(self, x1, y1, x2, y2, x3, y3, color):\n        \"\"\"\n        See the Processing function triangle():\n        https://processing.org/reference/triangle_.html\n        \"\"\"\n", "input": "", "output": "        self.context.set_source_rgb(*color)\n        self.context.move_to(self.tx(x1), self.ty(y1))\n        self.context.line_to(self.tx(x2), self.ty(y2))\n        self.context.line_to(self.tx(x3), self.ty(y3))\n        self.context.line_to(self.tx(x1), self.ty(y1))\n        self.context.fill()"}, {"instruction": "def validate(self, instance, value):\n        \"\"\"Checks if value is an open PNG file, valid filename, or png.Image\n\n        Returns an open bytestream of the image\n        \"\"\"\n", "input": "", "output": "        # Pass if already validated\n        if getattr(value, '__valid__', False):\n            return value\n        # Validate that value is PNG\n        if isinstance(value, png.Image):\n            pass\n        else:\n            value = super(ImagePNG, self).validate(instance, value)\n            try:\n                png.Reader(value).validate_signature()\n            except png.FormatError:\n                self.error(instance, value, extra='Open file is not PNG.')\n            value.seek(0)\n        # Write input to new bytestream\n        output = BytesIO()\n        output.name = self.filename\n        output.__valid__ = True\n        if isinstance(value, png.Image):\n            value.save(output)\n        else:\n            fid = value\n            fid.seek(0)\n            output.write(fid.read())\n            fid.close()\n        output.seek(0)\n        return output"}, {"instruction": "def send(self, s):\n        '''Send data to the subprocess' stdin.\n\n        Returns the number of bytes written.\n        '''\n", "input": "", "output": "        s = self._coerce_send_string(s)\n        self._log(s, 'send')\n\n        b = self._encoder.encode(s, final=False)\n        if PY3:\n            return self.proc.stdin.write(b)\n        else:\n            # On Python 2, .write() returns None, so we return the length of\n            # bytes written ourselves. This assumes they all got written.\n            self.proc.stdin.write(b)\n            return len(b)"}, {"instruction": "def try_imports(imports, fail_msg=None):\n    \"\"\"\n    Try import statements until one succeeds.\n\n    Parameters\n    ----------\n    imports : list\n        The list of import statements.\n    fail_msg : str\n        If not None and no statement succeeds, a `ValueError` is raised with\n        the given message, appended to all failed messages.\n\n    Returns\n    -------\n    locals : dict\n        The dictionary of imported modules.\n    \"\"\"\n", "input": "", "output": "    msgs = []\n    for imp in imports:\n        try:\n            exec imp\n            break\n\n        except Exception, inst:\n            msgs.append(str(inst))\n\n    else:\n        if fail_msg is not None:\n            msgs.append(fail_msg)\n            raise ValueError('\\n'.join(msgs))\n\n    return locals()"}, {"instruction": "def print_data(data):\n    \"\"\"Prints object key-value pairs in a custom format\n\n    :param data: The dict to print\n    :type data: dict\n    :rtype: None\n    \"\"\"\n", "input": "", "output": "    print(\", \".join([\"{}=>{}\".format(key, value) for key, value in data]))"}, {"instruction": "def join_session(self, sid):\n        \"\"\"Attach to an existing session.\"\"\"\n", "input": "", "output": "        self._rest.add_header('X-STC-API-Session', sid)\n        self._sid = sid\n        try:\n            status, data = self._rest.get_request('objects', 'system1',\n                                                  ['version', 'name'])\n        except resthttp.RestHttpError as e:\n            self._rest.del_header('X-STC-API-Session')\n            self._sid = None\n            raise RuntimeError('failed to join session \"%s\": %s' % (sid, e))\n\n        return data['version']"}, {"instruction": "def to_match(self):\n        \"\"\"Return a unicode object with the MATCH representation of this GlobalContextField.\"\"\"\n", "input": "", "output": "        self.validate()\n\n        mark_name, field_name = self.location.get_location_name()\n        validate_safe_string(mark_name)\n        validate_safe_string(field_name)\n\n        return u'%s.%s' % (mark_name, field_name)"}, {"instruction": "def locale_export():\n    \"\"\"Exports for dealing with Click-based programs and ASCII/Unicode errors.\n\n    RuntimeError: Click will abort further execution because Python 3 was\n    configured to use ASCII as encoding for the environment.\n    Consult https://click.palletsprojects.com/en/7.x/python3/ for mitigation steps.\n\n    Looks up available locales on the system to find an appropriate one to pick,\n    defaulting to C.UTF-8 which is globally available on newer systems.\n    \"\"\"\n", "input": "", "output": "    locale_to_use = \"C.UTF-8\"\n    try:\n        locales = subprocess.check_output([\"locale\", \"-a\"]).decode(errors=\"ignore\").split(\"\\n\")\n    except subprocess.CalledProcessError:\n        locales = []\n    for locale in locales:\n        if locale.lower().endswith((\"utf-8\", \"utf8\")):\n            locale_to_use = locale\n            break\n    return \"export LC_ALL=%s && export LANG=%s && \" % (locale_to_use, locale_to_use)"}, {"instruction": "def replace_nulls(hdrs):\n        \"\"\"Replace '' in hdrs.\"\"\"\n", "input": "", "output": "        ret = []\n        idx = 0\n        for hdr in hdrs:\n            if hdr == '':\n                ret.append(\"no_hdr{}\".format(idx))\n            else:\n                ret.append(hdr)\n        return ret"}, {"instruction": "def upsert(self, table: str, record: dict, create_cols: bool=False,\n               dtypes: list=None, pks=[\"id\"], namefields=[\"id\"]):\n        \"\"\"\n        Upsert a record in a table\n        \"\"\"\n", "input": "", "output": "        try:\n            self.db[table].upsert(record, pks, create_cols, dtypes)\n        except Exception as e:\n            self.err(e, \"Can not upsert data\")\n            return\n        names = \"\"\n        for el in namefields:\n            names += \" \" + record[el]\n        self.ok(\"Upserted record\"+names)"}, {"instruction": "def filter(self, record):\n        \"\"\"Add contextual information to the log record\n\n        :param record: the log record\n        :type record: :class:`logging.LogRecord`\n        :returns: True, if log should get sent\n        :rtype: :class:`bool`\n        :raises: None\n        \"\"\"\n", "input": "", "output": "        record.sitename = self.sitename\n        record.platform = self.platform\n        record.jobid = self.jobid\n        record.submitter = self.logname\n        record.jobname = self.jobname\n        record.queue = self.queue\n        record.fqdn = self.fqdn\n        return True"}, {"instruction": "def truncate(self, percentage):\n        \"\"\"\n        Truncate ``percentage`` / 2 [%] of whole time from first and last time.\n\n        :param float percentage: Percentage of truncate.\n\n        :Sample Code:\n            .. code:: python\n\n                from datetimerange import DateTimeRange\n                time_range = DateTimeRange(\n                    \"2015-03-22T10:00:00+0900\", \"2015-03-22T10:10:00+0900\")\n                time_range.is_output_elapse = True\n                print(time_range)\n                time_range.truncate(10)\n                print(time_range)\n        :Output:\n            .. parsed-literal::\n\n                2015-03-22T10:00:00+0900 - 2015-03-22T10:10:00+0900 (0:10:00)\n                2015-03-22T10:00:30+0900 - 2015-03-22T10:09:30+0900 (0:09:00)\n        \"\"\"\n", "input": "", "output": "\n        self.validate_time_inversion()\n\n        if percentage < 0:\n            raise ValueError(\"discard_percent must be greater or equal to zero: \" + str(percentage))\n\n        if percentage == 0:\n            return\n\n        discard_time = self.timedelta // int(100) * int(percentage / 2)\n\n        self.__start_datetime += discard_time\n        self.__end_datetime -= discard_time"}, {"instruction": "def get_element_centroids(self):\n        \"\"\"return the central points of all elements\n\n        Returns\n        -------\n        Nx2 array\n            x/z coordinates for all (N) elements\n        \"\"\"\n", "input": "", "output": "        centroids = np.vstack((\n            np.mean(self.grid['x'], axis=1), np.mean(self.grid['z'], axis=1)\n        )).T\n\n        return centroids"}, {"instruction": "def is_cnpjcpf(numero, estrito=False):\n    \"\"\"Uma vers\u00e3o conveniente para usar em testes condicionais. Apenas retorna\n    verdadeiro ou falso, conforme o argumento \u00e9 validado.\n\n    :param bool estrito: Padr\u00e3o ``False``, indica se apenas os d\u00edgitos do\n        n\u00famero dever\u00e3o ser considerados. Se verdadeiro, potenciais caracteres\n        que formam a m\u00e1scara ser\u00e3o removidos antes da valida\u00e7\u00e3o ser realizada.\n\n    \"\"\"\n", "input": "", "output": "    _numero = digitos(numero) if not estrito else numero\n    try:\n        cnpj(_numero)\n        return True\n    except NumeroCNPJError:\n        try:\n            cpf(_numero)\n            return True\n        except NumeroCPFError:\n            pass\n    return False"}, {"instruction": "def get_print_rect(self, grid_rect):\n        \"\"\"Returns wx.Rect that is correctly positioned on the print canvas\"\"\"\n", "input": "", "output": "\n        grid = self.grid\n\n        rect_x = grid_rect.x - \\\n            grid.GetScrollPos(wx.HORIZONTAL) * grid.GetScrollLineX()\n        rect_y = grid_rect.y - \\\n            grid.GetScrollPos(wx.VERTICAL) * grid.GetScrollLineY()\n\n        return wx.Rect(rect_x, rect_y, grid_rect.width, grid_rect.height)"}, {"instruction": "def replace(self, child, *nodes):\n        r\"\"\"Replace provided node with node(s).\n\n        :param TexNode child: Child node to replace\n        :param TexNode nodes: List of nodes to subtitute in\n\n        >>> from TexSoup import TexSoup\n        >>> soup = TexSoup(r'''\n        ... \\begin{itemize}\n        ...     \\item Hello\n        ...     \\item Bye\n        ... \\end{itemize}''')\n        >>> items = list(soup.find_all('item'))\n        >>> bye = items[1]\n        >>> soup.itemize.replace(soup.item, bye)\n        >>> soup.itemize\n        \\begin{itemize}\n            \\item Bye\n        \\item Bye\n        \\end{itemize}\n        \"\"\"\n", "input": "", "output": "        self.expr.insert(\n            self.expr.remove(child.expr),\n            *nodes)"}, {"instruction": "def check_positive_integer(name, value):\n    \"\"\"Check a value is a positive integer.\n\n    Returns the value if so, raises ValueError otherwise.\n\n    \"\"\"\n", "input": "", "output": "    try:\n        value = int(value)\n        is_positive = (value > 0)\n    except ValueError:\n        raise ValueError('%s should be an integer; got %r' % (name, value))\n\n    if is_positive:\n        return value\n    else:\n        raise ValueError('%s should be positive; got %r' % (name, value))"}, {"instruction": "def create(path, docker_compose):\n    '''\n    Create and validate a docker-compose file into a directory\n\n    path\n        Path where the docker-compose file will be stored on the server\n\n    docker_compose\n        docker_compose file\n\n    CLI Example:\n\n    .. code-block:: bash\n\n        salt myminion dockercompose.create /path/where/docker-compose/stored content\n    '''\n", "input": "", "output": "    if docker_compose:\n        ret = __write_docker_compose(path,\n                                     docker_compose,\n                                     already_existed=False)\n        if isinstance(ret, dict):\n            return ret\n    else:\n        return __standardize_result(False,\n                                    'Creating a docker-compose project failed, you must send a valid docker-compose file',\n                                    None, None)\n    return __standardize_result(True,\n                                'Successfully created the docker-compose file',\n                                {'compose.base_dir': path},\n                                None)"}, {"instruction": "def findall(dir=os.curdir):\n    \"\"\"\n    Find all files under 'dir' and return the list of full filenames.\n    Unless dir is '.', return full filenames with dir prepended.\n    \"\"\"\n", "input": "", "output": "    files = _find_all_simple(dir)\n    if dir == os.curdir:\n        make_rel = functools.partial(os.path.relpath, start=dir)\n        files = map(make_rel, files)\n    return list(files)"}, {"instruction": "def mode(self):\n        \"\"\"\n        Reading returns the currently selected mode. Writing sets the mode.\n        Generally speaking when the mode changes any sensor or motor devices\n        associated with the port will be removed new ones loaded, however this\n        this will depend on the individual driver implementing this class.\n        \"\"\"\n", "input": "", "output": "        self._mode, value = self.get_attr_string(self._mode, 'mode')\n        return value"}, {"instruction": "def _create_table(self, table_name):\n        ''' create sqlite's table for storing simple dictionaries\n        '''\n", "input": "", "output": "        if self.fieldnames:\n            sql_fields = []\n            for field in self._fields:\n                if field != '_id':\n                    if 'dblite' in self._fields[field]:\n                        sql_fields.append(' '.join([field, self._fields[field]['dblite']]))\n                    else:\n                        sql_fields.append(field)\n            sql_fields = ','.join(sql_fields)\n            SQL = 'CREATE TABLE IF NOT EXISTS %s (%s);' % (table_name, sql_fields)\n            try:\n                self._cursor.execute(SQL)\n            except sqlite3.OperationalError, err:\n                raise RuntimeError('Create table error, %s, SQL: %s' % (err, SQL))"}, {"instruction": "def cancel(self):\n        \"\"\"\n        Cancel itself and following NOTs as far as possible.\n        Returns the simplified expression.\n        \"\"\"\n", "input": "", "output": "        expr = self\n        while True:\n            arg = expr.args[0]\n            if not isinstance(arg, self.__class__):\n                return expr\n            expr = arg.args[0]\n            if not isinstance(expr, self.__class__):\n                return expr"}, {"instruction": "def get_formats(\n        self, token: dict = None, format_code: str = None, prot: str = \"https\"\n    ) -> dict:\n        \"\"\"Get formats.\n\n        :param str token: API auth token\n        :param str format_code: code of a specific format\n        :param str prot: https [DEFAULT] or http\n         (use it only for dev and tracking needs).\n        \"\"\"\n", "input": "", "output": "        # if specific format\n        if isinstance(format_code, str):\n            specific_format = \"/{}\".format(format_code)\n        else:\n            specific_format = \"\"\n\n        # search request\n        req_url = \"{}://v1.{}.isogeo.com/formats{}\".format(\n            prot, self.api_url, specific_format\n        )\n\n        req = self.get(\n            req_url, headers=self.header, proxies=self.proxies, verify=self.ssl\n        )\n\n        # checking response\n        checker.check_api_response(req)\n\n        # end of method\n        return req.json()"}, {"instruction": "def getControls(self):\n        '''\n        Calculates consumption for each consumer of this type using the consumption functions.\n\n        Parameters\n        ----------\n        None\n\n        Returns\n        -------\n        None\n        '''\n", "input": "", "output": "        cLvlNow = np.zeros(self.AgentCount) + np.nan\n        MPCnow = np.zeros(self.AgentCount) + np.nan\n        for t in range(self.T_cycle):\n            these = t == self.t_cycle\n            cLvlNow[these] = self.solution[t].cFunc(self.mLvlNow[these],self.pLvlNow[these])\n            MPCnow[these]  = self.solution[t].cFunc.derivativeX(self.mLvlNow[these],self.pLvlNow[these])\n        self.cLvlNow = cLvlNow\n        self.MPCnow  = MPCnow"}, {"instruction": "def add_selected(self, ):\n        \"\"\"Create a new reftrack with the selected element and type and add it to the root.\n\n        :returns: None\n        :rtype: None\n        :raises: NotImplementedError\n        \"\"\"\n", "input": "", "output": "        browser = self.shot_browser if self.browser_tabw.currentIndex() == 1 else self.asset_browser\n        selelements = browser.selected_indexes(2)\n        if not selelements:\n            return\n        seltypes = browser.selected_indexes(3)\n        if not seltypes:\n            return\n        elementi = selelements[0]\n        typi = seltypes[0]\n        if not elementi.isValid() or not typi.isValid():\n            return\n        element = elementi.internalPointer().internal_data()\n        typ = typi.internalPointer().internal_data()[0]\n\n        reftrack.Reftrack(self.root, self.refobjinter, typ=typ, element=element)"}, {"instruction": "def _full_pipeline(self):\n        \"\"\"Return the full aggregation pipeline for this ChangeStream.\"\"\"\n", "input": "", "output": "        options = self._pipeline_options()\n        full_pipeline = [{'$changeStream': options}]\n        full_pipeline.extend(self._pipeline)\n        return full_pipeline"}, {"instruction": "def parse_comments_for_file(filename):\n    \"\"\"\n    Return a list of all parsed comments in a file.  Mostly for testing &\n    interactive use.\n    \"\"\"\n", "input": "", "output": "    return [parse_comment(strip_stars(comment), next_line)\n            for comment, next_line in get_doc_comments(read_file(filename))]"}, {"instruction": "def load_fn_matches_ext(file_path, file_type):\n    \"\"\"\n    Check that the file extension matches the target extension given.\n\n    :param str file_path: Path to be checked\n    :param str file_type: Target extension\n    :return bool correct_ext: Extension match or does not match\n    \"\"\"\n", "input": "", "output": "    correct_ext = False\n    curr_ext = os.path.splitext(file_path)[1]\n    exts = [curr_ext, file_type]\n    try:\n        # special case: if file type is excel, both extensions are valid.\n        if \".xlsx\" in exts and \".xls\" in exts:\n            correct_ext = True\n        elif curr_ext == file_type:\n            correct_ext = True\n        else:\n            print(\"Use '{}' to load this file: {}\".format(FILE_TYPE_MAP[curr_ext][\"load_fn\"],\n                                                          os.path.basename(file_path)))\n    except Exception as e:\n        logger_misc.debug(\"load_fn_matches_ext: {}\".format(e))\n\n    return correct_ext"}, {"instruction": "def from_bytes(cls, data):\n        \"\"\"\n        I am so sorry.\n        \"\"\"\n", "input": "", "output": "        len_username = int.from_bytes(data[0:2], byteorder=\"big\")\n        offset_username = 2 + len_username\n        username = data[2:offset_username].decode(\"UTF-8\")\n        offset_password = 2 + offset_username\n        len_password = int.from_bytes(\n            data[offset_username:offset_password], byteorder=\"big\"\n        )\n        pass_begin = offset_password\n        pass_end = offset_password + len_password\n        password = data[pass_begin:pass_end].decode(\"UTF-8\")\n\n        return cls(username, password)"}, {"instruction": "def __add_options(parser):\n    \"\"\"\n    Add the `Configure` options to a option-parser instance or a\n    option group.\n    \"\"\"\n", "input": "", "output": "    parser.add_option('--upx-dir', default=None,\n                      help='Directory containing UPX.')\n    parser.add_option('-C', '--configfile',\n                      default=DEFAULT_CONFIGFILE,\n                      dest='configfilename',\n                      help='Name of generated configfile (default: %default)')"}, {"instruction": "def future(self, rev=None):\n        \"\"\"Return a Mapping of items after the given revision.\n\n        Default revision is the last one looked up.\n\n        \"\"\"\n", "input": "", "output": "        if rev is not None:\n            self.seek(rev)\n        return WindowDictFutureView(self._future)"}, {"instruction": "def union(*argv):\n        \"\"\"Returns union of sets as a new set. basically it's\n        Items are ordered by set1, set2, ...\n        \n        **\u4e2d\u6587\u6587\u6863**\n        \n        \u6c42\u591a\u4e2a\u6709\u5e8f\u96c6\u5408\u7684\u5e76\u96c6, \u6309\u7167\u7b2c\u4e00\u4e2a\u96c6\u5408, \u7b2c\u4e8c\u4e2a, ..., \u8fd9\u6837\u7684\u987a\u5e8f\u3002\n        \"\"\"\n", "input": "", "output": "        res = OrderedSet()\n        for ods in argv:\n            res = res | ods\n        return res"}, {"instruction": "def data_filler_simple_registration(self, number_of_rows, conn):\n        '''creates and fills the table with simple regis. information\n        '''\n", "input": "", "output": "        cursor = conn.cursor()\n\n        cursor.execute("}, {"instruction": "def bipartition(seq):\n    \"\"\"Return a list of bipartitions for a sequence.\n\n    Args:\n        a (Iterable): The sequence to partition.\n\n    Returns:\n        list[tuple[tuple]]: A list of tuples containing each of the two\n        partitions.\n\n    Example:\n        >>> bipartition((1,2,3))\n        [((), (1, 2, 3)), ((1,), (2, 3)), ((2,), (1, 3)), ((1, 2), (3,))]\n    \"\"\"\n", "input": "", "output": "    return [(tuple(seq[i] for i in part0_idx),\n             tuple(seq[j] for j in part1_idx))\n            for part0_idx, part1_idx in bipartition_indices(len(seq))]"}, {"instruction": "def _add_temporary_results(self, results, label):\n        \"\"\"Adds `results` to a temporary table with `label`.\n\n        :param results: results file\n        :type results: `File`\n        :param label: label to be associated with results\n        :type label: `str`\n\n        \"\"\"\n", "input": "", "output": "        NGRAM, SIZE, NAME, SIGLUM, COUNT, LABEL = constants.QUERY_FIELDNAMES\n        reader = csv.DictReader(results)\n        data = [(row[NGRAM], row[SIZE], row[NAME], row[SIGLUM], row[COUNT],\n                 label) for row in reader]\n        self._conn.executemany(constants.INSERT_TEMPORARY_RESULTS_SQL, data)"}, {"instruction": "def __copy_tree(src_dir, dest_dir):\r\n    \"\"\"\r\n    The shutil.copytree() or distutils.dir_util.copy_tree() will happen to report\r\n    error list below if we invoke it again and again ( at least in python 2.7.4 ):\r\n\r\n    IOError: [Errno 2] No such file or directory: ...\r\n\r\n    So we have to write our's copy_tree() for that purpose.\r\n    \"\"\"\n", "input": "", "output": "\r\n    if not os.path.exists(dest_dir):\r\n        os.makedirs(dest_dir)\r\n        shutil.copystat(src_dir, dest_dir)\r\n\r\n    for entry in os.listdir(src_dir):\r\n        from_path = os.path.join(src_dir, entry)\r\n        to_path = os.path.join(dest_dir, entry)\r\n        if os.path.isdir(from_path):\r\n            __copy_tree(from_path, to_path)\r\n        else:\r\n            shutil.copy2(from_path, to_path)"}, {"instruction": "def _searchservices(device, name=None, uuid=None, uuidbad=None):\n    \"\"\"\n    Searches the given IOBluetoothDevice using the specified parameters.\n    Returns an empty list if the device has no services.\n\n    uuid should be IOBluetoothSDPUUID object.\n    \"\"\"\n", "input": "", "output": "    if not isinstance(device, _IOBluetooth.IOBluetoothDevice):\n        raise ValueError(\"device must be IOBluetoothDevice, was %s\" % \\\n            type(device))\n\n    services = []\n    allservices = device.getServices()\n    if uuid:\n        gooduuids = (uuid, )\n    else:\n        gooduuids = ()\n    if uuidbad:\n        baduuids = (uuidbad, )\n    else:\n        baduuids = ()\n\n    if allservices is not None:\n        for s in allservices:\n            if gooduuids and not s.hasServiceFromArray_(gooduuids):\n                continue\n            if baduuids and s.hasServiceFromArray_(baduuids):\n                continue\n            if name is None or s.getServiceName() == name:\n                services.append(s)\n    return services"}, {"instruction": "def add_image_description(self, dict):\n        \"\"\"Add a dict to image description.\"\"\"\n", "input": "", "output": "        if self._ef is not None:\n            self._ef['0th'][piexif.ImageIFD.ImageDescription] = json.dumps(\n                dict)"}, {"instruction": "def mapred(self, transport, inputs, query, timeout):\n        \"\"\"\n        mapred(inputs, query, timeout)\n\n        Executes a MapReduce query.\n\n        .. note:: This request is automatically retried :attr:`retries`\n           times if it fails due to network error.\n\n        :param inputs: the input list/structure\n        :type inputs: list, dict\n        :param query: the list of query phases\n        :type query: list\n        :param timeout: the query timeout\n        :type timeout: integer, None\n        :rtype: mixed\n        \"\"\"\n", "input": "", "output": "        _validate_timeout(timeout)\n        return transport.mapred(inputs, query, timeout)"}, {"instruction": "def collapse_all(self):\n        \"\"\"Collapse all items.\"\"\"\n", "input": "", "output": "\n        def aux(item):\n            self.item(item, open=False)\n            children = self.get_children(item)\n            for c in children:\n                aux(c)\n\n        children = self.get_children(\"\")\n        for c in children:\n            aux(c)"}, {"instruction": "def public_decrypt(pub, message):\n    '''\n    Verify an M2Crypto-compatible signature\n\n    :param Crypto.PublicKey.RSA._RSAobj key: The RSA public key object\n    :param str message: The signed message to verify\n    :rtype: str\n    :return: The message (or digest) recovered from the signature, or an\n        empty string if the verification failed\n    '''\n", "input": "", "output": "    if HAS_M2:\n        return pub.public_decrypt(message, salt.utils.rsax931.RSA_X931_PADDING)\n    else:\n        verifier = salt.utils.rsax931.RSAX931Verifier(pub.exportKey('PEM'))\n        return verifier.verify(message)"}, {"instruction": "def load(self, client, webpy_app, course_factory, task_factory, database, user_manager, submission_manager, config):\n        \"\"\" Loads the plugin manager. Must be done after the initialisation of the client \"\"\"\n", "input": "", "output": "        self._app = webpy_app\n        self._task_factory = task_factory\n        self._database = database\n        self._user_manager = user_manager\n        self._submission_manager = submission_manager\n        self._loaded = True\n        for entry in config:\n            module = importlib.import_module(entry[\"plugin_module\"])\n            module.init(self, course_factory, client, entry)"}, {"instruction": "def build_blast_cmd(self, fname, dbname):\n        \"\"\"Return BLASTN command\"\"\"\n", "input": "", "output": "        return self.funcs.blastn_func(fname, dbname, self.outdir, self.exes.blast_exe)"}, {"instruction": "def get_name_str(self, element):\n        '''get_name_str\n\n        High-level api: Produce a string that represents the name of a node.\n\n        Parameters\n        ----------\n\n        element : `Element`\n            A node in model tree.\n\n        Returns\n        -------\n\n        str\n            A string that represents the name of a node.\n        '''\n", "input": "", "output": "\n        if element.get('diff') == 'added':\n            return self.model2.get_name_str(element)\n        else:\n            return self.model1.get_name_str(element)"}, {"instruction": "def subdirs(self, pattern=None, sort_key=lambda k: k, sort_reverse=False, abspath=False):\n        \"\"\" Return a sorted list containing relative path of all subdirs (recursively).\n\n        :type pattern: str\n        :param pattern: Unix style (glob like/gitignore like) pattern\n\n        :param sort_key: key argument for sorted\n\n        :param sort_reverse: reverse argument for sorted\n\n        :rtype: list\n        :return: List of all relative files paths.\n        \"\"\"\n", "input": "", "output": "        return sorted(self.itersubdirs(pattern, abspath=abspath), key=sort_key, reverse=sort_reverse)"}, {"instruction": "def chunks(l, n):\n    \"\"\"Yield successive n-sized chunks from l.\"\"\"\n", "input": "", "output": "    if n:\n        for i in xrange(0, len(l), n):\n            yield l[i:i + n]"}, {"instruction": "def tr(text, kword, color):\n    \"\"\" tr(text, keyword, color)\n    \"\"\"\n", "input": "", "output": "    return re.sub(kword, colorize(BgColor.Null, Base.Null, color, kword), text)"}, {"instruction": "def _file_md5(file_):\n    \"\"\"\n    Compute the md5 digest of a file in base64 encoding.\n    \"\"\"\n", "input": "", "output": "    md5 = hashlib.md5()\n    chunk_size = 128 * md5.block_size\n    for chunk in iter(lambda: file_.read(chunk_size), b''):\n        md5.update(chunk)\n    file_.seek(0)\n    byte_digest = md5.digest()\n    return base64.b64encode(byte_digest).decode()"}, {"instruction": "def calculate_lyapunov(self):\n        \"\"\"\n        Return the current Lyapunov Characteristic Number (LCN).\n        Note that you need to call init_megno() before the start of the simulation.\n        To get a timescale (the Lyapunov timescale), take the inverse of this quantity.\n        \"\"\"\n", "input": "", "output": "        if self._calculate_megno==0:\n            raise RuntimeError(\"Lyapunov Characteristic Number cannot be calculated. Make sure to call init_megno() after adding all particles but before integrating the simulation.\")\n\n        clibrebound.reb_tools_calculate_lyapunov.restype = c_double\n        return clibrebound.reb_tools_calculate_lyapunov(byref(self))"}, {"instruction": "def get_large_image(self, page=1):\n        \"\"\"\n        Downloads and returns the large sized image of a single page.\n\n        The page kwarg specifies which page to return. One is the default.\n        \"\"\"\n", "input": "", "output": "        url = self.get_large_image_url(page=page)\n        return self._get_url(url)"}, {"instruction": "def populate(self, obj=None, section=None, parse_types=True):\n        \"\"\"Set attributes in ``obj`` with ``setattr`` from the all values in\n        ``section``.\n\n        \"\"\"\n", "input": "", "output": "        section = self.default_section if section is None else section\n        obj = Settings() if obj is None else obj\n        is_dict = isinstance(obj, dict)\n        for k, v in self.get_options(section).items():\n            if parse_types:\n                if v == 'None':\n                    v = None\n                elif self.FLOAT_REGEXP.match(v):\n                    v = float(v)\n                elif self.INT_REGEXP.match(v):\n                    v = int(v)\n                elif self.BOOL_REGEXP.match(v):\n                    v = v == 'True'\n                else:\n                    m = self.EVAL_REGEXP.match(v)\n                    if m:\n                        evalstr = m.group(1)\n                        v = eval(evalstr)\n            logger.debug('setting {} => {} on {}'.format(k, v, obj))\n            if is_dict:\n                obj[k] = v\n            else:\n                setattr(obj, k, v)\n        return obj"}, {"instruction": "def _get_s3_key():\n    '''\n    Get AWS keys from pillar or config\n    '''\n", "input": "", "output": "\n    key = __opts__['s3.key'] if 's3.key' in __opts__ else None\n    keyid = __opts__['s3.keyid'] if 's3.keyid' in __opts__ else None\n    service_url = __opts__['s3.service_url'] \\\n        if 's3.service_url' in __opts__ \\\n        else None\n    verify_ssl = __opts__['s3.verify_ssl'] \\\n        if 's3.verify_ssl' in __opts__ \\\n        else None\n    kms_keyid = __opts__['aws.kmw.keyid'] if 'aws.kms.keyid' in __opts__ else None\n    location = __opts__['s3.location'] \\\n        if 's3.location' in __opts__ \\\n        else None\n    path_style = __opts__['s3.path_style'] \\\n        if 's3.path_style' in __opts__ \\\n        else None\n    https_enable = __opts__['s3.https_enable'] \\\n        if 's3.https_enable' in __opts__ \\\n        else None\n\n    return key, keyid, service_url, verify_ssl, kms_keyid, location, path_style, https_enable"}, {"instruction": "def _get_battery(self):\n        \"\"\"\n        Get the battery\n        \"\"\"\n", "input": "", "output": "        try:\n            battery = {\n                \"charge\": self._dev.charge(),\n                \"isCharging\": self._dev.isCharging() == 1,\n            }\n        except Exception:\n            return None\n\n        return battery"}, {"instruction": "def _visit(self, element, operation):\n        \"\"\"\n        Visit and execute a operation in element and descendants.\n\n        :param element: The element.\n        :type element: hatemile.util.html.htmldomelement.HTMLDOMElement\n        :param operation: The operation to be executed.\n        :type operation: function\n        \"\"\"\n", "input": "", "output": "\n        if self._is_valid_inherit_element(element):\n            if element.has_children_elements():\n                children = element.get_children_elements()\n                for child in children:\n                    self._visit(child, operation)\n            elif self._is_valid_element(element):\n                operation(element)"}, {"instruction": "def encode_request(name, max_size):\n    \"\"\" Encode request into client_message\"\"\"\n", "input": "", "output": "    client_message = ClientMessage(payload_size=calculate_size(name, max_size))\n    client_message.set_message_type(REQUEST_TYPE)\n    client_message.set_retryable(RETRYABLE)\n    client_message.append_str(name)\n    client_message.append_int(max_size)\n    client_message.update_frame_length()\n    return client_message"}, {"instruction": "def parse(self, requires_cfg=True):\n        \"\"\"Parse the configuration sources into `Bison`.\n\n        Args:\n            requires_cfg (bool): Specify whether or not parsing should fail\n                if a config file is not found. (default: True)\n        \"\"\"\n", "input": "", "output": "        self._parse_default()\n        self._parse_config(requires_cfg)\n        self._parse_env()"}, {"instruction": "def _title_uptodate(self,fullfile,pid,_title):\n        \"\"\"Check fb photo title against provided title,\n        returns true if they match\"\"\"\n", "input": "", "output": "        i=self.fb.get_object(pid)\n        if i.has_key('name'):\n            if _title == i['name']:\n                return True\n\n        return False"}, {"instruction": "def unwrap(self, message, signature):\n        \"\"\"\n        NTLM GSSUnwrap()\n        :param message: The message to be decrypted\n        :return: The decrypted message\n        \"\"\"\n", "input": "", "output": "        plain_text = _Ntlm2Session.decrypt(self, message)\n        _Ntlm2Session.verify(self, plain_text, signature)\n        return plain_text"}, {"instruction": "def set_value(self, label, value, takeable=False):\n        \"\"\"\n        Quickly set single value at passed label. If label is not contained, a\n        new object is created with the label placed at the end of the result\n        index\n\n        .. deprecated:: 0.21.0\n\n        Please use .at[] or .iat[] accessors.\n\n        Parameters\n        ----------\n        label : object\n            Partial indexing with MultiIndex not allowed\n        value : object\n            Scalar value\n        takeable : interpret the index as indexers, default False\n\n        Notes\n        -----\n        This method *always* returns a new object. It is not particularly\n        efficient but is provided for API compatibility with Series\n\n        Returns\n        -------\n        series : SparseSeries\n        \"\"\"\n", "input": "", "output": "        warnings.warn(\"set_value is deprecated and will be removed \"\n                      \"in a future release. Please use \"\n                      \".at[] or .iat[] accessors instead\", FutureWarning,\n                      stacklevel=2)\n        return self._set_value(label, value, takeable=takeable)"}, {"instruction": "def init_random(X, n_clusters, random_state):\n    \"\"\"K-means initialization using randomly chosen points\"\"\"\n", "input": "", "output": "    logger.info(\"Initializing randomly\")\n    idx = sorted(draw_seed(random_state, 0, len(X), size=n_clusters))\n    centers = X[idx].compute()\n    return centers"}, {"instruction": "def extract_features(self, phrase):\n        \"\"\"\n        This function will extract features from the phrase being used. \n        Currently, the feature we are extracting are unigrams of the text corpus.\n        \"\"\"\n", "input": "", "output": "        \n        words = nltk.word_tokenize(phrase)\n        features = {}\n        for word in words:\n            features['contains(%s)' % word] = (word in words)\n        return features"}, {"instruction": "def request_generic(self, act, coro, perform, complete):\r\n        \"\"\"\r\n        Performs an overlapped request (via `perform` callable) and saves\r\n        the token and the (`overlapped`, `perform`, `complete`) trio.\r\n        \"\"\"\n", "input": "", "output": "        overlapped = OVERLAPPED()\r\n        overlapped.object = act\r\n        self.add_token(act, coro, (overlapped, perform, complete))\r\n\r\n        rc, nbytes = perform(act, overlapped)\r\n        completion_key = c_long(0)\r\n        if rc == 0:\r\n            # ah geez, it didn't got in the iocp, we have a result!\r\n            pass\r\n\r\n\r\n            # ok this is weird, apparently this doesn't need to be requeued\r\n            #  - need to investigate why (TODO)\r\n            #~ PostQueuedCompletionStatus(\r\n                #~ self.iocp, # HANDLE CompletionPort\r\n                #~ nbytes, # DWORD dwNumberOfBytesTransferred\r\n                #~ byref(completion_key), # ULONG_PTR dwCompletionKey\r\n                #~ overlapped # LPOVERLAPPED lpOverlapped\r\n            #~ )\r\n        elif rc != WSA_IO_PENDING:\r\n            self.remove_token(act)\r\n            raise SocketError(rc, \"%s on %r\" % (ctypes.FormatError(rc), act))"}, {"instruction": "def find_java_home(cratedb_version: tuple) -> str:\n    \"\"\" Return a path to a JAVA_HOME suites for the given CrateDB version \"\"\"\n", "input": "", "output": "    if MIN_VERSION_FOR_JVM11 <= cratedb_version < (4, 0):\n        # Supports 8 to 11+, use whatever is set\n        return os.environ.get('JAVA_HOME', '')\n    if cratedb_version < MIN_VERSION_FOR_JVM11:\n        return _find_matching_java_home(lambda ver: ver[0] == 8)\n    else:\n        return _find_matching_java_home(lambda ver: ver[0] >= 11)"}, {"instruction": "def _try_instantiate(self, ipopo, factory, component):\n        # type: (Any, str, str) -> None\n        \"\"\"\n        Tries to instantiate a component from the queue. Hides all exceptions.\n\n        :param ipopo: The iPOPO service\n        :param factory: Component factory\n        :param component: Component name\n        \"\"\"\n", "input": "", "output": "        try:\n            # Get component properties\n            with self.__lock:\n                properties = self.__queue[factory][component]\n        except KeyError:\n            # Component not in queue\n            return\n        else:\n            try:\n                # Try instantiation\n                ipopo.instantiate(factory, component, properties)\n            except TypeError:\n                # Unknown factory: try later\n                pass\n            except ValueError as ex:\n                # Already known component\n                _logger.error(\"Component already running: %s\", ex)\n            except Exception as ex:\n                # Other error\n                _logger.exception(\"Error instantiating component: %s\", ex)"}, {"instruction": "def course_feature(catalog, soup):\n    \"\"\"Parses all the courses (AKA, the most important part).\n    \"\"\"\n", "input": "", "output": "    courses = {}\n    course_crns = {}\n    for course in soup.findAll('course'):\n        c = Course.from_soup_tag(course)\n        courses[str(c)] = c\n    catalog.courses = courses\n    catalog.courses\n    logger.info('Catalog has %d courses' % len(courses))"}, {"instruction": "def get_dataframe(self, *args, **kwargs):\n        \"\"\"\n        Retrieve data as a Pandas dataframe.\n\n        Args:\n            search: (dict) Search query like {\"categ_A\": \"val_A\", \"categ_B\": \"val_B\"},\n                documented at https://developer.mpds.io/#Categories\n            phases: (list) Phase IDs, according to the MPDS distinct phases concept\n            fields: (dict) Data of interest for C-, S-, and P-entries,\n                e.g. for phase diagrams: {'C': ['naxes', 'arity', 'shapes']},\n                documented at https://developer.mpds.io/#JSON-schemata\n            columns: (list) Column names for Pandas dataframe\n\n        Returns: (object) Pandas dataframe object containing the results\n        \"\"\"\n", "input": "", "output": "        columns = kwargs.get('columns')\n        if columns:\n            del kwargs['columns']\n        else:\n            columns = self.default_titles\n\n        return pd.DataFrame(self.get_data(*args, **kwargs), columns=columns)"}, {"instruction": "def cached(func):\n    \"\"\"Memoize a function result.\"\"\"\n", "input": "", "output": "    ret = None\n\n    def call_or_cache(*args, **kwargs):\n        nonlocal ret\n        if ret is None:\n            ret = func(*args, **kwargs)\n        return ret\n\n    return call_or_cache"}, {"instruction": "async def power(source, exponent):\n    \"\"\"Raise the elements of an asynchronous sequence to the given power.\"\"\"\n", "input": "", "output": "    async with streamcontext(source) as streamer:\n        async for item in streamer:\n            yield item ** exponent"}, {"instruction": "def random_id(length):\n    \"\"\"Generates a random ID of given length\"\"\"\n", "input": "", "output": "\n    def char():\n        "}, {"instruction": "def get_user_bookmarks(self, id, **data):\n        \"\"\"\n        GET /users/:id/bookmarks/\n        Gets all the user's saved events.\n        In order to update the saved events list, the user must unsave or save each event.\n        A user is authorized to only see his/her saved events.\n        \"\"\"\n", "input": "", "output": "        \n        return self.get(\"/users/{0}/bookmarks/\".format(id), data=data)"}, {"instruction": "def _insertDateIndex(date, l):\r\n    '''\r\n    returns the index to insert the given date in a list\r\n    where each items first value is a date\r\n    '''\n", "input": "", "output": "    return next((i for i, n in enumerate(l) if n[0] < date), len(l))"}, {"instruction": "def indent(self):\n        \"\"\"\n        Indents text at cursor position.\n        \"\"\"\n", "input": "", "output": "        cursor = self.editor.textCursor()\n        assert isinstance(cursor, QtGui.QTextCursor)\n        if cursor.hasSelection():\n            self.indent_selection(cursor)\n        else:\n            # simply insert indentation at the cursor position\n            tab_len = self.editor.tab_length\n            if cursor.positionInBlock() < self.min_column and not cursor.atBlockEnd():\n                cursor.movePosition(cursor.Right, cursor.MoveAnchor, self.min_column)\n            cursor.beginEditBlock()\n            if self.editor.use_spaces_instead_of_tabs:\n                nb_space_to_add = tab_len - (cursor.positionInBlock() - self.min_column) % tab_len\n                cursor.insertText(nb_space_to_add * \" \")\n            else:\n                cursor.insertText('\\t')\n            cursor.endEditBlock()\n            self.editor.setTextCursor(cursor)"}, {"instruction": "def checkOutputPath(self, output_path):\n        \"\"\"\n        Create or clean up output path\n        \"\"\"\n", "input": "", "output": "        if not output_path:\n            # output_path = self.output_path_DEFAULT\n            output_path = os.path.join(self.output_path_DEFAULT,\n                                       slugify(unicode(self.title)))\n        if os.path.exists(output_path):\n            shutil.rmtree(output_path)\n        os.makedirs(output_path)\n        return output_path"}, {"instruction": "def stop(self):  # pylint: disable=no-self-use\n        \"\"\"Wrapper to stop the CherryPy server\n\n        :return: None\n        \"\"\"\n", "input": "", "output": "        cherrypy.log(\"Stopping CherryPy engine (current state: %s)...\" % cherrypy.engine.state)\n        try:\n            cherrypy.engine.exit()\n        except RuntimeWarning:\n            pass\n        except SystemExit:\n            cherrypy.log('SystemExit raised: shutting down bus')\n        cherrypy.log(\"Stopped\")"}, {"instruction": "def abivalidate_inputs(self):\n        \"\"\"\n        Run ABINIT in dry mode to validate all the inputs of the flow.\n\n        Return:\n            (isok, tuples)\n\n            isok is True if all inputs are ok.\n            tuples is List of `namedtuple` objects, one for each task in the flow.\n            Each namedtuple has the following attributes:\n\n                retcode: Return code. 0 if OK.\n                log_file:  log file of the Abinit run, use log_file.read() to access its content.\n                stderr_file: stderr file of the Abinit run. use stderr_file.read() to access its content.\n\n        Raises:\n            `RuntimeError` if executable is not in $PATH.\n        \"\"\"\n", "input": "", "output": "        if not self.allocated:\n            self.allocate()\n\n        isok, tuples = True, []\n        for task in self.iflat_tasks():\n            t = task.input.abivalidate()\n            if t.retcode != 0: isok = False\n            tuples.append(t)\n\n        return isok, tuples"}, {"instruction": "def get_creators(self, *args, **kwargs):\n        \"\"\"Fetches lists of creators.\n        \n        get /v1/public/creators\n        \n        :returns:  CreatorDataWrapper\n\n        >>> m = Marvel(public_key, private_key)\n        >>> cdw = m.get_creators(lastName=\"Lee\", orderBy=\"firstName,-modified\", limit=\"5\", offset=\"15\")\n        >>> print cdw.data.total\n        25\n        >>> print cdw.data.results[0].fullName\n        Alvin Lee\n        \"\"\"\n", "input": "", "output": "        \n        response = json.loads(self._call(Creator.resource_url(), self._params(kwargs)).text)\n        return CreatorDataWrapper(self, response)"}, {"instruction": "def lazy_property(fn):\n    \"\"\"\n    Decorator that makes a property lazy-evaluated whilst preserving\n    docstrings.\n\n    Args:\n        fn (function): the property in question\n\n    Returns:\n        evaluated version of the property.\n    \"\"\"\n", "input": "", "output": "    attr_name = '_lazy_' + fn.__name__\n\n    @property\n    @wraps(fn)\n    def _lazy_property(self):\n        if not hasattr(self, attr_name):\n            setattr(self, attr_name, fn(self))\n        return getattr(self, attr_name)\n    return _lazy_property"}, {"instruction": "def process(self):\n        \"\"\"Process current event.\"\"\"\n", "input": "", "output": "        try:\n            self.receiver(self)\n        # TODO RESTException\n        except Exception as e:\n            current_app.logger.exception('Could not process event.')\n            self.response_code = 500\n            self.response = dict(status=500, message=str(e))\n        return self"}, {"instruction": "def initialize_segment_register_x64(self, state, concrete_target):\n        \"\"\"\n        Set the fs register in the angr to the value of the fs register in the concrete process\n\n        :param state:               state which will be modified\n        :param concrete_target:     concrete target that will be used to read the fs register\n        :return: None\n        \"\"\"\n", "input": "", "output": "        _l.debug(\"Synchronizing fs segment register\")\n        state.regs.fs = self._read_fs_register_x64(concrete_target)"}, {"instruction": "def instance_path_for(name, identifier_type, identifier_key=None):\n    \"\"\"\n    Get a path for thing.\n\n    \"\"\"\n", "input": "", "output": "    return \"/{}/<{}:{}>\".format(\n        name_for(name),\n        identifier_type,\n        identifier_key or \"{}_id\".format(name_for(name)),\n    )"}, {"instruction": "def visit(self, node):\n        '''The main visit function. Visits the passed-in node and calls\n        finalize.\n        '''\n", "input": "", "output": "        for token in self.itervisit(node):\n            pass\n        result = self.finalize()\n        if result is not self:\n            return result"}, {"instruction": "def composition(mol):\n    \"\"\"Molecular composition in dict format\n    (ex. Glucose {'C': 6, 'H': 12, 'O': 6}).\n    \"\"\"\n", "input": "", "output": "    mol.require(\"Valence\")\n    c = Counter()\n    for _, a in mol.atoms_iter():\n        c += a.composition()\n    return c"}, {"instruction": "def expand_paths(path):\n    \"\"\"When given a path with brackets, expands it to return all permutations\n       of the path with expanded brackets, similar to ant.\n\n       >>> expand_paths('../{a,b}/{c,d}')\n       ['../a/c', '../a/d', '../b/c', '../b/d']\n       >>> expand_paths('../{a,b}/{a,b}.py')\n       ['../a/a.py', '../a/b.py', '../b/a.py', '../b/b.py']\n       >>> expand_paths('../{a,b,c}/{a,b,c}')\n       ['../a/a', '../a/b', '../a/c', '../b/a', '../b/b', '../b/c', '../c/a', '../c/b', '../c/c']\n       >>> expand_paths('test')\n       ['test']\n       >>> expand_paths('')\n    \"\"\"\n", "input": "", "output": "    pr = itertools.product\n    parts = MAGIC_BRACKETS.findall(path)\n\n    if not path:\n        return\n\n    if not parts:\n        return [path]\n\n    permutations = [[(p[0], i, 1) for i in p[1].split(',')] for p in parts]\n    return [_replace_all(path, i) for i in pr(*permutations)]"}, {"instruction": "def get_odoo_args(self, ctx):\n        \"\"\"Return a list of Odoo command line arguments from the Click context.\"\"\"\n", "input": "", "output": "        config = ctx.params.get(\"config\")\n        addons_path = ctx.params.get(\"addons_path\")\n        database = ctx.params.get(\"database\")\n        log_level = ctx.params.get(\"log_level\")\n        logfile = ctx.params.get(\"logfile\")\n\n        odoo_args = []\n\n        if config:\n            odoo_args.extend([\"--config\", config])\n        if addons_path:\n            odoo_args.extend([\"--addons-path\", addons_path])\n        if database:\n            odoo_args.extend([\"--database\", database])\n        if log_level:\n            odoo_args.extend([\"--log-level\", log_level])\n        if logfile:\n            odoo_args.extend([\"--logfile\", logfile])\n\n        return odoo_args"}, {"instruction": "def _connectToWP(self):\n        \"\"\"Establish the actual TCP connection to Flickr\"\"\"\n", "input": "", "output": "\n        if self.connected_to_wp:\n            logger.debug(\"Already connected to wp\")\n            return True\n\n        # Load config from file\n        info=json.load(open(WP_LOGIN_FILE,'r'))\n        self.wp = Client(info['url'],\\\n                info['username'],\\\n                info['password'])\n\n        logger.debug(\"Connecting to wp\")\n\n        self.connected_to_wp=True\n\n        return True"}, {"instruction": "def service(cls):\n    '''\n    Marks the decorated class as a singleton ``service``.\n\n    Injects following classmethods:\n\n        .. py:method:: .get(context)\n\n            Returns a singleton instance of the class for given ``context``\n\n            :param context: context to look in\n            :type context: :class:`Context`\n            :returns: ``cls``\n    '''\n", "input": "", "output": "\n    if not cls:\n        return None\n\n    # Inject methods\n    def get(cls, context):\n        return context.get_service(cls)\n    cls.get = get.__get__(cls)\n\n    log.debug('Registering [%s] (service)', get_fqdn(cls))\n\n    return cls"}, {"instruction": "def get_url(\n    width, height=None, background_color=\"cccccc\",\n    text_color=\"969696\", text=None, random_background_color=False\n):\n    \"\"\"\n    Craft the URL for a placeholder image.\n\n    You can customize the background color, text color and text using\n    the optional keyword arguments\n\n    If you want to use a random color pass in random_background_color as True.\n    \"\"\"\n", "input": "", "output": "    if random_background_color:\n        background_color = _get_random_color()\n\n    # If height is not provided, presume it is will be a square\n    if not height:\n        height = width\n    d = dict(\n        width=width,\n        height=height,\n        bcolor=background_color,\n        tcolor=text_color\n    )\n    url = URL % d\n    if text:\n        text = text.replace(\" \", \"+\")\n        url = url + \"?text=\" + text\n    return url"}, {"instruction": "def get_full_import_name(import_from, name):\n    \"\"\"Get the full path of a name from a ``from x import y`` statement.\n\n    :param import_from: The astroid node to resolve the name of.\n    :type import_from: astroid.nodes.ImportFrom\n    :param name:\n    :type name: str\n\n    :returns: The full import path of the name.\n    :rtype: str\n    \"\"\"\n", "input": "", "output": "    partial_basename = resolve_import_alias(name, import_from.names)\n\n    module_name = import_from.modname\n    if import_from.level:\n        module = import_from.root()\n        assert isinstance(module, astroid.nodes.Module)\n        module_name = module.relative_to_absolute_name(\n            import_from.modname, level=import_from.level\n        )\n\n    return \"{}.{}\".format(module_name, partial_basename)"}, {"instruction": "def commit(self):\n        \"\"\":return: Commit object the tag ref points to\n        \n        :raise ValueError: if the tag points to a tree or blob\"\"\"\n", "input": "", "output": "        obj = self.object\n        while obj.type != 'commit':\n            if obj.type == \"tag\":\n                # it is a tag object which carries the commit as an object - we can point to anything\n                obj = obj.object\n            else:\n                raise ValueError((\"Cannot resolve commit as tag %s points to a %s object - \" +\n                                  \"use the `.object` property instead to access it\") % (self, obj.type))\n        return obj"}, {"instruction": "def cmpxchg(self, ptr, cmp, val, ordering, failordering=None, name=''):\n        \"\"\"\n        Atomic compared-and-set:\n            atomic {\n                old = *ptr\n                success = (old == cmp)\n                if (success)\n                    *ptr = val\n                }\n            name = { old, success }\n\n        If failordering is `None`, the value of `ordering` is used.\n        \"\"\"\n", "input": "", "output": "        failordering = ordering if failordering is None else failordering\n        inst = instructions.CmpXchg(self.block, ptr, cmp, val, ordering,\n                                    failordering, name=name)\n        self._insert(inst)\n        return inst"}, {"instruction": "def guard_retract(analysis):\n    \"\"\" Return whether the transition \"retract\" can be performed or not\n    \"\"\"\n", "input": "", "output": "    # Cannot retract if there are dependents that cannot be retracted\n    if not is_transition_allowed(analysis.getDependents(), \"retract\"):\n        return False\n\n    dependencies = analysis.getDependencies()\n    if not dependencies:\n        return True\n\n    # Cannot retract if all dependencies have been verified\n    if all(map(lambda an: IVerified.providedBy(an), dependencies)):\n        return False\n\n    return True"}, {"instruction": "def get_storage_usage(access_token, subscription_id, location):\n    '''Returns storage usage and quota information for the specified subscription.\n\n    Args:\n        access_token (str): A valid Azure authentication token.\n        subscription_id (str): Azure subscription id.\n\n    Returns:\n        HTTP response. JSON body of storage account usage.\n    '''\n", "input": "", "output": "    endpoint = ''.join([get_rm_endpoint(),\n                        '/subscriptions/', subscription_id,\n                        '/providers/Microsoft.Storage/locations/', location,\n                        '/usages',\n                        '?api-version=', STORAGE_API])\n    return do_get(endpoint, access_token)"}, {"instruction": "def get_extents(self):\n        \"\"\"Return the extents of the recording-surface.\n\n        :returns:\n            A ``(x, y, width, height)`` tuple of floats,\n            or :obj:`None` if the surface is unbounded.\n\n        *New in cairo 1.12*\n\n        \"\"\"\n", "input": "", "output": "        extents = ffi.new('cairo_rectangle_t *')\n        if cairo.cairo_recording_surface_get_extents(self._pointer, extents):\n            return (extents.x, extents.y, extents.width, extents.height)"}, {"instruction": "def _cellrepr(value, allow_formulas):\n    \"\"\"\n    Get a string representation of dataframe value.\n\n    :param :value: the value to represent\n    :param :allow_formulas: if True, allow values starting with '='\n            to be interpreted as formulas; otherwise, escape\n            them with an apostrophe to avoid formula interpretation.\n    \"\"\"\n", "input": "", "output": "    if pd.isnull(value) is True:\n        return \"\"\n    if isinstance(value, float):\n        value = repr(value)\n    else:\n        value = str(value)\n    if (not allow_formulas) and value.startswith('='):\n        value = \"'%s\" % value\n    return value"}, {"instruction": "def get_datas(callback, macs=[], run_flag=RunFlag(), bt_device=''):\n        \"\"\"\n        Get data for all ruuvitag sensors or sensors in the MAC's list.\n\n        Args:\n            callback (func): callback funcion to be called when new data is received\n            macs (list): MAC addresses\n            run_flag (object): RunFlag object. Function executes while run_flag.running\n            bt_device (string): Bluetooth device id\n        \"\"\"\n", "input": "", "output": "\n        log.info('Get latest data for sensors. Stop with Ctrl+C.')\n        log.info('MACs: %s', macs)\n\n        for new_data in RuuviTagSensor._get_ruuvitag_datas(macs, None, run_flag, bt_device):\n            callback(new_data)"}, {"instruction": "def _colored_time(self, time_taken, color=None):\n        \"\"\"Get formatted and colored string for a given time taken.\"\"\"\n", "input": "", "output": "        if self.timer_no_color:\n            return \"{0:0.4f}s\".format(time_taken)\n\n        return _colorize(\"{0:0.4f}s\".format(time_taken), color)"}, {"instruction": "def parse_kv(args):\n    ''' convert a string of key/value items to a dict '''\n", "input": "", "output": "\n    options = {}\n    if args is not None:\n        # attempting to split a unicode here does bad things\n        vargs = shlex.split(str(args), posix=True)\n        for x in vargs:\n            if x.find(\"=\") != -1:\n                k, v = x.split(\"=\",1)\n                options[k]=v\n    return options"}, {"instruction": "def col(s, c, bg=0, no_reset=0):\n    \"\"\"\n    print col('foo', 124) -> red 'foo' on the terminal\n    c = color, s the value to colorize \"\"\"\n", "input": "", "output": "    reset = reset_col\n    if no_reset:\n        reset = ''\n    for _strt, _end, _col in ((code_start, code_end, H2),\n                              (stng_start, stng_end, H2),\n                              (emph_start, emph_end, H3)):\n        if _strt in s:\n            # inline code:\n            s = s.replace(_strt, col('', _col, bg=background, no_reset=1))\n            s = s.replace(_end , col('', c, no_reset=1))\n\n    s =  '\\033[38;5;%sm%s%s' % (c, s, reset)\n    if bg:\n        pass\n        #s = col_bg(bg) + s\n    return s"}, {"instruction": "def remove_description(self, id, **kwargs):  # noqa: E501\n        \"\"\"Remove description from a specific source  # noqa: E501\n\n          # noqa: E501\n        This method makes a synchronous HTTP request by default. To make an\n        asynchronous HTTP request, please pass async_req=True\n        >>> thread = api.remove_description(id, async_req=True)\n        >>> result = thread.get()\n\n        :param async_req bool\n        :param str id: (required)\n        :return: ResponseContainer\n                 If the method is called asynchronously,\n                 returns the request thread.\n        \"\"\"\n", "input": "", "output": "        kwargs['_return_http_data_only'] = True\n        if kwargs.get('async_req'):\n            return self.remove_description_with_http_info(id, **kwargs)  # noqa: E501\n        else:\n            (data) = self.remove_description_with_http_info(id, **kwargs)  # noqa: E501\n            return data"}, {"instruction": "def _download(self, force_overwrite=False, verbose=False):\n        \"\"\"\n        Download the file if it's not already there.\n        We shouldn't *need* to overwrite; the xml is not supposed to update.\n        \"\"\"\n", "input": "", "output": "        if not force_overwrite:\n            # If the file is already there, we're done\n            if os.path.isfile(self.filepath):\n                if verbose:\n                    print(\n                        \"File already available at %s -- skipping\"\n                        % (self.filepath)\n                    )\n                return False\n        stream_download(self.URL, self.filepath, verbose=verbose)\n        return True"}, {"instruction": "def post(self, url, body=None):\n        \"\"\"Sends this `Resource` instance to the service with a\n        ``POST`` request to the given URL. Takes an optional body\"\"\"\n", "input": "", "output": "        response = self.http_request(url, 'POST', body or self, {'Content-Type': 'application/xml; charset=utf-8'})\n        if response.status not in (200, 201, 204):\n            self.raise_http_error(response)\n\n        self._url = response.getheader('Location')\n\n        if response.status in (200, 201):\n            response_xml = response.read()\n            logging.getLogger('recurly.http.response').debug(response_xml)\n            self.update_from_element(ElementTree.fromstring(response_xml))"}, {"instruction": "def save(self, list_file):\n    \"\"\"Saves the current list of annotations to the given file.\n\n    **Parameters:**\n\n    ``list_file`` : str\n      The name of a list file to write the currently stored list into\n    \"\"\"\n", "input": "", "output": "    bob.io.base.create_directories_safe(os.path.dirname(list_file))\n    with open(list_file, 'w') as f:\n      for i in range(len(self.image_paths)):\n        f.write(self.image_paths[i])\n        for bbx in self.bounding_boxes[i]:\n          f.write(\"\\t[%f %f %f %f]\" % (bbx.top_f, bbx.left_f, bbx.size_f[0], bbx.size_f[1]))\n        f.write(\"\\n\")"}, {"instruction": "def make_temp(string, suffix='', decode=True, delete=True):\n    \"\"\" xmlsec needs files in some cases where only strings exist, hence the\n    need for this function. It creates a temporary file with the\n    string as only content.\n\n    :param string: The information to be placed in the file\n    :param suffix: The temporary file might have to have a specific\n        suffix in certain circumstances.\n    :param decode: The input string might be base64 coded. If so it\n        must, in some cases, be decoded before being placed in the file.\n    :return: 2-tuple with file pointer ( so the calling function can\n        close the file) and filename (which is for instance needed by the\n        xmlsec function).\n    \"\"\"\n", "input": "", "output": "    ntf = NamedTemporaryFile(suffix=suffix, delete=delete)\n    # Python3 tempfile requires byte-like object\n    if not isinstance(string, six.binary_type):\n        string = string.encode('utf-8')\n\n    if decode:\n        ntf.write(base64.b64decode(string))\n    else:\n        ntf.write(string)\n    ntf.seek(0)\n    return ntf, ntf.name"}, {"instruction": "async def insert(self, **kwargs):\n\t\t\"\"\"\n\t\tAccepts request object, retrieves data from the one`s body\n\t\tand creates new account. \n\t\t\"\"\"\n", "input": "", "output": "\t\t\n\t\tif kwargs:\n\t\t\t# Create autoincrement for account\n\t\t\tpk = await self.autoincrement()\n\t\t\tkwargs.update({\"id\": pk})\n\n\t\t\t# Create account with received data and autoincrement\n\t\t\tawait self.collection.insert_one(kwargs)\n\n\t\t\trow = await self.collection.find_one({\"id\": pk})\n\n\t\telse:\n\t\t\trow = None\n\n\t\tif row:\n\t\t\treturn {i:row[i] for i in row if i != \"_id\"}\n\t\telse:\n\t\t\treturn {\"error\":500, \n\t\t\t\t\t\"reason\":\"Not created\"}"}, {"instruction": "def run_blast_commands(ncbicommandline_method, **keywords):\n    \"\"\"Runs blastplus/tblastn search, collects result and pass as a xml temporary file.  \"\"\"\n", "input": "", "output": "\n    # temporary files for output\n    blast_out_tmp = tempfile.NamedTemporaryFile(mode=\"w+\",delete=False)\n    keywords['out'] = blast_out_tmp.name\n\n    # unpack query temp file object\n    query_file_object_tmp = keywords['query']\n    keywords['query'] = query_file_object_tmp.name\n\n    stderr = ''\n    error_string = ''\n    try:\n        # formating blastplus command\n        blastplusx_cline = ncbicommandline_method(**keywords)\n        stdout, stderr = blastplusx_cline()\n\n    except ApplicationError as e:\n        error_string = \"Runtime error: \" + stderr + \"\\n\" + e.cmd\n\n    # remove query temp file\n    os.unlink(query_file_object_tmp.name)\n    # os.remove(query_file_object_tmp.name)\n\n    return blast_out_tmp, error_string"}, {"instruction": "def dict_filter(d, exclude=[]):\n    \"\"\"\n    Exclude specified keys from a nested dict\n    \"\"\"\n", "input": "", "output": "\n    if isinstance(d, list):\n        ret = []\n        for e in d:\n            ret.append(dict_filter(e, exclude))\n        return ret\n    elif isinstance(d, dict):\n        ret = {}\n        for k, v in d.items():\n            if isinstance(k, builtin_str):\n                k = str(k)\n\n            assert isinstance(k, str)\n            if k in exclude:\n                continue\n            ret[k] = dict_filter(v, exclude)\n        return ret\n\n    return d"}, {"instruction": "def tangent(obj, params, **kwargs):\n    \"\"\" Evaluates the tangent vector of the curves or surfaces at the input parameter values.\n\n    This function is designed to evaluate tangent vectors of the B-Spline and NURBS shapes at single or\n    multiple parameter positions.\n\n    :param obj: input shape\n    :type obj: abstract.Curve or abstract.Surface\n    :param params: parameters\n    :type params: float, list or tuple\n    :return: a list containing \"point\" and \"vector\" pairs\n    :rtype: tuple\n    \"\"\"\n", "input": "", "output": "    normalize = kwargs.get('normalize', True)\n    if isinstance(obj, abstract.Curve):\n        if isinstance(params, (list, tuple)):\n            return ops.tangent_curve_single_list(obj, params, normalize)\n        else:\n            return ops.tangent_curve_single(obj, params, normalize)\n    if isinstance(obj, abstract.Surface):\n        if isinstance(params[0], float):\n            return ops.tangent_surface_single(obj, params, normalize)\n        else:\n            return ops.tangent_surface_single_list(obj, params, normalize)"}, {"instruction": "def requeue(self):\n        \"\"\"Reject this message and put it back on the queue.\n\n        You must not use this method as a means of selecting messages\n        to process.\n\n        :raises MessageStateError: If the message has already been\n            acknowledged/requeued/rejected.\n\n        \"\"\"\n", "input": "", "output": "        if self.acknowledged:\n            raise self.MessageStateError(\n                \"Message already acknowledged with state: %s\" % self._state)\n        self.backend.requeue(self.delivery_tag)\n        self._state = \"REQUEUED\""}, {"instruction": "def vertex_fingerprints(self):\n        \"\"\"A fingerprint for each vertex\n\n           The result is invariant under permutation of the vertex indexes.\n           Vertices that are symmetrically equivalent will get the same\n           fingerprint, e.g. the hydrogens in methane would get the same\n           fingerprint.\n        \"\"\"\n", "input": "", "output": "        return self.get_vertex_fingerprints(\n            [self.get_vertex_string(i) for i in range(self.num_vertices)],\n            [self.get_edge_string(i) for i in range(self.num_edges)],\n        )"}, {"instruction": "def simple_layer_stack(include_encdec_attention,\n                       num_layers=6,\n                       d_ff=2048,\n                       num_heads=8,\n                       d_kv=128,\n                       dropout_rate=0.1):\n  \"\"\"Create a layer stack.\n\n  Args:\n    include_encdec_attention: a boolean\n    num_layers: an integer\n    d_ff: an integer\n    num_heads: an integer\n    d_kv: an integer\n    dropout_rate: a float\n\n  Returns:\n    a LayerStack\n  \"\"\"\n", "input": "", "output": "  ret = []\n  for _ in xrange(num_layers):\n    ret.append(\n        transformer_layers.SelfAttention(\n            num_heads=num_heads,\n            key_value_size=d_kv,\n            attention_kwargs={\"dropout_rate\": dropout_rate}))\n    if include_encdec_attention:\n      ret.append(\n          transformer_layers.EncDecAttention(\n              num_heads=num_heads,\n              key_value_size=d_kv,\n              attention_kwargs={\"dropout_rate\": dropout_rate}))\n    ret.append(\n        transformer_layers.DenseReluDense(\n            hidden_size=d_ff,\n            dropout_rate=dropout_rate))\n  return transformer.LayerStack(ret)"}, {"instruction": "def execute(self, **kwargs):\n    \"\"\" commit the current statements from add()\n    \"\"\"\n", "input": "", "output": "    assert self.resp is None, \"Transaction already committed\"\n    try:\n      self.resp = self.db.tx(list(self.edn_iter), **kwargs)\n    except Exception:\n      self.resp = False\n      raise\n    else:\n      self.resolve()\n      self.adds = None\n      self.tmpents = None\n    return self.resp"}, {"instruction": "def session_to_hour(timestamp):\n    \"\"\":param timestamp: as string in YYYYMMDDHHmmSS format\n    :return string in YYYYMMDDHH format\"\"\"\n", "input": "", "output": "    t = datetime.strptime(timestamp, SYNERGY_SESSION_PATTERN)\n    return t.strftime(SYNERGY_HOURLY_PATTERN)"}, {"instruction": "def _notebook(trigger, note_store):\n        \"\"\"\n        :param trigger:\u00a0trigger object\n        :param note_store: note_store object\n        :return: note object\n        \"\"\"\n", "input": "", "output": "        note = Types.Note()\n        if trigger.notebook:\n            # get the notebookGUID ...\n            notebook_id = EvernoteMgr.get_notebook(note_store, trigger.notebook)\n            # create notebookGUID if it does not exist then return its id\n            note.notebookGuid = EvernoteMgr.set_notebook(note_store, trigger.notebook, notebook_id)\n\n            if trigger.tag:\n                # ... and get the tagGUID if a tag has been provided\n                tag_id = EvernoteMgr.get_tag(note_store, trigger.tag)\n                if tag_id is False:\n                    tag_id = EvernoteMgr.set_tag(note_store, trigger.tag, tag_id)\n                    # set the tag to the note if a tag has been provided\n                    if tag_id:\n                        note.tagGuids = tag_id\n\n            logger.debug(\"notebook that will be used %s\", trigger.notebook)\n        return note"}, {"instruction": "def SetSchema(self, reader):\n        \"\"\"Use XSD Schema to validate the document as it is processed.\n          Activation is only possible before the first Read(). if\n          @schema is None, then Schema validation is desactivated. @\n          The @schema should not be freed until the reader is\n           deallocated or its use has been deactivated. \"\"\"\n", "input": "", "output": "        if reader is None: reader__o = None\n        else: reader__o = reader._o\n        ret = libxml2mod.xmlTextReaderSetSchema(reader__o, self._o)\n        return ret"}, {"instruction": "def ws010(self, value=None):\n        \"\"\"  Corresponds to IDD Field `ws010`\n        Wind speed corresponding to 1.0% annual cumulative frequency of occurrence\n\n        Args:\n            value (float): value for IDD Field `ws010`\n                Unit: m/s\n                if `value` is None it will not be checked against the\n                specification and is assumed to be a missing value\n\n        Raises:\n            ValueError: if `value` is not a valid value\n        \"\"\"\n", "input": "", "output": "        if value is not None:\n            try:\n                value = float(value)\n            except ValueError:\n                raise ValueError('value {} need to be of type float '\n                                 'for field `ws010`'.format(value))\n\n        self._ws010 = value"}, {"instruction": "def iresolve(self, *keys):\n        '''\n        Iterates over resolved instances for given provider keys.\n\n        :param keys: Provider keys\n        :type keys: tuple\n        :return: Iterator of resolved instances\n        :rtype: generator\n        '''\n", "input": "", "output": "        for key in keys:\n            missing = self.get_missing_deps(key)\n            if missing:\n                raise UnresolvableError(\"Missing dependencies for %s: %s\" % (key, missing))\n\n            provider = self._providers.get(key)\n            if not provider:\n                raise UnresolvableError(\"Provider does not exist for %s\" % key)\n\n            yield provider()"}, {"instruction": "def all_stats(klass, account, ids, metric_groups, **kwargs):\n        \"\"\"\n        Pulls a list of metrics for a specified set of object IDs.\n        \"\"\"\n", "input": "", "output": "        params = klass._standard_params(ids, metric_groups, **kwargs)\n\n        resource = klass.RESOURCE_SYNC.format(account_id=account.id)\n        response = Request(account.client, 'get', resource, params=params).perform()\n        return response.body['data']"}, {"instruction": "def libvlc_video_get_track_description(p_mi):\n    '''Get the description of available video tracks.\n    @param p_mi: media player.\n    @return: list with description of available video tracks, or NULL on error.\n    '''\n", "input": "", "output": "    f = _Cfunctions.get('libvlc_video_get_track_description', None) or \\\n        _Cfunction('libvlc_video_get_track_description', ((1,),), None,\n                    ctypes.POINTER(TrackDescription), MediaPlayer)\n    return f(p_mi)"}, {"instruction": "def get_athlete_stats(self, athlete_id=None):\n        \"\"\"\n        Returns Statistics for the athlete.\n        athlete_id must be the id of the authenticated athlete or left blank.\n        If it is left blank two requests will be made - first to get the\n        authenticated athlete's id and second to get the Stats.\n\n        http://strava.github.io/api/v3/athlete/#stats\n\n        :return: A model containing the Stats\n        :rtype: :py:class:`stravalib.model.AthleteStats`\n        \"\"\"\n", "input": "", "output": "        if athlete_id is None:\n            athlete_id = self.get_athlete().id\n\n        raw = self.protocol.get('/athletes/{id}/stats', id=athlete_id)\n        # TODO: Better error handling - this will return a 401 if this athlete\n        #       is not the authenticated athlete.\n\n        return model.AthleteStats.deserialize(raw)"}, {"instruction": "def _unparse_a_tags(cls, attrs_dict):\n        \"\"\" Iterates over the dictionary\n\n        :param: attrs_dict a dict of attributes\n        :returns:   a SimpleXMLElement list containing <a> tags\n        \"\"\"\n", "input": "", "output": "        prop_tags = []\n\n        for k, v in attrs_dict.items():\n            node = {cls.ATTRNAME_PROPERTY: k, '_content': utils.auto_type(v)}\n            prop_tags.append(node)\n\n        return prop_tags"}, {"instruction": "def pitching_stats(start_season, end_season=None, league='all', qual=1, ind=1):\n    \"\"\"\n    Get season-level pitching data from FanGraphs. \n\n    ARGUMENTS:\n    start_season : int : first season you want data for (or the only season if you do not specify an end_season)\n    end_season : int : final season you want data for \n    league : \"all\", \"nl\", or \"al\"\n    qual: minimum number of pitches thrown to be included in the data (integer). Use the string 'y' for fangraphs default.\n    ind : int : =1 if you want individual season-level data, =0 if you want a player's aggreagate data over all seasons in the query\n    \"\"\"\n", "input": "", "output": "    if start_season is None:\n        raise ValueError(\"You need to provide at least one season to collect data for. Try pitching_leaders(season) or pitching_leaders(start_season, end_season).\")\n    if end_season is None:\n        end_season = start_season\n    soup = get_soup(start_season=start_season, end_season=end_season, league=league, qual=qual, ind=ind)\n    table = get_table(soup, ind)\n    return table"}, {"instruction": "def get_refinement_options(self):\n        \"\"\" Returns possible specializations for the upper values in the taxonomy \"\"\"\n", "input": "", "output": "        domain = self.get_domain()\n        for upper_value in self.upper:\n            for suc in domain.successors(upper_value):\n                yield suc"}, {"instruction": "def get_content_type_charset(self, default='UTF-8'):\n\t\t\"\"\"\n\t\tInspect the Content-Type header to retrieve the charset that the client\n\t\thas specified.\n\n\t\t:param str default: The default charset to return if none exists.\n\t\t:return: The charset of the request.\n\t\t:rtype: str\n\t\t\"\"\"\n", "input": "", "output": "\t\tencoding = default\n\t\theader = self.headers.get('Content-Type', '')\n\t\tidx = header.find('charset=')\n\t\tif idx > 0:\n\t\t\tencoding = (header[idx + 8:].split(' ', 1)[0] or encoding)\n\t\treturn encoding"}, {"instruction": "async def delete(self, key, param=None):\n        \"\"\"\n        delete cache corresponding to identity\n        generated from key and param\n        \"\"\"\n", "input": "", "output": "        identity = self._gen_identity(key, param)\n        return await self.client.delete(identity)"}, {"instruction": "def SetQualifier(self, *args, **kwargs):\n        \"\"\"Create or modify a qualifier type in the local repository of this\n        class.\n\n        For a description of the parameters, see\n        :meth:`pywbem.WBEMConnection.SetQualifier`.\n        \"\"\"\n", "input": "", "output": "\n        qual = args[0] if args else kwargs['QualifierDeclaration']\n        try:\n            self.qualifiers[self.default_namespace][qual.name] = qual\n        except KeyError:\n            self.qualifiers[self.default_namespace] = \\\n                NocaseDict({qual.name: qual})"}, {"instruction": "def clear_jobs():\n    '''Clear old jobs\n\n    :param days: Jobs for how many days should be kept (default: 10)\n    :type days: integer\n\n    :statuscode 200: no error\n    :statuscode 403: not authorized to delete jobs\n    :statuscode 409: an error occurred\n    '''\n", "input": "", "output": "    if not is_authorized():\n        return json.dumps({'error': 'not authorized'}), 403, headers\n\n    days = flask.request.args.get('days', None)\n    return _clear_jobs(days)"}, {"instruction": "def as_rgb(self):\n        \"\"\"\n        Return a color palette with RGB values instead of hex codes.\n        \"\"\"\n", "input": "", "output": "        rgb = [mpl.colors.colorConverter.to_rgb(hex) for hex in self]\n        return ColorPalette(rgb)"}, {"instruction": "def _construct_key(self, rule_id: str, spacy_rule_id:int) -> int:\n        \"\"\"\n        Use a mapping to store the information about rule_id for each matches, create the mapping key here\n        Args:\n            rule_id: str\n            spacy_rule_id:int\n\n        Returns: int\n        \"\"\"\n", "input": "", "output": "\n        hash_key = (rule_id, spacy_rule_id)\n        hash_v = hash(hash_key) + sys.maxsize + 1\n        self._hash_map[hash_v] = hash_key\n        return hash_v"}, {"instruction": "def get_all_repos(self):\n        \"\"\"Gets user repos\n        :return: List of all user repositories (public, orgs and private)\n        \"\"\"\n", "input": "", "output": "        url = \"https://api.github.com/user/repos\"\n        params = {\n            \"access_token\": GITHUB_TOKEN\n        }  # add auth params\n        url = add_params_to_url(url, params)\n        return self._get_repos(url)"}, {"instruction": "def list_devices(self):\n        \"\"\"List devices in the ALDB.\"\"\"\n", "input": "", "output": "        if self.plm.devices:\n            for addr in self.plm.devices:\n                device = self.plm.devices[addr]\n                if device.address.is_x10:\n                    _LOGGING.info('Device: %s %s', device.address.human,\n                                  device.description)\n                else:\n                    _LOGGING.info('Device: %s cat: 0x%02x subcat: 0x%02x '\n                                  'desc: %s, model: %s',\n                                  device.address.human, device.cat,\n                                  device.subcat, device.description,\n                                  device.model)\n        else:\n            _LOGGING.info('No devices found')\n            if not self.plm.transport:\n                _LOGGING.info('IM connection has not been made.')\n                _LOGGING.info('Use `connect [device]` to open the connection')"}, {"instruction": "def create_media_asset(access_token, name, options=\"0\"):\n    '''Create Media Service Asset.\n\n    Args:\n        access_token (str): A valid Azure authentication token.\n        name (str): Media Service Asset Name.\n        options (str): Media Service Options.\n\n    Returns:\n        HTTP response. JSON body.\n    '''\n", "input": "", "output": "    path = '/Assets'\n    endpoint = ''.join([ams_rest_endpoint, path])\n    body = '{\"Name\": \"' + name + '\", \"Options\": \"' + str(options) + '\"}'\n    return do_ams_post(endpoint, path, body, access_token)"}, {"instruction": "def dump_conndata(self):\n        \"\"\"Developer tool for debugging forensics.\"\"\"\n", "input": "", "output": "        attrs = vars(self)\n        return ', '.join(\"%s: %s\" % item for item in attrs.items())"}, {"instruction": "def send(self, data):\n        \"\"\"\n        Sends data to the server.\n        \"\"\"\n", "input": "", "output": "        self.logger.debug('Send data: {}'.format(data))\n\n        if not self.connected:\n            self.logger.warning('Connection not established. Return...')\n            return\n\n        self.websocket.send(json.dumps(data))"}, {"instruction": "def get_dates(self):\n        \"\"\" Returns a list of acquisition times from tile info data\n\n        :return: List of acquisition times in the order returned by WFS service.\n        :rtype: list(datetime.datetime)\n        \"\"\"\n", "input": "", "output": "        return [datetime.datetime.strptime('{}T{}'.format(tile_info['properties']['date'],\n                                                          tile_info['properties']['time'].split('.')[0]),\n                                           '%Y-%m-%dT%H:%M:%S') for tile_info in self]"}, {"instruction": "def match_string(self, stype):\n        \"\"\"Match string type.\"\"\"\n", "input": "", "output": "\n        return not (stype - self.string_types) or bool(stype & self.wild_string_types)"}, {"instruction": "def evaluate(self, x, y, flux, x_0, y_0, sigma):\n        \"\"\"Model function Gaussian PSF model.\"\"\"\n", "input": "", "output": "\n        return (flux / 4 *\n                ((self._erf((x - x_0 + 0.5) / (np.sqrt(2) * sigma)) -\n                  self._erf((x - x_0 - 0.5) / (np.sqrt(2) * sigma))) *\n                 (self._erf((y - y_0 + 0.5) / (np.sqrt(2) * sigma)) -\n                  self._erf((y - y_0 - 0.5) / (np.sqrt(2) * sigma)))))"}, {"instruction": "def copy_table(from_table, name):\n    \"\"\"\n    Copy a table.\n\n    Based on `Table.tometadata`, but simplified to remove constraints and indexes.\n\n    \"\"\"\n", "input": "", "output": "    metadata = from_table.metadata\n\n    if name in metadata.tables:\n        return metadata.tables[name]\n\n    schema = metadata.schema\n\n    columns = [\n        copy_column(column, schema)\n        for column in from_table.columns\n        if should_copy(column)\n    ]\n\n    return Table(\n        name,\n        metadata,\n        schema=schema,\n        comment=from_table.comment,\n        *columns,\n        **from_table.kwargs,\n    )"}, {"instruction": "async def pause_writing(self):\n        \"\"\"Pause writing.\"\"\"\n", "input": "", "output": "        self._restart_writer = False\n        if self._writer_task:\n            self._writer_task.remove_done_callback(self.restart_writing)\n            self._writer_task.cancel()\n            await self._writer_task\n            await asyncio.sleep(0, loop=self._loop)"}, {"instruction": "def raw_corpus_rouge2(hypotheses: Iterable[str], references: Iterable[str]) -> float:\n    \"\"\"\n    Simple wrapper around ROUGE-2 implementation.\n\n    :param hypotheses: Hypotheses stream.\n    :param references: Reference stream.\n    :return: ROUGE-2 score as float between 0 and 1.\n    \"\"\"\n", "input": "", "output": "    return rouge.rouge_2(hypotheses, references)"}, {"instruction": "def next(self):\n        \"\"\"\n        Returns the next sequence of results, given stride and n.\n        \n        \"\"\"\n", "input": "", "output": "        try:\n            results = self._stride_buffer.pop()\n        except (IndexError, AttributeError):\n            self._rebuffer()\n            results = self._stride_buffer.pop()\n        if not results:\n            raise StopIteration\n        return results"}, {"instruction": "def guest_deploy(self, userid, image_name, transportfiles=None,\n                     remotehost=None, vdev=None, hostname=None):\n        \"\"\" Deploy the image to vm.\n\n        :param userid: (str) the user id of the vm\n        :param image_name: (str) the name of image that used to deploy the vm\n        :param transportfiles: (str) the files that used to customize the vm\n        :param remotehost: the server where the transportfiles located, the\n               format is username@IP, eg nova@192.168.99.1\n        :param vdev: (str) the device that image will be deploy to\n        :param hostname: (str) the hostname of the vm. This parameter will be\n               ignored if transportfiles present.\n        \"\"\"\n", "input": "", "output": "        action = (\"deploy image '%(img)s' to guest '%(vm)s'\" %\n                  {'img': image_name, 'vm': userid})\n        with zvmutils.log_and_reraise_sdkbase_error(action):\n            self._vmops.guest_deploy(userid, image_name, transportfiles,\n                                     remotehost, vdev, hostname)"}, {"instruction": "def build_indentation_list(parser: str = 'github'):\n    r\"\"\"Create a data structure that holds the state of indentations.\n\n    :parameter parser: decides the length of the list.\n         Defaults to ``github``.\n    :type parser: str\n    :returns: indentation_list, a list that contains the state of\n         indentations given a header type.\n    :rtype: list\n    :raises: a built-in exception.\n    \"\"\"\n", "input": "", "output": "    indentation_list = list()\n\n    if (parser == 'github' or parser == 'cmark' or parser == 'gitlab'\n            or parser == 'commonmarker' or parser == 'redcarpet'):\n        for i in range(0, md_parser[parser]['header']['max_levels']):\n            indentation_list.append(False)\n\n    return indentation_list"}, {"instruction": "def acquire_lock(self, name):\n        \"\"\"\n        Wait for a lock with name.\n        This will prevent other processes from acquiring the lock with\n        the name while it is held. Thus they will wait in the position\n        where they are acquiring the lock until the process that has it\n        releases it.\n        \"\"\"\n", "input": "", "output": "        if self._remotelib:\n            try:\n                while not self._remotelib.run_keyword('acquire_lock',\n                                                      [name, self._my_id], {}):\n                    time.sleep(0.1)\n                    logger.debug('waiting for lock to release')\n                return True\n            except RuntimeError:\n                logger.warn('no connection')\n                self.__remotelib = None\n        return _PabotLib.acquire_lock(self, name, self._my_id)"}, {"instruction": "def histogram_summary(tag, values, bins):\n    \"\"\"Outputs a `Summary` protocol buffer with a histogram.\n    Adding a histogram summary makes it possible to visualize the data's distribution in\n    TensorBoard. See detailed explanation of the TensorBoard histogram dashboard at\n    https://www.tensorflow.org/get_started/tensorboard_histograms\n    This op reports an `InvalidArgument` error if any value is not finite.\n    Adapted from the TensorFlow function `histogram()` at\n    https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/summary/summary.py\n\n    Parameters\n    ----------\n        tag : str\n            A name for the summary of the histogram. Will also serve as a series name in\n            TensorBoard.\n        values : MXNet `NDArray` or `numpy.ndarray`\n            Values for building the histogram.\n\n    Returns\n    -------\n        A `Summary` protobuf of the histogram.\n    \"\"\"\n", "input": "", "output": "    tag = _clean_tag(tag)\n    values = _make_numpy_array(values)\n    hist = _make_histogram(values.astype(float), bins)\n    return Summary(value=[Summary.Value(tag=tag, histo=hist)])"}, {"instruction": "def list_courses(args):\n    \"\"\"\n    List enrolled courses.\n\n    @param args: Command-line arguments.\n    @type args: namedtuple\n    \"\"\"\n", "input": "", "output": "    session = get_session()\n    login(session, args.username, args.password)\n    extractor = CourseraExtractor(session)\n    courses = extractor.list_courses()\n    logging.info('Found %d courses', len(courses))\n    for course in courses:\n        logging.info(course)"}, {"instruction": "def get_gateway_socket(gateway):\r\n    \"\"\"Takes a gateway address string and returns a non-blocking UDP\r\n       socket to communicate with its NAT-PMP implementation on\r\n       NATPMP_PORT.\r\n       \r\n       e.g. addr = get_gateway_socket('10.0.1.1')\r\n    \"\"\"\n", "input": "", "output": "    if not gateway:\r\n        raise NATPMPNetworkError(NATPMP_GATEWAY_NO_VALID_GATEWAY,\r\n                                 error_str(NATPMP_GATEWAY_NO_VALID_GATEWAY))\r\n    response_socket = socket.socket(socket.AF_INET, socket.SOCK_DGRAM)\r\n    response_socket.setblocking(0)\r\n    response_socket.connect((gateway, NATPMP_PORT))\r\n    return response_socket"}, {"instruction": "def cube2matrix(data_cube):\n    r\"\"\"Cube to Matrix\n\n    This method transforms a 3D cube to a 2D matrix\n\n    Parameters\n    ----------\n    data_cube : np.ndarray\n        Input data cube, 3D array\n\n    Returns\n    -------\n    np.ndarray 2D matrix\n\n    Examples\n    --------\n    >>> from modopt.base.transform import cube2matrix\n    >>> a = np.arange(16).reshape((4, 2, 2))\n    >>> cube2matrix(a)\n    array([[ 0,  4,  8, 12],\n           [ 1,  5,  9, 13],\n           [ 2,  6, 10, 14],\n           [ 3,  7, 11, 15]])\n\n    \"\"\"\n", "input": "", "output": "\n    return data_cube.reshape([data_cube.shape[0]] +\n                             [np.prod(data_cube.shape[1:])]).T"}, {"instruction": "def _cz_gate(self, lines):\n        \"\"\"\n        Return the TikZ code for an n-controlled Z-gate.\n\n        :param lines: List of all qubits involved.\n        :type: list[int]\n        \"\"\"\n", "input": "", "output": "        line = lines[0]\n        delta_pos = self._gate_offset(Z)\n        gate_width = self._gate_width(Z)\n        gate_str = self._phase(line, self.pos[line])\n\n        for ctrl in lines[1:]:\n            gate_str += self._phase(ctrl, self.pos[line])\n            gate_str += self._line(ctrl, line)\n\n        new_pos = self.pos[line] + delta_pos + gate_width\n        for i in lines:\n            self.op_count[i] += 1\n        for i in range(min(lines), max(lines) + 1):\n            self.pos[i] = new_pos\n        return gate_str"}, {"instruction": "def read_networks(folder):\n    \"\"\"\n    Read perseus network collection folder format\n    \n    >>> network_table, networks = read_networks(folder)\n    \n    :param folder: Path to network collection\n    :returns: Network table and dictionary with 'name', 'edge_table', and 'node_table' keys.\n    \"\"\"\n", "input": "", "output": "    network_table = read_perseus(path.join(folder, \"networks.txt\"))\n    networks = {}\n    for name, guid in network_table[['Name', 'GUID']].values:\n        networks[guid] = {\n                'name': name,\n                'guid': guid,\n                'node_table': read_perseus(path.join(folder, \"{}_nodes.txt\".format(guid))),\n                'edge_table': read_perseus(path.join(folder, \"{}_edges.txt\".format(guid)))\n                }\n    return network_table, networks"}, {"instruction": "def sky2vec(cls, sky):\n        \"\"\"\n        Convert sky positions in to 3d-vectors on the unit sphere.\n\n        Parameters\n        ----------\n        sky : numpy.array\n            Sky coordinates as an array of (ra,dec)\n\n        Returns\n        -------\n        vec : numpy.array\n            Unit vectors as an array of (x,y,z)\n\n        See Also\n        --------\n        :func:`AegeanTools.regions.Region.vec2sky`\n        \"\"\"\n", "input": "", "output": "        theta_phi = cls.sky2ang(sky)\n        theta, phi = map(np.array, list(zip(*theta_phi)))\n        vec = hp.ang2vec(theta, phi)\n        return vec"}, {"instruction": "def hit(self, pt):\n        \"\"\"Find the view (self, child, or None) under the point `pt`.\"\"\"\n", "input": "", "output": "\n        if self.hidden or not self._enabled:\n            return None\n\n        if not self.frame.collidepoint(pt):\n            return None\n\n        local_pt = (pt[0] - self.frame.topleft[0],\n                    pt[1] - self.frame.topleft[1])\n\n        for child in reversed(self.children):   # front to back\n            hit_view = child.hit(local_pt)\n            if hit_view is not None:\n                return hit_view\n\n        return self"}, {"instruction": "def _output_ret(self, ret, out, retcode=0):\n        '''\n        Print the output from a single return to the terminal\n        '''\n", "input": "", "output": "        import salt.output\n        # Handle special case commands\n        if self.config['fun'] == 'sys.doc' and not isinstance(ret, Exception):\n            self._print_docs(ret)\n        else:\n            # Determine the proper output method and run it\n            salt.output.display_output(ret,\n                                       out=out,\n                                       opts=self.config,\n                                       _retcode=retcode)\n        if not ret:\n            sys.stderr.write('ERROR: No return received\\n')\n            sys.exit(2)"}, {"instruction": "def compile(self, expr, params=None, limit=None):\n        \"\"\"\n        Translate expression to one or more queries according to backend target\n\n        Returns\n        -------\n        output : single query or list of queries\n        \"\"\"\n", "input": "", "output": "        query_ast = self._build_ast_ensure_limit(expr, limit, params=params)\n        return query_ast.compile()"}, {"instruction": "def key(self):\n        \"\"\"\n        Example::\n\n            /browse/homes/ca/ -> ca\n            /browse/homes/ca/los-angeles-county/ -> los-angeles-county\n            /browse/homes/ca/los-angeles-county/91001/ -> 91001\n            /browse/homes/ca/los-angeles-county/91001/tola-ave_5038895/ -> tola-ave_5038895\n\n        :return:\n        \"\"\"\n", "input": "", "output": "\n        return [part.strip() for part in self.href.split(\"/\") if part.strip()][\n            -1]"}, {"instruction": "def create_or_edit(self, id, seq, resource): # pylint: disable=invalid-name,redefined-builtin\n        \"\"\"Create or edit a highlight.\n\n        :param id: Result ID as an int.\n        :param seq: TestResult sequence ID as an int.\n        :param resource: :class:`highlights.Highlight <highlights.Highlight>` object\n        :return: :class:`highlights.Highlight <highlights.Highlight>` object\n        :rtype: highlights.Highlight\n        \"\"\"\n", "input": "", "output": "        schema = HighlightSchema(exclude=('id', 'seq'))\n        json = self.service.encode(schema, resource)\n\n        schema = HighlightSchema()\n        resp = self.service.edit(self._base(id, seq), resource.line, json)\n        return self.service.decode(schema, resp)"}, {"instruction": "def isfile(self, version=None, *args, **kwargs):\n        '''\n        Check whether the path exists and is a file\n        '''\n", "input": "", "output": "        version = _process_version(self, version)\n\n        path = self.get_version_path(version)\n        self.authority.fs.isfile(path, *args, **kwargs)"}, {"instruction": "def write_sheet(writer, name, df, index=False):\n    \"\"\"Write a pandas DataFrame to an ExcelWriter,\n    auto-formatting column width depending on maxwidth of data and colum header\n\n    Parameters\n    ----------\n    writer: pandas.ExcelWriter\n        an instance of a pandas ExcelWriter\n    name: string\n        name of the sheet to be written\n    df: pandas.DataFrame\n        a pandas DataFrame to be written to the sheet\n    index: boolean, default False\n        flag whether index should be written to the sheet\n    \"\"\"\n", "input": "", "output": "    if index:\n        df = df.reset_index()\n    df.to_excel(writer, name, index=False)\n    worksheet = writer.sheets[name]\n    for i, col in enumerate(df.columns):\n        if df.dtypes[col].name.startswith(('float', 'int')):\n            width = len(str(col)) + 2\n        else:\n            width = max([df[col].map(lambda x: len(str(x or 'None'))).max(),\n                         len(col)]) + 2\n        xls_col = '{c}:{c}'.format(c=NUMERIC_TO_STR[i])\n        worksheet.set_column(xls_col, width)"}, {"instruction": "def delete_build_configuration(id=None, name=None):\n    \"\"\"\n    Delete an existing BuildConfiguration\n    :param id:\n    :param name:\n    :return:\n    \"\"\"\n", "input": "", "output": "    data = delete_build_configuration_raw(id, name)\n    if data:\n        return utils.format_json(data)"}, {"instruction": "def from_yaml(data):\n    \"\"\"\n    Interpolate the provided data and return a dict.\n\n    Currently, this is used to reinterpolate the `molecule.yml` inside an\n    Ansible playbook.  If there were any interpolation errors, they would\n    have been found and raised earlier.\n\n    :return: dict\n    \"\"\"\n", "input": "", "output": "    molecule_env_file = os.environ['MOLECULE_ENV_FILE']\n\n    env = os.environ.copy()\n    env = config.set_env_from_file(env, molecule_env_file)\n\n    i = interpolation.Interpolator(interpolation.TemplateWithDefaults, env)\n    interpolated_data = i.interpolate(data)\n\n    return util.safe_load(interpolated_data)"}, {"instruction": "def qualified_name(self):\n        '''return the fully qualified name (`<module>.<interface>#<operation>`)'''\n", "input": "", "output": "        return '{0}.{1}#{2}'.format(self.module.name, self.interface.name, self.name)"}, {"instruction": "def create_hparams():\n  \"\"\"Create hparams.\"\"\"\n", "input": "", "output": "  if FLAGS.use_tpu and \"tpu\" not in FLAGS.hparams_set:\n    tf.logging.warn(\"Not all hyperparameter sets work on TPU. \"\n                    \"Prefer hparams_sets with a '_tpu' suffix, \"\n                    \"e.g. transformer_tpu, if available for your model.\")\n  hparams_path = os.path.join(FLAGS.output_dir, \"hparams.json\")\n  return trainer_lib.create_hparams(FLAGS.hparams_set, FLAGS.hparams,\n                                    hparams_path=hparams_path)"}, {"instruction": "def handle(self, *args, **options):\n        \"\"\"\n        Iterates a command over all registered schemata.\n        \"\"\"\n", "input": "", "output": "        if options['schema_name']:\n            # only run on a particular schema\n            connection.set_schema_to_public()\n            self.execute_command(get_tenant_model().objects.get(schema_name=options['schema_name']), self.COMMAND_NAME,\n                                 *args, **options)\n        else:\n            for tenant in get_tenant_model().objects.all():\n                if not (options['skip_public'] and tenant.schema_name == get_public_schema_name()):\n                    self.execute_command(tenant, self.COMMAND_NAME, *args, **options)"}, {"instruction": "def _pval_from_bootci(boot, estimate):\n    \"\"\"Compute p-value from bootstrap distribution.\n    Similar to the pval function in the R package mediation.\n    Note that this is less accurate than a permutation test because the\n    bootstrap distribution is not conditioned on a true null hypothesis.\n    \"\"\"\n", "input": "", "output": "    if estimate == 0:\n        out = 1\n    else:\n        out = 2 * min(sum(boot > 0), sum(boot < 0)) / len(boot)\n    return min(out, 1)"}, {"instruction": "def as_cwd():\n    \"\"\" Use workdir.options.path as a temporary working directory \"\"\"\n", "input": "", "output": "    _set_log_level()\n    owd = os.getcwd()\n    logger.debug('entering working directory: ' + options.path)\n    os.chdir(os.path.expanduser(options.path))\n    yield\n    logger.debug('returning to original directory: ' + owd)\n    os.chdir(owd)"}, {"instruction": "def _prune_invalid_time_reductions(spec):\n    \"\"\"Prune time reductions of spec with no time dimension.\"\"\"\n", "input": "", "output": "    valid_reductions = []\n    if not spec['var'].def_time and spec['dtype_out_time'] is not None:\n        for reduction in spec['dtype_out_time']:\n            if reduction not in _TIME_DEFINED_REDUCTIONS:\n                valid_reductions.append(reduction)\n            else:\n                msg = (\"Var {0} has no time dimension \"\n                       \"for the given time reduction \"\n                       \"{1} so this calculation will \"\n                       \"be skipped\".format(spec['var'].name, reduction))\n                logging.info(msg)\n    else:\n        valid_reductions = spec['dtype_out_time']\n    return valid_reductions"}, {"instruction": "def get_foreign_keys_in_altered_table(self, diff):\n        \"\"\"\n        :param diff: The table diff\n        :type diff: eloquent.dbal.table_diff.TableDiff\n\n        :rtype: list\n        \"\"\"\n", "input": "", "output": "        foreign_keys = diff.from_table.get_foreign_keys()\n        column_names = self.get_column_names_in_altered_table(diff)\n\n        for key, constraint in foreign_keys.items():\n            changed = False\n            local_columns = []\n            for column_name in constraint.get_local_columns():\n                normalized_column_name = column_name.lower()\n                if normalized_column_name not in column_names:\n                    del foreign_keys[key]\n                    break\n                else:\n                    local_columns.append(column_names[normalized_column_name])\n                    if column_name != column_names[normalized_column_name]:\n                        changed = True\n\n            if changed:\n                pass\n\n        return foreign_keys"}, {"instruction": "def expire_queues(self):\n        '''\n        Expires old queue_dict keys that have not been used in a long time.\n        Prevents slow memory build up when crawling lots of different domains\n        '''\n", "input": "", "output": "        curr_time = time.time()\n        for key in list(self.queue_dict):\n            diff = curr_time - self.queue_dict[key][1]\n            if diff > self.queue_timeout:\n                self.logger.debug(\"Expiring domain queue key \" + key)\n                del self.queue_dict[key]\n                if key in self.queue_keys:\n                    self.queue_keys.remove(key)"}, {"instruction": "def _CopyDateFromString(self, date_string):\n    \"\"\"Copies a date from a string.\n\n    Args:\n      date_string (str): date value formatted as: YYYY-MM-DD\n\n    Returns:\n      tuple[int, int, int]: year, month, day of month.\n\n    Raises:\n      ValueError: if the date string is invalid or not supported.\n    \"\"\"\n", "input": "", "output": "    date_string_length = len(date_string)\n\n    # The date string should at least contain 'YYYY-MM-DD'.\n    if date_string_length < 10:\n      raise ValueError('Date string too short.')\n\n    if date_string[4] != '-' or date_string[7] != '-':\n      raise ValueError('Invalid date string.')\n\n    try:\n      year = int(date_string[0:4], 10)\n    except ValueError:\n      raise ValueError('Unable to parse year.')\n\n    try:\n      month = int(date_string[5:7], 10)\n    except ValueError:\n      raise ValueError('Unable to parse month.')\n\n    try:\n      day_of_month = int(date_string[8:10], 10)\n    except ValueError:\n      raise ValueError('Unable to parse day of month.')\n\n    days_per_month = self._GetDaysPerMonth(year, month)\n    if day_of_month < 1 or day_of_month > days_per_month:\n      raise ValueError('Day of month value out of bounds.')\n\n    return year, month, day_of_month"}, {"instruction": "def assertTraceDoesNotContain(response, message):\n    \"\"\"\n    Raise TestStepFail if response.verify_trace finds message from response traces.\n\n    :param response: Response. Must contain method verify_trace\n    :param message: Message to look for\n    :return: Nothing\n    :raises: AttributeError if response does not contain verify_trace method.\n    TestStepFail if verify_trace returns True.\n    \"\"\"\n", "input": "", "output": "    if not hasattr(response, \"verify_trace\"):\n        raise AttributeError(\"Response object does not contain verify_trace method!\")\n    if response.verify_trace(message, False):\n        raise TestStepFail('Assert: Message(s) \"%s\" in response' % message)"}, {"instruction": "def handle(self, *args, **options):\n        \"\"\"Django command handler.\"\"\"\n", "input": "", "output": "        self.verbosity = int(options.get('verbosity'))\n        self.quiet = options.get('quiet')\n        self._set_logger_level()\n\n        self.servername = options.get('servername')\n        self.decrypt = options.get('decrypt')\n        self.uncompress = options.get('uncompress')\n\n        self.filename = options.get('input_filename')\n        self.path = options.get('input_path')\n\n        self.replace = options.get('replace')\n        self.passphrase = options.get('passphrase')\n        self.interactive = options.get('interactive')\n\n        self.storage = get_storage()\n        self.media_storage = get_storage_class()()\n        self._restore_backup()"}, {"instruction": "def info(self):\n        \"\"\"return info about configuration\"\"\"\n", "input": "", "output": "        uri = ','.join(x['hostname'] for x in self.routers)\n        mongodb_uri = 'mongodb://' + uri\n        result = {'id': self.id,\n                  'shards': self.members,\n                  'configsvrs': self.configsvrs,\n                  'routers': self.routers,\n                  'mongodb_uri': mongodb_uri,\n                  'orchestration': 'sharded_clusters'}\n        if self.login:\n            result['mongodb_auth_uri'] = self.mongodb_auth_uri(uri)\n        return result"}, {"instruction": "def equivalent_to(std_function):\n    \"\"\"\n    Decorates a cloud object compatible function\n    to provides fall back to standard function if\n    used on local files.\n\n    Args:\n        std_function (function): standard function to\n            used with local files.\n\n    Returns:\n        function: new function\n    \"\"\"\n", "input": "", "output": "\n    def decorate(cos_function):\n        "}, {"instruction": "def track_exists(self, localdir):\n        \"\"\" Check if track exists in local directory. \"\"\"\n", "input": "", "output": "        path = glob.glob(self.gen_localdir(localdir) +\n                         self.gen_filename() + \"*\")\n        if len(path) > 0 and os.path.getsize(path[0]) > 0:\n            return True\n        return False"}, {"instruction": "def p_array_literal_2(self, p):\n        \"\"\"array_literal : LBRACKET element_list RBRACKET\n                         | LBRACKET element_list COMMA elision_opt RBRACKET\n        \"\"\"\n", "input": "", "output": "        items = p[2]\n        if len(p) == 6:\n            items.extend(p[4])\n        p[0] = ast.Array(items=items)"}, {"instruction": "def _wrap_parse(code, filename):\n        \"\"\"\n        async wrapper is required to avoid await calls raising a SyntaxError\n        \"\"\"\n", "input": "", "output": "        code = 'async def wrapper():\\n' + indent(code, ' ')\n        return ast.parse(code, filename=filename).body[0].body[0].value"}, {"instruction": "def filter_nodes(graph: BELGraph, node_predicates: NodePredicates) -> Iterable[BaseEntity]:\n    \"\"\"Apply a set of predicates to the nodes iterator of a BEL graph.\"\"\"\n", "input": "", "output": "    concatenated_predicate = concatenate_node_predicates(node_predicates=node_predicates)\n    for node in graph:\n        if concatenated_predicate(graph, node):\n            yield node"}, {"instruction": "def get_log_events(awsclient, log_group_name, log_stream_name, start_ts=None):\n    \"\"\"Get log events for the specified log group and stream.\n    this is used in tenkai output instance diagnostics\n\n    :param log_group_name: log group name\n    :param log_stream_name: log stream name\n    :param start_ts: timestamp\n    :return:\n    \"\"\"\n", "input": "", "output": "    client_logs = awsclient.get_client('logs')\n\n    request = {\n        'logGroupName': log_group_name,\n        'logStreamName': log_stream_name\n    }\n    if start_ts:\n        request['startTime'] = start_ts\n\n    # TODO exhaust the events!\n    # TODO use all_pages !\n    response = client_logs.get_log_events(**request)\n\n    if 'events' in response and response['events']:\n        return [{'timestamp': e['timestamp'], 'message': e['message']}\n                for e in response['events']]"}, {"instruction": "def _is_json(string):\n        \"\"\"Test if input string is in JSON format.\n\n        :param string: Input string.\n        :type string: :py:class:`str` or :py:class:`bytes`\n        :return: Input string if in JSON format or False otherwise.\n        :rtype: :py:class:`str` or :py:obj:`False`\n        \"\"\"\n", "input": "", "output": "        try:\n            if isinstance(string, bytes):\n                string = string.decode(\"utf-8\")\n                json.loads(string)\n            elif isinstance(string, str):\n                json.loads(string)\n            else:\n                raise TypeError(\"Expecting <class 'str'> or <class 'bytes'>, but {} was passed\".format(type(string)))\n            return string\n\n        except ValueError:\n            return False"}, {"instruction": "def get_topic_list(num=10, top_items=False):\n    \"\"\"\n    Returns a list of top recent topics, excluding less valuable forums.\n    Default is 10 topics.\n    Can be sorted for most active topics by votes and post count.\n    Usage:\n    {% get_topic_list 5 as topics %}\n    {% get_topic_list 7 top_items=True as topics %}\n    \"\"\"\n", "input": "", "output": "    excluded_forum_ids = [3, 7, 10, 12, 15, 16, 17, 18, 19, 23]\n    topics = Topic.objects.exclude(forum_id__in=excluded_forum_ids).order_by('-id')[0:num]\n    if top_items:\n        topics = sorted(list(topics), key=lambda t: (t.forum_id, -t.votes, -t.post_count))\n    return topics"}, {"instruction": "def _pairwise_chisq(self):\n        \"\"\"Pairwise comparisons (Chi-Square) along axis, as numpy.ndarray.\n\n        Returns a list of square and symmetric matrices of test statistics for the null\n        hypothesis that each vector along *axis* is equal to each other.\n        \"\"\"\n", "input": "", "output": "        return [\n            self._chi_squared(\n                mr_subvar_proportions,\n                self._margin[idx],\n                self._opposite_axis_margin[idx]\n                / np.sum(self._opposite_axis_margin[idx]),\n            )\n            for (idx, mr_subvar_proportions) in enumerate(self._proportions)\n        ]"}, {"instruction": "def processlist(**connection_args):\n    '''\n    Retrieves the processlist from the MySQL server via\n    \"SHOW FULL PROCESSLIST\".\n\n    Returns: a list of dicts, with each dict representing a process:\n\n    .. code-block:: python\n\n        {'Command': 'Query',\n        'Host': 'localhost',\n        'Id': 39,\n        'Info': 'SHOW FULL PROCESSLIST',\n        'Rows_examined': 0,\n        'Rows_read': 1,\n        'Rows_sent': 0,\n        'State': None,\n        'Time': 0,\n        'User': 'root',\n        'db': 'mysql'}\n\n    CLI Example:\n\n    .. code-block:: bash\n\n        salt '*' mysql.processlist\n\n    '''\n", "input": "", "output": "    ret = []\n\n    dbc = _connect(**connection_args)\n    if dbc is None:\n        return []\n    cur = dbc.cursor()\n    _execute(cur, 'SHOW FULL PROCESSLIST')\n    hdr = [c[0] for c in cur.description]\n    for _ in range(cur.rowcount):\n        row = cur.fetchone()\n        idx_r = {}\n        for idx_j in range(len(hdr)):\n            idx_r[hdr[idx_j]] = row[idx_j]\n        ret.append(idx_r)\n    cur.close()\n    return ret"}, {"instruction": "def _lats(self):\n        \"\"\"Return the latitudes (in degrees) of the gridded data.\"\"\"\n", "input": "", "output": "        lats = _np.linspace(90.0, -90.0 + 180.0 / self.nlat, num=self.nlat)\n        return lats"}, {"instruction": "def get_genus_type_metadata(self):\n        \"\"\"Overrides get_genus_type_metadata of extended object\"\"\"\n", "input": "", "output": "        metadata = dict(self.my_osid_object_form._genus_type_metadata)\n        metadata.update({'read_only': True})\n        return Metadata(**metadata)"}, {"instruction": "def txt_line_iterator(txt_path):\n  \"\"\"Iterate through lines of file.\"\"\"\n", "input": "", "output": "  with tf.gfile.Open(txt_path) as f:\n    for line in f:\n      yield line.strip()"}, {"instruction": "def split_input(args):\n    \"\"\"Split query input into local files and URLs.\"\"\"\n", "input": "", "output": "    args['files'] = []\n    args['urls'] = []\n    for arg in args['query']:\n        if os.path.isfile(arg):\n            args['files'].append(arg)\n        else:\n            args['urls'].append(arg.strip('/'))"}, {"instruction": "def split_filename(filename):\n    \"\"\"\n    Received a standard style rpm fullname and returns\n    name, version, release, epoch, arch\n    Example: foo-1.0-1.i386.rpm returns foo, 1.0, 1, i386\n             1:bar-9-123a.ia64.rpm returns bar, 9, 123a, 1, ia64\n\n    This function replaces rpmUtils.miscutils.splitFilename, see\n    https://bugzilla.redhat.com/1452801\n    \"\"\"\n", "input": "", "output": "\n    # Remove .rpm suffix\n    if filename.endswith('.rpm'):\n        filename = filename.split('.rpm')[0]\n\n    # is there an epoch?\n    components = filename.split(':')\n    if len(components) > 1:\n        epoch = components[0]\n    else:\n        epoch = ''\n\n    # Arch is the last item after .\n    arch = filename.rsplit('.')[-1]\n    remaining = filename.rsplit('.%s' % arch)[0]\n    release = remaining.rsplit('-')[-1]\n    version = remaining.rsplit('-')[-2]\n    name = '-'.join(remaining.rsplit('-')[:-2])\n\n    return name, version, release, epoch, arch"}, {"instruction": "def head(self, n=10):\n        \"\"\"\n        Display the top of the file.\n\n        Args:\n            n (int): Number of lines to display\n        \"\"\"\n", "input": "", "output": "        r = self.__repr__().split('\\n')\n        print('\\n'.join(r[:n]), end=' ')"}, {"instruction": "def _findLocation(self, reference_name, start, end):\n        \"\"\"\n        return a location key form the locationMap\n        \"\"\"\n", "input": "", "output": "        try:\n            # TODO - sequence_annotations does not have build?\n            return self._locationMap['hg19'][reference_name][start][end]\n        except:\n            return None"}, {"instruction": "def find_content(self, text):\n        \"\"\"Find content.\"\"\"\n", "input": "", "output": "\n        for m in self.pattern.finditer(self.norm_nl(text)):\n            self.evaluate(m)"}, {"instruction": "def pprint(self, file_=sys.stdout):\n        \"\"\"Print the code block to stdout.\n        Does syntax highlighting if possible.\n        \"\"\"\n", "input": "", "output": "\n        code = []\n        if self._deps:\n            code.append(\"# dependencies:\")\n        for k, v in _compat.iteritems(self._deps):\n            code.append(\"#   %s: %r\" % (k, v))\n        code.append(str(self))\n        code = \"\\n\".join(code)\n\n        if file_.isatty():\n            try:\n                from pygments import highlight\n                from pygments.lexers import PythonLexer\n                from pygments.formatters import TerminalFormatter\n            except ImportError:\n                pass\n            else:\n                formatter = TerminalFormatter(bg=\"dark\")\n                lexer = PythonLexer()\n                file_.write(highlight(code, lexer, formatter))\n                return\n        file_.write(code + \"\\n\")"}, {"instruction": "def mutation(self, config=None, info=None, save_dir=None):\n        \"\"\"\n        Parameters\n        ----------\n        config : str\n        info : str\n        save_dir : str\n        \"\"\"\n", "input": "", "output": "        self.result = None\n        self.config = config\n        self.restore_dir = self.save_dir\n        self.save_dir = save_dir\n        self.info = info"}, {"instruction": "def make_static_url(\n        cls, settings: Dict[str, Any], path: str, include_version: bool = True\n    ) -> str:\n        \"\"\"Constructs a versioned url for the given path.\n\n        This method may be overridden in subclasses (but note that it\n        is a class method rather than an instance method).  Subclasses\n        are only required to implement the signature\n        ``make_static_url(cls, settings, path)``; other keyword\n        arguments may be passed through `~RequestHandler.static_url`\n        but are not standard.\n\n        ``settings`` is the `Application.settings` dictionary.  ``path``\n        is the static path being requested.  The url returned should be\n        relative to the current host.\n\n        ``include_version`` determines whether the generated URL should\n        include the query string containing the version hash of the\n        file corresponding to the given ``path``.\n\n        \"\"\"\n", "input": "", "output": "        url = settings.get(\"static_url_prefix\", \"/static/\") + path\n        if not include_version:\n            return url\n\n        version_hash = cls.get_version(settings, path)\n        if not version_hash:\n            return url\n\n        return \"%s?v=%s\" % (url, version_hash)"}, {"instruction": "def readFAM(basefilename,usecols=None):\n    \"\"\"\n    helper method for speeding up read FAM\n    \"\"\"\n", "input": "", "output": "    fam = basefilename+'.fam'\n    fam = SP.loadtxt(fam,dtype=bytes,usecols=usecols)\n    return fam"}, {"instruction": "def _remove_unicode_encoding(xml_file):\n    '''\n    attempts to remove the \"encoding='unicode'\" from an xml file\n    as lxml does not support that on a windows node currently\n    see issue #38100\n    '''\n", "input": "", "output": "    with salt.utils.files.fopen(xml_file, 'rb') as f:\n        xml_content = f.read()\n    modified_xml = re.sub(r' encoding=[\\'\"]+unicode[\\'\"]+', '', xml_content.decode('utf-16'), count=1)\n    xmltree = lxml.etree.parse(six.StringIO(modified_xml))\n    return xmltree"}, {"instruction": "def _initialize_repo_cache():\n    \"\"\"Initialize the repository cache used for scraping.\n\n    Retrieves a list of repositories with their provider and last scraping time\n    from Elasticsearch.\n    This list can be used to check which repos need to be scraped (e.g. after\n    a specific amount of time).\n    \"\"\"\n", "input": "", "output": "    LOGGER.info(\"Initializing repository cache\")\n    # Initialize Repo Cache\n    repo_cache = {}\n\n    # Get all repos from Elasticsearch\n    for hit in GitRepo.search().query(\"match_all\").scan():\n        # TODO (fschmidt): Maybe we can use this list as cache for the whole\n        # scraper-webhook part.\n        # This way, we could reduce the amount of operations needed for GitHub\n        # and ElasticSearch\n        repo_cache[hit.repo_name] = hit.to_dict(skip_empty=False)\n\n    return repo_cache"}, {"instruction": "def copy(self, deep=True, data=None):\n        \"\"\"Returns a copy of this object.\n\n        `deep` is ignored since data is stored in the form of\n        pandas.Index, which is already immutable. Dimensions, attributes\n        and encodings are always copied.\n\n        Use `data` to create a new object with the same structure as\n        original but entirely new data.\n\n        Parameters\n        ----------\n        deep : bool, optional\n            Deep is always ignored.\n        data : array_like, optional\n            Data to use in the new object. Must have same shape as original.\n\n        Returns\n        -------\n        object : Variable\n            New object with dimensions, attributes, encodings, and optionally\n            data copied from original.\n        \"\"\"\n", "input": "", "output": "        if data is None:\n            data = self._data\n        else:\n            data = as_compatible_data(data)\n            if self.shape != data.shape:\n                raise ValueError(\"Data shape {} must match shape of object {}\"\n                                 .format(data.shape, self.shape))\n        return type(self)(self.dims, data, self._attrs,\n                          self._encoding, fastpath=True)"}, {"instruction": "def open(hdfs_path, mode=\"r\", buff_size=0, replication=0, blocksize=0,\n         user=None, encoding=None, errors=None):\n    \"\"\"\n    Open a file, returning an :class:`~.file.hdfs_file` object.\n\n    ``hdfs_path`` and ``user`` are passed to :func:`~path.split`,\n    while the other args are passed to the :class:`~.file.hdfs_file`\n    constructor.\n    \"\"\"\n", "input": "", "output": "    host, port, path_ = path.split(hdfs_path, user)\n    fs = hdfs(host, port, user)\n    return fs.open_file(path_, mode, buff_size, replication, blocksize,\n                        encoding, errors)"}, {"instruction": "def register_schema(self, directory, path):\n        \"\"\"Register a json-schema.\n\n        :param directory: root directory path.\n        :param path: schema path, relative to the root directory.\n        \"\"\"\n", "input": "", "output": "        self.schemas[path] = os.path.abspath(directory)"}, {"instruction": "def get_condition(self, condition_id):\n        \"\"\"Retrieve the condition for a condition_id.\n\n        :param condition_id: id of the condition, str\n        :return:\n        \"\"\"\n", "input": "", "output": "        condition = self.contract_concise.getCondition(condition_id)\n        if condition and len(condition) == 7:\n            return ConditionValues(*condition)\n\n        return None"}, {"instruction": "def delete_refs(repo, refs, dry_run=False):\n    \"\"\"Note that only the ref to a tag can be explicitly removed.  The tag\n    object will leave on until it's gargabe collected.\"\"\"\n", "input": "", "output": "\n    assert isinstance(repo, github.Repository.Repository), type(repo)\n\n    debug(\"removing {n} refs from {repo}\".format(\n        n=len(refs),\n        repo=repo.full_name)\n    )\n\n    for r in refs:\n        debug(\"  deleting {ref}\".format(ref=r.ref))\n        if dry_run:\n            debug('    (noop)')\n            continue\n\n        r.delete()"}, {"instruction": "def wrap_sequence(sequence, books=None, tensor_shape=None):\n  \"\"\"Creates an input layer representing the given sequence of tensors.\n\n  Args:\n    sequence: A sequence of tensors.\n    books: The bookkeeper.\n    tensor_shape: An optional shape that will be set on the Tensor or verified\n      to match the tensor.\n  Returns:\n    A layer.\n  \"\"\"\n", "input": "", "output": "  if books is None:\n    books = bookkeeper.for_default_graph()\n  my_sequence = [\n      wrap(t, books=books, tensor_shape=tensor_shape) for t in sequence]\n  return Layer(books, sequence=my_sequence, name=my_sequence[0].name)"}, {"instruction": "def translate_sites(self, indices, vector, frac_coords=True,\n                        to_unit_cell=True):\n        \"\"\"\n        Translate specific sites by some vector, keeping the sites within the\n        unit cell.\n\n        Args:\n            indices: Integer or List of site indices on which to perform the\n                translation.\n            vector: Translation vector for sites.\n            frac_coords (bool): Whether the vector corresponds to fractional or\n                cartesian coordinates.\n            to_unit_cell (bool): Whether new sites are transformed to unit\n                cell\n        \"\"\"\n", "input": "", "output": "        if not isinstance(indices, collections.abc.Iterable):\n            indices = [indices]\n\n        for i in indices:\n            site = self._sites[i]\n            if frac_coords:\n                fcoords = site.frac_coords + vector\n            else:\n                fcoords = self._lattice.get_fractional_coords(\n                    site.coords + vector)\n            if to_unit_cell:\n                fcoords = np.mod(fcoords, 1)\n            self._sites[i].frac_coords = fcoords"}, {"instruction": "def _startReapingProcesses(self):\n        \"\"\"\n        Start a LoopingCall that calls reapAllProcesses.\n        \"\"\"\n", "input": "", "output": "        lc = LoopingCall(self._reapAllProcesses)\n        lc.clock = self._reactor\n        lc.start(0.1, False)"}, {"instruction": "def remove_query_param(self, key, value=None):\n        \"\"\"\n        Remove a query param from a URL\n\n        Set the value parameter if removing from a list.\n\n        :param string key: The key to delete\n        :param string value: The value of the param to delete (of more than one)\n        \"\"\"\n", "input": "", "output": "        parse_result = self.query_params()\n        if value is not None:\n            index = parse_result[key].index(value)\n            del parse_result[key][index]\n        else:\n            del parse_result[key]\n        return URL._mutate(self, query=unicode_urlencode(parse_result, doseq=True))"}, {"instruction": "def unflatten(obj):\n  '''\n  TODO: add docs\n  '''\n", "input": "", "output": "  if not isdict(obj):\n    raise ValueError(\n      'only dict-like objects can be unflattened, not %r' % (obj,))\n  ret = dict()\n  sub = dict()\n  for key, value in obj.items():\n    if '.' not in key and '[' not in key:\n      ret[key] = value\n      continue\n    if '.' in key and '[' in key:\n      idx = min(key.find('.'), key.find('['))\n    elif '.' in key:\n      idx = key.find('.')\n    else:\n      idx = key.find('[')\n    prefix = key[:idx]\n    if prefix not in sub:\n      sub[prefix] = dict()\n    sub[prefix][key[idx:]] = value\n  for pfx, values in sub.items():\n    if pfx in ret:\n      raise ValueError(\n        'conflicting scalar vs. structure for prefix: %s' % (pfx,))\n    ret[pfx] = _relunflatten(pfx, values)\n  return ret"}, {"instruction": "def create_and_register_access_db(filename: str,\n                                  dsn: str,\n                                  description: str) -> bool:\n    \"\"\"\n    (Windows only.)\n    Creates a Microsoft Access database and registers it with ODBC.\n\n    Args:\n        filename: filename of the database to create\n        dsn: ODBC data source name to create\n        description: description of the database\n\n    Returns:\n        bool: was the DSN created?\n    \"\"\"\n", "input": "", "output": "    fullfilename = os.path.abspath(filename)\n    create_string = fullfilename + \" General\"\n    # ... filename, space, sort order (\"General\" for English)\n    return (create_user_dsn(access_driver, CREATE_DB=create_string) and\n            register_access_db(filename, dsn, description))"}, {"instruction": "def next(self):\n        \"\"\"Next point in iteration\n        \"\"\"\n", "input": "", "output": "        if self.probability == 1:\n            x, y = next(self.scan)\n        else:\n            while True:\n                x, y = next(self.scan)\n                if random.random() <= self.probability: break\n        return x, y"}, {"instruction": "def pattern(head, *args, mode=1, wc_name=None, conditions=None, **kwargs) \\\n        -> Pattern:\n    \"\"\"'Flat' constructor for the Pattern class\n\n    Positional and keyword arguments are mapped into `args` and `kwargs`,\n    respectively. Useful for defining rules that match an instantiated\n    Expression with specific arguments\n    \"\"\"\n", "input": "", "output": "    if len(args) == 0:\n        args = None\n    if len(kwargs) == 0:\n        kwargs = None\n    return Pattern(head, args, kwargs, mode=mode, wc_name=wc_name,\n                   conditions=conditions)"}, {"instruction": "def disconnect(self):\n        \"\"\"Disconnect from event stream.\"\"\"\n", "input": "", "output": "        _LOGGING.debug('Disconnecting from stream: %s', self.name)\n        self.kill_thrd.set()\n        self.thrd.join()\n        _LOGGING.debug('Event stream thread for %s is stopped', self.name)\n        self.kill_thrd.clear()"}, {"instruction": "def prepare_sparse_params(self, param_rowids):\n        '''Prepares the module for processing a data batch by pulling row_sparse\n        parameters from kvstore to all devices based on rowids.\n\n        Parameters\n        ----------\n        param_rowids : dict of str to NDArray of list of NDArrays\n        '''\n", "input": "", "output": "        if not self._kvstore:\n            return\n        assert(isinstance(param_rowids, dict))\n        for param_name, rowids in param_rowids.items():\n            if isinstance(rowids, (tuple, list)):\n                rowids_1d = []\n                for r in rowids:\n                    rowids_1d.append(r.reshape((-1,)).astype(np.int64))\n                rowid = mx.nd.concat(*rowids_1d, dim=0)\n            else:\n                rowid = rowids\n            param_idx = self._exec_group.param_names.index(param_name)\n            param_val = self._exec_group.param_arrays[param_idx]\n            self._kvstore.row_sparse_pull(param_name, param_val, row_ids=rowid,\n                                          priority=-param_idx)"}, {"instruction": "async def track_event(event, state, service_name):\n    \"\"\"\n    Store state of events in memory\n    :param event: Event object\n    :param state: EventState object\n    :param service_name: Name of service name\n    \"\"\"\n", "input": "", "output": "    redis = await aioredis.create_redis(\n        (EVENT_TRACKING_HOST, 6379), loop=loop)\n\n    now = datetime.utcnow()\n    event_id = event.event_id\n\n    tracking_data = json.dumps({\n        \"event_id\": event_id,\n        \"timestamp\": str(now),\n        \"state\": state\n    })\n    await redis.rpush(service_name, tracking_data)\n    redis.close()\n    await redis.wait_closed()"}, {"instruction": "def populate_all_metadata():\n    \"\"\" Create metadata instances for all models in seo_models if empty.\n        Once you have created a single metadata instance, this will not run.\n        This is because it is a potentially slow operation that need only be\n        done once. If you want to ensure that everything is populated, run the\n        populate_metadata management command.\n    \"\"\"\n", "input": "", "output": "    for Metadata in registry.values():\n        InstanceMetadata = Metadata._meta.get_model('modelinstance')\n        if InstanceMetadata is not None:\n            for model in Metadata._meta.seo_models:\n                populate_metadata(model, InstanceMetadata)"}, {"instruction": "def hisat2_general_stats_table(self):\n        \"\"\" Take the parsed stats from the HISAT2 report and add it to the\n        basic stats table at the top of the report \"\"\"\n", "input": "", "output": "\n        headers = OrderedDict()\n        headers['overall_alignment_rate'] = {\n            'title': '% Aligned',\n            'description': 'overall alignment rate',\n            'max': 100,\n            'min': 0,\n            'suffix': '%',\n            'scale': 'YlGn'\n        }\n        self.general_stats_addcols(self.hisat2_data, headers)"}, {"instruction": "def incidental (self):\n        \"\"\" Returns incidental properties.\n        \"\"\"\n", "input": "", "output": "        result = [p for p in self.lazy_properties if p.feature.incidental]\n        result.extend(self.incidental_)\n        return result"}, {"instruction": "def show_state_usage(queue=False, **kwargs):\n    '''\n    Retrieve the highstate data from the salt master to analyse used and unused states\n\n    Custom Pillar data can be passed with the ``pillar`` kwarg.\n\n    CLI Example:\n\n    .. code-block:: bash\n\n        salt '*' state.show_state_usage\n    '''\n", "input": "", "output": "    conflict = _check_queue(queue, kwargs)\n    if conflict is not None:\n        return conflict\n    pillar = kwargs.get('pillar')\n    pillar_enc = kwargs.get('pillar_enc')\n    if pillar_enc is None \\\n            and pillar is not None \\\n            and not isinstance(pillar, dict):\n        raise SaltInvocationError(\n            'Pillar data must be formatted as a dictionary, unless pillar_enc '\n            'is specified.'\n        )\n\n    st_ = salt.state.HighState(__opts__, pillar, pillar_enc=pillar_enc)\n    st_.push_active()\n\n    try:\n        ret = st_.compile_state_usage()\n    finally:\n        st_.pop_active()\n    _set_retcode(ret)\n    return ret"}, {"instruction": "def get_client(self):\n        \"\"\"\n        Retrieves or creates a client instance from this configuration object. If instantiated from this configuration,\n        the resulting object is also cached in the property ``client`` and a reference to this configuration is stored\n        on the client object.\n\n        :return: Client object instance.\n        :rtype: docker.client.Client\n        \"\"\"\n", "input": "", "output": "        client = self._client\n        if not client:\n            self._client = client = self.client_constructor(**self.get_init_kwargs())\n            client.client_configuration = self\n            # Client might update the version number after construction.\n            updated_version = getattr(client, 'api_version', None)\n            if updated_version:\n                self.version = updated_version\n        return client"}, {"instruction": "def logpost(self, theta, t=None):\n        \"\"\"Posterior log-density at given parameter values. \n\n        Parameters\n        ----------\n        theta: dict-like\n            theta['par'] is a ndarray containing the N values for parameter par\n        t: int \n            time (if set to None, the full posterior is returned)\n\n        Returns\n        -------\n        l: float numpy.ndarray\n            the N log-likelihood values \n        \"\"\"\n", "input": "", "output": "        return self.prior.logpdf(theta) + self.loglik(theta, t)"}, {"instruction": "def retention_load(self, forced=False):\n        \"\"\"Call hook point 'load_retention'.\n        Retention modules will read retention (from file, db etc)\n\n        :param forced: is load forced?\n        :type forced: bool\n        :return: None\n        \"\"\"\n", "input": "", "output": "        # If we set the retention update to 0, we do not want to manage retention\n        # If we are not forced (like at stopping)\n        if self.pushed_conf.retention_update_interval == 0 and not forced:\n            logger.debug(\"Should have loaded retention but it is not enabled\")\n            return\n\n        _t0 = time.time()\n        self.hook_point('load_retention')\n        statsmgr.timer('hook.retention-load', time.time() - _t0)\n\n        self.add(make_monitoring_log('INFO', 'RETENTION LOAD: %s' % self.my_daemon.name))\n        logger.info('Retention data loaded: %.2f seconds', time.time() - _t0)"}, {"instruction": "def copy_dataset_files(self, ds, incver=False, cb=None, **kwargs):\n        \"\"\"\n        Copy only files and configs into the database.\n        :param ds: The source dataset to copy\n        :param cb: A progress callback, taking two parameters: cb(message, num_records)\n        :return:\n        \"\"\"\n", "input": "", "output": "        from ambry.orm import File\n\n        tables = [File]\n\n        return self._copy_dataset_copy(ds, tables, incver, cb, **kwargs)"}, {"instruction": "def ecdsa_private_key(privkey_str=None, compressed=None):\n    \"\"\"\n    Make a private key, but enforce the following rule:\n    * unless the key's hex encoding specifically ends in '01', treat it as uncompressed.\n    \"\"\"\n", "input": "", "output": "    if compressed is None:\n        compressed = False\n        if privkey_str is not None:\n            if len(privkey_str) == 66 and privkey_str[-2:] == '01':\n                compressed = True\n\n    return _ECPrivateKey(privkey_str, compressed=compressed)"}, {"instruction": "def node_changed(self, node):\n        \"\"\"\n        Calls :meth:`QAbstractItemModel.dataChanged` with given Node index.\n\n        :param node: Node.\n        :type node: AbstractCompositeNode or GraphModelNode\n        :return: Method success.\n        :rtype: bool\n        \"\"\"\n", "input": "", "output": "\n        index = self.get_node_index(node)\n        if index is not None:\n            self.dataChanged.emit(index, index)\n            return True\n        else:\n            return False"}, {"instruction": "def SeriesXmlRewriterFactory(chart_type, chart_data):\n    \"\"\"\n    Return a |_BaseSeriesXmlRewriter| subclass appropriate to *chart_type*.\n    \"\"\"\n", "input": "", "output": "    XL_CT = XL_CHART_TYPE\n\n    RewriterCls = {\n        # There are 73 distinct chart types, only specify non-category\n        # types, others default to _CategorySeriesXmlRewriter. Stock-type\n        # charts are multi-plot charts, so no guaratees on how they turn\n        # out.\n        XL_CT.BUBBLE:                       _BubbleSeriesXmlRewriter,\n        XL_CT.BUBBLE_THREE_D_EFFECT:        _BubbleSeriesXmlRewriter,\n        XL_CT.XY_SCATTER:                   _XySeriesXmlRewriter,\n        XL_CT.XY_SCATTER_LINES:             _XySeriesXmlRewriter,\n        XL_CT.XY_SCATTER_LINES_NO_MARKERS:  _XySeriesXmlRewriter,\n        XL_CT.XY_SCATTER_SMOOTH:            _XySeriesXmlRewriter,\n        XL_CT.XY_SCATTER_SMOOTH_NO_MARKERS: _XySeriesXmlRewriter,\n    }.get(chart_type, _CategorySeriesXmlRewriter)\n\n    return RewriterCls(chart_data)"}, {"instruction": "def getResponse(self, http_request, request):\n        \"\"\"\n        Processes the AMF request, returning an AMF response.\n\n        @param http_request: The underlying HTTP Request.\n        @type http_request: U{HTTPRequest<http://docs.djangoproject.com\n            /en/dev/ref/request-response/#httprequest-objects>}\n        @param request: The AMF Request.\n        @type request: L{Envelope<pyamf.remoting.Envelope>}\n        @rtype: L{Envelope<pyamf.remoting.Envelope>}\n        \"\"\"\n", "input": "", "output": "        response = remoting.Envelope(request.amfVersion)\n\n        for name, message in request:\n            http_request.amf_request = message\n\n            processor = self.getProcessor(message)\n            response[name] = processor(message, http_request=http_request)\n\n        return response"}, {"instruction": "def ngrok_url():\n    \"\"\"\n    If ngrok is running, it exposes an API on port 4040. We can use that\n    to figure out what URL it has assigned, and suggest that to the user.\n    https://ngrok.com/docs#list-tunnels\n    \"\"\"\n", "input": "", "output": "    try:\n        ngrok_resp = requests.get(\"http://localhost:4040/api/tunnels\")\n    except requests.ConnectionError:\n        # I guess ngrok isn't running.\n        return None\n    ngrok_data = ngrok_resp.json()\n    secure_urls = [\n        tunnel[\"public_url\"]\n        for tunnel in ngrok_data[\"tunnels\"]\n        if tunnel[\"proto\"] == \"https\"\n    ]\n    return secure_urls[0]"}, {"instruction": "def create(context, name):\n    \"\"\"create(context, name)\n\n    Create a tag.\n\n    >>> dcictl tag-create [OPTIONS]\n\n    :param string name: Name of the tag [required]\n    \"\"\"\n", "input": "", "output": "\n    result = tag.create(context, name=name)\n    utils.format_output(result, context.format)"}, {"instruction": "def dump(self, output, close_after_write=True):\n        \"\"\"Write a worksheet to the current workbook.\n\n        Args:\n            output (str):\n                Path to the workbook file to write.\n            close_after_write (bool, optional):\n                Close the workbook after write.\n                Defaults to |True|.\n        \"\"\"\n", "input": "", "output": "\n        self.open(output)\n        try:\n            self.make_worksheet(self.table_name)\n            self.write_table()\n        finally:\n            if close_after_write:\n                self.close()"}, {"instruction": "def signout(self, redirect_url = \"/\"):\r\n        \"\"\"\r\n            \u6ce8\u9500\u767b\u5f55\u72b6\u6001\r\n\r\n            \u53c2\u6570:\r\n                redirect_url    \u8df3\u8f6c\u94fe\u63a5\uff0c\u4e3a None \u65f6\u4e0d\u8df3\u8f6c (Ajax \u53ef\u80fd\u7528\u5f97\u5230)\u3002\r\n        \"\"\"\n", "input": "", "output": "        self.clear_cookie(self._USER_NAME)\r\n        if redirect_url: self.redirect(redirect_url)"}, {"instruction": "def remove_tag(self, tag):\n        \"\"\"Remove a user's tag from this object.\"\"\"\n", "input": "", "output": "\n        if isinstance(tag, Tag):\n            tag = tag.get_name()\n\n        params = self._get_params()\n        params[\"tag\"] = tag\n\n        self._request(self.ws_prefix + \".removeTag\", False, params)"}, {"instruction": "def _parse_value(self, stats, field):\n        \"\"\"\n        Pull the specified value from the HTML contents.\n\n        Given a field, find the corresponding HTML tag for that field and parse\n        its value before returning the value as a string. A couple fields, such\n        as 'conference' and 'team_abbreviation' don't follow a standard parsing\n        scheme and need to be handled differently to get the correct value.\n\n        Parameters\n        ----------\n        stats : PyQuery object\n            A PyQuery object containing all stats in HTML format for a\n            particular player.\n        field : string\n            A string of the field to parse from the HTML.\n\n        Returns\n        -------\n        string\n            Returns the desired value as a string.\n        \"\"\"\n", "input": "", "output": "        if field == 'conference':\n            value = self._parse_conference(stats)\n        elif field == 'team_abbreviation':\n            value = self._parse_team_abbreviation(stats)\n        else:\n            value = utils._parse_field(PLAYER_SCHEME, stats, field)\n        return value"}, {"instruction": "def _checkObjectsToLearn(self, objects):\n    \"\"\"\n    Checks that objects have the correct format before being sent to the\n    experiment.\n    \"\"\"\n", "input": "", "output": "    for objectName, sensationList in objects.iteritems():\n      if objectName not in self.objects:\n        raise ValueError(\n          \"Invalid object name \\\"{}\\\" sent to experiment\".format(objectName)\n        )\n\n      for sensations in sensationList:\n        if set(sensations.keys()) != set(range(self.numColumns)):\n          raise ValueError(\n            \"Invalid number of cortical column sensations sent to experiment\"\n          )\n        for pair in sensations.values():\n          if not isinstance(pair, tuple) or len(pair) != 2 or \\\n                  not isinstance(pair[0], set) or \\\n                  not isinstance(pair[1], set):\n            raise ValueError(\"Invalid SDR's sent to experiment\")"}, {"instruction": "def disable_selinux():\n    \"\"\" disables selinux \"\"\"\n", "input": "", "output": "\n    if contains(filename='/etc/selinux/config',\n                text='SELINUX=enforcing'):\n        sed('/etc/selinux/config',\n            'SELINUX=enforcing', 'SELINUX=disabled', use_sudo=True)\n\n    if contains(filename='/etc/selinux/config',\n                text='SELINUX=permissive'):\n        sed('/etc/selinux/config',\n            'SELINUX=permissive', 'SELINUX=disabled', use_sudo=True)\n\n    if sudo('getenforce').lower() != 'disabled':\n        with settings(warn_only=True, capture=True):\n            sudo('/sbin/reboot')\n        sleep_for_one_minute()"}, {"instruction": "def output_size(self) -> Tuple[Sequence[Shape], Sequence[Shape], Sequence[Shape], int]:\n        '''Returns the simulation output size.'''\n", "input": "", "output": "        return self._cell.output_size"}, {"instruction": "def add_timescale(self, timescale):\n        \"\"\"\n        Add a time scale to the plot.\n\n        :param timescale: the timescale to be added\n        :type  timescale: :class:`~aeneas.plotter.PlotTimeScale`\n        :raises: TypeError: if ``timescale`` is not an instance of :class:`~aeneas.plotter.PlotTimeScale`\n        \"\"\"\n", "input": "", "output": "        if not isinstance(timescale, PlotTimeScale):\n            self.log_exc(u\"timescale must be an instance of PlotTimeScale\", None, True, TypeError)\n        self.timescale = timescale\n        self.log(u\"Added timescale\")"}, {"instruction": "def ChargeColorMapping(maptype='jet', reverse=False):\n    \"\"\"Maps amino-acid charge at neutral pH to colors. \n    Currently does not use the keyword arguments for *maptype*\n    or *reverse* but accepts these arguments to be consistent\n    with KyteDoolittleColorMapping and MWColorMapping for now.\"\"\"\n", "input": "", "output": "\n    pos_color = '#FF0000'\n    neg_color = '#0000FF'\n    neut_color = '#000000'\n\n    mapping_d = {'A':neut_color,'R':pos_color,'N':neut_color,\\\n                 'D':neg_color,'C':neut_color,'Q':neut_color,\\\n                 'E':neg_color,'G':neut_color,'H':pos_color,\\\n                 'I':neut_color,'L':neut_color,'K':pos_color,\\\n                 'M':neut_color,'F':neut_color,'P':neut_color,\\\n                 'S':neut_color,'T':neut_color,'W':neut_color,\\\n                 'Y':neut_color,'V':neut_color}\n\n    return (None, mapping_d, None)"}, {"instruction": "def to_dict(obj):\n    \"\"\"\n    Create a filtered dict from the given object.\n\n    Note: This function is currently specific to the FailureLine model.\n    \"\"\"\n", "input": "", "output": "    if not isinstance(obj.test, str):\n        # TODO: can we handle this in the DB?\n        # Reftests used to use tuple indicies, which we can't support.\n        # This is fixed upstream, but we also need to handle it here to allow\n        # for older branches.\n        return\n\n    keys = [\n        'id',\n        'job_guid',\n        'test',\n        'subtest',\n        'status',\n        'expected',\n        'message',\n        'best_classification',\n        'best_is_verified',\n    ]\n\n    all_fields = obj.to_dict()\n    return {k: v for k, v in all_fields.items() if k in keys}"}, {"instruction": "def init():\n    \"\"\" Initialize the lib\n\n    Function-design use is to be able to test language settings changes\n    \"\"\"\n", "input": "", "output": "\n    if settings.USE_L10N:\n        locale = settings.LANGUAGE_CODE.replace('-', '_')\n        try:\n            humanize.i18n.activate(locale)\n        except FileNotFoundError:\n            pass  # Just let it to the default locale\n\n    HUMANIZE_FUNC_LIST = [\n        'naturalday',\n        'naturaltime',\n        'ordinal',\n        'intword',\n        'naturaldelta',\n        'intcomma',\n        'apnumber',\n        'fractional',\n        'naturalsize',\n        'naturaldate'\n    ]\n\n    # registers all humanize functions as template tags\n    for funcname in HUMANIZE_FUNC_LIST:\n        func = getattr(humanize, funcname)\n        register.filter(funcname, func, is_safe=True)"}, {"instruction": "def connectionMade(self):\n        \"\"\"Keep a reference to the protocol on the factory, and uses the\n        factory's store to find multiplexed connection factories.\n\n        Unfortunately, we can't add the protocol by TLS certificate\n        fingerprint, because the TLS handshake won't have completed\n        yet, so ``self.transport.getPeerCertificate()`` is still\n        ``None``.\n\n        \"\"\"\n", "input": "", "output": "        self.factory.protocols.add(self)\n        self._factories = multiplexing.FactoryDict(self.store)\n        super(AMP, self).connectionMade()"}, {"instruction": "def reftrack_version_data(rt, role):\n    \"\"\"Return the data for the version that is loaded by the reftrack\n\n    :param rt: the :class:`jukeboxcore.reftrack.Reftrack` holds the data\n    :type rt: :class:`jukeboxcore.reftrack.Reftrack`\n    :param role: item data role\n    :type role: QtCore.Qt.ItemDataRole\n    :returns: data for the version\n    :rtype: depending on role\n    :raises: None\n    \"\"\"\n", "input": "", "output": "    tfi = rt.get_taskfileinfo()\n    if not tfi:\n        return\n    return filesysitemdata.taskfileinfo_version_data(tfi, role)"}, {"instruction": "def _get_task_descriptor_info(self, courseid, taskid):\n        \"\"\"\n        :param courseid: the course id of the course\n        :param taskid: the task id of the task\n        :raise InvalidNameException, TaskNotFoundException\n        :return: a tuple, containing:\n            (descriptor filename,\n             task file manager for the descriptor)\n        \"\"\"\n", "input": "", "output": "        if not id_checker(courseid):\n            raise InvalidNameException(\"Course with invalid name: \" + courseid)\n        if not id_checker(taskid):\n            raise InvalidNameException(\"Task with invalid name: \" + taskid)\n\n        task_fs = self.get_task_fs(courseid, taskid)\n        for ext, task_file_manager in self._task_file_managers.items():\n            if task_fs.exists(\"task.\"+ext):\n                return \"task.\" + ext, task_file_manager\n\n        raise TaskNotFoundException()"}, {"instruction": "def ping(hostname: str, timeout_s: int = 5) -> bool:\n    \"\"\"\n    Pings a host, using OS tools.\n\n    Args:\n        hostname: host name or IP address\n        timeout_s: timeout in seconds\n\n    Returns:\n        was the ping successful?\n\n    \"\"\"\n", "input": "", "output": "    if sys.platform == \"win32\":\n        timeout_ms = timeout_s * 1000\n        args = [\n            \"ping\",\n            hostname,\n            \"-n\", \"1\",  # ping count\n            \"-w\", str(timeout_ms),  # timeout\n        ]\n    elif sys.platform.startswith('linux'):\n        args = [\n            \"ping\",\n            hostname,\n            \"-c\", \"1\",  # ping count\n            \"-w\", str(timeout_s),  # timeout\n        ]\n    else:\n        raise AssertionError(\"Don't know how to ping on this operating system\")\n    proc = subprocess.Popen(args,\n                            stdout=subprocess.PIPE, stderr=subprocess.PIPE)\n    proc.communicate()\n    retcode = proc.returncode\n    return retcode == 0"}, {"instruction": "def bbox_vert_aligned_right(box1, box2):\n    \"\"\"\n    Returns true if the right boundary of both boxes is within 2 pts\n    \"\"\"\n", "input": "", "output": "    if not (box1 and box2):\n        return False\n    return abs(box1.right - box2.right) <= 2"}, {"instruction": "def _parse_json_with_fieldnames(self):\n        \"\"\" Parse the raw JSON with all attributes/methods defined in the class, except for the\n            ones defined starting with '_' or flagged in cls._TO_EXCLUDE.\n\n            The final result is stored in self.json\n        \"\"\"\n", "input": "", "output": "        for key in dir(self):\n            if not key.startswith('_') and key not in self._TO_EXCLUDE:\n                self.fieldnames.append(key)\n                value = getattr(self, key)\n                if value:\n                    self.json[key] = value\n        # Add OK attribute even if value is \"False\"\n        self.json['ok'] = self.ok"}, {"instruction": "def make_oracle(q0, q1, secret_function):\n    \"\"\" Gates implementing the secret function f(x).\"\"\"\n", "input": "", "output": "\n    # coverage: ignore\n    if secret_function[0]:\n        yield [CNOT(q0, q1), X(q1)]\n\n    if secret_function[1]:\n        yield CNOT(q0, q1)"}, {"instruction": "def _FormatDataToken(self, token_data):\n    \"\"\"Formats a data token as a dictionary of values.\n\n    Args:\n      token_data (bsm_token_data_data): AUT_DATA token data.\n\n    Returns:\n      dict[str, str]: token values.\n    \"\"\"\n", "input": "", "output": "    format_string = bsmtoken.BSM_TOKEN_DATA_PRINT.get(\n        token_data.data_format, 'UNKNOWN')\n\n    if token_data.data_format == 4:\n      data = bytes(bytearray(token_data.data)).split(b'\\x00')[0]\n      data = data.decode('utf-8')\n    else:\n      data = ''.join(['{0:02x}'.format(byte) for byte in token_data.data])\n    return {\n        'format': format_string,\n        'data': data}"}, {"instruction": "def stat(filename):\n    \"\"\"Returns os.stat for a given file, adjusting the timestamps as appropriate.\"\"\"\n", "input": "", "output": "    import os\n    try:\n        # on the host, lstat won't try to follow symlinks\n        rstat = os.lstat(filename)\n    except:\n        rstat = os.stat(filename)\n    return rstat[:7] + tuple(tim + TIME_OFFSET for tim in rstat[7:])"}, {"instruction": "def re_parser(self, scode, *args):\n        \"\"\"\n        args: [arg1, arg2]\n\n        arg[0] = a valid regex pattern\n\n        arg[1] : if startswith('@') call sub; if startswith('$') call finditer,\n                 $0, $1 means group index.\n\n        return an ensure_list\n        \"\"\"\n", "input": "", "output": "\n        def gen_match(matches, num):\n            for match in matches:\n                yield match.group(num)\n\n        scode = self.ensure_str(scode)\n        assert self._re.match(\n            '^@|^\\$\\d+', args[1]), ValueError('args1 should match ^@|^\\$\\d+')\n        arg1, arg2 = args[1][0], args[1][1:]\n        com = self._re.compile(args[0])\n        if arg1 == '@':\n            result = com.sub(arg2, scode)\n            return self.ensure_list(result)\n        else:\n            result = com.finditer(scode)\n            return gen_match(result, int(arg2))"}, {"instruction": "def walk(self, visitor):\n        \"\"\"\n        Walk the branch and call the visitor function\n        on each node.\n        @param visitor: A function.\n        @return: self\n        @rtype: L{Element}\n        \"\"\"\n", "input": "", "output": "        visitor(self)\n        for c in self.children:\n            c.walk(visitor)\n        return self"}, {"instruction": "def get_host_address(host=None, default_address=DEFAULT_HOST_IP):\n    \"\"\"\n    Returns the given host address.\n\n    :param host: Host to retrieve the address.\n    :type host: unicode\n    :param default_address: Default address if the host is unreachable.\n    :type default_address: unicode\n    :return: Host address.\n    :rtype: unicode\n    \"\"\"\n", "input": "", "output": "\n    try:\n        return unicode(socket.gethostbyname(host or socket.gethostname()),\n                       Constants.default_codec,\n                       Constants.codec_error)\n    except Exception as error:\n        return default_address"}, {"instruction": "def _get_iterator(self):\n        \"\"\"The iterator passed in can take several forms: a class that can be\n        instantiated and then iterated over; a function that when called\n        returns an iterator; an actual iterator/generator or an iterable\n        collection.  This function sorts all that out and returns an iterator\n        that can be used\"\"\"\n", "input": "", "output": "        try:\n            return self.job_param_source_iter(self.config)\n        except TypeError:\n            try:\n                return self.job_param_source_iter()\n            except TypeError:\n                return self.job_param_source_iter"}, {"instruction": "def n_sections(neurites, neurite_type=NeuriteType.all, iterator_type=Tree.ipreorder):\n    '''Number of sections in a collection of neurites'''\n", "input": "", "output": "    return sum(1 for _ in iter_sections(neurites,\n                                        iterator_type=iterator_type,\n                                        neurite_filter=is_type(neurite_type)))"}, {"instruction": "def extract_mfd(dstore, what):\n    \"\"\"\n    Display num_ruptures by magnitude for event based calculations.\n    Example: http://127.0.0.1:8800/v1/calc/30/extract/event_based_mfd\n    \"\"\"\n", "input": "", "output": "    dd = collections.defaultdict(int)\n    for rup in dstore['ruptures'].value:\n        dd[rup['mag']] += 1\n    dt = numpy.dtype([('mag', float), ('freq', int)])\n    magfreq = numpy.array(sorted(dd.items(), key=operator.itemgetter(0)), dt)\n    return magfreq"}, {"instruction": "def _send(self, value, mode):\n        \"\"\"Send the specified value to the display with automatic 4bit / 8bit\n        selection. The rs_mode is either ``RS_DATA`` or ``RS_INSTRUCTION``.\"\"\"\n", "input": "", "output": "\n        # Assemble the parameters sent to the pigpio script\n        params = [mode]\n        params.extend([(value >> i) & 0x01 for i in range(8)])\n        # Switch off pigpio's exceptions, so that we get the return codes\n        pigpio.exceptions = False\n        while True:\n            ret = self.pi.run_script(self._writescript, params)\n            if ret >= 0:\n                break\n            elif ret != pigpio.PI_SCRIPT_NOT_READY:\n                raise pigpio.error(pigpio.error_text(ret))\n            # If pigpio script is not ready, sleep and try again\n            c.usleep(1)\n        # Switch on pigpio's exceptions\n        pigpio.exceptions = True"}, {"instruction": "def get_point_source_fluxes(self, id, energies, tag=None):\n        \"\"\"\n        Get the fluxes from the id-th point source\n\n        :param id: id of the source\n        :param energies: energies at which you need the flux\n        :param tag: a tuple (integration variable, a, b) specifying the integration to perform. If this\n        parameter is specified then the returned value will be the average flux for the source computed as the integral\n        between a and b over the integration variable divided by (b-a). The integration variable must be an independent\n        variable contained in the model. If b is None, then instead of integrating the integration variable will be\n        set to a and the model evaluated in a.\n        :return: fluxes\n        \"\"\"\n", "input": "", "output": "\n        return self._point_sources.values()[id](energies, tag=tag)"}, {"instruction": "def transform(self, vector):\n        \"\"\"\n        Computes the Hadamard product of the vector.\n        \"\"\"\n", "input": "", "output": "        if isinstance(vector, RDD):\n            vector = vector.map(_convert_to_vector)\n\n        else:\n            vector = _convert_to_vector(vector)\n        return callMLlibFunc(\"elementwiseProductVector\", self.scalingVector, vector)"}, {"instruction": "def parse_args():\n  \"\"\"Parses command line arguments.\"\"\"\n", "input": "", "output": "  parser = ArgumentParser(description=\"ModelBase builder\")\n  subparsers = parser.add_subparsers()\n\n  sql_parser = subparsers.add_parser(\n    \"get-query\",\n    description=\"Usage: e.g. psql -c \\\"copy ($(python3 lib/generate_models.py get-query)) to \" +\n                \"stdout with csv header\\\" DB_NAME postgres\")\n  sql_parser.set_defaults(func=print_sql_query)\n\n  gen_parser = subparsers.add_parser(\"generate\")\n  gen_parser.add_argument(\"filename\", nargs=\"?\", help=\"Read this file for input, or STDIN if not \" \\\n                                                      \"given\")\n  gen_parser.add_argument(\"-i\", \"--indent\", default=\"  \")\n  gen_parser.add_argument(\"-c\", \"--created-at-col-name\", default=\"created_at\")\n  gen_parser.add_argument(\"-u\", \"--updated-at-col-name\", default=\"updated_at\")\n  gen_parser.set_defaults(func=generate_models)\n\n  args = parser.parse_args()\n\n  if hasattr(args, \"func\"):\n    return args\n  else:\n    arg_parser.print_help()\n    sys.exit(1)"}, {"instruction": "def ComputeRoot(hashes):\n        \"\"\"\n        Compute the root hash.\n\n        Args:\n            hashes (list): the list of hashes to build the root from.\n\n        Returns:\n            bytes: the root hash.\n        \"\"\"\n", "input": "", "output": "        if not len(hashes):\n            raise Exception('Hashes must have length')\n        if len(hashes) == 1:\n            return hashes[0]\n\n        tree = MerkleTree(hashes)\n        return tree.Root.Hash"}, {"instruction": "def geometric_center(coords, periodic):\n    '''Geometric center taking into account periodic boundaries'''\n", "input": "", "output": "    max_vals = periodic\n    theta = 2 * np.pi * (coords / max_vals)\n    eps = np.cos(theta) * max_vals / (2 * np.pi)\n    zeta = np.sin(theta) * max_vals / (2 * np.pi)\n\n    eps_avg = eps.sum(axis=0)\n    zeta_avg = zeta.sum(axis=0)\n    theta_avg = np.arctan2(-zeta_avg, -eps_avg) + np.pi\n\n    return theta_avg * max_vals / (2 * np.pi)"}, {"instruction": "def get_debug_option(packagename):\n    \"\"\" Determines if the build is in debug mode.\n\n    Returns\n    -------\n    debug : bool\n        True if the current build was started with the debug option, False\n        otherwise.\n\n    \"\"\"\n", "input": "", "output": "\n    try:\n        current_debug = get_pkg_version_module(packagename,\n                                               fromlist=['debug'])[0]\n    except (ImportError, AttributeError):\n        current_debug = None\n\n    # Only modify the debug flag if one of the build commands was explicitly\n    # run (i.e. not as a sub-command of something else)\n    dist = get_dummy_distribution()\n    if any(cmd in dist.commands for cmd in ['build', 'build_ext']):\n        debug = bool(get_distutils_build_option('debug'))\n    else:\n        debug = bool(current_debug)\n\n    if current_debug is not None and current_debug != debug:\n        build_ext_cmd = dist.get_command_class('build_ext')\n        build_ext_cmd._force_rebuild = True\n\n    return debug"}, {"instruction": "def _docf(self, tag, val):\n        \"\"\"\n        Callback used as the handler argument to process_docs(). This converts\n        Stone doc references to Sphinx-friendly annotations.\n        \"\"\"\n", "input": "", "output": "        if tag == 'type':\n            return ':class:`{}`'.format(val)\n        elif tag == 'route':\n            if self.args.route_method:\n                return ':meth:`%s`' % self.args.route_method.format(\n                    ns=self.cur_namespace.name, route=fmt_func(val))\n            else:\n                return val\n        elif tag == 'link':\n            anchor, link = val.rsplit(' ', 1)\n            return '`{} <{}>`_'.format(anchor, link)\n        elif tag == 'val':\n            if val == 'null':\n                return 'None'\n            elif val == 'true' or val == 'false':\n                return '``{}``'.format(val.capitalize())\n            else:\n                return val\n        elif tag == 'field':\n            return '``{}``'.format(val)\n        else:\n            raise RuntimeError('Unknown doc ref tag %r' % tag)"}, {"instruction": "def followers(self):\n        \"\"\" :class:`Feed <pypump.models.feed.Feed>` with all\n        :class:`Person <pypump.models.person.Person>` objects following the person.\n\n        Example:\n            >>> alice = pump.Person('alice@example.org')\n            >>> for follower in alice.followers[:2]:\n            ...     print(follower.id)\n            ...\n            acct:bob@example.org\n            acct:carol@example.org\n        \"\"\"\n", "input": "", "output": "        if self._followers is None:\n            self._followers = Followers(self.links['followers'], pypump=self._pump)\n        return self._followers"}, {"instruction": "def build(self):\n        \"\"\"\n        Builds the query string, which can be used for a search query\n\n        :return: the query string\n        \"\"\"\n", "input": "", "output": "        if self.es_version == '1':\n            if len(self.filters) > 0:\n                return {\n                    'filtered': {\n                        'query': self.query,\n                        'filter': {\n                            'and': self.filters\n                        }\n                    }\n                }\n            else:\n                return self.query\n        else:\n            query = {\n                'bool': {\n                    'must': self.query\n                }\n            }\n            if len(self.filters) > 0:\n                query[\"bool\"][\"filter\"] = self.filters\n            return query"}, {"instruction": "def reorient(self, up, look):\n        '''\n        Reorient the mesh by specifying two vectors.\n\n        up: The foot-to-head direction.\n        look: The direction the body is facing.\n\n        In the result, the up will end up along +y, and look along +z\n        (i.e. facing towards a default OpenGL camera).\n\n        '''\n", "input": "", "output": "        from blmath.geometry.transform import rotation_from_up_and_look\n        from blmath.numerics import as_numeric_array\n\n        up = as_numeric_array(up, (3,))\n        look = as_numeric_array(look, (3,))\n\n        if self.v is not None:\n            self.v = np.dot(rotation_from_up_and_look(up, look), self.v.T).T"}, {"instruction": "def check_images(data):\n    \"\"\"\n    Check and reformat input images if needed\n    \"\"\"\n", "input": "", "output": "    if isinstance(data, ndarray):\n        data = fromarray(data)\n    \n    if not isinstance(data, Images):\n        data = fromarray(asarray(data))\n\n    if len(data.shape) not in set([3, 4]):\n        raise Exception('Number of image dimensions %s must be 2 or 3' % (len(data.shape)))\n\n    return data"}, {"instruction": "def load(self):\r\n        \"\"\"\r\n        Loads the children for this record item.\r\n        \r\n        :return     <bool> | changed\r\n        \"\"\"\n", "input": "", "output": "        if self.__loaded:\r\n            return False\r\n        \r\n        self.__loaded = True\r\n        self.setChildIndicatorPolicy(self.DontShowIndicatorWhenChildless)\r\n        \r\n        # loads the children for this widget\r\n        tree = self.treeWidget()\r\n        if tree.groupBy():\r\n            grps = self.childRecords().grouped(tree.groupBy())\r\n            for grp, records in grps.items():\r\n                tree.createGroupItem(grp, records, self)\r\n        else:\r\n            for record in self.childRecords():\r\n                tree.createRecordItem(record, self)\r\n        \r\n        return True"}, {"instruction": "def is_twss(self, phrase):\n        \"\"\"\n        The magic function- this accepts a phrase and tells you if it\n        classifies as an entendre\n        \"\"\"\n", "input": "", "output": "        featureset = self.extract_features(phrase)\n        return self.classifier.classify(featureset)"}, {"instruction": "def get_all_licenses(self):\n        \"\"\"Retrieve license type, key, installation date, etc.\"\"\"\n", "input": "", "output": "        data = self._execute_command('GET_ALL_LICENSES', 'RIB_INFO', 'read')\n        d = {}\n        for key, val in data['GET_ALL_LICENSES']['LICENSE'].items():\n            if isinstance(val, dict):\n                d[key] = data['GET_ALL_LICENSES']['LICENSE'][key]['VALUE']\n        return d"}, {"instruction": "def PROFILE_RAUTIAN(sg0,GamD,Gam0,Shift0,anuVC,eta,sg):\n    \"\"\"\n    # Rautian profile based on HTP.\n    # Input parameters:\n    #      sg0     : Unperturbed line position in cm-1 (Input).\n    #      GamD    : Doppler HWHM in cm-1 (Input)\n    #      Gam0    : Speed-averaged line-width in cm-1 (Input).       \n    #      anuVC   : Velocity-changing frequency in cm-1 (Input).\n    #      Shift0  : Speed-averaged line-shift in cm-1 (Input).\n    #      sg      : Current WaveNumber of the Computation in cm-1 (Input).\n    \"\"\"\n", "input": "", "output": "    return pcqsdhc(sg0,GamD,Gam0,cZero,Shift0,cZero,anuVC,cZero,sg)"}, {"instruction": "def get_group(self):\n        \"\"\"Get the group of the Dataset.\n\n        Returns\n        -------\n        group : numpy array or None\n            Group size of each group.\n        \"\"\"\n", "input": "", "output": "        if self.group is None:\n            self.group = self.get_field('group')\n            if self.group is not None:\n                # group data from LightGBM is boundaries data, need to convert to group size\n                self.group = np.diff(self.group)\n        return self.group"}, {"instruction": "def html_page_context(app, pagename, templatename, context, doctree):\n    ''' Collect page names for the sitemap as HTML pages are built.\n\n    '''\n", "input": "", "output": "    site = context['SITEMAP_BASE_URL']\n    version = context['version']\n    app.sitemap_links.add(site + version + '/' + pagename + \".html\")"}, {"instruction": "def logger(self):\n        ''' Lazy logger '''\n", "input": "", "output": "        if self.__logger is None:\n            self.__logger = logging.getLogger(self.__name)\n        return self.__logger"}, {"instruction": "def _connect_control_flow_node(control_flow_node, next_node):\n    \"\"\"Connect a ControlFlowNode properly to the next_node.\"\"\"\n", "input": "", "output": "    for last in control_flow_node.last_nodes:\n        if isinstance(next_node, ControlFlowNode):\n            last.connect(next_node.test)  # connect to next if test case\n        elif isinstance(next_node, AssignmentCallNode):\n            call_node = next_node.call_node\n            inner_most_call_node = _get_inner_most_function_call(call_node)\n            last.connect(inner_most_call_node)\n        else:\n            last.connect(next_node)"}, {"instruction": "def await_metadata_by_name(self, name, metadata_key, timeout, caster=None):\n    \"\"\"Block up to a timeout for process metadata to arrive on disk.\n\n    :param string name: The ProcessMetadataManager identity/name (e.g. 'pantsd').\n    :param string metadata_key: The metadata key (e.g. 'pid').\n    :param int timeout: The deadline to write metadata.\n    :param type caster: A type-casting callable to apply to the read value (e.g. int, str).\n    :returns: The value of the metadata key (read from disk post-write).\n    :raises: :class:`ProcessMetadataManager.Timeout` on timeout.\n    \"\"\"\n", "input": "", "output": "    file_path = self._metadata_file_path(name, metadata_key)\n    self._wait_for_file(file_path, timeout=timeout)\n    return self.read_metadata_by_name(name, metadata_key, caster)"}, {"instruction": "def max_len(iterable, minimum=0):\n    \"\"\"Return the len() of the longest item in ``iterable`` or ``minimum``.\n\n    >>> max_len(['spam', 'ham'])\n    4\n\n    >>> max_len([])\n    0\n\n    >>> max_len(['ham'], 4)\n    4\n    \"\"\"\n", "input": "", "output": "    try:\n        result = max(map(len, iterable))\n    except ValueError:\n        result = minimum\n    return minimum if result < minimum else result"}, {"instruction": "def is_not_null_predicate(\n    raw_crash, dumps, processed_crash, processor, key=''\n):\n    \"\"\"a predicate that converts the key'd source to boolean.\n\n    parameters:\n        raw_crash - dict\n        dumps - placeholder in a fat interface - unused\n        processed_crash - placeholder in a fat interface - unused\n        processor - placeholder in a fat interface - unused\n    \"\"\"\n", "input": "", "output": "    try:\n        return bool(raw_crash[key])\n    except KeyError:\n        return False"}, {"instruction": "def start_worker(self):\n        \"\"\"Trigger new process as a RQ worker.\"\"\"\n", "input": "", "output": "        if not self.include_rq:\n            return None\n\n        worker = Worker(queues=self.queues,\n                        connection=self.connection)\n        worker_pid_path = current_app.config.get(\n            \"{}_WORKER_PID\".format(self.config_prefix), 'rl_worker.pid'\n        )\n\n        try:\n            worker_pid_file = open(worker_pid_path, 'r')\n            worker_pid = int(worker_pid_file.read())\n            print(\"Worker already started with PID=%d\" % worker_pid)\n            worker_pid_file.close()\n\n            return worker_pid\n\n        except (IOError, TypeError):\n            self.worker_process = Process(target=worker_wrapper, kwargs={\n                'worker_instance': worker,\n                'pid_path': worker_pid_path\n            })\n            self.worker_process.start()\n            worker_pid_file = open(worker_pid_path, 'w')\n            worker_pid_file.write(\"%d\" % self.worker_process.pid)\n            worker_pid_file.close()\n\n            print(\"Start a worker process with PID=%d\" %\n                  self.worker_process.pid)\n\n            return self.worker_process.pid"}, {"instruction": "def add_lock(packages, root=None, **kwargs):  # pylint: disable=unused-argument\n    '''\n    Add a package lock. Specify packages to lock by exact name.\n\n    root\n        operate on a different root directory.\n\n    CLI Example:\n\n    .. code-block:: bash\n\n        salt '*' pkg.add_lock <package name>\n        salt '*' pkg.add_lock <package1>,<package2>,<package3>\n        salt '*' pkg.add_lock pkgs='[\"foo\", \"bar\"]'\n    '''\n", "input": "", "output": "    salt.utils.versions.warn_until('Sodium', 'This function is deprecated. Please use hold() instead.')\n    locks = list_locks(root)\n    added = []\n    try:\n        packages = list(__salt__['pkg_resource.parse_targets'](packages)[0].keys())\n    except MinionError as exc:\n        raise CommandExecutionError(exc)\n\n    for pkg in packages:\n        if not locks.get(pkg):\n            added.append(pkg)\n\n    if added:\n        __zypper__(root=root).call('al', *added)\n\n    return {'added': len(added), 'packages': added}"}, {"instruction": "def _join_host_port(host, port):\n    \"\"\"Adapted golang's net.JoinHostPort\"\"\"\n", "input": "", "output": "    template = \"%s:%s\"\n    host_requires_bracketing = ':' in host or '%' in host\n    if host_requires_bracketing:\n        template = \"[%s]:%s\"\n    return template % (host, port)"}, {"instruction": "def _error_on_missing_application(self, params):\n        \"\"\"Raise an ApplicationNotFoundError if the app is not accessible\n\n        In this case, checks for the java runtime and the RDP jar file.\n        \"\"\"\n", "input": "", "output": "        if not (os.path.exists('java') or which('java')):\n            raise ApplicationNotFoundError(\n                \"Cannot find java runtime. Is it installed? Is it in your \"\n                \"path?\")\n        jar_fp = self._get_jar_fp()\n        if jar_fp is None:\n            raise ApplicationNotFoundError(\n                \"JAR file not found in current directory and the RDP_JAR_PATH \"\n                \"environment variable is not set.  Please set RDP_JAR_PATH to \"\n                \"the full pathname of the JAR file.\")\n        if not os.path.exists(jar_fp):\n            raise ApplicationNotFoundError(\n                \"JAR file %s does not exist.\" % jar_fp)"}, {"instruction": "def _check_bounds(self, v):\n        \"\"\"Check which values are out of bounds.\n\n        Raises\n        ------\n        ValueError:\n\n        \"\"\"\n", "input": "", "output": "        below_bounds = v < self._x[0]\n        above_bounds = v > self._x[-1]\n\n        if self.bounds_error and below_bounds.any():\n            raise ValueError(\"A value in x_new is below the interpolation \"\n                \"range.\")\n        if self.bounds_error and above_bounds.any():\n            raise ValueError(\"A value in x_new is above the interpolation \"\n                \"range.\")\n\n        return below_bounds, above_bounds"}, {"instruction": "def _init_metadata(self):\n        \"\"\"stub\"\"\"\n", "input": "", "output": "        super(edXNumericResponseQuestionFormRecord, self)._init_metadata()\n        QuestionTextFormRecord._init_metadata(self)\n        QuestionFilesFormRecord._init_metadata(self)"}, {"instruction": "def to_json(self, value):\n        \"\"\"\n        Serialize the data, ensuring that it is valid XML (or None).\n\n        Raises an lxml.etree.XMLSyntaxError if it is a basestring but not valid\n        XML.\n        \"\"\"\n", "input": "", "output": "        if self._enable_enforce_type:\n            value = self.enforce_type(value)\n        return super(XMLString, self).to_json(value)"}, {"instruction": "def update_bounds_boxes(self):\n        \"\"\"\n        updates bounds boxes with bounds of current specimen and fit\n        \"\"\"\n", "input": "", "output": "        if self.s not in list(self.Data.keys()):\n            self.select_specimen(list(self.Data.keys())[0])\n        self.T_list = self.Data[self.s]['zijdblock_steps']\n        if self.current_fit:\n            self.tmin_box.SetItems(self.T_list)\n            self.tmax_box.SetItems(self.T_list)\n            if type(self.current_fit.tmin) == str and type(self.current_fit.tmax) == str:\n                self.tmin_box.SetStringSelection(self.current_fit.tmin)\n                self.tmax_box.SetStringSelection(self.current_fit.tmax)\n        if self.ie_open:\n            self.ie.update_bounds_boxes(self.T_list)"}, {"instruction": "def get_cloud_masks(self, threshold=None, non_valid_value=False):\n        \"\"\" The binary cloud mask is computed on the fly. Be cautious. The pixels without valid data are assigned\n        non_valid_value.\n\n        :param threshold: A float from [0,1] specifying threshold\n        :type threshold: float\n        :param non_valid_value: Value which will be assigned to pixels without valid data\n        :type non_valid_value: int in range `[-254, 255]`\n        :return: Binary cloud masks of shape `(times, height, width)` and `dtype=numpy.int8`\n        :rtype: numpy.ndarray\n        \"\"\"\n", "input": "", "output": "        self.get_probability_masks()\n\n        cloud_masks = self.cloud_detector.get_mask_from_prob(self.probability_masks, threshold)\n        cloud_masks[~self.valid_data] = non_valid_value\n\n        return cloud_masks"}, {"instruction": "def update_firmware(self,\n                        hardware_id,\n                        ipmi=True,\n                        raid_controller=True,\n                        bios=True,\n                        hard_drive=True):\n        \"\"\"Update hardware firmware.\n\n        This will cause the server to be unavailable for ~20 minutes.\n\n        :param int hardware_id: The ID of the hardware to have its firmware\n                                updated.\n        :param bool ipmi: Update the ipmi firmware.\n        :param bool raid_controller: Update the raid controller firmware.\n        :param bool bios: Update the bios firmware.\n        :param bool hard_drive: Update the hard drive firmware.\n\n        Example::\n\n            # Check the servers active transactions to see progress\n            result = mgr.update_firmware(hardware_id=1234)\n        \"\"\"\n", "input": "", "output": "\n        return self.hardware.createFirmwareUpdateTransaction(\n            bool(ipmi), bool(raid_controller), bool(bios), bool(hard_drive), id=hardware_id)"}, {"instruction": "def get_inters_direct(r, L, R_cut):\n    '''\n    Return points within a given cut-off of each other,\n    in a periodic system.\n\n    Uses a direct algorithm, which may be very slow for large numbers of\n    points.\n\n    Parameters\n    ----------\n    r: array, shape (n, d) where d is one of (2, 3).\n        A set of n point coordinates.\n        Coordinates are assumed to lie in [-L / 2, L / 2].\n    L: float.\n        Bounds of the system.\n    R_cut: float.\n        The maximum distance within which to consider points to lie\n        near each other.\n\n    Returns\n    -------\n    inters: array, shape (n, n)\n        Indices of the nearby points.\n        For each particle indexed by the first axis,\n        the second axis lists each index of a nearby point.\n    intersi: array, shape (n,)\n        Total number of nearby points.\n        This array should be used to index `inters`, as for point `i`,\n        elements in `inters[i]` beyond `intersi[i]` have no well-defined value.\n    '''\n", "input": "", "output": "    _cell_list.cell_list_direct.make_inters(r.T, L, R_cut)\n    return _parse_inters()"}, {"instruction": "def generatePlugins(widgetPath = None, buildPath = None):\r\n    \"\"\"\r\n    Generates all the plugin files for the system and imports them.\r\n    \r\n    :param      widgetPath | <str> || None\r\n                buildPath  | <str> || None\r\n    \"\"\"\n", "input": "", "output": "    if widgetPath is None:\r\n        widgetPath = WIDGET_PATH\r\n        \r\n    if buildPath is None:\r\n        buildPath = BUILD_PATH\r\n    \r\n    for basepath in widgetPath.split(os.path.pathsep):\r\n        if not basepath:\r\n            continue\r\n            \r\n        # load packaged widgets\r\n        for filepath in glob.glob(os.path.join(basepath, '*/__init__.py')):\r\n            generatePlugins(os.path.dirname(filepath), buildPath)\r\n        \r\n        # load module widgets\r\n        for filepath in glob.glob(os.path.join(basepath, '*.py')):\r\n            generatePlugin(filepath, buildPath)"}, {"instruction": "def has_next_assessment_section(self, assessment_section_id):\n        \"\"\"Tests if there is a next assessment section in the assessment following the given assessment section ``Id``.\n\n        arg:    assessment_section_id (osid.id.Id): ``Id`` of the\n                ``AssessmentSection``\n        return: (boolean) - ``true`` if there is a next section,\n                ``false`` otherwise\n        raise:  IllegalState - ``has_assessment_begun()`` is ``false``\n        raise:  NotFound - ``assessment_taken_id`` is not found\n        raise:  NullArgument - ``assessment_taken_id`` is ``null``\n        raise:  OperationFailed - unable to complete request\n        raise:  PermissionDenied - authorization failure occurred\n        *compliance: mandatory -- This method must be implemented.*\n\n        \"\"\"\n", "input": "", "output": "        try:\n            self.get_next_assessment_section(assessment_section_id)\n        except errors.IllegalState:\n            return False\n        else:\n            return True"}, {"instruction": "def _cryptography_cipher(key, iv):\n    \"\"\"Build a cryptography TripleDES Cipher object.\n\n    :param bytes key: Encryption key\n    :param bytesiv iv: Initialization vector\n    :returns: TripleDES Cipher instance\n    :rtype: cryptography.hazmat.primitives.ciphers.Cipher\n    \"\"\"\n", "input": "", "output": "    return Cipher(\n        algorithm=algorithms.TripleDES(key),\n        mode=modes.CBC(iv),\n        backend=default_backend()\n    )"}, {"instruction": "def _check_scalar_vertical_extents(self, ds, z_variable):\n        '''\n        Check the scalar value of Z compared to the vertical extents which\n        should also be equivalent\n\n        :param netCDF4.Dataset ds: An open netCDF dataset\n        :param str z_variable: Name of the variable representing the Z-Axis\n        '''\n", "input": "", "output": "        vert_min = ds.geospatial_vertical_min\n        vert_max = ds.geospatial_vertical_max\n        msgs = []\n        total = 2\n\n        zvalue = ds.variables[z_variable][:].item()\n        if not np.isclose(vert_min, vert_max):\n            msgs.append(\"geospatial_vertical_min != geospatial_vertical_max for scalar depth values, %s != %s\" % (\n                vert_min,\n                vert_max\n            ))\n\n        if not np.isclose(vert_max, zvalue):\n            msgs.append(\"geospatial_vertical_max != %s values, %s != %s\" % (\n                z_variable,\n                vert_max,\n                zvalue\n            ))\n\n        return Result(BaseCheck.MEDIUM,\n                      (total - len(msgs), total),\n                      'geospatial_vertical_extents_match',\n                      msgs)"}, {"instruction": "def get_users_in_organization(self, organization_id, start=0, limit=50):\n        \"\"\"\n        Get all the users of a specified organization\n\n        :param organization_id: str\n        :param start: OPTIONAL: int\n        :param limit: OPTIONAL: int\n        :return: Users list in organization\n        \"\"\"\n", "input": "", "output": "        url = 'rest/servicedeskapi/organization/{}/user'.format(organization_id)\n        params = {}\n        if start is not None:\n            params['start'] = int(start)\n        if limit is not None:\n            params['limit'] = int(limit)\n\n        return self.get(url, headers=self.experimental_headers, params=params)"}, {"instruction": "def hasChannelType(self, chan):\n        \"\"\"Returns True if chan is among the supported channel types.\n        \n        @param app: Module name.\n        @return:    Boolean \n        \n        \"\"\"\n", "input": "", "output": "        if self._chantypes is None:\n            self._initChannelTypesList()\n        return chan in self._chantypes"}, {"instruction": "def connection_required(func):\n        \"\"\"Decorator to specify that a target connection is required in order\n        for the given method to be used.\n\n        Args:\n          func (function): function being decorated\n\n        Returns:\n          The wrapper function.\n        \"\"\"\n", "input": "", "output": "        @functools.wraps(func)\n        def wrapper(self, *args, **kwargs):\n            "}, {"instruction": "def to_xdr_object(self):\n        \"\"\"Creates an XDR Memo object for a transaction with MEMO_RETURN.\"\"\"\n", "input": "", "output": "        return Xdr.types.Memo(\n            type=Xdr.const.MEMO_RETURN, retHash=self.memo_return)"}, {"instruction": "def create_url(urlbase, urlargd, escape_urlargd=True, urlhash=None):\n    \"\"\"Creates a W3C compliant URL. Output will look like this:\n    'urlbase?param1=value1&amp;param2=value2'\n    @param urlbase: base url (e.g. config.CFG_SITE_URL/search)\n    @param urlargd: dictionary of parameters. (e.g. p={'recid':3, 'of'='hb'}\n    @param escape_urlargd: boolean indicating if the function should escape\n                           arguments (e.g. < becomes &lt; or \" becomes &quot;)\n    @param urlhash: hash string to add at the end of the link\n    \"\"\"\n", "input": "", "output": "    separator = '&amp;'\n    output = urlbase\n    if urlargd:\n        output += '?'\n        if escape_urlargd:\n            arguments = [escape(quote(str(key)), quote=True) + '=' +\n                         escape(quote(str(urlargd[key])), quote=True)\n                         for key in urlargd.keys()]\n        else:\n            arguments = [str(key) + '=' + str(urlargd[key])\n                         for key in urlargd.keys()]\n        output += separator.join(arguments)\n    if urlhash:\n        output += \"#\" + escape(quote(str(urlhash)))\n    return output"}, {"instruction": "def load(self):\n        \"\"\" Return the model from the store \"\"\"\n", "input": "", "output": "\n        filters = [Filter(self.field, 'eq', self.rid)]\n        store = goldman.sess.store\n\n        self._is_loaded = True\n        self.models = store.search(self.rtype, filters=filters)\n\n        return self.models"}, {"instruction": "def _read_imu(self):\n        \"\"\"\n        Internal. Tries to read the IMU sensor three times before giving up\n        \"\"\"\n", "input": "", "output": "\n        self._init_imu()  # Ensure imu is initialised\n\n        attempts = 0\n        success = False\n\n        while not success and attempts < 3:\n            success = self._imu.IMURead()\n            attempts += 1\n            time.sleep(self._imu_poll_interval)\n\n        return success"}, {"instruction": "def _compute_distance_scaling(self, rup, dists, C):\n        \"\"\"\n        Compute distance-scaling term, equations (3) and (4), pag 107.\n        \"\"\"\n", "input": "", "output": "        Mref = 4.5\n        Rref = 1.0\n        R = np.sqrt(dists.rjb ** 2 + C['h'] ** 2)\n        return (C['c1'] + C['c2'] * (rup.mag - Mref)) * np.log(R / Rref) + \\\n            C['c3'] * (R - Rref)"}, {"instruction": "def get(self, *args, **kwargs):\n        '''\n        /label/s/view\n        '''\n", "input": "", "output": "        url_arr = self.parse_url(args[0])\n\n        if len(url_arr) == 2:\n            if url_arr[0] == 'remove':\n                self.remove_redis_keyword(url_arr[1])\n            else:\n                self.list(url_arr[0], url_arr[1])\n        elif len(url_arr) == 3:\n            self.list(url_arr[0], url_arr[1], url_arr[2])\n        else:\n            return False"}, {"instruction": "def match_repository_configuration(url, page_size=10, page_index=0, sort=\"\"):\n    \"\"\"\n    Search for Repository Configurations based on internal or external url with exact match\n    \"\"\"\n", "input": "", "output": "    content = match_repository_configuration_raw(url, page_size, page_index, sort)\n    if content:\n        return utils.format_json_list(content)"}, {"instruction": "def get_pr_review_status(pr: PullRequestDetails) -> Any:\n    \"\"\"\n    References:\n        https://developer.github.com/v3/pulls/reviews/#list-reviews-on-a-pull-request\n    \"\"\"\n", "input": "", "output": "    url = (\"https://api.github.com/repos/{}/{}/pulls/{}/reviews\"\n           \"?access_token={}\".format(pr.repo.organization,\n                                     pr.repo.name,\n                                     pr.pull_id,\n                                     pr.repo.access_token))\n    response = requests.get(url)\n\n    if response.status_code != 200:\n        raise RuntimeError(\n            'Get review failed. Code: {}. Content: {}.'.format(\n                response.status_code, response.content))\n\n    return json.JSONDecoder().decode(response.content.decode())"}, {"instruction": "def _handle_tag_text(self, text):\n        \"\"\"Handle regular *text* inside of an HTML open tag.\"\"\"\n", "input": "", "output": "        next = self._read(1)\n        if not self._can_recurse() or text not in self.MARKERS:\n            self._emit_text(text)\n        elif text == next == \"{\":\n            self._parse_template_or_argument()\n        elif text == next == \"[\":\n            self._parse_wikilink()\n        elif text == \"<\":\n            self._parse_tag()\n        else:\n            self._emit_text(text)"}, {"instruction": "def cacheable(self):\n        \"\"\"Return the cacheable attribute of the BFD file being processed.\"\"\"\n", "input": "", "output": "        if not self._ptr:\n            raise BfdException(\"BFD not initialized\")\n\n        return _bfd.get_bfd_attribute(self._ptr, BfdAttributes.CACHEABLE)"}, {"instruction": "def highpass(cutoff):\n  \"\"\"\n  This strategy uses an exponential approximation for cut-off frequency\n  calculation, found by matching the single pole and single zero Laplace\n  highpass filter.\n  \"\"\"\n", "input": "", "output": "  R = thub(exp(-cutoff), 2)\n  G = (R + 1) / 2\n  return G * (1 - z ** -1) / (1 - R * z ** -1)"}, {"instruction": "def urlEncodeAndJoin(self, seq, sepr=','):\n        '''sepr.join(urlencode(seq))\n        Args:\n            seq: string list to be urlencoded\n            sepr: join seq with sepr\n        Returns:\n            str\n        '''\n", "input": "", "output": "        try:\n            from urllib.parse import quote_plus as encode\n            return sepr.join([encode(x, encoding=CHARSET_UTF8) for x in seq])\n        except ImportError:\n            from urllib import quote as encode\n            return sepr.join([i for i in map(lambda x: encode(x), seq)])"}, {"instruction": "def search(self, keyword, p_index=''):\n        '''\n        perform searching.\n        '''\n", "input": "", "output": "        if p_index == '' or p_index == '-1':\n            current_page_number = 1\n        else:\n            current_page_number = int(p_index)\n        res_all = self.ysearch.get_all_num(keyword)\n        results = self.ysearch.search_pager(\n            keyword,\n            page_index=current_page_number,\n            doc_per_page=CMS_CFG['list_num']\n        )\n        page_num = int(res_all / CMS_CFG['list_num'])\n        kwd = {'title': '\u67e5\u627e\u7ed3\u679c',\n               'pager': '',\n               'count': res_all,\n               'keyword': keyword,\n               'catid': '',\n               'current_page': current_page_number}\n        self.render('misc/search/search_list.html',\n                    kwd=kwd,\n                    srecs=results,\n                    pager=gen_pager_bootstrap_url(\n                        '/search/{0}'.format(keyword),\n                        page_num,\n                        current_page_number\n                    ),\n                    userinfo=self.userinfo,\n                    cfg=CMS_CFG)"}, {"instruction": "def to_np(*args):\n    \"\"\" convert GPU arras to numpy and return them\"\"\"\n", "input": "", "output": "    if len(args) > 1:\n        return (cp.asnumpy(x) for x in args)\n    else:\n        return cp.asnumpy(args[0])"}, {"instruction": "def _api_group_for_type(cls):\n    \"\"\"\n    Determine which Kubernetes API group a particular PClass is likely to\n    belong with.\n\n    This is basically nonsense.  The question being asked is wrong.  An\n    abstraction has failed somewhere.  Fixing that will get rid of the need\n    for this.\n    \"\"\"\n", "input": "", "output": "    _groups = {\n        (u\"v1beta1\", u\"Deployment\"): u\"extensions\",\n        (u\"v1beta1\", u\"DeploymentList\"): u\"extensions\",\n        (u\"v1beta1\", u\"ReplicaSet\"): u\"extensions\",\n        (u\"v1beta1\", u\"ReplicaSetList\"): u\"extensions\",\n    }\n    key = (\n        cls.apiVersion,\n        cls.__name__.rsplit(u\".\")[-1],\n    )\n    group = _groups.get(key, None)\n    return group"}, {"instruction": "def to_bytes(value):\n    \"\"\" str to bytes (py3k) \"\"\"\n", "input": "", "output": "    vtype = type(value)\n\n    if vtype == bytes or vtype == type(None):\n        return value\n\n    try:\n        return vtype.encode(value)\n    except UnicodeEncodeError:\n        pass\n    return value"}, {"instruction": "def set_custom_image(user_context, app_id, image_path):\n  \"\"\"Sets the custom image for `app_id` to be the image located at\n  `image_path`. If there already exists a custom image for `app_id` it will\n  be deleted. Returns True is setting the image was successful.\"\"\"\n", "input": "", "output": "  if image_path is None:\n    return False\n\n  if not os.path.exists(image_path):\n    return False\n\n  (root, ext) = os.path.splitext(image_path)\n  if not is_valid_extension(ext):\n    # TODO: Maybe log that this happened?\n    return False\n  # If we don't remove the old image then theres no guarantee that Steam will\n  # show our new image when it launches.\n  if has_custom_image(user_context, app_id):\n    img = get_custom_image(user_context, app_id)\n    assert(img is not None)\n    os.remove(img)\n  \n  # Set the new image\n  parent_dir = paths.custom_images_directory(user_context)\n  new_path = os.path.join(parent_dir, app_id + ext)\n  shutil.copyfile(image_path, new_path)\n  return True"}, {"instruction": "async def unformat(self):\n        \"\"\"Unformat this block device.\"\"\"\n", "input": "", "output": "        self._data = await self._handler.unformat(\n            system_id=self.node.system_id, id=self.id)"}, {"instruction": "def _int(ctx, number):\n    \"\"\"\n    Rounds a number down to the nearest integer\n    \"\"\"\n", "input": "", "output": "    return conversions.to_integer(conversions.to_decimal(number, ctx).to_integral_value(ROUND_FLOOR), ctx)"}, {"instruction": "def update_handler(feeds):\n\t\t'''Update all cross-referencing filters results for feeds and others, related to them.\n\t\t\tIntended to be called from non-Feed update hooks (like new Post saving).'''\n", "input": "", "output": "\t\t# Check if this call is a result of actions initiated from\n\t\t#  one of the hooks in a higher frame (resulting in recursion).\n\t\tif Feed._filters_update_handler_lock: return\n\t\treturn Feed._filters_update_handler(Feed, feeds, force=True)"}, {"instruction": "def make_private(self, client=None):\n        \"\"\"Update blob's ACL, revoking read access for anonymous users.\n\n        :type client: :class:`~google.cloud.storage.client.Client` or\n                      ``NoneType``\n        :param client: Optional. The client to use.  If not passed, falls back\n                       to the ``client`` stored on the blob's bucket.\n        \"\"\"\n", "input": "", "output": "        self.acl.all().revoke_read()\n        self.acl.save(client=client)"}, {"instruction": "def Escape(self, string=\"\", **_):\n    \"\"\"Support standard string escaping.\"\"\"\n", "input": "", "output": "    # Translate special escapes:\n    self.stack[-1] += self.STRING_ESCAPES.get(string, string)"}, {"instruction": "def visit_Import(self, node):\n        \"\"\" Register imported modules and usage symbols.  \"\"\"\n", "input": "", "output": "        for alias in node.names:\n            alias_name = tuple(alias.name.split('.'))\n            self.imports.add(alias_name[0])\n            if alias.asname:\n                self.symbols[alias.asname] = alias_name\n            else:\n                self.symbols[alias_name[0]] = alias_name[:1]\n            self.update = True\n        return None"}, {"instruction": "def import_sanitizer(sanitizer):\n    \"\"\"\n    Imports the sanitizer python module.\n\n    :param sanitizer: Sanitizer python module file.\n    :type sanitizer: unicode\n    :return: Module.\n    :rtype: object\n    \"\"\"\n", "input": "", "output": "\n    directory = os.path.dirname(sanitizer)\n    not directory in sys.path and sys.path.append(directory)\n\n    namespace = __import__(foundations.strings.get_splitext_basename(sanitizer))\n    if hasattr(namespace, \"bleach\"):\n        return namespace\n    else:\n        raise foundations.exceptions.ProgrammingError(\n            \"{0} | '{1}' is not a valid sanitizer module file!\".format(sanitizer))"}, {"instruction": "def timestamp_file():\n    \"\"\"Opens a file for tracking the time of the last version check\"\"\"\n", "input": "", "output": "    config_dir = os.path.join(\n        os.path.expanduser(\"~\"), BaseGlobalConfig.config_local_dir\n    )\n\n    if not os.path.exists(config_dir):\n        os.mkdir(config_dir)\n\n    timestamp_file = os.path.join(config_dir, \"cumulus_timestamp\")\n\n    try:\n        with open(timestamp_file, \"r+\") as f:\n            yield f\n    except IOError:  # file does not exist\n        with open(timestamp_file, \"w+\") as f:\n            yield f"}, {"instruction": "def transcript_sequence(self, transcript_id):\n        \"\"\"Return cDNA nucleotide sequence of transcript, or None if\n        transcript doesn't have cDNA sequence.\n        \"\"\"\n", "input": "", "output": "        if self.transcript_sequences is None:\n            raise ValueError(\n                \"No transcript FASTA supplied to this Genome: %s\" % self)\n        return self.transcript_sequences.get(transcript_id)"}, {"instruction": "def _generate_overview_note(pass_count, only_warning_count, error_count, total_count):\n    \"\"\" Generates and returns the HTML note that provides a summary of validation status. \"\"\"\n", "input": "", "output": "\n    note_html = ['<div class=\"progress\">']\n    pbars = [\n        [ float(error_count), 'danger', 'had errors' ],\n        [ float(only_warning_count), 'warning', 'had warnings' ],\n        [ float(pass_count), 'success', 'passed' ]\n    ]\n    for b in pbars:\n        if b[0]:\n            note_html.append(\n                '<div class=\"progress-bar progress-bar-{pbcol}\" style=\"width: {pct}%\" data-toggle=\"tooltip\" title=\"{count} {sample} {txt}\">{count}</div>'. \\\n                format(\n                    pbcol = b[1],\n                    count = int(b[0]),\n                    pct = (b[0]/float(total_count))*100.0,\n                    txt = b[2],\n                    sample = 'samples' if b[0] > 1 else 'sample'\n                )\n            )\n    note_html.append('</div>')\n\n    return \"\\n\".join(note_html)"}, {"instruction": "def update_datastore(self, schema=None, primary_key=None,\n                         path=None):\n        # type: (Optional[List[Dict]], Optional[str], Optional[str]) -> None\n        \"\"\"For tabular data, update a resource in the HDX datastore which enables data preview in HDX. If no schema is provided\n        all fields are assumed to be text. If path is not supplied, the file is first downloaded from HDX.\n\n        Args:\n            schema (List[Dict]): List of fields and types of form {'id': 'FIELD', 'type': 'TYPE'}. Defaults to None.\n            primary_key (Optional[str]): Primary key of schema. Defaults to None.\n            path (Optional[str]): Local path to file that was uploaded. Defaults to None.\n\n        Returns:\n            None\n        \"\"\"\n", "input": "", "output": "        self.create_datastore(schema, primary_key, 2, path=path)"}, {"instruction": "def validate(self):\n        \"\"\"  Confirms the current token is still valid.\n        Returns True if it is valid, False otherwise. \"\"\"\n", "input": "", "output": "\n        try:\n            resp = self.request().get(self.validate_url, verify=self.verifySSL).json()\n        except TokenExpiredError:\n            return False\n        except AttributeError:\n            return False\n\n        if 'error' in resp:\n            return False\n        return True"}, {"instruction": "def match_one(template, image, options=None):\n    \"\"\"\n    Match template and find exactly one match in the Image using specified features.\n\n    :param template: Template Image\n    :param image: Search Image\n    :param options: Options include\n        - features: List of options for each feature\n    :return: (Box, Score) Bounding box of the matched object, Heatmap value\n    \"\"\"\n", "input": "", "output": "    heatmap, scale = multi_feat_match(template, image, options)\n\n    min_val, _, min_loc, _ = cv.minMaxLoc(heatmap)\n    top_left = tuple(scale * x for x in min_loc)\n    score = min_val\n\n    h, w = template.shape[:2]\n    return Box(top_left[0], top_left[1], w, h), score"}, {"instruction": "def _delete(self, pk):\n        \"\"\"\n            Delete function logic, override to implement different logic\n            deletes the record with primary_key = pk\n\n            :param pk:\n                record primary key to delete\n        \"\"\"\n", "input": "", "output": "        item = self.datamodel.get(pk, self._base_filters)\n        if not item:\n            abort(404)\n        try:\n            self.pre_delete(item)\n        except Exception as e:\n            flash(str(e), \"danger\")\n        else:\n            if self.datamodel.delete(item):\n                self.post_delete(item)\n            flash(*self.datamodel.message)\n            self.update_redirect()"}, {"instruction": "def reentrancies(self):\n        \"\"\"\n        Return a mapping of variables to their re-entrancy count.\n\n        A re-entrancy is when more than one edge selects a node as its\n        target. These graphs are rooted, so the top node always has an\n        implicit entrancy. Only nodes with re-entrancies are reported,\n        and the count is only for the entrant edges beyond the first.\n        Also note that these counts are for the interpreted graph, not\n        for the linearized form, so inverted edges are always\n        re-entrant.\n        \"\"\"\n", "input": "", "output": "        entrancies = defaultdict(int)\n        entrancies[self.top] += 1  # implicit entrancy to top\n        for t in self.edges():\n            entrancies[t.target] += 1\n        return dict((v, cnt - 1) for v, cnt in entrancies.items() if cnt >= 2)"}, {"instruction": "def _join_signals(self):\n        r\"\"\"Join N 1D signals into one N-dimensional signal.\"\"\"\n", "input": "", "output": "        joined = dict()\n        for name in self.signals:\n            name_base = name.rsplit('_', 1)[0]\n            names = joined.get(name_base, list())\n            names.append(name)\n            joined[name_base] = names\n        for name_base, names in joined.items():\n            if len(names) > 1:\n                names = sorted(names)  # ensure dim ordering (_0, _1, etc.)\n                signal_nd = np.stack([self.signals[n] for n in names], axis=1)\n                self.signals[name_base] = signal_nd\n                for name in names:\n                    del self.signals[name]"}, {"instruction": "def _parse_contract(self, player_info):\n        \"\"\"\n        Parse the player's contract.\n\n        Depending on the player's contract status, a contract table is located\n        at the bottom of the stats page and includes player wages by season. If\n        found, create a dictionary housing the wages by season.\n\n        Parameters\n        ----------\n        player_info : PyQuery object\n            A PyQuery object containing the HTML from the player's stats page.\n        \"\"\"\n", "input": "", "output": "        contract = {}\n\n        salary_table = player_info('table#br-salaries')\n        for row in salary_table('tbody tr').items():\n            if 'class=\"spacer partial_table\"' in str(row):\n                continue\n            year = row('th[data-stat=\"year_ID\"]').text()\n            if year.strip() == '':\n                continue\n            age = row('td[data-stat=\"age\"]').text()\n            team = self._parse_team_name(str(row('td[data-stat=\"team_name\"]')))\n            salary = row('td[data-stat=\"Salary\"]').text()\n            contract[year] = {\n                'age': age,\n                'team': team,\n                'salary': salary\n            }\n        setattr(self, '_contract', contract)"}, {"instruction": "def _create_latent_variables(self):\n        \"\"\" Creates model latent variables\n\n        Returns\n        ----------\n        None (changes model attributes)\n        \"\"\"\n", "input": "", "output": "        for parm in range(self.z_no):\n            self.latent_variables.add_z('Sigma^2 ' + self.X_names[parm], fam.Flat(transform='exp'), fam.Normal(0,3))"}, {"instruction": "def set_virtualization_realm_type(self):\n        \"\"\"Sets the virtualization realm type from deployment properties\n\n        :return: None\n        \"\"\"\n", "input": "", "output": "        log = logging.getLogger(self.cls_logger + '.set_virtualization_realm_type')\n        self.virtualization_realm_type = self.get_value('cons3rt.deploymentRun.virtRealm.type')\n        log.info('Found virtualization realm type : {t}'.format(t=self.virtualization_realm_type))"}, {"instruction": "def read_long_description(readme_file):\n    \"\"\" Read package long description from README file \"\"\"\n", "input": "", "output": "    try:\n        import pypandoc\n    except (ImportError, OSError) as exception:\n        print('No pypandoc or pandoc: %s' % (exception,))\n        if sys.version_info.major == 3:\n            handle = open(readme_file, encoding='utf-8')\n        else:\n            handle = open(readme_file)\n        long_description = handle.read()\n        handle.close()\n        return long_description\n    else:\n        return pypandoc.convert(readme_file, 'rst')"}, {"instruction": "def min_periodic_distance(self, xyz0, xyz1):\n        \"\"\"Vectorized distance calculation considering minimum image.\n\n        Parameters\n        ----------\n        xyz0 : np.ndarray, shape=(3,), dtype=float\n            Coordinates of first point\n        xyz1 : np.ndarray, shape=(3,), dtype=float\n            Coordinates of second point\n\n        Returns\n        -------\n        float\n            Vectorized distance between the two points following minimum\n            image convention\n\n        \"\"\"\n", "input": "", "output": "        d = np.abs(xyz0 - xyz1)\n        d = np.where(d > 0.5 * self.periodicity, self.periodicity - d, d)\n        return np.sqrt((d ** 2).sum(axis=-1))"}, {"instruction": "def is_volatile(self):\n        \"\"\"\n        True if combination of field access properties result in a field that\n        should be interpreted as volatile.\n        (Any hardware-writable field is inherently volatile)\n        \"\"\"\n", "input": "", "output": "\n        hw = self.get_property('hw')\n        return (\n            (hw in (rdltypes.AccessType.rw, rdltypes.AccessType.rw1,\n                    rdltypes.AccessType.w, rdltypes.AccessType.w1))\n            or self.get_property('counter')\n            or (self.get_property('next') is not None)\n            or self.get_property('hwset')\n            or self.get_property('hwclr')\n        )"}, {"instruction": "def rbac_policy_update(request, policy_id, **kwargs):\n    \"\"\"Update a RBAC Policy.\n\n    :param request: request context\n    :param policy_id: target policy id\n    :param target_tenant: target tenant of the policy\n    :return: RBACPolicy object\n    \"\"\"\n", "input": "", "output": "    body = {'rbac_policy': kwargs}\n    rbac_policy = neutronclient(request).update_rbac_policy(\n        policy_id, body=body).get('rbac_policy')\n    return RBACPolicy(rbac_policy)"}, {"instruction": "def collaborations(self, key, value):\n    \"\"\"Populate the ``collaborations`` key.\"\"\"\n", "input": "", "output": "    result = []\n\n    for g_value in force_list(value.get('g')):\n        collaborations = normalize_collaboration(g_value)\n        if len(collaborations) == 1:\n            result.append({\n                'record': get_record_ref(maybe_int(value.get('0')), 'experiments'),\n                'value': collaborations[0],\n            })\n        else:\n            result.extend({'value': collaboration} for collaboration in collaborations)\n\n    return result"}, {"instruction": "def parse_response_start_line(line: str) -> ResponseStartLine:\n    \"\"\"Returns a (version, code, reason) tuple for an HTTP 1.x response line.\n\n    The response is a `collections.namedtuple`.\n\n    >>> parse_response_start_line(\"HTTP/1.1 200 OK\")\n    ResponseStartLine(version='HTTP/1.1', code=200, reason='OK')\n    \"\"\"\n", "input": "", "output": "    line = native_str(line)\n    match = re.match(\"(HTTP/1.[0-9]) ([0-9]+) ([^\\r]*)\", line)\n    if not match:\n        raise HTTPInputError(\"Error parsing response start line\")\n    return ResponseStartLine(match.group(1), int(match.group(2)), match.group(3))"}, {"instruction": "def set_compression_pool_size(pool_size):\n    \"\"\"\n    Set the size of the compression workers thread pool.\n    If the pool is already created, it waits until all jobs are finished, and then proceeds with setting the new size.\n\n    Parameters\n    ----------\n        pool_size : `int`\n            The size of the pool (must be a positive integer)\n\n    Returns\n    -------\n    `None`\n    \"\"\"\n", "input": "", "output": "    pool_size = int(pool_size)\n    if pool_size < 1:\n        raise ValueError(\"The compression thread pool size cannot be of size {}\".format(pool_size))\n\n    global _compress_thread_pool\n    if _compress_thread_pool is not None:\n        _compress_thread_pool.close()\n        _compress_thread_pool.join()\n    _compress_thread_pool = ThreadPool(pool_size)"}, {"instruction": "def mkdir(self, pathobj, _):\n        \"\"\"\n        Creates remote directory\n        Note that this operation is not recursive\n        \"\"\"\n", "input": "", "output": "        if not pathobj.drive or not pathobj.root:\n            raise RuntimeError(\"Full path required: '%s'\" % str(pathobj))\n\n        if pathobj.exists():\n            raise OSError(17, \"File exists: '%s'\" % str(pathobj))\n\n        url = str(pathobj) + '/'\n        text, code = self.rest_put(url, session=pathobj.session, verify=pathobj.verify, cert=pathobj.cert)\n\n        if not code == 201:\n            raise RuntimeError(\"%s %d\" % (text, code))"}, {"instruction": "def contract_to_dict(contract):\n    \"\"\"Convert an IBPy Contract object to a dict containing any non-default values.\"\"\"\n", "input": "", "output": "    default = Contract()\n    return {field: val for field, val in vars(contract).items() if val != getattr(default, field, None)}"}, {"instruction": "def add_bits_to_path(path_, filename_prefix=None, extension=None):\r\n    \"\"\"\r\n    Adds prefix/suffix to filename\r\n\r\n    Arguments:\r\n        path_ -- path to file\r\n        filename_prefix -- prefix to be added to file name\r\n        extension -- extension to be added to file name. The dot is automatically added, such as\r\n            \"ext\" and \".ext\" will have the same effect\r\n\r\n    Examples:\r\n        > add_bits_to_path(\"/home/user/file\", \"prefix-\")\r\n        /home/user/prefix-file\r\n\r\n        > add_bits_to_path(\"/home/user/file\", None, \".ext\")\r\n        /home/user/file.ext\r\n\r\n        > add_bits_to_path(\"/home/user/file\", None, \"ext\")  # dot in extension is optional\r\n        /home/user/file.ext\r\n\r\n        > add_bits_to_path(\"/home/user/\", None, \".ext\")\r\n        /home/user/.ext\r\n    \"\"\"\n", "input": "", "output": "\r\n    dir_, basename = os.path.split(path_)\r\n\r\n    if filename_prefix:\r\n        basename = filename_prefix+basename\r\n    if extension:\r\n        if not extension.startswith(\".\"):\r\n            extension = \".\"+extension\r\n        basename = basename+extension\r\n\r\n    return os.path.join(dir_, basename)"}, {"instruction": "def exclude_args(parser, args, excluded_args, target):\n    \"\"\"Delete options that are not appropriate for a following code path; exit\n    with an error if excluded options were passed in by the user.\n\n    argparse generates a namespace with all options it knows, but not every\n    attribute should be passed to all code paths (i.e. options about\n    interpolation should not reach `run_from_ufos()`). This function can be run\n    before entering a particular code path to clean up the kwargs passed to it.\n\n    Exit with an error message if the user actually passed the options in.\n    \"\"\"\n", "input": "", "output": "    msg = '\"%s\" option invalid for %s'\n    for argname in excluded_args:\n        if argname not in args:\n            continue\n        if args[argname]:\n            optname = \"--%s\" % argname.replace(\"_\", \"-\")\n            parser.error(msg % (optname, target))\n        del args[argname]"}, {"instruction": "def iter(self, match=\"*\", count=1000):\n        \"\"\" Iterates the set of keys in :prop:key_prefix in :prop:_client\n            @match: #str pattern to match after the :prop:key_prefix\n            @count: the user specified the amount of work that should be done\n                at every call in order to retrieve elements from the collection\n\n            -> yields redis keys within this instance\n        \"\"\"\n", "input": "", "output": "        replace_this = self.key_prefix+\":\"\n        for key in self._client.scan_iter(\n           match=\"{}:{}\".format(self.key_prefix, match), count=count):\n            yield self._decode(key).replace(replace_this, \"\", 1)"}, {"instruction": "async def is_user_authorized(self):\n        \"\"\"\n        Returns ``True`` if the user is authorized.\n        \"\"\"\n", "input": "", "output": "        if self._authorized is None:\n            try:\n                # Any request that requires authorization will work\n                await self(functions.updates.GetStateRequest())\n                self._authorized = True\n            except errors.RPCError:\n                self._authorized = False\n\n        return self._authorized"}, {"instruction": "def rm_keys_from_dict(d, keys):\n    \"\"\"\n    Given a dictionary and a key list, remove any data in the dictionary with the given keys.\n\n    :param dict d: Metadata\n    :param list keys: Keys to be removed\n    :return dict d: Metadata\n    \"\"\"\n", "input": "", "output": "    # Loop for each key given\n    for key in keys:\n        # Is the key in the dictionary?\n        if key in d:\n            try:\n                d.pop(key, None)\n            except KeyError:\n                # Not concerned with an error. Keep going.\n                pass\n    return d"}, {"instruction": "def setupDock(self):\n        \"\"\"Setup empty Dock at startup. \"\"\"\n", "input": "", "output": "        self.dock = QtWidgets.QDockWidget(\"Classes\", self)\n        self.dock.setWidget(self.tree)\n        self.dock.setFeatures(QtWidgets.QDockWidget.NoDockWidgetFeatures)\n        self.addDockWidget(QtCore.Qt.LeftDockWidgetArea, self.dock)"}, {"instruction": "def __send_exc_clear(self, log_if_exc_set=None):\n        \"\"\"Clear send exception and time. If exception was previously was set, optionally log log_if_exc_set at INFO\n        level.\n        \"\"\"\n", "input": "", "output": "        if not (log_if_exc_set is None or self.__send_exc is None):\n            logger.info(log_if_exc_set)\n        self.__send_exc_time = None\n        self.__send_exc = None"}, {"instruction": "def has_gap_in_elf_shndx(self):\n        \"\"\"Return the has gap in elf shndx attribute of the BFD file being\n        processed.\n        \"\"\"\n", "input": "", "output": "        if not self._ptr:\n            raise BfdException(\"BFD not initialized\")\n\n        return _bfd.get_bfd_attribute(\n            self._ptr, BfdAttributes.HAS_GAP_IN_ELF_SHNDX)"}, {"instruction": "def strip_spaces_and_quotes(value):\n    \"\"\"Remove invalid whitespace and/or single pair of dquotes and return None\n    for empty strings.\n\n    Used to prepare cookie values, path, and domain attributes in a way which\n    tolerates simple formatting mistakes and standards variations.\n    \"\"\"\n", "input": "", "output": "    value = value.strip() if value else \"\"\n    if value and len(value) > 1 and (value[0] == value[-1] == '\"'):\n        value = value[1:-1]\n    if not value:\n        value = \"\"\n    return value"}, {"instruction": "def _f16_to_32bit(ins):\n    \"\"\" If any of the operands within the ins(truction) are numeric,\n    convert them to its 32bit representation, otherwise leave them\n    as they are.\n    \"\"\"\n", "input": "", "output": "    ins.quad = [x for x in ins.quad]\n    for i in range(2, len(ins.quad)):\n        if is_float(ins.quad[i]):\n            de, hl = f16(ins.quad[i])\n            ins.quad[i] = str((de << 16) | hl)\n\n    ins.quad = tuple(ins.quad)\n    return ins"}, {"instruction": "def process_tree_files(tree):\n    \"\"\" process_tree_files: Download files from nodes\n        Args:\n            tree (ChannelManager): manager to handle communication to Kolibri Studio\n        Returns: None\n    \"\"\"\n", "input": "", "output": "    # Fill in values necessary for next steps\n    config.LOGGER.info(\"Processing content...\")\n    files_to_diff = tree.process_tree(tree.channel)\n    config.SUSHI_BAR_CLIENT.report_statistics(files_to_diff, topic_count=tree.channel.get_topic_count())\n    tree.check_for_files_failed()\n    return files_to_diff, config.FAILED_FILES"}, {"instruction": "def call(self, itemMethod):\n        \"\"\"\n        Invoke the given bound item method in the batch process.\n\n        Return a Deferred which fires when the method has been invoked.\n        \"\"\"\n", "input": "", "output": "        item = itemMethod.im_self\n        method = itemMethod.im_func.func_name\n        return self.batchController.getProcess().addCallback(\n            CallItemMethod(storepath=item.store.dbdir,\n                           storeid=item.storeID,\n                           method=method).do)"}, {"instruction": "def initialize(self, store):\r\n        \"\"\"Common initialization of handlers happens here. If additional\r\n        initialization is required, this method must either be called with\r\n        ``super`` or the child class must assign the ``store`` attribute and\r\n        register itself with the store.\r\n\r\n        \"\"\"\n", "input": "", "output": "        assert isinstance(store, stores.BaseStore)\r\n        self.messages = Queue()\r\n        self.store = store\r\n        self.store.register(self)"}, {"instruction": "def save_graph(cn_topo, filename, showintfs=False, showaddrs=False):\n    '''\n    Save the topology to an image file \n    '''\n", "input": "", "output": "    __do_draw(cn_topo, showintfs=showintfs, showaddrs=showaddrs)\n    pyp.savefig(filename)"}, {"instruction": "def _make_proxy(self, varname, parent=None, constructor=MlabObjectProxy):\n        \"\"\"Creates a proxy for a variable.\n\n        XXX create and cache nested proxies also here.\n        \"\"\"\n", "input": "", "output": "        # FIXME why not just use gensym here?\n        proxy_val_name = \"PROXY_VAL%d__\" % self._proxy_count\n        self._proxy_count += 1\n        mlabraw.eval(self._session, \"%s = %s;\" % (proxy_val_name, varname))\n        res = constructor(self, proxy_val_name, parent)\n        self._proxies[proxy_val_name] = res\n        return res"}, {"instruction": "def to_xml(self):\n        '''\n        Returns an XMLi representation of the shipping details.\n        @return: Element\n        '''\n", "input": "", "output": "        for n, v in {\"recipient\": self.recipient}.items():\n            if is_empty_or_none(v):\n                raise ValueError(\"'%s' attribute cannot be empty or None.\" % n)\n\n        doc = Document()\n        root = doc.createElement(\"shipping\")\n        root.appendChild(self.recipient.to_xml(\"recipient\"))\n        return root"}, {"instruction": "def loc_adjacent_to_opponent_king(self, location, position):\n        \"\"\"\n        Finds if 2 kings are touching given the position of one of the kings.\n\n        :type: location: Location\n        :type: position: Board\n        :rtype: bool\n        \"\"\"\n", "input": "", "output": "        for fn in self.cardinal_directions:\n            try:\n                if isinstance(position.piece_at_square(fn(location)), King) and \\\n                        position.piece_at_square(fn(location)).color != self.color:\n                    return True\n\n            except IndexError:\n                pass\n\n        return False"}, {"instruction": "def add(self, dpid, ports):\n        \"\"\"add a setting of a bonding i/f.\n        'add' method takes the corresponding args in this order.\n\n        ========= =====================================================\n        Attribute Description\n        ========= =====================================================\n        dpid      datapath id.\n\n        ports     a list of integer values that means the ports face\n                  with the slave i/fs.\n        ========= =====================================================\n\n        if you want to use multi LAG, call 'add' method more than once.\n        \"\"\"\n", "input": "", "output": "        assert isinstance(ports, list)\n        assert len(ports) >= 2\n        ifs = {}\n        for port in ports:\n            ifs[port] = {'enabled': False, 'timeout': 0}\n        bond = {dpid: ifs}\n        self._bonds.append(bond)"}, {"instruction": "def get_comments_content_object(parser, token):\n    \"\"\"\n    Get a limited set of comments for a given object.\n    Defaults to a limit of 5. Setting the limit to -1 disables limiting.\n\n    usage:\n\n        {% get_comments_content_object for form_object as variable_name %}\n\n    \"\"\"\n", "input": "", "output": "    keywords = token.contents.split()\n    if len(keywords) != 5:\n        raise template.TemplateSyntaxError(\n            \"'%s' tag takes exactly 2 arguments\" % (keywords[0],))\n    if keywords[1] != 'for':\n        raise template.TemplateSyntaxError(\n            \"first argument to '%s' tag must be 'for'\" % (keywords[0],))\n    if keywords[3] != 'as':\n        raise template.TemplateSyntaxError(\n            \"first argument to '%s' tag must be 'as'\" % (keywords[0],))\n    return GetCommentsContentObject(keywords[2], keywords[4])"}, {"instruction": "def login(container):\n    \"\"\"Log into container.\"\"\"\n", "input": "", "output": "    columns, lines = shutil.get_terminal_size()  # Temporary\n    try:\n        subprocess.check_call([\n            \"docker\", \"exec\",\n            \"--env\", f\"COLUMNS={str(columns)},LINES={str(lines)}\",  # Temporary\n            \"--env\", f\"LINES={str(lines)}\",  # Temporary\n            \"--interactive\",\n            \"--tty\",\n            container,\n            \"bash\",\n            \"--login\"\n        ])\n    except subprocess.CalledProcessError:\n        raise RuntimeError() from None"}, {"instruction": "def set_size(self, size):\n        \"\"\"Set figure size\"\"\"\n", "input": "", "output": "        w, h = size\n        self.root.set('width', w)\n        self.root.set('height', h)"}, {"instruction": "def CreateFeedItemAddOperation(name, price, date, ad_customizer_feed):\n  \"\"\"Creates a FeedItemOperation.\n\n  The generated FeedItemOperation will create a FeedItem with the specified\n  values when sent to FeedItemService.mutate.\n\n  Args:\n    name: the value for the name attribute of the FeedItem.\n    price: the value for the price attribute of the FeedItem.\n    date: the value for the date attribute of the FeedItem.\n    ad_customizer_feed: the AdCustomizerFeed we're associating the FeedItems\n        with.\n\n  Returns:\n    A new FeedItemOperation for adding a FeedItem.\n  \"\"\"\n", "input": "", "output": "  feed_item = {\n      'feedId': ad_customizer_feed['feedId'],\n      'attributeValues': [\n          {\n              'feedAttributeId': ad_customizer_feed['feedAttributes'][0]['id'],\n              'stringValue': name\n          },\n          {\n              'feedAttributeId': ad_customizer_feed['feedAttributes'][1]['id'],\n              'stringValue': price\n          },\n          {\n              'feedAttributeId': ad_customizer_feed['feedAttributes'][2]['id'],\n              'stringValue': date\n          }\n      ]\n  }\n\n  operation = {\n      'operator': 'ADD',\n      'operand': feed_item\n  }\n\n  return operation"}, {"instruction": "def Axn(mt, x, n):\n    \"\"\" (A^1)x:n : Returns the EPV (net single premium) of a term insurance. \"\"\"\n", "input": "", "output": "    return (mt.Mx[x] - mt.Mx[x + n]) / mt.Dx[x]"}, {"instruction": "def discussions_notifications(user):\n    '''Notify user about open discussions'''\n", "input": "", "output": "    notifications = []\n\n    # Only fetch required fields for notification serialization\n    # Greatly improve performances and memory usage\n    qs = discussions_for(user).only('id', 'created', 'title', 'subject')\n\n    # Do not dereference subject (so it's a DBRef)\n    # Also improve performances and memory usage\n    for discussion in qs.no_dereference():\n        notifications.append((discussion.created, {\n            'id': discussion.id,\n            'title': discussion.title,\n            'subject': {\n                'id': discussion.subject['_ref'].id,\n                'type': discussion.subject['_cls'].lower(),\n            }\n        }))\n\n    return notifications"}, {"instruction": "async def connect(self, timeout=None, ssl=None):\n        \"\"\"\n        Establishes a connection with the server.\n        \"\"\"\n", "input": "", "output": "        await self._connect(timeout=timeout, ssl=ssl)\n        self._connected = True\n\n        self._send_task = self._loop.create_task(self._send_loop())\n        self._recv_task = self._loop.create_task(self._recv_loop())"}, {"instruction": "def reset(self):\n    \"\"\"Reset simulated and real environments.\"\"\"\n", "input": "", "output": "    self._frame_counter = 0\n    ob_real = self.real_env.reset()\n    # Initialize simulated environment with frames from real one.\n    self.sim_env.add_to_initial_stack(ob_real)\n    for _ in range(3):\n      ob_real, _, _, _ = self.real_env.step(self.name_to_action_num[\"NOOP\"])\n      self.sim_env.add_to_initial_stack(ob_real)\n    ob_sim = self.sim_env.reset()\n    assert np.all(ob_real == ob_sim)\n    self._last_step_tuples = self._pack_step_tuples((ob_real, 0, False, {}),\n                                                    (ob_sim, 0, False, {}))\n    self.set_zero_cumulative_rewards()\n    ob, _, _, _ = self._player_step_tuple(self._last_step_tuples)\n    return ob"}, {"instruction": "def from_tuples(cls, tups):\n        \"\"\"\n        Create a new IntervalTree from an iterable of 2- or 3-tuples,\n         where the tuple lists begin, end, and optionally data.\n        \"\"\"\n", "input": "", "output": "        ivs = [Interval(*t) for t in tups]\n        return IntervalTree(ivs)"}, {"instruction": "def addprojecthook(self, project_id, url, push=False, issues=False, merge_requests=False, tag_push=False):\n        \"\"\"\n        add a hook to a project\n\n        :param project_id: project id\n        :param url: url of the hook\n        :return: True if success\n        \"\"\"\n", "input": "", "output": "        data = {\n            'id': project_id,\n            'url': url,\n            'push_events': int(bool(push)),\n            'issues_events': int(bool(issues)),\n            'merge_requests_events': int(bool(merge_requests)),\n            'tag_push_events': int(bool(tag_push)),\n        }\n\n        request = requests.post(\n            '{0}/{1}/hooks'.format(self.projects_url, project_id),\n            headers=self.headers, data=data, verify=self.verify_ssl, auth=self.auth, timeout=self.timeout)\n\n        if request.status_code == 201:\n            return request.json()\n        else:\n            return False"}, {"instruction": "def contains(value: Union[str, 'Type']) -> bool:\n        \"\"\" Checks if a type is defined \"\"\"\n", "input": "", "output": "        if isinstance(value, str):\n            return any(value.lower() == i.value for i in Type)\n\n        return any(value == i for i in Type)"}, {"instruction": "def _aix_memdata():\n    '''\n    Return the memory information for AIX systems\n    '''\n", "input": "", "output": "    grains = {'mem_total': 0, 'swap_total': 0}\n    prtconf = salt.utils.path.which('prtconf')\n    if prtconf:\n        for line in __salt__['cmd.run'](prtconf, python_shell=True).splitlines():\n            comps = [x for x in line.strip().split(' ') if x]\n            if len(comps) > 2 and 'Memory' in comps[0] and 'Size' in comps[1]:\n                grains['mem_total'] = int(comps[2])\n                break\n    else:\n        log.error('The \\'prtconf\\' binary was not found in $PATH.')\n\n    swap_cmd = salt.utils.path.which('swap')\n    if swap_cmd:\n        swap_data = __salt__['cmd.run']('{0} -s'.format(swap_cmd)).split()\n        try:\n            swap_total = (int(swap_data[-2]) + int(swap_data[-6])) * 4\n        except ValueError:\n            swap_total = None\n        grains['swap_total'] = swap_total\n    else:\n        log.error('The \\'swap\\' binary was not found in $PATH.')\n    return grains"}, {"instruction": "def get_intercom_data(self):\n        \"\"\"Specify the data sent to Intercom API according to event type\"\"\"\n", "input": "", "output": "        data = {\n            \"event_name\": self.get_type_display(),  # event type\n            \"created_at\": calendar.timegm(self.created.utctimetuple()),  # date\n            \"metadata\": self.metadata\n        }\n        if self.user:\n            data[\"user_id\"] = self.user.intercom_id\n        return data"}, {"instruction": "def wavefunction(self) -> pyquil.Wavefunction:\n        \"\"\"\n        Return the wavefunction of a completed program.\n        \"\"\"\n", "input": "", "output": "        assert self.status == 'done'\n        assert self._ket is not None\n        wavefn = state_to_wavefunction(self._ket)\n        return wavefn"}, {"instruction": "def _to_dict(self):\n        \"\"\"\n        Converts object into a dictionary.\n        \"\"\"\n", "input": "", "output": "        for i, tag in enumerate(self.tags):\n            if tag in (\"\", None):\n                self.tags.pop(i)\n\n        data = {\n            'name': self.name,\n            'referenceId': self.reference_id,\n            'shortDescription': self.short_description,\n            'longDescription': self.long_description,\n            'itemState': self.item_state,\n            'linkURL': self.link_url,\n            'linkText': self.link_text,\n            'tags': self.tags,\n            'economics': self.economics,\n            'id': self.id,\n            'end_date': _make_tstamp(self.end_date),\n            'start_date': _make_tstamp(self.start_date)}\n        if len(self.renditions) > 0:\n            data['renditions'] = []\n            for r in self.renditions:\n                data['renditions'].append(r.to_dict())\n        if len(self.metadata) > 0:\n            data['customFields'] = {}\n            for meta in self.metadata:\n                data['customFields'][meta['key']] = meta['value']\n        [data.pop(key) for key in data.keys() if data[key] == None]\n        return data"}, {"instruction": "def get_remaining_width(sample_string, max_terminal_width=None):\n    \"\"\"Returns the number of characters available if sample string were to be printed in the terminal.\n\n    Positional arguments:\n    sample_string -- gets the length of this string.\n\n    Keyword arguments:\n    max_terminal_width -- limit the overall width of everything to these many characters.\n\n    Returns:\n    Integer.\n    \"\"\"\n", "input": "", "output": "    if max_terminal_width is not None:\n        available_width = min(terminal_width(), max_terminal_width)\n    else:\n        available_width = terminal_width()\n    return available_width - len(sample_string)"}, {"instruction": "def register_algorithm(self, alg_id, alg_obj):\n        \"\"\"\n        Registers a new Algorithm for use when creating and verifying tokens.\n        \"\"\"\n", "input": "", "output": "        if alg_id in self._algorithms:\n            raise ValueError('Algorithm already has a handler.')\n\n        if not isinstance(alg_obj, Algorithm):\n            raise TypeError('Object is not of type `Algorithm`')\n\n        self._algorithms[alg_id] = alg_obj\n        self._valid_algs.add(alg_id)"}, {"instruction": "def update_node_count(self, node, add_to_count):\n        \"\"\"\\\n        stores how many decent nodes are under a parent node\n        \"\"\"\n", "input": "", "output": "        current_score = 0\n        count_string = self.parser.getAttribute(node, 'gravityNodes')\n        if count_string:\n            current_score = int(count_string)\n\n        new_score = current_score + add_to_count\n        self.parser.setAttribute(node, \"gravityNodes\", str(new_score))"}, {"instruction": "def _find_aux_coord_vars(self, ds, refresh=False):\n        '''\n        Returns a list of auxiliary coordinate variables\n\n        An auxiliary coordinate variable is any netCDF variable that contains\n        coordinate data, but is not a coordinate variable (in the sense of the term\n        defined by CF).\n\n        :param netCDF4.Dataset ds: An open netCDF dataset\n        :param bool refresh: if refresh is set to True, the cache is\n                             invalidated.\n        :rtype: list\n        :return: List of variable names (str) that are defined to be auxiliary\n                 coordinate variables.\n        '''\n", "input": "", "output": "        if self._aux_coords.get(ds, None) and refresh is False:\n            return self._aux_coords[ds]\n\n        self._aux_coords[ds] = cfutil.get_auxiliary_coordinate_variables(ds)\n        return self._aux_coords[ds]"}, {"instruction": "def options(self, parser, env):\n        \"\"\"Register commandline options.\n        \"\"\"\n", "input": "", "output": "        parser.add_option(\n            \"--epdb\", action=\"store_true\", dest=\"epdb_debugErrors\",\n            default=env.get('NOSE_EPDB', False),\n            help=\"Drop into extended debugger on errors\")\n        parser.add_option(\n            \"--epdb-failures\", action=\"store_true\",\n            dest=\"epdb_debugFailures\",\n            default=env.get('NOSE_EPDB_FAILURES', False),\n            help=\"Drop into extended debugger on failures\")"}, {"instruction": "def rel_path(name, available_tools):\n        \"\"\"\n        Extracts relative path to a tool (from the main cloned directory) out\n        of available_tools based on the name it is given\n        \"\"\"\n", "input": "", "output": "        if name == '@' or name == '.' or name == '/':\n            name = ''\n        multi_tool = '@' in name\n        for tool in available_tools:\n            t_name = tool[0].lower()\n            if multi_tool:\n                if name.split('@')[-1] == t_name.split('@')[-1]:\n                    return t_name, t_name\n            else:\n                if name == t_name.split('/')[-1]:\n                    return t_name, tool[0]\n                elif name == '' and t_name.split('@')[-1] == 'unspecified':\n                    return '', ''\n        return None, None"}, {"instruction": "def handle(self):\n        \"\"\"\n        Executes the actual Stratum program.\n        \"\"\"\n", "input": "", "output": "        self.output = PyStratumStyle(self.input, self.output)\n\n        command = self.get_application().find('constants')\n        ret = command.execute(self.input, self.output)\n        if ret:\n            return ret\n\n        command = self.get_application().find('loader')\n        ret = command.execute(self.input, self.output)\n        if ret:\n            return ret\n\n        command = self.get_application().find('wrapper')\n        ret = command.execute(self.input, self.output)\n\n        self.output.writeln('')\n\n        return ret"}, {"instruction": "def select(table, index_track, field_name, op, value, includeMissing):\n    '''Modifies the table and index_track lists based on the comparison.\n    '''\n", "input": "", "output": "    result = []\n    result_index = []\n    counter = 0\n    for row in table:\n        if detect_fields(field_name, convert_to_dict(row)):\n            final_value = get_value(row, field_name)\n            if do_op(final_value, op, value):\n                result.append(row)\n                result_index.append(index_track[counter])\n        else:\n            if includeMissing:\n                result.append(row)\n                result_index.append(index_track[counter])\n        counter += 1\n    #table = result\n    #index_track = result_index\n    return (result, result_index)"}, {"instruction": "def u_edit(*args):\n    \"\"\"\n    Edits given paths into Umbra.\n\n    :param \\*args: Arguments.\n    :type \\*args: \\*\n    :return: Definition success.\n    :rtype: bool\n    \"\"\"\n", "input": "", "output": "\n    paths = []\n    for path in args:\n        if not os.path.exists(path):\n            continue\n\n        paths.append(os.path.abspath(path))\n\n    if not paths:\n        return\n\n    connection = socket.socket(socket.AF_INET, socket.SOCK_STREAM)\n    connection.connect((socket.gethostbyname(socket.gethostname()), 16384))\n    connection.send(\"{0}<!RE>\".format(\"\\n\".join(COMMAND_TEMPLATE).format(paths)))\n    connection.close()\n    return True"}, {"instruction": "def uf(sigla):\n    \"\"\"\n    Valida a sigla da Unidade Federativa. Se n\u00e3o for uma sigla de UF v\u00e1lida,\n    ser\u00e1 lan\u00e7ada a exce\u00e7\u00e3o :exc:`UnidadeFederativaError`.\n    \"\"\"\n", "input": "", "output": "    if not sigla in [s for s, i, n, r in UNIDADES_FEDERACAO]:\n        raise UnidadeFederativaError('Estado (sigla) UF \"%s\" '\n                'inexistente' % sigla)"}, {"instruction": "def sonTraceRootPath():\n    \"\"\"\n    function for finding external location\n    \"\"\"\n", "input": "", "output": "    import sonLib.bioio\n    i = os.path.abspath(sonLib.bioio.__file__)\n    return os.path.split(os.path.split(os.path.split(i)[0])[0])[0]"}, {"instruction": "def _object_to_json(obj):\n        \"\"\"Convert objects that cannot be natively serialized into JSON\n        into their string representation\n\n        For datetime based objects convert them into their ISO formatted\n        string as specified by :meth:`datetime.datetime.isoformat`.\n\n        :param obj: object to convert into a JSON via getting its string\n            representation.\n        :type obj: object\n\n        :return: String value representing the given object ready to be\n            encoded into a JSON.\n        :rtype: str\n        \"\"\"\n", "input": "", "output": "        if isinstance(obj, datetime.datetime):\n            return obj.isoformat()\n        return repr(obj)"}, {"instruction": "def put(self):\n        \"\"\"Update a credential by file path\"\"\"\n", "input": "", "output": "        cred_payload = utils.uni_to_str(json.loads(request.get_data()))\n        return self.manager.update_credential(cred_payload)"}, {"instruction": "def assertFileSizeAlmostEqual(\n            self, filename, size, places=None, msg=None, delta=None):\n        '''Fail if ``filename`` does not have the given ``size`` as\n        determined by their difference rounded to the given number of\n        decimal ``places`` (default 7) and comparing to zero, or if\n        their difference is greater than a given ``delta``.\n\n        Parameters\n        ----------\n        filename : str, bytes, file-like\n        size : int, float\n        places : int\n        msg : str\n            If not provided, the :mod:`marbles.mixins` or\n            :mod:`unittest` standard message will be used.\n        delta : int, float\n\n        Raises\n        ------\n        TypeError\n            If ``filename`` is not a str or bytes object and is not\n            file-like.\n        '''\n", "input": "", "output": "        fsize = self._get_file_size(filename)\n        self.assertAlmostEqual(\n                fsize, size, places=places, msg=msg, delta=delta)"}, {"instruction": "async def try_trigger_before_first_request_functions(self) -> None:\n        \"\"\"Trigger the before first request methods.\"\"\"\n", "input": "", "output": "        if self._got_first_request:\n            return\n\n        # Reverse the teardown functions, so as to match the expected usage\n        self.teardown_appcontext_funcs = list(reversed(self.teardown_appcontext_funcs))\n        for key, value in self.teardown_request_funcs.items():\n            self.teardown_request_funcs[key] = list(reversed(value))\n        for key, value in self.teardown_websocket_funcs.items():\n            self.teardown_websocket_funcs[key] = list(reversed(value))\n\n        async with self._first_request_lock:\n            if self._got_first_request:\n                return\n            for function in self.before_first_request_funcs:\n                await function()\n            self._got_first_request = True"}, {"instruction": "def val(self, strictkey):\n        \"\"\"\n        Return a chunk referencing a value in a mapping with the key 'key'.\n        \"\"\"\n", "input": "", "output": "        ruamelkey = self.ruamelindex(strictkey)\n        return self._select(self._pointer.val(ruamelkey, strictkey))"}, {"instruction": "def submit(self, func, *args, **kwargs):\n        \"\"\"Submit a function for serialized execution on sqs\n        \"\"\"\n", "input": "", "output": "        self.op_sequence += 1\n        self.sqs.send_message(\n            QueueUrl=self.map_queue,\n            MessageBody=utils.dumps({'args': args, 'kwargs': kwargs}),\n            MessageAttributes={\n                'sequence_id': {\n                    'StringValue': str(self.op_sequence),\n                    'DataType': 'Number'},\n                'op': {\n                    'StringValue': named(func),\n                    'DataType': 'String',\n                },\n                'ser': {\n                    'StringValue': 'json',\n                    'DataType': 'String'}}\n        )\n\n        self.futures[self.op_sequence] = f = SQSFuture(\n            self.op_sequence)\n        return f"}, {"instruction": "def set(self, name, value):\n        \"\"\"\n        Sets the value of the field `name` to `value`, which is `True` or\n        `False`.\n        \"\"\"\n", "input": "", "output": "        flag = self.flags[name]\n        self._value = (self.value | flag) if value else (self.value & ~flag)"}, {"instruction": "def RIBNextHopLimitExceeded_RIBNextHopLimit(self, **kwargs):\n        \"\"\"Auto Generated Code\n        \"\"\"\n", "input": "", "output": "        config = ET.Element(\"config\")\n        RIBNextHopLimitExceeded = ET.SubElement(config, \"RIBNextHopLimitExceeded\", xmlns=\"http://brocade.com/ns/brocade-notification-stream\")\n        RIBNextHopLimit = ET.SubElement(RIBNextHopLimitExceeded, \"RIBNextHopLimit\")\n        RIBNextHopLimit.text = kwargs.pop('RIBNextHopLimit')\n\n        callback = kwargs.pop('callback', self._callback)\n        return callback(config)"}, {"instruction": "def get_empty_dirs(self, path):\n        \"\"\"Return a list of empty directories in path.\"\"\"\n", "input": "", "output": "        empty_dirs = []\n        for i in os.listdir(path):\n            child_path = os.path.join(path, i)\n            if i == '.git' or os.path.isfile(child_path) or os.path.islink(child_path):  # noqa\n                continue\n            if self.path_only_contains_dirs(child_path):\n                empty_dirs.append(i)\n        return empty_dirs"}, {"instruction": "def _detect_available_configs():\n        \"\"\"\n        Returns all currently used channels as well as\n        one other currently unused channel.\n\n        .. note::\n\n            This method will run into problems if thousands of\n            autodetected busses are used at once.\n\n        \"\"\"\n", "input": "", "output": "        with channels_lock:\n            available_channels = list(channels.keys())\n\n        # find a currently unused channel\n        get_extra = lambda: \"channel-{}\".format(randint(0, 9999))\n        extra = get_extra()\n        while extra in available_channels:\n            extra = get_extra()\n\n        available_channels += [extra]\n\n        return [\n            {'interface': 'virtual', 'channel': channel}\n            for channel in available_channels\n        ]"}, {"instruction": "def get(cls, device_server_id, custom_headers=None):\n        \"\"\"\n        Get one of your DeviceServers.\n\n        :type api_context: context.ApiContext\n        :type device_server_id: int\n        :type custom_headers: dict[str, str]|None\n\n        :rtype: BunqResponseDeviceServer\n        \"\"\"\n", "input": "", "output": "\n        if custom_headers is None:\n            custom_headers = {}\n\n        api_client = client.ApiClient(cls._get_api_context())\n        endpoint_url = cls._ENDPOINT_URL_READ.format(device_server_id)\n        response_raw = api_client.get(endpoint_url, {}, custom_headers)\n\n        return BunqResponseDeviceServer.cast_from_bunq_response(\n            cls._from_json(response_raw, cls._OBJECT_TYPE_GET)\n        )"}, {"instruction": "def comment (self, data):\n        \"\"\"\n        Print HTML comment.\n\n        @param data: the comment\n        @type data: string\n        @return: None\n        \"\"\"\n", "input": "", "output": "        data = data.encode(self.encoding, \"ignore\")\n        self.fd.write(\"<!--%s-->\" % data)"}, {"instruction": "def process_ndex_network(network_id, username=None, password=None,\n                         require_grounding=True):\n    \"\"\"Process an NDEx network into Statements.\n\n    Parameters\n    ----------\n    network_id : str\n        NDEx network ID.\n    username : str\n        NDEx username.\n    password : str\n        NDEx password.\n    require_grounding: bool\n        Whether network nodes lacking grounding information should be included\n        among the extracted Statements (default is True).\n\n    Returns\n    -------\n    NdexCxProcessor\n        Processor containing Statements. Returns None if there if the HTTP\n        status code indicates an unsuccessful request.\n    \"\"\"\n", "input": "", "output": "    nd = ndex2.client.Ndex2(username=username, password=password)\n    res = nd.get_network_as_cx_stream(network_id)\n    if res.status_code != 200:\n        logger.error('Problem downloading network: status code %s' %\n                     res.status_code)\n        logger.error('Response: %s' % res.text)\n        return None\n    json_list = res.json()\n    summary = nd.get_network_summary(network_id)\n    return process_cx(json_list, summary=summary,\n                      require_grounding=require_grounding)"}, {"instruction": "def _disbatch_runner_async(self, chunk):\n        '''\n        Disbatch runner client_async commands\n        '''\n", "input": "", "output": "        pub_data = self.saltclients['runner'](chunk)\n        raise tornado.gen.Return(pub_data)"}, {"instruction": "def instagram_config(self, id, secret, scope=None, **_):\n        \"\"\" Get config dictionary for instagram oauth \"\"\"\n", "input": "", "output": "        scope = scope if scope else 'basic'\n        token_params = dict(scope=scope)\n\n        config = dict(\n            # request_token_url=None,\n            access_token_url='/oauth/access_token/',\n            authorize_url='/oauth/authorize/',\n            base_url='https://api.instagram.com/',\n            consumer_key=id,\n            consumer_secret=secret,\n            request_token_params=token_params\n        )\n        return config"}, {"instruction": "def rgb(color,default=(0,0,0)):\n    \"\"\" return rgb tuple for named color in rgb.txt or a hex color \"\"\"\n", "input": "", "output": "    c = color.lower()\n    if c[0:1] == '#' and len(c)==7:\n        r,g,b = c[1:3], c[3:5], c[5:]\n        r,g,b = [int(n, 16) for n in (r, g, b)]\n        return (r,g,b)\n\n    if c.find(' ')>-1:    c = c.replace(' ','')\n    if c.find('gray')>-1: c = c.replace('gray','grey')\n    if c in x11_colors.keys():  return x11_colors[c]\n    return default"}, {"instruction": "def tag(self, repository_tag, tags=[]):\n        \"\"\"\n        Tags image with one or more tags.\n\n        Raises exception on failure.\n        \"\"\"\n", "input": "", "output": "        if not isinstance(repository_tag, six.string_types):\n            raise TypeError('repository_tag must be a string')\n\n        if not isinstance(tags, list):\n            raise TypeError('tags must be a list.')\n\n        if ':' in repository_tag:\n            repository, tag = repository_tag.split(':')\n            tags.append(tag)\n        else:\n            repository = repository_tag\n\n            if not tags:\n                tags.append('latest')\n\n        for tag in tags:\n            repo_tag = \"{0}:{1}\".format(repository, tag)\n\n            if repo_tag not in self.repo_tags:\n                logger.info(\"Tagging Image: {0} Repo Tag: {1}\".format(self.identifier, repo_tag))\n                self.repo_tags = self.repo_tags + (repo_tag, )\n\n                # always going to force tags until a feature is added to allow users to specify.\n                try:\n                    self.client.tag(self.id, repository, tag)\n                except:\n                    self.client.tag(self.id, repository, tag, force=True)"}, {"instruction": "def db_for_write(self, model, **hints):\n        \"\"\"\n        Prevent write actions on read-only tables.\n\n        Raises:\n            WriteNotSupportedError: If models.sf_access is ``read_only``.\n\n        \"\"\"\n", "input": "", "output": "        try:\n            if model.sf_access == READ_ONLY:\n                raise WriteNotSupportedError(\"%r is a read-only model.\" % model)\n        except AttributeError:\n            pass\n        return None"}, {"instruction": "def _set_verbosity(self):\n        '''\n        Sets the appropriate verbosity.\n        Must be called after self._test_target_files so that self.target_files is properly set.\n        '''\n", "input": "", "output": "        # If more than one target file was specified, enable verbose mode; else, there is\n        # nothing in some outputs to indicate which scan corresponds to which\n        # file.\n        if len(self.target_files) > 1 and not self.verbose:\n            self.verbose = True"}, {"instruction": "def run(self, app):\n        \"\"\"Function starts the web server given configuration.\"\"\"\n", "input": "", "output": "        GlimLog.info('Glim server started on %s environment' % self.args.env)\n        try:\n            kwargs = Config.get('app.server.options')\n            run(app.wsgi,\n                host=Config.get('app.server.host'),\n                port=Config.get('app.server.port'),\n                debug=Config.get('app.server.debugger'),\n                reloader=Config.get('app.server.reloader'),\n                server=Config.get('app.server.wsgi'),\n                **kwargs)\n        except Exception as e:\n            print(traceback.format_exc())\n            exit()"}, {"instruction": "def job_info(self, **kwargs):\n        \"\"\"\n        Get the information about the jobs returned by a particular search.\n        See the [GetJobs][] documentation for more info.\n\n        [GetJobs]: http://casjobs.sdss.org/casjobs/services/jobs.asmx?op=GetJobs\n\n        \"\"\"\n", "input": "", "output": "        search = \";\".join([\"%s : %s\"%(k, str(kwargs[k])) for k in kwargs])\n        params = {\"owner_wsid\": self.userid, \"owner_pw\": self.password,\n                \"conditions\": search, \"includeSystem\": False}\n        r = self._send_request(\"GetJobs\", params=params)\n        results = []\n        for n in minidom.parseString(r.text).getElementsByTagName(\"CJJob\"):\n            results.append({})\n            for e in n.childNodes:\n                if e.nodeType != e.TEXT_NODE:\n                    results[-1][e.tagName] = e.firstChild.data\n        return results"}, {"instruction": "def _shape(self):\n        \"\"\" Returns the shape of the data array associated with this file.\"\"\"\n", "input": "", "output": "        hdu = self.open()\n        _shape = hdu.shape\n        if not self.inmemory:\n            self.close()\n            del hdu\n        return _shape"}, {"instruction": "def _request(self, typ, id=0, method='GET', params=None, data=None, url=None):\n        \"\"\"\n        send the request, return response obj\n        \"\"\"\n", "input": "", "output": "\n        headers = { \"Accept\": \"application/json\" }\n        auth = None\n\n        if self.user:\n            auth = (self.user, self.password)\n\n        if not url:\n            if id:\n                url = \"%s/%s/%s\" % (self.url, typ, id)\n            else:\n                url = \"%s/%s\" % (self.url, typ)\n\n        return requests.request(method, url, params=params, data=data, auth=auth, headers=headers)"}, {"instruction": "def cartesian_to_spherical(vectors):\n    \"\"\"\n    Return the spherical coordinates for coordinates in Cartesian space.\n\n    This function does an opposite to :func:`spherical_to_cartesian`.\n\n    :param vectors:\n        Array of 3d vectors in Cartesian space of shape (..., 3)\n    :returns:\n        Tuple of three arrays of the same shape as ``vectors`` representing\n        longitude (decimal degrees), latitude (decimal degrees) and depth (km)\n        in specified order.\n    \"\"\"\n", "input": "", "output": "    rr = numpy.sqrt(numpy.sum(vectors * vectors, axis=-1))\n    xx, yy, zz = vectors.T\n    lats = numpy.degrees(numpy.arcsin((zz / rr).clip(-1., 1.)))\n    lons = numpy.degrees(numpy.arctan2(yy, xx))\n    depths = EARTH_RADIUS - rr\n    return lons.T, lats.T, depths"}, {"instruction": "def contains_one_of(self, elements):\n        \"\"\"\n        Ensures :attr:`subject` contains exactly one of *elements*, which must be an iterable.\n        \"\"\"\n", "input": "", "output": "        if sum(e in self._subject for e in elements) != 1:\n            raise self._error_factory(_format(\"Expected {} to have exactly one of {}\", self._subject, elements))\n        return ChainInspector(self._subject)"}, {"instruction": "def get_automation(self, automation_id, refresh=False):\n        \"\"\"Get a single automation.\"\"\"\n", "input": "", "output": "        if self._automations is None:\n            self.get_automations()\n            refresh = False\n\n        automation = self._automations.get(str(automation_id))\n\n        if automation and refresh:\n            automation.refresh()\n\n        return automation"}, {"instruction": "def show_run(command_history_id):\n    \"\"\"\n    Show detailed command history by its ID.\n    \"\"\"\n", "input": "", "output": "    from pprint import pprint\n    from .config import ConfigStore\n    from .database import DataBase\n    db = DataBase(ConfigStore().db_path)\n    with db.connection():\n        for ch_id in command_history_id:\n            crec = db.get_full_command_record(ch_id)\n            pprint(crec.__dict__)\n            print(\"\")"}, {"instruction": "def _floatize_x(x, new_x):\n    \"\"\" Make x and new_x float.\n    This is particulary useful for datetime dtype.\n    x, new_x: tuple of np.ndarray\n    \"\"\"\n", "input": "", "output": "    x = list(x)\n    new_x = list(new_x)\n    for i in range(len(x)):\n        if _contains_datetime_like_objects(x[i]):\n            # Scipy casts coordinates to np.float64, which is not accurate\n            # enough for datetime64 (uses 64bit integer).\n            # We assume that the most of the bits are used to represent the\n            # offset (min(x)) and the variation (x - min(x)) can be\n            # represented by float.\n            xmin = x[i].values.min()\n            x[i] = x[i]._to_numeric(offset=xmin, dtype=np.float64)\n            new_x[i] = new_x[i]._to_numeric(offset=xmin, dtype=np.float64)\n    return x, new_x"}, {"instruction": "def _read_depth_image(self):\n        \"\"\" Reads a depth image from the device \"\"\"\n", "input": "", "output": "        # read raw uint16 buffer\n        im_arr = self._depth_stream.read_frame()\n        raw_buf = im_arr.get_buffer_as_uint16()\n        buf_array = np.array([raw_buf[i] for i in range(PrimesenseSensor.DEPTH_IM_WIDTH * PrimesenseSensor.DEPTH_IM_HEIGHT)])\n\n        # convert to image in meters\n        depth_image = buf_array.reshape(PrimesenseSensor.DEPTH_IM_HEIGHT,\n                                        PrimesenseSensor.DEPTH_IM_WIDTH)\n        depth_image = depth_image * MM_TO_METERS # convert to meters\n        if self._flip_images:\n            depth_image = np.flipud(depth_image)\n        else:\n            depth_image = np.fliplr(depth_image)\n        return DepthImage(depth_image, frame=self._frame)"}, {"instruction": "def __upload(self, resource, bytes):\n        \"\"\"Performs a single chunk upload.\"\"\"\n", "input": "", "output": "\n        # note: string conversion required here due to open encoding bug in requests-oauthlib.\n        headers = {\n            'x-ton-expires': http_time(self.options.get('x-ton-expires', self._DEFAULT_EXPIRE)),\n            'content-length': str(self._file_size),\n            'content-type': self.content_type\n        }\n\n        return Request(self._client, 'post', resource,\n                       domain=self._DEFAULT_DOMAIN, headers=headers, body=bytes).perform()"}, {"instruction": "def _iterparse(xmlfile):\n    \"\"\"\n    Avoid bug in python 3.{2,3}. See http://bugs.python.org/issue9257.\n\n    :param xmlfile: XML file or file-like object\n    \"\"\"\n", "input": "", "output": "    try:\n        return ET.iterparse(xmlfile, events=(\"start-ns\", ))\n    except TypeError:\n        return ET.iterparse(xmlfile, events=(b\"start-ns\", ))"}, {"instruction": "def add_database_args(parser):\n    '''\n    Add a standard set of database arguments for argparse\n    '''\n", "input": "", "output": "    parser.add_argument(\n        'url',\n        nargs='?',\n        default='sqlite:///ncbi_taxonomy.db',\n        type=sqlite_default(),\n        help=('Database string URI or filename.  If no database scheme '\n              'specified \\\"sqlite:///\\\" will be prepended. [%(default)s]'))\n    db_parser = parser.add_argument_group(title='database options')\n\n    # TODO: better description of what --schema does\n    db_parser.add_argument(\n        '--schema',\n        help=('Name of SQL schema in database to query '\n              '(if database flavor supports this).'))\n\n    return parser"}, {"instruction": "def findnode(obj, path=''):\n    \"\"\"Returns a Node pointing to obj.\n    \n    If obj is a ctypes-derived class, an UnboundNode is returned.  If obj is\n    an instance of such a class, then a BoundNode will be returned.\n    \n    If the optional path is provided, it is a string to look up searching\n    down the original source node, such as '.overhead.window[2].page'\n    \"\"\"\n", "input": "", "output": "    if isclass(obj):\n        node = _createunbound(obj)\n    else:\n        node = _createbound(obj)\n    \n    # And walk it down.\n    pathparts = re.split(r'\\]?(?:[[.]|$)', path)\n    for part in pathparts:\n        if not part:    continue\n        try:\n            idx = int(part)\n            node = node[idx]\n        except ValueError:\n            node = node[part]\n    return node"}, {"instruction": "def ls(self, src, extra_args=[]):\n        '''List files in a directory'''\n", "input": "", "output": "        src = [self._full_hdfs_path(x) for x in src]\n        output = self._getStdOutCmd([self._hadoop_cmd, 'fs', '-ls'] + extra_args + src, True)\n        return self._transform_ls_output(output, self.hdfs_url)"}, {"instruction": "def keywords(s, top=10, **kwargs):\n    \"\"\" Returns a sorted list of keywords in the given string.\n    \"\"\"\n", "input": "", "output": "    return parser.find_keywords(s, top=top, frequency=parser.frequency)"}, {"instruction": "def kunc_dPdV(v, v0, k0, k0p, order=5, precision=1.e-5):\n    \"\"\"\n    calculate dP/dV for numerical calculation of bulk modulus\n    according to test this differs from analytical result by 1.e-5\n\n    :param v: unit-cell volume in A^3\n    :param v0: unit-cell volume in A^3 at 1 bar\n    :param k0: bulk modulus at reference conditions\n    :param k0p: pressure derivative of bulk modulus at reference conditions\n    :param precision: precision for numerical calc (default = 1.e-5 * v0)\n    :return: dP/dV\n    \"\"\"\n", "input": "", "output": "    def f_scalar(v, v0, k0, k0p, order=order, precision=1.e-5):\n        return derivative(kunc_p, v, args=(v0, k0, k0p, order),\n                          dx=v0 * precision)\n    f_v = np.vectorize(f_scalar, excluded=[1, 2, 3, 4, 5])\n    return f_v(v, v0, k0, k0p, order=order, precision=precision)"}, {"instruction": "def get_filebase(path, pattern):\n    \"\"\"Get the end of *path* of same length as *pattern*.\"\"\"\n", "input": "", "output": "    # A pattern can include directories\n    tail_len = len(pattern.split(os.path.sep))\n    return os.path.join(*str(path).split(os.path.sep)[-tail_len:])"}, {"instruction": "def half_light_radius_source(self, kwargs_source, center_x=0, center_y=0, deltaPix=None, numPix=None):\n        \"\"\"\n        computes numerically the half-light-radius of the deflector light and the total photon flux\n\n        :param kwargs_source:\n        :return:\n        \"\"\"\n", "input": "", "output": "        if numPix is None:\n            numPix = 1000\n        if deltaPix is None:\n            deltaPix = 0.005\n        x_grid, y_grid = util.make_grid(numPix=numPix, deltapix=deltaPix)\n        x_grid += center_x\n        y_grid += center_y\n        source_light = self.SourceModel.surface_brightness(x_grid, y_grid, kwargs_source)\n        R_h = analysis_util.half_light_radius(source_light, x_grid, y_grid, center_x=center_x, center_y=center_y)\n        return R_h"}, {"instruction": "def with_router(func):\n    \"\"\"\n    Decorator version of :func:`run_with_router`. Example:\n\n    .. code-block:: python\n\n        @with_router\n        def do_stuff(router, arg):\n            pass\n\n        do_stuff(blah, 123)\n    \"\"\"\n", "input": "", "output": "    def wrapper(*args, **kwargs):\n        return run_with_router(func, *args, **kwargs)\n    if mitogen.core.PY3:\n        wrapper.func_name = func.__name__\n    else:\n        wrapper.func_name = func.func_name\n    return wrapper"}, {"instruction": "def cloudant_iam(account_name, api_key, **kwargs):\n    \"\"\"\n    Provides a context manager to create a Cloudant session using IAM\n    authentication and provide access to databases, docs etc.\n\n    :param account_name: Cloudant account name.\n    :param api_key: IAM authentication API key.\n\n    For example:\n\n    .. code-block:: python\n\n        # cloudant context manager\n        from cloudant import cloudant_iam\n\n        with cloudant_iam(ACCOUNT_NAME, API_KEY) as client:\n            # Context handles connect() and disconnect() for you.\n            # Perform library operations within this context.  Such as:\n            print client.all_dbs()\n            # ...\n\n    \"\"\"\n", "input": "", "output": "    cloudant_session = Cloudant.iam(account_name, api_key, **kwargs)\n\n    cloudant_session.connect()\n    yield cloudant_session\n    cloudant_session.disconnect()"}, {"instruction": "def getoptS(X, Y, M_E, E):\r\n    ''' Find Sopt given X, Y\r\n    '''\n", "input": "", "output": "    n, r = X.shape\r\n    C = np.dot(np.dot(X.T, M_E), Y)\r\n    C = C.flatten()\r\n\r\n    A = np.zeros((r * r, r * r))\r\n    for i in range(r):\r\n        for j in range(r):\r\n            ind = j * r + i\r\n            temp = np.dot(\r\n                np.dot(X.T, np.dot(X[:, i, None], Y[:, j, None].T) * E), Y)\r\n            A[:, ind] = temp.flatten()\r\n\r\n    S = np.linalg.solve(A, C)\r\n\r\n    return np.reshape(S, (r, r)).T"}, {"instruction": "def update_rec(self, rec, name, value):\n        \"\"\"Update current GOTerm with optional record.\"\"\"\n", "input": "", "output": "        # 'def' is a reserved word in python, do not use it as a Class attr.\n        if name == \"def\":\n            name = \"defn\"\n\n        # If we have a relationship, then we will split this into a further\n        # dictionary.\n\n        if hasattr(rec, name):\n            if name not in self.attrs_scalar:\n                if name not in self.attrs_nested:\n                    getattr(rec, name).add(value)\n                else:\n                    self._add_nested(rec, name, value)\n            else:\n                raise Exception(\"ATTR({NAME}) ALREADY SET({VAL})\".format(\n                    NAME=name, VAL=getattr(rec, name)))\n        else: # Initialize new GOTerm attr\n            if name in self.attrs_scalar:\n                setattr(rec, name, value)\n            elif name not in self.attrs_nested:\n                setattr(rec, name, set([value]))\n            else:\n                name = '_{:s}'.format(name)\n                setattr(rec, name, defaultdict(list))\n                self._add_nested(rec, name, value)"}, {"instruction": "def update(self, new_email_address, name, access_level, password=None):\n        \"\"\"Updates the details for a person. Password is optional and is only updated if supplied.\"\"\"\n", "input": "", "output": "        params = {\"email\": self.email_address}\n        body = {\n            \"EmailAddress\": new_email_address,\n            \"Name\": name,\n            \"AccessLevel\": access_level,\n            \"Password\": password}\n        response = self._put(\"/clients/%s/people.json\" % self.client_id,\n                             body=json.dumps(body), params=params)\n        # Update self.email_address, so this object can continue to be used\n        # reliably\n        self.email_address = new_email_address"}, {"instruction": "def get_street(street, areacode, api=None):\n    \"\"\"\n    Retrieve streets in a given bounding area\n\n    :param overpy.Overpass api: First street of intersection\n    :param String street: Name of street\n    :param String areacode: The OSM id of the bounding area\n    :return: Parsed result\n    :raises overpy.exception.OverPyException: If something bad happens.\n    \"\"\"\n", "input": "", "output": "    if api is None:\n        api = overpy.Overpass()\n\n    query = "}, {"instruction": "def iter(self, bucket):\n        \"\"\"https://github.com/frictionlessdata/tableschema-bigquery-py#storage\n        \"\"\"\n", "input": "", "output": "\n        # Get schema/data\n        schema = tableschema.Schema(self.describe(bucket))\n        table_name = self.__mapper.convert_bucket(bucket)\n        response = self.__service.tabledata().list(\n            projectId=self.__project,\n            datasetId=self.__dataset,\n            tableId=table_name).execute()\n\n        # Collect rows\n        rows = []\n        for fields in response['rows']:\n            row = [field['v'] for field in fields['f']]\n            rows.append(row)\n\n        # Sort rows\n        # TODO: provide proper sorting solution\n        rows = sorted(rows, key=lambda row: row[0] if row[0] is not None else 'null')\n\n        # Emit rows\n        for row in rows:\n            row = self.__mapper.restore_row(row, schema=schema)\n            yield row"}, {"instruction": "def card_auth(self, auth_mode, block_address, key, uid):\n        \"\"\"\n        Authenticates to use specified block address. Tag must be selected using select_tag(uid) before auth.\n        auth_mode -- RFID.auth_a or RFID.auth_b\n        key -- list or tuple with six bytes key\n        uid -- list or tuple with four bytes tag ID\n        Returns error state.\n        \"\"\"\n", "input": "", "output": "        buf = []\n        buf.append(auth_mode)\n        buf.append(block_address)\n\n        for i in range(len(key)):\n            buf.append(key[i])\n\n        for i in range(4):\n            buf.append(uid[i])\n\n        (error, back_data, back_length) = self.card_write(self.mode_auth, buf)\n        if not (self.dev_read(0x08) & 0x08) != 0:\n            error = True\n\n        if not error:\n            self.authed = True\n\n        return error"}, {"instruction": "def _send(self, message):\n        \"\"\"\n        A helper method that does the actual sending\n\n        :param SmsMessage message: SmsMessage class instance.\n        :returns: True if message is sent else False\n        :rtype: bool\n        \"\"\"\n", "input": "", "output": "\n        params = {\n            'from': message.from_phone, \n            'to': \",\".join(message.to),\n            'text': message.body,\n            'api_key': self.get_api_key(),\n            'api_secret': self.get_api_secret(),\n        }\n\n\n        print(params)\n\n        logger.debug(\"POST to %r with body: %r\", NEXMO_API_URL, params)\n\n        return self.parse(NEXMO_API_URL, requests.post(NEXMO_API_URL, data=params))"}, {"instruction": "def _minimize_constraints_fun_summation(x):\n    '''\n    Minimize constraints fun summation\n    '''\n", "input": "", "output": "    summation = sum([x[i] for i in CONSTRAINT_PARAMS_IDX])\n    return CONSTRAINT_UPPERBOUND >= summation >= CONSTRAINT_LOWERBOUND"}, {"instruction": "def locate(command, on):\n    \"\"\"Locate the command's man page.\"\"\"\n", "input": "", "output": "    location = find_page_location(command, on)\n    click.echo(location)"}, {"instruction": "def get_collection(cls):\n        \"\"\"Return a reference to the database collection for the class\"\"\"\n", "input": "", "output": "\n        # By default the collection returned will be the published collection,\n        # however if the `draft` flag has been set against the global context\n        # (e.g `g`) then the collection returned will contain draft documents.\n\n        if g.get('draft'):\n            return getattr(\n                cls.get_db(),\n                '{collection}_draft'.format(collection=cls._collection)\n                )\n\n        return getattr(cls.get_db(), cls._collection)"}, {"instruction": "def license(self, license_id: str, token: dict = None, prot: str = \"https\") -> dict:\n        \"\"\"Get details about a specific license.\n\n        :param str token: API auth token\n        :param str license_id: license UUID\n        :param str prot: https [DEFAULT] or http\n         (use it only for dev and tracking needs).\n        \"\"\"\n", "input": "", "output": "        # handling request parameters\n        payload = {\"lid\": license_id}\n\n        # search request\n        license_url = \"{}://v1.{}.isogeo.com/licenses/{}\".format(\n            prot, self.api_url, license_id\n        )\n        license_req = self.get(\n            license_url,\n            headers=self.header,\n            params=payload,\n            proxies=self.proxies,\n            verify=self.ssl,\n        )\n\n        # checking response\n        checker.check_api_response(license_req)\n\n        # end of method\n        return license_req.json()"}, {"instruction": "def proofMethods():\n    \"\"\"\n    Run the full protocol including proof generation and verification.\n    \"\"\"\n", "input": "", "output": "    r, x = blind(m)\n    y,kw,tTilde = eval(w,t,x,msk,s)\n\n    # Proof in Gt/Gt\n    pi = proveGt(x, tTilde, kw, y)\n    verifyGt(x, tTilde, y, pi, errorOnFail=True)\n\n    # Proof in G1/Gt\n    pi = proveG1(x, tTilde, kw, y)\n    verifyG1(x, tTilde, y, pi, errorOnFail=True)\n\n    z = deblind(r, y)"}, {"instruction": "def delete_assessment(self, assessment):\n        \"\"\"\n        To delete a Assessment\n        :param assessment: string\n        \"\"\"\n", "input": "", "output": "        response = self.http.delete('/Assessment/' + str(assessment))\n        return response"}, {"instruction": "def _get_from_riak(self, key):\n        \"\"\"\n        Args:\n            key (str): riak key\n        Returns:\n            (tuple): riak obj json data and riak key\n        \"\"\"\n", "input": "", "output": "\n        obj = self.bucket.get(key)\n\n        if obj.exists:\n            return obj.data, obj.key\n\n        raise ObjectDoesNotExist(\"%s %s\" % (key, self.compiled_query))"}, {"instruction": "def sso(user, desired_username, name, email, profile_fields=None):\n    \"\"\"\n    Create a user, if the provided `user` is None, from the parameters.\n    Then log the user in, and return it.\n    \"\"\"\n", "input": "", "output": "    if not user:\n        if not settings.REGISTRATION_OPEN:\n            raise SSOError('Account registration is closed')\n        user = _create_desired_user(desired_username)\n        _configure_user(user, name, email, profile_fields)\n\n    if not user.is_active:\n        raise SSOError('Account disabled')\n\n    # login() expects the logging in backend to be set on the user.\n    # We are bypassing login, so fake it.\n    user.backend = settings.AUTHENTICATION_BACKENDS[0]\n    return user"}, {"instruction": "def ifaces(cls, name):\n        \"\"\" Get vlan attached ifaces. \"\"\"\n", "input": "", "output": "        ifaces = Iface.list({'vlan_id': cls.usable_id(name)})\n        ret = []\n        for iface in ifaces:\n            ret.append(Iface.info(iface['id']))\n        return ret"}, {"instruction": "def get_report(self):\n    \"\"\" describe the graph\n\n    :returns: report\n    :rtype: string\n    \"\"\"\n", "input": "", "output": "    ostr = ''\n    ostr += \"Nodes: \"+str(len(self.__nodes.keys()))+\"\\n\"\n    ostr += \"Edges: \"+str(len(self.__edges.keys()))+\"\\n\"\n    return ostr"}, {"instruction": "def apply_to_segmentlist(self, seglist):\n\t\t\"\"\"\n\t\tApply our low and high windows to the segments in a\n\t\tsegmentlist.\n\t\t\"\"\"\n", "input": "", "output": "\t\tfor i, seg in enumerate(seglist):\n\t\t\tseglist[i] = seg.__class__(seg[0] - self.low_window, seg[1] + self.high_window)"}, {"instruction": "def keys_by_alg_and_usage(self, issuer, alg, usage):\n        \"\"\"\n        Find all keys that can be used for a specific crypto algorithm and\n        usage by key owner.\n\n        :param issuer: Key owner\n        :param alg: a crypto algorithm\n        :param usage: What the key should be used for\n        :return: A possibly empty list of keys\n        \"\"\"\n", "input": "", "output": "        if usage in [\"sig\", \"ver\"]:\n            ktype = jws_alg2keytype(alg)\n        else:\n            ktype = jwe_alg2keytype(alg)\n\n        return self.get(usage, ktype, issuer)"}, {"instruction": "def preprocess_input(userinput):\n  \"\"\"\n  <Purpose>\n    Preprocess the raw command line input string.\n\n  <Arguments>\n    The raw command line input string.  We assume it is pre-stripped.\n\n  <Side Effects>\n    The string will be processed by each module that has a defined preprocessor.\n\n  <Exceptions>\n    None\n\n  <Returns>\n    The preprocessed string.\n  \"\"\"\n", "input": "", "output": "  for module in get_enabled_modules():\n    # Not every module has a preprocessor...\n    if 'input_preprocessor' in module_data[module]:\n      userinput = module_data[module]['input_preprocessor'](userinput)\n  return userinput"}, {"instruction": "def initialize_segment_register_x64(self, state, concrete_target):\n        \"\"\"\n        Set the gs register in the angr to the value of the fs register in the concrete process\n\n        :param state:               state which will be modified\n        :param concrete_target:     concrete target that will be used to read the fs register\n        :return: None\n       \"\"\"\n", "input": "", "output": "        _l.debug(\"Synchronizing gs segment register\")\n        state.regs.gs = self._read_gs_register_x64(concrete_target)"}, {"instruction": "def get_readwrite_instance(cls, working_dir, restore=False, restore_block_height=None):\n        \"\"\"\n        Get a read/write instance to the db, without the singleton check.\n        Used for low-level operations like db restore.\n        Not used in the steady state behavior of the system.\n        \"\"\"\n", "input": "", "output": "        log.warning(\"!!! Getting raw read/write DB instance !!!\")\n\n        import virtualchain_hooks\n        db_path = virtualchain.get_db_filename(virtualchain_hooks, working_dir)\n        db = BlockstackDB(db_path, DISPOSITION_RW, working_dir, get_genesis_block())\n        rc = db.db_setup()\n        if not rc:\n            if restore:\n                # restore from backup instead of bailing out\n                log.debug(\"Restoring from unclean shutdown\")\n                rc = db.db_restore(block_number=restore_block_height)\n                if rc:\n                    return db\n                else:\n                    log.error(\"Failed to restore from unclean shutdown\")\n\n            db.close()\n            raise Exception(\"Failed to set up db\")\n\n        return db"}, {"instruction": "def from_hertz(self, hertz, standard_pitch=440):\n        \"\"\"Set the Note name and pitch, calculated from the hertz value.\n\n        The standard_pitch argument can be used to set the pitch of A-4,\n        from which the rest is calculated.\n        \"\"\"\n", "input": "", "output": "        value = ((log((float(hertz) * 1024) / standard_pitch, 2) +\n            1.0 / 24) * 12 + 9)  # notes.note_to_int(\"A\")\n        self.name = notes.int_to_note(int(value) % 12)\n        self.octave = int(value / 12) - 6\n        return self"}, {"instruction": "def CanSetRoleTo(self, Role):\n        \"\"\"Checks if the new role can be applied to the member.\n\n        :Parameters:\n          Role : `enums`.chatMemberRole*\n            New chat member role.\n\n        :return: True if the new role can be applied, False otherwise.\n        :rtype: bool\n        \"\"\"\n", "input": "", "output": "        t = self._Owner._Alter('CHATMEMBER', self.Id, 'CANSETROLETO', Role,\n                               'ALTER CHATMEMBER CANSETROLETO')\n        return (chop(t, 1)[-1] == 'TRUE')"}, {"instruction": "def query(self, queryEngine, query=None, vendorSpecific=None, **kwargs):\n        \"\"\"See Also: queryResponse()\n\n        Args:\n          queryEngine:\n          query:\n          vendorSpecific:\n          **kwargs:\n\n        Returns:\n\n        \"\"\"\n", "input": "", "output": "        response = self.queryResponse(queryEngine, query, vendorSpecific, **kwargs)\n        return self._read_stream_response(response)"}, {"instruction": "def replay_scope(self, sess):\n    \"\"\"Enters a replay scope that unsets it at the end.\"\"\"\n", "input": "", "output": "    current_replay = self.replay(sess)\n    try:\n      self.set_replay(sess, True)\n      yield\n    finally:\n      self.set_replay(sess, current_replay)"}, {"instruction": "def runUAT(self, args):\n\t\t\"\"\"\n\t\tRuns the Unreal Automation Tool with the supplied arguments\n\t\t\"\"\"\n", "input": "", "output": "\t\tUtility.run([self.getRunUATScript()] + args, cwd=self.getEngineRoot(), raiseOnError=True)"}, {"instruction": "def reflected_light_intensity(self):\n        \"\"\"\n        A measurement of the reflected light intensity, as a percentage.\n        \"\"\"\n", "input": "", "output": "        self._ensure_mode(self.MODE_REFLECT)\n        return self.value(0) * self._scale('REFLECT')"}, {"instruction": "def find_compatible_interpreters(pex_python_path=None, compatibility_constraints=None):\n  \"\"\"Find all compatible interpreters on the system within the supplied constraints and use\n     PEX_PYTHON_PATH if it is set. If not, fall back to interpreters on $PATH.\n  \"\"\"\n", "input": "", "output": "  if pex_python_path:\n    interpreters = []\n    for binary in pex_python_path.split(os.pathsep):\n      try:\n        interpreters.append(PythonInterpreter.from_binary(binary))\n      except Executor.ExecutionError:\n        print(\"Python interpreter %s in PEX_PYTHON_PATH failed to load properly.\" % binary,\n          file=sys.stderr)\n    if not interpreters:\n      die('PEX_PYTHON_PATH was defined, but no valid interpreters could be identified. Exiting.')\n  else:\n    # We may have been invoked with a specific interpreter not on the $PATH, make sure our\n    # sys.executable is included as a candidate in this case.\n    interpreters = OrderedSet([PythonInterpreter.get()])\n\n    # Add all qualifying interpreters found in $PATH.\n    interpreters.update(PythonInterpreter.all())\n\n  return list(\n    matched_interpreters(interpreters, compatibility_constraints)\n    if compatibility_constraints\n    else interpreters\n  )"}, {"instruction": "def diam_floc_vel_term(ConcAl, ConcClay, coag, material,\n                       DIM_FRACTAL, VelTerm, Temp):\n    \"\"\"Calculate floc diamter as a function of terminal velocity.\"\"\"\n", "input": "", "output": "    WaterDensity = pc.density_water(Temp).magnitude\n    return (material.Diameter * (((18 * VelTerm * PHI_FLOC\n                          * pc.viscosity_kinematic(Temp).magnitude\n                          )\n                         / (pc.gravity.magnitude * material.Diameter**2)\n                         )\n                         * (WaterDensity\n                            / (dens_floc_init(ConcAl, ConcClay, coag,\n                                              material).magnitude\n                               - WaterDensity\n                               )\n                            )\n                        ) ** (1 / (DIM_FRACTAL - 1))\n            )"}, {"instruction": "def get_report(session, vehicle_index):\n    \"\"\"Get vehicle health report summary.\"\"\"\n", "input": "", "output": "    vhr = get_vehicle_health_report(session, vehicle_index)\n    if 'reportCard' not in vhr:\n        raise MoparError(\"no vhr found\")\n    return _traverse_report(vhr['reportCard'])"}, {"instruction": "def scalar_summary(tag, scalar):\n    \"\"\"Outputs a `Summary` protocol buffer containing a single scalar value.\n    The generated Summary has a Tensor.proto containing the input Tensor.\n    Adapted from the TensorFlow function `scalar()` at\n    https://github.com/tensorflow/tensorflow/blob/r1.6/tensorflow/python/summary/summary.py\n\n    Parameters\n    ----------\n      tag : str\n          A name for the generated summary. Will also serve as the series name in TensorBoard.\n      scalar : int, MXNet `NDArray`, or `numpy.ndarray`\n          A scalar value or an ndarray of shape (1,).\n\n    Returns\n    -------\n      A `Summary` protobuf of the `scalar` value.\n\n    Raises\n    ------\n      ValueError: If the scalar has the wrong shape or type.\n    \"\"\"\n", "input": "", "output": "    tag = _clean_tag(tag)\n    scalar = _make_numpy_array(scalar)\n    assert(scalar.squeeze().ndim == 0), 'scalar should be 0D'\n    scalar = float(scalar)\n    return Summary(value=[Summary.Value(tag=tag, simple_value=scalar)])"}, {"instruction": "def to_pixel(self, wcs, mode='all'):\n        \"\"\"\n        Convert the aperture to a `RectangularAnnulus` object defined in\n        pixel coordinates.\n\n        Parameters\n        ----------\n        wcs : `~astropy.wcs.WCS`\n            The world coordinate system (WCS) transformation to use.\n\n        mode : {'all', 'wcs'}, optional\n            Whether to do the transformation including distortions\n            (``'all'``; default) or only including only the core WCS\n            transformation (``'wcs'``).\n\n        Returns\n        -------\n        aperture : `RectangularAnnulus` object\n            A `RectangularAnnulus` object.\n        \"\"\"\n", "input": "", "output": "\n        pixel_params = self._to_pixel_params(wcs, mode=mode)\n        return RectangularAnnulus(**pixel_params)"}, {"instruction": "def get(self,table, sys_id):\n        \"\"\"\n        get a single record by table name and sys_id\n        returns a dict (the json map) for python 3.4\n        \"\"\"\n", "input": "", "output": "        result = self.table_api_get(table, sys_id)\n        return self.to_record(result, table)"}, {"instruction": "def _format_executable(lines, element, spacer=\"\"):\n    \"\"\"Performs formatting specific to a Subroutine or Function code\n    element for relevant docstrings.\n    \"\"\"\n", "input": "", "output": "    rlines = []\n    rlines.append(element.signature)\n    _format_summary(rlines, element)\n\n    rlines.append(\"\")\n    rlines.append(\"PARAMETERS\")\n    for p in element.ordered_parameters:\n        _format_value_element(rlines, p)\n\n    rlines.append(\"\")\n    _format_generic(rlines, element, [\"summary\"])\n\n    #Subroutines can have embedded types and functions which need to be handled.\n    if len(element.types) > 0:\n        rlines.append(\"\\nEMBEDDED TYPES\")\n        for key, value in list(element.types.items()):\n            _format_type(rlines, value, \"  \")\n\n    if len(element.executables) > 0:\n        rlines.append(\"\\nEMBEDDED EXECUTABLES\")\n        for key, value in list(element.executables.items()):\n            _format_executable(rlines, value, \"  \")\n\n    lines.extend([spacer + l for l in rlines])"}, {"instruction": "def fast_hash(self):\n        \"\"\"\n        Get a CRC32 or xxhash.xxh64 reflecting the DataStore.\n\n        Returns\n        ------------\n        hashed: int, checksum of data\n        \"\"\"\n", "input": "", "output": "        fast = sum(i.fast_hash() for i in self.data.values())\n        return fast"}, {"instruction": "def get_attributes(self):\n        \"\"\"\n        Used by the uni_form_tags to get helper attributes\n        \"\"\"\n", "input": "", "output": "        items = {}\n        items['form_method'] = self.form_method.strip()\n        items['form_tag'] = self.form_tag\n        items['form_style'] = self.form_style.strip()\n        \n        if self.form_action:\n            items['form_action'] = self.form_action.strip()\n        if self.form_id:\n            items['id'] = self.form_id.strip()\n        if self.form_class:\n            items['class'] = self.form_class.strip()\n        if self.inputs:\n            items['inputs'] = self.inputs\n        if self.form_error_title:\n            items['form_error_title'] = self.form_error_title.strip()\n        if self.formset_error_title:\n            items['formset_error_title'] = self.formset_error_title.strip()\n        return items"}, {"instruction": "def extract_files_from_dict(d):\n    \"\"\"Return any file objects from the provided dict.\n\n    >>> extract_files_from_dict({\n    ... 'oauth_token': 'foo',\n    ... 'track': {\n    ...   'title': 'bar',\n    ...   'asset_data': open('setup.py', 'rb')\n    ...  }})  # doctest:+ELLIPSIS\n    {'track': {'asset_data': <...}}\n    \"\"\"\n", "input": "", "output": "    files = {}\n    for key, value in six.iteritems(d):\n        if isinstance(value, dict):\n            files[key] = extract_files_from_dict(value)\n        elif is_file_like(value):\n            files[key] = value\n    return files"}, {"instruction": "def cat_net_img(url='', indent=4, img_height=0, img_cache_dir='/tmp', use_cache=False):\n    \"\"\"\n        - \u4f18\u5148 \u4ece\u5fae\u535a\u7f13\u5b58\u76ee\u5f55\u8bfb\u53d6\u56fe\u7247\n        - \u5982\u679c\u5931\u8d25, \u518d\u4ece\u76f8\u5e94\u7684url\u8bfb\u53d6\u7167\u7247\n\n    :param use_cache: ``\u4f7f\u7528\u7f13\u5b58``\n    :type use_cache:\n    :param img_cache_dir:\n    :type img_cache_dir:\n    :param url:\n    :type url:\n    :param indent:\n    :type indent:\n    :param img_height:\n    :type img_height:\n    :return:\n    :rtype:\n    \"\"\"\n", "input": "", "output": "    name = url.split('/')[-1]\n    pth = os.path.join(img_cache_dir, name)\n\n    # \u5982\u679c\u4e0d\u4f7f\u7528\u7f13\u5b58 \u6216\u8005 \u6587\u4ef6\u4e0d\u5b58\u5728, \u5219\u5148\u4e0b\u8f7d\u5230\u672c\u5730, \u7136\u540e\u518d\u8bfb\u53d6\n    if not use_cache or not is_file_ok(pth):\n        raw = requests.get(url)\n        write_file(raw.content, pth)\n\n    with textui.indent(indent, quote=' {}'.format(' ')):\n        textui.puts(cat_img_by_path(pth, img_height))"}, {"instruction": "def codepoints2bitstream(codepoints):\n    \"\"\"\n    >>> list(codepoints2bitstream([0x48,0x45]))\n    [0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0]\n    >>> list(codepoints2bitstream(0x48))\n    [0, 0, 0, 1, 0, 0, 1, 0]\n    \"\"\"\n", "input": "", "output": "    if isinstance(codepoints, int):\n        codepoints = [codepoints]\n    for codepoint in codepoints:\n        bit_string = byte2bit_string(codepoint)\n        for bit in bit_string:\n            yield int(bit)"}, {"instruction": "def _reduce_boolean_pair(self, config_dict, key1, key2):\n        \"\"\"Ensure only one key with a boolean value is present in dict.\n\n        :param config_dict: dict -- dictionary of config or kwargs\n        :param key1: string -- first key name\n        :param key2: string -- second key name\n        :raises: BooleansToReduceHaveSameValue\n        \"\"\"\n", "input": "", "output": "\n        if key1 in config_dict and key2 in config_dict \\\n                and config_dict[key1] == config_dict[key2]:\n            msg = 'Boolean pair, %s and %s, have same value: %s. If both ' \\\n                'are given to this method, they cannot be the same, as this ' \\\n                'method cannot decide which one should be True.' \\\n                % (key1, key2, config_dict[key1])\n            raise BooleansToReduceHaveSameValue(msg)\n        elif key1 in config_dict and not config_dict[key1]:\n            config_dict[key2] = True\n            config_dict.pop(key1)\n        elif key2 in config_dict and not config_dict[key2]:\n            config_dict[key1] = True\n            config_dict.pop(key2)\n        return config_dict"}, {"instruction": "def sendmail(self, to, message):\n        \"\"\"Send mail to one or more recipients. The required arguments are a\n        list of RFC 822 to-address strings (a bare string will be treated as a\n        list with 1 address), and a message string.\n\n        \"\"\"\n", "input": "", "output": "\n        # If we were passed a bare string as the To: address, convert it to\n        # a single element list.\n        if isinstance(to, str):\n            to = [ to, ]\n\n        # Send one email with the appropriate recipient list.\n        server = self._smtp_server()\n        server.sendmail(self.get_rfc2822_address(), to, message)\n        server.quit()"}, {"instruction": "def asset(self, asset_id, asset_type, action='GET'):\n        \"\"\"\n        Gets the asset with the provided id\n        Args:\n            asset_id: The id of the asset to be retrieved\n            asset_type: (str) Either PHONE, HANDLER, or URL\n            action:\n\n        Returns:\n\n        \"\"\"\n", "input": "", "output": "        if not self.can_update():\n            self._tcex.handle_error(910, [self.type])\n\n        if asset_type == 'PHONE':\n            return self.tc_requests.adversary_phone_asset(\n                self.api_type, self.api_sub_type, self.unique_id, asset_id, action=action\n            )\n        if asset_type == 'HANDLER':\n            return self.tc_requests.adversary_handle_asset(\n                self.api_type, self.api_sub_type, self.unique_id, asset_id, action=action\n            )\n        if asset_type == 'URL':\n            return self.tc_requests.adversary_url_asset(\n                self.api_type, self.api_sub_type, self.unique_id, asset_id, action=action\n            )\n        self._tcex.handle_error(\n            925, ['asset_type', 'assets', 'asset_type', 'asset_type', asset_type]\n        )\n        return None"}, {"instruction": "def calculate_cardinality(\n        angle,\n        earthquake_hazard=None,\n        place_exposure=None\n):\n    \"\"\"Simple postprocessor where we compute the cardinality of an angle.\n\n    :param angle: Bearing angle.\n    :type angle: float\n\n    :param earthquake_hazard: The hazard to use.\n    :type earthquake_hazard: str\n\n    :param place_exposure: The exposure to use.\n    :type place_exposure: str\n\n    :return: Cardinality text.\n    :rtype: str\n    \"\"\"\n", "input": "", "output": "    # this method could still be improved later, since the acquisition interval\n    # is a bit strange, i.e the input angle of 22.499\u00b0 will return `N` even\n    # though 22.5\u00b0 is the direction for `NNE`\n    _ = earthquake_hazard, place_exposure  # NOQA\n\n    direction_list = tr(\n        'N,NNE,NE,ENE,E,ESE,SE,SSE,S,SSW,SW,WSW,W,WNW,NW,NNW'\n    ).split(',')\n\n    bearing = float(angle)\n    direction_count = len(direction_list)\n    direction_interval = 360. / direction_count\n    index = int(floor(bearing / direction_interval))\n    index %= direction_count\n    return direction_list[index]"}, {"instruction": "def cli(yamlfile, format, classes, directory):\n    \"\"\" Generate a UML representation of a biolink model \"\"\"\n", "input": "", "output": "    print(YumlGenerator(yamlfile, format).serialize(classes=classes, directory=directory), end=\"\")"}, {"instruction": "def get_prep_value(self, value):\n        \"\"\"Returns field's value prepared for saving into a database.\"\"\"\n", "input": "", "output": "\n        if isinstance(value, LocalizedValue):\n            prep_value = LocalizedValue()\n            for k, v in value.__dict__.items():\n                if v is None:\n                    prep_value.set(k, '')\n                else:\n                    # Need to convert File objects provided via a form to\n                    # unicode for database insertion\n                    prep_value.set(k, six.text_type(v))\n            return super().get_prep_value(prep_value)\n        return super().get_prep_value(value)"}, {"instruction": "def protocol_authenticate(self, account=None):\n        \"\"\"\n        Low-level API to perform protocol-level authentication on protocols\n        that support it.\n\n        .. HINT::\n           In most cases, you want to use the login() method instead, as\n           it automatically chooses the best login method for each protocol.\n\n        :type  account: Account\n        :param account: An account object, like login().\n        \"\"\"\n", "input": "", "output": "        with self._get_account(account) as account:\n            user = account.get_name()\n            password = account.get_password()\n            key = account.get_key()\n            if key is None:\n                self._dbg(1, \"Attempting to authenticate %s.\" % user)\n                self._protocol_authenticate(user, password)\n            else:\n                self._dbg(1, \"Authenticate %s with key.\" % user)\n                self._protocol_authenticate_by_key(user, key)\n        self.proto_authenticated = True"}, {"instruction": "def iso8601_date(d):\n    \"\"\"\n    Return a string representation of a date that the Twilio API understands\n    Format is YYYY-MM-DD. Returns None if d is not a string, datetime, or date\n    \"\"\"\n", "input": "", "output": "    if d == values.unset:\n        return d\n    elif isinstance(d, datetime.datetime):\n        return str(d.date())\n    elif isinstance(d, datetime.date):\n        return str(d)\n    elif isinstance(d, str):\n        return d"}, {"instruction": "def find_undeclared_variables(ast):\n    \"\"\"Returns a set of all variables in the AST that will be looked up from\n    the context at runtime.  Because at compile time it's not known which\n    variables will be used depending on the path the execution takes at\n    runtime, all variables are returned.\n\n    >>> from jinja2 import Environment, meta\n    >>> env = Environment()\n    >>> ast = env.parse('{% set foo = 42 %}{{ bar + foo }}')\n    >>> meta.find_undeclared_variables(ast) == set(['bar'])\n    True\n\n    .. admonition:: Implementation\n\n       Internally the code generator is used for finding undeclared variables.\n       This is good to know because the code generator might raise a\n       :exc:`TemplateAssertionError` during compilation and as a matter of\n       fact this function can currently raise that exception as well.\n    \"\"\"\n", "input": "", "output": "    codegen = TrackingCodeGenerator(ast.environment)\n    codegen.visit(ast)\n    return codegen.undeclared_identifiers"}, {"instruction": "def find_class_files(self):\n        \"\"\"Find compiled class files recursively in the root path\n\n        :return: list of absolute file paths\n        \"\"\"\n", "input": "", "output": "        files = self._find_files()\n        self.announce(\n            \"found '{}' compiled class files in '{}'\".format(\n                len(files), self.root\n            )\n        )\n        return files"}, {"instruction": "def format_command(\n    command_args,  # type: List[str]\n    command_output,  # type: str\n):\n    # type: (...) -> str\n    \"\"\"\n    Format command information for logging.\n    \"\"\"\n", "input": "", "output": "    text = 'Command arguments: {}\\n'.format(command_args)\n\n    if not command_output:\n        text += 'Command output: None'\n    elif logger.getEffectiveLevel() > logging.DEBUG:\n        text += 'Command output: [use --verbose to show]'\n    else:\n        if not command_output.endswith('\\n'):\n            command_output += '\\n'\n        text += (\n            'Command output:\\n{}'\n            '-----------------------------------------'\n        ).format(command_output)\n\n    return text"}, {"instruction": "def i18n(msg, event=None, lang='en', domain='backend'):\n    \"\"\"Gettext function wrapper to return a message in a specified language by domain\n\n    To use internationalization (i18n) on your messages, import it as '_' and use as usual.\n    Do not forget to supply the client's language setting.\"\"\"\n", "input": "", "output": "\n    if event is not None:\n        language = event.client.language\n    else:\n        language = lang\n\n    domain = Domain(domain)\n    return domain.get(language, msg)"}, {"instruction": "def mkdir(self, name=None, folder_id='0'):\n\t\t'''Create a folder with a specified \"name\" attribute.\n\t\t\tfolder_id allows to specify a parent folder.'''\n", "input": "", "output": "\t\treturn self( 'folders', method='post', encode='json',\n\t\t\tdata=dict(name=name, parent=dict(id=folder_id)) )"}, {"instruction": "def _sumterm_prime(lexer):\n    \"\"\"Return a sum term' expression, eliminates left recursion.\"\"\"\n", "input": "", "output": "    tok = next(lexer)\n    # '|' XORTERM SUMTERM'\n    if isinstance(tok, OP_or):\n        xorterm = _xorterm(lexer)\n        sumterm_prime = _sumterm_prime(lexer)\n        if sumterm_prime is None:\n            return xorterm\n        else:\n            return ('or', xorterm, sumterm_prime)\n    # null\n    else:\n        lexer.unpop_token(tok)\n        return None"}, {"instruction": "def into_buffer(self, buf, offset):\n    \"\"\"Produce a framed/packed SBP message into the provided buffer and offset.\n\n    \"\"\"\n", "input": "", "output": "    self.payload = containerize(exclude_fields(self))\n    self.parser = MsgEphemerisGPSDepF._parser\n    self.stream_payload.reset(buf, offset)\n    return self.pack_into(buf, offset, self._build_payload)"}, {"instruction": "def set_volume(self, volume):\n        \"\"\"Set a new volume level.\"\"\"\n", "input": "", "output": "        if volume > 100 or volume < 0:\n            raise Exception('Bad request to volume control. '\n                            'Must be between 0 and 100')\n        params = ('<InstanceID>0</InstanceID><Channel>Master</Channel>'\n                  '<DesiredVolume>{}</DesiredVolume>').format(volume)\n        self.soap_request(URL_CONTROL_DMR, URN_RENDERING_CONTROL,\n                          'SetVolume', params)"}, {"instruction": "def parse_structure(self, node):\n        \"\"\"\n        Parses <Structure>\n\n        @param node: Node containing the <Structure> element\n        @type node: xml.etree.Element\n        \"\"\"\n", "input": "", "output": "\n        self.current_structure = self.current_component_type.structure\n        self.process_nested_tags(node)\n        self.current_structure = None"}, {"instruction": "def decode(self, offset):\n        \"\"\"Decode a section of the data section starting at offset\n\n        Arguments:\n        offset -- the location of the data structure to decode\n        \"\"\"\n", "input": "", "output": "        new_offset = offset + 1\n        (ctrl_byte,) = struct.unpack(b'!B', self._buffer[offset:new_offset])\n        type_num = ctrl_byte >> 5\n        # Extended type\n        if not type_num:\n            (type_num, new_offset) = self._read_extended(new_offset)\n\n        (size, new_offset) = self._size_from_ctrl_byte(\n            ctrl_byte, new_offset, type_num)\n        return self._type_decoder[type_num](self, size, new_offset)"}, {"instruction": "def missing_nodes(self):\n        \"\"\"The set of targets known as dependencies but not yet defined.\"\"\"\n", "input": "", "output": "        missing = set()\n        for target_addr, target_attrs in self.graph.node.items():\n            if 'target_obj' not in target_attrs:\n                missing.add(target_addr)\n        return missing"}, {"instruction": "def _build_url(self):\n        \"\"\"Build url based on searching by date or by show.\"\"\"\n", "input": "", "output": "        url_params = [\n            BASE_URL, self.category + ' ratings', self.day, self.year, self.month\n        ]\n\n        return SEARCH_URL.format(*url_params)"}, {"instruction": "def search_registered_query_for_facets(self, **kwargs):  # noqa: E501\n        \"\"\"Lists the values of one or more facets over the customer's non-deleted derived metric definition  # noqa: E501\n\n          # noqa: E501\n        This method makes a synchronous HTTP request by default. To make an\n        asynchronous HTTP request, please pass async_req=True\n        >>> thread = api.search_registered_query_for_facets(async_req=True)\n        >>> result = thread.get()\n\n        :param async_req bool\n        :param FacetsSearchRequestContainer body:\n        :return: ResponseContainerFacetsResponseContainer\n                 If the method is called asynchronously,\n                 returns the request thread.\n        \"\"\"\n", "input": "", "output": "        kwargs['_return_http_data_only'] = True\n        if kwargs.get('async_req'):\n            return self.search_registered_query_for_facets_with_http_info(**kwargs)  # noqa: E501\n        else:\n            (data) = self.search_registered_query_for_facets_with_http_info(**kwargs)  # noqa: E501\n            return data"}, {"instruction": "def render(self, template, **data):\n        \"\"\"Renders the template using Jinja2 with given data arguments.\n\n        \"\"\"\n", "input": "", "output": "        if(type(template) != str):\n            raise TypeError(\"String expected\")\n        \n        env = Environment(\n            loader=FileSystemLoader(os.getcwd() + '/View'),\n            autoescape=select_autoescape()\n        )\n\n        template = env.get_template(template)\n        return self.finish(template.render(data))"}, {"instruction": "def post_optimization_step(self, batch_info, device, model, rollout):\n        \"\"\" Steps to take after optimization has been done\"\"\"\n", "input": "", "output": "        if batch_info.aggregate_batch_number % self.target_update_frequency == 0:\n            self.target_model.load_state_dict(model.state_dict())\n            self.target_model.eval()"}, {"instruction": "def path_pieces(vault_path):\n    \"\"\"Will return a two part tuple comprising of the vault path\n    and the key with in the stored object\"\"\"\n", "input": "", "output": "    path_bits = vault_path.split('/')\n    path = '/'.join(path_bits[0:len(path_bits) - 1])\n    key = path_bits[len(path_bits) - 1]\n    return path, key"}, {"instruction": "arg.toByte", "input": "Java", "output": "public static java.lang.Byte toByte(Object arg) throws NoSuchMethodException {\n        if (arg instanceof java.lang.Integer) return boxToByte((byte)unboxToInt(arg));\n        if (arg instanceof java.lang.Character) return boxToByte((byte)unboxToChar(arg));\n        if (arg instanceof java.lang.Byte) return (java.lang.Byte)arg;\n        if (arg instanceof java.lang.Long) return boxToByte((byte)unboxToLong(arg));\n        if (arg instanceof java.lang.Short) return boxToByte((byte)unboxToShort(arg));\n        if (arg instanceof java.lang.Float) return boxToByte((byte)unboxToFloat(arg));\n        if (arg instanceof java.lang.Double) return boxToByte((byte)unboxToDouble(arg));\n        throw new NoSuchMethodException();\n    }"}, {"instruction": "Read some bytes from the stream.\n@param b The byte array that will hold the contents from the stream.\n@param off The offset into the byte array at which bytes will start to be\nplaced.\n@param len the maximum number of bytes that can be returned.\n@return The number of bytes returned or -1 if the end of stream has been\nreached.\n@see java.io.InputStream#read(byte[], int, int)\n@throws IOException if an IO problem occurs.", "input": "Java", "output": "@Override\n    public int read (byte[] b, int off, int len) throws IOException {\n\n        if (closed) {\n            throw new IOException(\"Attempted read from closed stream.\");\n        }\n\n        if (eof) {\n            return -1;\n        }\n        if (pos >= chunkSize) {\n            nextChunk();\n            if (eof) {\n                return -1;\n            }\n        }\n        len = Math.min(len, chunkSize - pos);\n        int count = in.read(b, off, len);\n        pos += count;\n        return count;\n    }"}, {"instruction": "Gets the selected alerts from the {@link #treeAlert alerts tree}.\n\n@param allAlerts {@code true} if it should return all selected alerts, {@code false} to just return the first selected\nalert.\n@return a {@code Set} with the selected alerts, never {@code null}.", "input": "Java", "output": "private Set<Alert> getSelectedAlertsImpl(boolean allAlerts) {\r\n\t\tTreePath[] paths = getTreeAlert().getSelectionPaths();\r\n\t\tif (paths == null || paths.length == 0) {\r\n\t\t\treturn Collections.emptySet();\r\n\t\t}\r\n\r\n\t\tSet<Alert> alerts = new HashSet<>();\r\n\t\tif (!allAlerts) {\r\n\t\t\tDefaultMutableTreeNode alertNode = (DefaultMutableTreeNode) paths[0].getLastPathComponent();\r\n\t\t\talerts.add((Alert) alertNode.getUserObject());\r\n\t\t\treturn alerts;\r\n\t\t}\r\n\r\n\t\tfor (int i = 0; i < paths.length; i++) {\r\n\t\t\tDefaultMutableTreeNode alertNode = (DefaultMutableTreeNode) paths[i].getLastPathComponent();\r\n\t\t\tif (alertNode.getChildCount() == 0) {\r\n\t\t\t\talerts.add((Alert) alertNode.getUserObject());\r\n\t\t\t\tcontinue;\r\n\t\t\t}\r\n\t\t\tfor (int j = 0; j < alertNode.getChildCount(); j++) {\r\n\t\t\t\tDefaultMutableTreeNode node = (DefaultMutableTreeNode) alertNode.getChildAt(j);\r\n\t\t\t\talerts.add((Alert) node.getUserObject());\r\n\t\t\t}\r\n\t\t}\r\n\t\treturn alerts;\r\n\t}"}, {"instruction": "\u8ba1\u7b97\u5bf9\u8c61\u957f\u5ea6\uff0c\u5982\u679c\u662f\u5b57\u7b26\u4e32\u8c03\u7528\u5176length\u51fd\u6570\uff0c\u96c6\u5408\u7c7b\u8c03\u7528\u5176size\u51fd\u6570\uff0c\u6570\u7ec4\u8c03\u7528\u5176length\u5c5e\u6027\uff0c\u5176\u4ed6\u53ef\u904d\u5386\u5bf9\u8c61\u904d\u5386\u8ba1\u7b97\u957f\u5ea6<br>\n\u652f\u6301\u7684\u7c7b\u578b\u5305\u62ec\uff1a\n<ul>\n<li>CharSequence</li>\n<li>Map</li>\n<li>Iterator</li>\n<li>Enumeration</li>\n<li>Array</li>\n</ul>\n\n@param obj \u88ab\u8ba1\u7b97\u957f\u5ea6\u7684\u5bf9\u8c61\n@return \u957f\u5ea6", "input": "Java", "output": "public static int length(Object obj) {\r\n\t\tif (obj == null) {\r\n\t\t\treturn 0;\r\n\t\t}\r\n\t\tif (obj instanceof CharSequence) {\r\n\t\t\treturn ((CharSequence) obj).length();\r\n\t\t}\r\n\t\tif (obj instanceof Collection) {\r\n\t\t\treturn ((Collection<?>) obj).size();\r\n\t\t}\r\n\t\tif (obj instanceof Map) {\r\n\t\t\treturn ((Map<?, ?>) obj).size();\r\n\t\t}\r\n\r\n\t\tint count;\r\n\t\tif (obj instanceof Iterator) {\r\n\t\t\tIterator<?> iter = (Iterator<?>) obj;\r\n\t\t\tcount = 0;\r\n\t\t\twhile (iter.hasNext()) {\r\n\t\t\t\tcount++;\r\n\t\t\t\titer.next();\r\n\t\t\t}\r\n\t\t\treturn count;\r\n\t\t}\r\n\t\tif (obj instanceof Enumeration) {\r\n\t\t\tEnumeration<?> enumeration = (Enumeration<?>) obj;\r\n\t\t\tcount = 0;\r\n\t\t\twhile (enumeration.hasMoreElements()) {\r\n\t\t\t\tcount++;\r\n\t\t\t\tenumeration.nextElement();\r\n\t\t\t}\r\n\t\t\treturn count;\r\n\t\t}\r\n\t\tif (obj.getClass().isArray() == true) {\r\n\t\t\treturn Array.getLength(obj);\r\n\t\t}\r\n\t\treturn -1;\r\n\t}"}, {"instruction": "Evaluate on a directory containing a set of DataSet objects to be loaded with a {@link DataSetLoader}.\nUses default batch size of {@link #DEFAULT_EVAL_SCORE_BATCH_SIZE}\n@param path Path/URI to the directory containing the datasets to load\n@return Evaluation", "input": "Java", "output": "public <T extends Evaluation> T evaluate(String path, int batchSize, DataSetLoader loader){\n        JavaRDD<String> paths;\n        try {\n            paths = SparkUtils.listPaths(sc, path);\n        } catch (IOException e) {\n            throw new RuntimeException(\"Error listing paths in directory\", e);\n        }\n\n        JavaRDD<DataSet> rdd = paths.map(new LoadDataSetFunction(loader, new RemoteFileSourceFactory(BroadcastHadoopConfigHolder.get(sc))));\n        return (T)doEvaluation(rdd, batchSize, new org.deeplearning4j.eval.Evaluation())[0];\n    }"}, {"instruction": "Removes decision from ldap attribute set.\n\n@param ldapConsent the ldap attribute holding consent decisions\n@param decisionId  the decision Id\n@return the new decision set", "input": "Java", "output": "private static Set<String> removeDecision(final LdapAttribute ldapConsent, final long decisionId) {\n        val result = new HashSet<String>();\n        if (ldapConsent.size() != 0) {\n            ldapConsent.getStringValues()\n                .stream()\n                .map(LdapConsentRepository::mapFromJson)\n                .filter(d -> d.getId() != decisionId)\n                .map(LdapConsentRepository::mapToJson)\n                .filter(Objects::nonNull)\n                .forEach(result::add);\n        }\n        return result;\n    }"}, {"instruction": "Cancel Hadoop Tokens\n\n@param logger logger handler", "input": "Java", "output": "public void cancelHadoopTokens(final Logger logger) {\n    if (tokenFile == null) {\n      return;\n    }\n    try {\n      hadoopSecurityManager.cancelTokens(tokenFile, userToProxy, logger);\n    } catch (HadoopSecurityManagerException e) {\n      logger.error(e.getCause() + e.getMessage());\n    } catch (Exception e) {\n      logger.error(e.getCause() + e.getMessage());\n    }\n    if (tokenFile.exists()) {\n      tokenFile.delete();\n    }\n  }"}, {"instruction": "To assist boot failure script, record the number of boot attempts.\nThis file gets deleted in case of successful boot.\n\n@see BootFailure", "input": "Java", "output": "private void recordBootAttempt(File home) {\n        try (OutputStream o=Files.newOutputStream(BootFailure.getBootFailureFile(home).toPath(), StandardOpenOption.CREATE, StandardOpenOption.APPEND)) {\n            o.write((new Date().toString() + System.getProperty(\"line.separator\", \"\\n\")).toString().getBytes());\n        } catch (IOException | InvalidPathException e) {\n            LOGGER.log(WARNING, \"Failed to record boot attempts\",e);\n        }\n    }"}, {"instruction": "Register SQL.\n\n@param sql SQL\n@param parametersCount parameters count\n@return statement ID", "input": "Java", "output": "public synchronized int register(final String sql, final int parametersCount) {\n        Integer result = statementIdAssigner.get(sql);\n        if (null != result) {\n            return result;\n        }\n        result = sequence.incrementAndGet();\n        statementIdAssigner.putIfAbsent(sql, result);\n        binaryStatements.putIfAbsent(result, new MySQLBinaryStatement(sql, parametersCount));\n        return result;\n    }"}, {"instruction": "Initializes the process that periodically fetches CRL data.", "input": "Java", "output": "@SneakyThrows\n    @SuppressWarnings(\"FutureReturnValueIgnored\")\n    public void init() {\n        if (!validateConfiguration()) {\n            return;\n        }\n\n        val results = this.fetcher.fetch(getResources());\n        ResourceCRLRevocationChecker.this.addCrls(results);\n\n        final Runnable scheduledFetcher = () -> {\n            try {\n                val fetchedResults = getFetcher().fetch(getResources());\n                ResourceCRLRevocationChecker.this.addCrls(fetchedResults);\n            } catch (final Exception e) {\n                LOGGER.debug(e.getMessage(), e);\n            }\n        };\n\n        this.scheduler.scheduleAtFixedRate(\n            scheduledFetcher,\n            this.refreshInterval,\n            this.refreshInterval,\n            TimeUnit.SECONDS);\n\n    }"}, {"instruction": "Parse a template fragment.\n\n@param fragment to parse\n@param query if the fragment is part of a query string.", "input": "Java", "output": "private void parseFragment(String fragment, boolean query) {\n    ChunkTokenizer tokenizer = new ChunkTokenizer(fragment);\n\n    while (tokenizer.hasNext()) {\n      /* check to see if we have an expression or a literal */\n      String chunk = tokenizer.next();\n\n      if (chunk.startsWith(\"{\")) {\n        /* it's an expression, defer encoding until resolution */\n        FragmentType type = (query) ? FragmentType.QUERY : FragmentType.PATH_SEGMENT;\n\n        Expression expression = Expressions.create(chunk, type);\n        if (expression == null) {\n          this.templateChunks.add(Literal.create(encode(chunk, query)));\n        } else {\n          this.templateChunks.add(expression);\n        }\n      } else {\n        /* it's a literal, pct-encode it */\n        this.templateChunks.add(Literal.create(encode(chunk, query)));\n      }\n    }\n  }"}, {"instruction": "Returns a {@link Set} view of the mappings contained in this map.  The\nset is backed by the map, so changes to the map are reflected in the\nset, and vice-versa.  The set supports element removal, which removes\nthe corresponding mapping from the map, via the\n<tt>Iterator.remove</tt>, <tt>Set.remove</tt>, <tt>removeAll</tt>,\n<tt>retainAll</tt>, and <tt>clear</tt> operations.  It does not support\nthe <tt>add</tt> or <tt>addAll</tt> operations.\n\n<p>The view's <tt>iterator</tt> is a \"weakly consistent\" iterator\nthat will never throw {@link ConcurrentModificationException},\nand guarantees to traverse elements as they existed upon\nconstruction of the iterator, and may (but is not guaranteed to)\nreflect any modifications subsequent to construction.\n\n<p><strong>Warning:</strong> the iterator associated with this Set\nrequires the creation of {@link java.util.Map.Entry} objects with each\niteration.  The org.cliffc.high_scale_lib.NonBlockingHashMap\ndoes not normally create or using {@link java.util.Map.Entry} objects so\nthey will be created soley to support this iteration.  Iterating using\n{@link #keySet} or {@link #values} will be more efficient.  In addition,\nthis version requires <strong>auto-boxing</strong> the keys.", "input": "Java", "output": "public Set<Map.Entry<Long,TypeV>> entrySet() {\n    return new AbstractSet<Map.Entry<Long,TypeV>>() {\n      public void    clear   (          ) {        NonBlockingHashMapLong.this.clear( ); }\n      public int     size    (          ) { return NonBlockingHashMapLong.this.size ( ); }\n      public boolean remove( final Object o ) {\n        if (!(o instanceof Map.Entry)) return false;\n        final Map.Entry<?,?> e = (Map.Entry<?,?>)o;\n        return NonBlockingHashMapLong.this.remove(e.getKey(), e.getValue());\n      }\n      public boolean contains(final Object o) {\n        if (!(o instanceof Map.Entry)) return false;\n        final Map.Entry<?,?> e = (Map.Entry<?,?>)o;\n        TypeV v = get(e.getKey());\n        return v.equals(e.getValue());\n      }\n      public Iterator<Map.Entry<Long,TypeV>> iterator() { return new SnapshotE(); }\n    };\n  }"}, {"instruction": "Create a new read only representation of headers used by servers.\n@param validateHeaders {@code true} will run validation on each header name/value pair to ensure protocol\ncompliance.\n@param status The value for {@link PseudoHeaderName#STATUS}.\n@param otherHeaders A an array of key:value pairs. Must not contain any\n<a href=\"https://tools.ietf.org/html/rfc7540#section-8.1.2.1\">pseudo headers</a>\nor {@code null} names/values.\nA copy will <strong>NOT</strong> be made of this array. If the contents of this array\nmay be modified externally you are responsible for passing in a copy.\n@return a new read only representation of headers used by servers.", "input": "Java", "output": "public static ReadOnlyHttp2Headers serverHeaders(boolean validateHeaders,\n                                                     AsciiString status,\n                                                     AsciiString... otherHeaders) {\n        return new ReadOnlyHttp2Headers(validateHeaders,\n                                        new AsciiString[] { PseudoHeaderName.STATUS.value(), status },\n                                        otherHeaders);\n    }"}, {"instruction": "Identifying certificate for this host. {@code keyCertChainInputStream} and {@code keyInputStream} may\nbe {@code null} for client contexts, which disables mutual authentication.\n\n@param keyCertChainInputStream an input stream for an X.509 certificate chain in PEM format\n@param keyInputStream an input stream for a PKCS#8 private key in PEM format\n@param keyPassword the password of the {@code keyInputStream}, or {@code null} if it's not\npassword-protected", "input": "Java", "output": "public SslContextBuilder keyManager(InputStream keyCertChainInputStream, InputStream keyInputStream,\n            String keyPassword) {\n        X509Certificate[] keyCertChain;\n        PrivateKey key;\n        try {\n            keyCertChain = SslContext.toX509Certificates(keyCertChainInputStream);\n        } catch (Exception e) {\n            throw new IllegalArgumentException(\"Input stream not contain valid certificates.\", e);\n        }\n        try {\n            key = SslContext.toPrivateKey(keyInputStream, keyPassword);\n        } catch (Exception e) {\n            throw new IllegalArgumentException(\"Input stream does not contain valid private key.\", e);\n        }\n        return keyManager(key, keyPassword, keyCertChain);\n    }"}, {"instruction": "Checks if the current security principal has the permission to create top level items within the specified\nitem group.\n<p>\nThis is just a convenience function.\n@param c the container of the item.\n@param d the descriptor of the item to be created.\n@throws AccessDeniedException\nif the user doesn't have the permission.\n@since 1.607", "input": "Java", "output": "public final void checkCreatePermission(@Nonnull ItemGroup c,\n                                            @Nonnull TopLevelItemDescriptor d) {\n        Authentication a = Jenkins.getAuthentication();\n        if (a == SYSTEM) {\n            return;\n        }\n        if (!hasCreatePermission(a, c, d)) {\n            throw new AccessDeniedException(Messages.AccessDeniedException2_MissingPermission(a.getName(),\n                    Item.CREATE.group.title+\"/\"+Item.CREATE.name + Item.CREATE + \"/\" + d.getDisplayName()));\n        }\n    }"}, {"instruction": "As per {@link #execBackwards(Map)}, but the set of gradients to calculate can be specified manually.<br>\nFor example, to calculate the gradient for placeholder variable \"myPlaceholder\", use\n{@code execBackwards(placeholders, Arrays.asList(myPlaceholder.gradient().getVarName())}.\n\n@param placeholders Values for the placeholder variables in the graph. For graphs without placeholders, use null or an empty map\n@param variableGradNamesList Names of the gradient variables to calculate", "input": "Java", "output": "public void execBackwards(Map<String,INDArray> placeholders, List<String> variableGradNamesList){\n        if (getFunction(\"grad\") == null) {\n            createGradFunction();\n        }\n\n        log.trace(\"About to execute backward function\");\n\n        //Edge case: if no variables, no variable gradients to calculate...\n        if(variableGradNamesList.isEmpty()){\n            log.warn(\"Skipping gradient calculation (backward pass) - no variables to be calculated (variableGradNamesList is empty)\");\n            return;\n        }\n\n        sameDiffFunctionInstances.get(\"grad\").exec(placeholders, variableGradNamesList);\n    }"}, {"instruction": "{@inheritDoc}\nGets the remote ip from the request, and invokes spnego if it isn't filtered.\n\n@param context the request context\n@return {@link #yes()} if spnego should be invoked and ip isn't filtered,\n{@link #no()} otherwise.", "input": "Java", "output": "@Override\n    protected Event doExecute(final RequestContext context) {\n        val remoteIp = getRemoteIp(context);\n        LOGGER.debug(\"Current user IP [{}]\", remoteIp);\n        if (shouldDoSpnego(remoteIp)) {\n            LOGGER.info(\"Spnego should be activated for [{}]\", remoteIp);\n            return yes();\n        }\n        LOGGER.info(\"Spnego should is skipped for [{}]\", remoteIp);\n        return no();\n    }"}, {"instruction": "Configures the file input format by reading the file path from the configuration.\n\n@see org.apache.flink.api.common.io.InputFormat#configure(org.apache.flink.configuration.Configuration)", "input": "Java", "output": "@Override\n\tpublic void configure(Configuration parameters) {\n\n\t\tif (getFilePaths().length == 0) {\n\t\t\t// file path was not specified yet. Try to set it from the parameters.\n\t\t\tString filePath = parameters.getString(FILE_PARAMETER_KEY, null);\n\t\t\tif (filePath == null) {\n\t\t\t\tthrow new IllegalArgumentException(\"File path was not specified in input format or configuration.\");\n\t\t\t} else {\n\t\t\t\tsetFilePath(filePath);\n\t\t\t}\n\t\t}\n\n\t\tif (!this.enumerateNestedFiles) {\n\t\t\tthis.enumerateNestedFiles = parameters.getBoolean(ENUMERATE_NESTED_FILES_FLAG, false);\n\t\t}\n\t}"}, {"instruction": "Determine the SCM url and branch that is set for the component. Information\nis gathered with the assumption that the data is stored in options.url and\noptions.branch.\n\n@param component\n@return\t\t\tthe {@link RepoBranch} that the component uses", "input": "Java", "output": "protected RepoBranch getComponentRepoBranch(Component component) {\r\n        CollectorItem item = component.getFirstCollectorItemForType(CollectorType.SCM);\r\n        if (item == null) {\r\n        \tlogger.warn(\"Error encountered building pipeline: could not find scm collector item for dashboard.\");\r\n        \treturn new RepoBranch(\"\", \"\", RepoType.Unknown);\r\n        }\r\n        \r\n        // TODO find a better way?\r\n        String url = (String)item.getOptions().get(\"url\");\r\n        String branch = (String)item.getOptions().get(\"branch\");\r\n        \r\n        return new RepoBranch(url, branch, RepoType.Unknown);\r\n\t}"}, {"instruction": "Extract the OAuth bearer token from a header.\n\n@param request The request.\n@return The token, or null if no OAuth authorization header was supplied.", "input": "Java", "output": "protected String extractHeaderToken(HttpServletRequest request) {\n\t\tEnumeration<String> headers = request.getHeaders(\"Authorization\");\n\t\twhile (headers.hasMoreElements()) { // typically there is only one (most servers enforce that)\n\t\t\tString value = headers.nextElement();\n\t\t\tif ((value.toLowerCase().startsWith(OAuth2AccessToken.BEARER_TYPE.toLowerCase()))) {\n\t\t\t\tString authHeaderValue = value.substring(OAuth2AccessToken.BEARER_TYPE.length()).trim();\n\t\t\t\t// Add this here for the auth details later. Would be better to change the signature of this method.\n\t\t\t\trequest.setAttribute(OAuth2AuthenticationDetails.ACCESS_TOKEN_TYPE,\n\t\t\t\t\t\tvalue.substring(0, OAuth2AccessToken.BEARER_TYPE.length()).trim());\n\t\t\t\tint commaIndex = authHeaderValue.indexOf(',');\n\t\t\t\tif (commaIndex > 0) {\n\t\t\t\t\tauthHeaderValue = authHeaderValue.substring(0, commaIndex);\n\t\t\t\t}\n\t\t\t\treturn authHeaderValue;\n\t\t\t}\n\t\t}\n\n\t\treturn null;\n\t}"}, {"instruction": "\u8c03\u6574\u4e00\u4e0b\u7ebf\u7a0b\u6c60", "input": "Java", "output": "private void adjustPoolSize(DbLoadContext context) {\n        Pipeline pipeline = context.getPipeline();\n        int newPoolSize = pipeline.getParameters().getLoadPoolSize();\n        if (newPoolSize != poolSize) {\n            poolSize = newPoolSize;\n            if (executor instanceof ThreadPoolExecutor) {\n                ThreadPoolExecutor pool = (ThreadPoolExecutor) executor;\n                pool.setCorePoolSize(newPoolSize);\n                pool.setMaximumPoolSize(newPoolSize);\n            }\n        }\n    }"}, {"instruction": "Parse all Http request params", "input": "Java", "output": "private Pair<String, String>[] getAllParams(final HttpServletRequest req) {\n    final List<Pair<String, String>> allParams = new LinkedList<>();\n\n    final Iterator it = req.getParameterMap().entrySet().iterator();\n    while (it.hasNext()) {\n      final Map.Entry pairs = (Map.Entry) it.next();\n      for (final Object value : (String[]) pairs.getValue()) {\n        allParams.add(new Pair<>((String) pairs.getKey(), (String) value));\n      }\n    }\n\n    return allParams.toArray(new Pair[allParams.size()]);\n  }"}, {"instruction": "Replace deprecated configuration properties for {@link FlinkKinesisProducer}.\nThis should be remove along with deprecated keys", "input": "Java", "output": "public static Properties replaceDeprecatedProducerKeys(Properties configProps) {\n\t\t// Replace deprecated key\n\t\tif (configProps.containsKey(ProducerConfigConstants.COLLECTION_MAX_COUNT)) {\n\t\t\tconfigProps.setProperty(COLLECTION_MAX_COUNT,\n\t\t\t\t\tconfigProps.getProperty(ProducerConfigConstants.COLLECTION_MAX_COUNT));\n\t\t\tconfigProps.remove(ProducerConfigConstants.COLLECTION_MAX_COUNT);\n\t\t}\n\t\t// Replace deprecated key\n\t\tif (configProps.containsKey(ProducerConfigConstants.AGGREGATION_MAX_COUNT)) {\n\t\t\tconfigProps.setProperty(AGGREGATION_MAX_COUNT,\n\t\t\t\t\tconfigProps.getProperty(ProducerConfigConstants.AGGREGATION_MAX_COUNT));\n\t\t\tconfigProps.remove(ProducerConfigConstants.AGGREGATION_MAX_COUNT);\n\t\t}\n\t\treturn configProps;\n\t}"}, {"instruction": "Returns the interconnectDiagnostics for the specified interconnect.\n\n<p>Sample code:\n\n<pre><code>\ntry (InterconnectClient interconnectClient = InterconnectClient.create()) {\nProjectGlobalInterconnectName interconnect = ProjectGlobalInterconnectName.of(\"[PROJECT]\", \"[INTERCONNECT]\");\nInterconnectsGetDiagnosticsResponse response = interconnectClient.getDiagnosticsInterconnect(interconnect);\n}\n</code></pre>\n\n@param interconnect Name of the interconnect resource to query.\n@throws com.google.api.gax.rpc.ApiException if the remote call fails", "input": "Java", "output": "@BetaApi\n  public final InterconnectsGetDiagnosticsResponse getDiagnosticsInterconnect(\n      ProjectGlobalInterconnectName interconnect) {\n\n    GetDiagnosticsInterconnectHttpRequest request =\n        GetDiagnosticsInterconnectHttpRequest.newBuilder()\n            .setInterconnect(interconnect == null ? null : interconnect.toString())\n            .build();\n    return getDiagnosticsInterconnect(request);\n  }"}, {"instruction": "(and return Long.MIN_VALUE but this is a valid long return value).", "input": "Java", "output": "private static Long attemptUUIDParseLow(BufferedString str) {\n    final byte[] buf = str.getBuffer();\n    int i=str.getOffset();\n    if( i+36 > buf.length ) return markBad(str);\n    long lo=0;\n    lo = get2(lo,buf,(i+=2)-2);\n    lo = get2(lo,buf,(i+=2)-2);\n    lo = get2(lo,buf,(i+=2)-2);\n    lo = get2(lo,buf,(i+=2)-2);\n    if( buf[i++]!='-' ) return markBad(str);\n    lo = get2(lo,buf,(i+=2)-2);\n    lo = get2(lo,buf,(i+=2)-2);\n    if( buf[i++]!='-' ) return markBad(str);\n    lo = get2(lo,buf,(i+=2)-2);\n    return attemptUUIDParseEnd(str, lo, buf, i);\n  }"}, {"instruction": "\u8fc7\u6ee4<br>\n\u8fc7\u6ee4\u8fc7\u7a0b\u901a\u8fc7\u4f20\u5165\u7684Editor\u5b9e\u73b0\u6765\u8fd4\u56de\u9700\u8981\u7684\u5143\u7d20\u5185\u5bb9\uff0c\u8fd9\u4e2aEditor\u5b9e\u73b0\u53ef\u4ee5\u5b9e\u73b0\u4ee5\u4e0b\u529f\u80fd\uff1a\n\n<pre>\n1\u3001\u8fc7\u6ee4\u51fa\u9700\u8981\u7684\u5bf9\u8c61\uff0c\u5982\u679c\u8fd4\u56denull\u8868\u793a\u8fd9\u4e2a\u5143\u7d20\u5bf9\u8c61\u629b\u5f03\n2\u3001\u4fee\u6539\u5143\u7d20\u5bf9\u8c61\uff0c\u8fd4\u56de\u96c6\u5408\u4e2d\u4e3a\u4fee\u6539\u540e\u7684\u5bf9\u8c61\n</pre>\n\n@param <K> Key\u7c7b\u578b\n@param <V> Value\u7c7b\u578b\n@param map Map\n@param editor \u7f16\u8f91\u5668\u63a5\u53e3\n@return \u8fc7\u6ee4\u540e\u7684Map", "input": "Java", "output": "public static <K, V> Map<K, V> filter(Map<K, V> map, Editor<Entry<K, V>> editor) {\r\n\t\tif(null == map || null == editor) {\r\n\t\t\treturn map;\r\n\t\t}\r\n\t\t\r\n\t\tfinal Map<K, V> map2 = ObjectUtil.clone(map);\r\n\t\tif (isEmpty(map2)) {\r\n\t\t\treturn map2;\r\n\t\t}\r\n\r\n\t\tmap2.clear();\r\n\t\tEntry<K, V> modified;\r\n\t\tfor (Entry<K, V> entry : map.entrySet()) {\r\n\t\t\tmodified = editor.edit(entry);\r\n\t\t\tif (null != modified) {\r\n\t\t\t\tmap2.put(modified.getKey(), modified.getValue());\r\n\t\t\t}\r\n\t\t}\r\n\t\treturn map2;\r\n\t}"}, {"instruction": "\u6267\u884c\u6bd4\u8f83<br>\n\u6309\u7167\u6bd4\u8f83\u5668\u94fe\u7684\u987a\u5e8f\u5206\u522b\u6bd4\u8f83\uff0c\u5982\u679c\u6bd4\u8f83\u51fa\u76f8\u7b49\u5219\u8f6c\u5411\u4e0b\u4e00\u4e2a\u6bd4\u8f83\u5668\uff0c\u5426\u5219\u76f4\u63a5\u8fd4\u56de\n\n@param o1 \u7b2c\u4e00\u4e2a\u5bf9\u8c61\n@param o2 \u7b2c\u4e8c\u4e2a\u5bf9\u8c61\n@return -1, 0, or 1\n@throws UnsupportedOperationException \u5982\u679c\u6bd4\u8f83\u5668\u94fe\u4e3a\u7a7a\uff0c\u65e0\u6cd5\u5b8c\u6210\u6bd4\u8f83", "input": "Java", "output": "@Override\r\n\tpublic int compare(final E o1, final E o2) throws UnsupportedOperationException {\r\n\t\tif (lock == false) {\r\n\t\t\tcheckChainIntegrity();\r\n\t\t\tlock = true;\r\n\t\t}\r\n\t\t\r\n\t\tfinal Iterator<Comparator<E>> comparators = chain.iterator();\r\n\t\tComparator<? super E> comparator;\r\n\t\tint retval;\r\n\t\tfor (int comparatorIndex = 0; comparators.hasNext(); ++comparatorIndex) {\r\n\t\t\tcomparator = comparators.next();\r\n\t\t\tretval = comparator.compare(o1, o2);\r\n\t\t\tif (retval != 0) {\r\n\t\t\t\t// invert the order if it is a reverse sort\r\n\t\t\t\tif (true == orderingBits.get(comparatorIndex)) {\r\n\t\t\t\t\tretval = (retval > 0) ? -1 : 1;\r\n\t\t\t\t}\r\n\t\t\t\treturn retval;\r\n\t\t\t}\r\n\t\t}\r\n\r\n\t\t// if comparators are exhausted, return 0\r\n\t\treturn 0;\r\n\t}"}, {"instruction": "Converts StreamConfiguration into StreamConfig.\n\n@param scope the stream's scope\n@param streamName The Stream Name\n@param configModel The stream configuration.\n@return StreamConfig instance.", "input": "Java", "output": "public static final StreamConfig decode(String scope, String streamName, final StreamConfiguration configModel) {\n        Preconditions.checkNotNull(configModel, \"configModel\");\n        final StreamConfig.Builder builder = StreamConfig.newBuilder()\n                .setStreamInfo(createStreamInfo(scope, streamName))\n                .setScalingPolicy(decode(configModel.getScalingPolicy()));\n        if (configModel.getRetentionPolicy() != null) {\n            builder.setRetentionPolicy(decode(configModel.getRetentionPolicy()));\n        }\n        return builder.build();\n    }"}, {"instruction": "Put cas response attributes into model.\n\n@param model              the model\n@param attributes         the attributes\n@param registeredService  the registered service\n@param attributesRenderer the attributes renderer", "input": "Java", "output": "protected void putCasResponseAttributesIntoModel(final Map<String, Object> model,\n                                                     final Map<String, Object> attributes,\n                                                     final RegisteredService registeredService,\n                                                     final CasProtocolAttributesRenderer attributesRenderer) {\n\n        LOGGER.trace(\"Beginning to encode attributes for the response\");\n        val encodedAttributes = this.protocolAttributeEncoder.encodeAttributes(attributes, registeredService);\n\n        LOGGER.debug(\"Encoded attributes for the response are [{}]\", encodedAttributes);\n        putIntoModel(model, CasProtocolConstants.VALIDATION_CAS_MODEL_ATTRIBUTE_NAME_ATTRIBUTES, encodedAttributes);\n\n        val formattedAttributes = attributesRenderer.render(encodedAttributes);\n        putIntoModel(model, CasProtocolConstants.VALIDATION_CAS_MODEL_ATTRIBUTE_NAME_FORMATTED_ATTRIBUTES, formattedAttributes);\n    }"}, {"instruction": "Handles the logout processing.\n\n<p>\nThe default implementation erases the session and do a few other clean up, then\nredirect the user to the URL specified by {@link #getPostLogOutUrl(StaplerRequest, Authentication)}.\n\n@since 1.314", "input": "Java", "output": "public void doLogout(StaplerRequest req, StaplerResponse rsp) throws IOException, ServletException {\n        HttpSession session = req.getSession(false);\n        if(session!=null)\n            session.invalidate();\n        Authentication auth = SecurityContextHolder.getContext().getAuthentication();\n        SecurityContextHolder.clearContext();\n\n        // reset remember-me cookie\n        Cookie cookie = new Cookie(ACEGI_SECURITY_HASHED_REMEMBER_ME_COOKIE_KEY,\"\");\n        cookie.setMaxAge(0);\n        cookie.setSecure(req.isSecure());\n        cookie.setHttpOnly(true);\n        cookie.setPath(req.getContextPath().length()>0 ? req.getContextPath() : \"/\");\n        rsp.addCookie(cookie);\n\n        rsp.sendRedirect2(getPostLogOutUrl(req,auth));\n    }"}, {"instruction": "Returns the caches as a {@link Single}.\n\n@return The caches as a {@link Single}", "input": "Java", "output": "@Read\n    public Single<Map<String, Object>> getCaches() {\n        return Flowable.fromIterable(cacheManager.getCacheNames())\n                       .flatMapMaybe(n -> Flowable.fromPublisher(cacheManager.getCache(n).getCacheInfo()).firstElement())\n                       .reduce(new HashMap<>(), (seed, info) -> {\n                           seed.put(info.getName(), info.get());\n                           return seed;\n                       }).map(objectObjectHashMap -> Collections.singletonMap(\n                           NAME, objectObjectHashMap\n                       ));\n    }"}, {"instruction": "/*\n(non-Javadoc)\n@see java.util.Map#containsValue(java.lang.Object)", "input": "Java", "output": "@Override\n    public boolean containsValue(Object value) {\n        if (value == null) {\n            throw new NullPointerException();\n        }\n\n        for (Map.Entry<K, CachedValue<K, V>> entry : map.entrySet()) {\n            CachedValue<K, V> cachedValue = entry.getValue();\n            if (cachedValue.getValue().equals(value)) {\n                if (isValueExpired(cachedValue)) {\n                    if (map.remove(cachedValue.getKey(), cachedValue)) {\n                        onValueRemove(cachedValue);\n                    }\n                } else {\n                    readValue(cachedValue);\n                    return true;\n                }\n            }\n        }\n        return false;\n    }"}, {"instruction": "Closes the queue and prevents any other access to it. Any blocked call to takeAllItems() will fail with InterruptedException.\n\n@return If the queue has any more items in it, these will be returned here in the order in which they were inserted.\nThe items are guaranteed not to be returned both here and via take()/poll().", "input": "Java", "output": "public Queue<T> close() {\n        CompletableFuture<Queue<T>> pending = null;\n        Queue<T> result = null;\n        synchronized (this.contents) {\n            if (!this.closed) {\n                this.closed = true;\n                pending = this.pendingTake;\n                this.pendingTake = null;\n                result = fetch(this.contents.size());\n            }\n        }\n\n        // Cancel any pending poll request.\n        if (pending != null) {\n            pending.cancel(true);\n        }\n\n        return result != null ? result : new LinkedList<>();\n    }"}, {"instruction": "Copy the values of an {@link Iterator} to the target {@link CallStreamObserver} while properly\naccounting for outbound flow-control.  After calling this method, {@code target} should no\nlonger be used.\n\n<p>For clients this method is safe to call inside {@link ClientResponseObserver#beforeStart},\non servers it is safe to call inside the service method implementation.\n</p>\n\n@param source of values expressed as an {@link Iterator}.\n@param target {@link CallStreamObserver} which accepts values from the source.", "input": "Java", "output": "public static <V> void copyWithFlowControl(final Iterator<V> source,\n      final CallStreamObserver<V> target) {\n    Preconditions.checkNotNull(source, \"source\");\n    Preconditions.checkNotNull(target, \"target\");\n\n    final class FlowControllingOnReadyHandler implements Runnable {\n      private boolean completed;\n\n      @Override\n      public void run() {\n        if (completed) {\n          return;\n        }\n\n        while (target.isReady() && source.hasNext()) {\n          target.onNext(source.next());\n        }\n\n        if (!source.hasNext()) {\n          completed = true;\n          target.onCompleted();\n        }\n      }\n    }\n\n    target.setOnReadyHandler(new FlowControllingOnReadyHandler());\n  }"}, {"instruction": "Deletes all alerts of this node and all child nodes recursively.", "input": "Java", "output": "public void deleteAllAlerts() {\r\n        for(int i = 0; i < getChildCount(); i++) {\r\n            ((SiteNode) getChildAt(i)).deleteAllAlerts();\r\n        }\r\n\r\n        if (!alerts.isEmpty()) {\r\n            alerts.clear();\r\n            highestAlert = null;\r\n            calculateHighestAlert = false;\r\n        \tif (this.siteMap != null) {\r\n        \t\t// Deleting alert might affect the nodes visibility in a filtered tree\r\n        \t\tsiteMap.applyFilter(this);\r\n        \t}\r\n            nodeChanged();\r\n        }\r\n    }"}, {"instruction": "Returns a new initialized {@link OpenSslX509KeyManagerFactory} which will provide its private key by using the\n{@link OpenSslPrivateKeyMethod}.", "input": "Java", "output": "public static OpenSslX509KeyManagerFactory newKeyless(X509Certificate... certificateChain)\n            throws CertificateException, IOException,\n            KeyStoreException, NoSuchAlgorithmException, UnrecoverableKeyException {\n        KeyStore store = new OpenSslKeyStore(certificateChain.clone(), true);\n        store.load(null, null);\n        OpenSslX509KeyManagerFactory factory = new OpenSslX509KeyManagerFactory();\n        factory.init(store, null);\n        return factory;\n    }"}, {"instruction": "\u5bf9\u6bd4\u4e24\u4e2aReader\u7684\u5185\u5bb9\u662f\u5426\u4e00\u81f4<br>\n\u5185\u90e8\u4f1a\u8f6c\u6362\u6d41\u4e3a {@link BufferedInputStream}\n\n@param input1 \u7b2c\u4e00\u4e2areader\n@param input2 \u7b2c\u4e8c\u4e2areader\n@return \u4e24\u4e2a\u6d41\u7684\u5185\u5bb9\u4e00\u81f4\u8fd4\u56detrue\uff0c\u5426\u5219false\n@throws IORuntimeException IO\u5f02\u5e38\n@since 4.0.6", "input": "Java", "output": "public static boolean contentEquals(Reader input1, Reader input2) throws IORuntimeException {\r\n\t\tinput1 = getReader(input1);\r\n\t\tinput2 = getReader(input2);\r\n\r\n\t\ttry {\r\n\t\t\tint ch = input1.read();\r\n\t\t\twhile (EOF != ch) {\r\n\t\t\t\tint ch2 = input2.read();\r\n\t\t\t\tif (ch != ch2) {\r\n\t\t\t\t\treturn false;\r\n\t\t\t\t}\r\n\t\t\t\tch = input1.read();\r\n\t\t\t}\r\n\r\n\t\t\tint ch2 = input2.read();\r\n\t\t\treturn ch2 == EOF;\r\n\t\t} catch (IOException e) {\r\n\t\t\tthrow new IORuntimeException(e);\r\n\t\t}\r\n\t}"}, {"instruction": "Sets the position of the response panel. Should be considered a hint, not all workbench layouts might use this setting.\n<p>\nIf the position is already set no further action is taken, otherwise updates the main tool bar buttons, the workbench\npanel and the configurations file.\n\n@param position the new position of the response panel\n@throws IllegalArgumentException if the given parameter is {@code null}.\n@since 2.5.0\n@see #getResponsePanelPosition()\n@see #setWorkbenchLayout(org.parosproxy.paros.view.WorkbenchPanel.Layout)", "input": "Java", "output": "public void setResponsePanelPosition(WorkbenchPanel.ResponsePanelPosition position) {\r\n\t\tif (position == null) {\r\n\t\t\tthrow new IllegalArgumentException(\"Parameter position must not be null.\");\r\n\t\t}\r\n\r\n\t\tif (responsePanelPosition == position) {\r\n\t\t\treturn;\r\n\t\t}\r\n\r\n\t\tresponsePanelPosition = position;\r\n\r\n\t\tswitch (position) {\r\n\t\tcase PANEL_ABOVE:\r\n\t\t\taboveResponsePanelPositionButton.setSelected(true);\r\n\t\t\tbreak;\r\n\t\tcase PANELS_SIDE_BY_SIDE:\r\n\t\t\tpanelsResponsePanelPositionButton.setSelected(true);\r\n\t\t\tbreak;\r\n\t\tcase TAB_SIDE_BY_SIDE:\r\n\t\t\ttabSideBySideResponsePanelPositionButton.setSelected(true);\r\n\t\t\tbreak;\r\n\t\tcase TABS_SIDE_BY_SIDE:\r\n\t\tdefault:\r\n\t\t\ttabsResponsePanelPositionButton.setSelected(true);\r\n\t\t}\r\n\r\n\t\tgetWorkbench().setResponsePanelPosition(responsePanelPosition);\r\n\t\toptions.getViewParam().setResponsePanelPosition(responsePanelPosition.toString());\r\n\t}"}, {"instruction": "\u6839\u636e\u7ed9\u5b9a\u7684\u56fe\u7247\u683c\u5f0f\u6216\u8005\u6269\u5c55\u540d\u83b7\u53d6{@link ImageWriter}\uff0c\u5982\u679c\u672a\u627e\u5230\u5408\u9002\u7684Writer\uff0c\u8fd4\u56denull\n\n@param formatName \u56fe\u7247\u683c\u5f0f\u6216\u6269\u5c55\u540d\uff0c\u4f8b\u5982\"jpg\"\u3001\"png\"\n@return {@link ImageWriter}\n@since 4.3.2", "input": "Java", "output": "public static ImageWriter getWriter(String formatName) {\r\n\t\tImageWriter writer = null;\r\n\t\tIterator<ImageWriter> iter = ImageIO.getImageWritersByFormatName(formatName);\r\n\t\tif (iter.hasNext()) {\r\n\t\t\twriter = iter.next();\r\n\t\t}\r\n\t\tif (null == writer) {\r\n\t\t\t// \u5c1d\u8bd5\u6269\u5c55\u540d\u83b7\u53d6\r\n\t\t\titer = ImageIO.getImageWritersBySuffix(formatName);\r\n\t\t\tif (iter.hasNext()) {\r\n\t\t\t\twriter = iter.next();\r\n\t\t\t}\r\n\t\t}\r\n\t\treturn writer;\r\n\t}"}, {"instruction": "------------------------------------------------------------------------", "input": "Java", "output": "@Override\n\tpublic void report() {\n\t\t// instead of locking here, we tolerate exceptions\n\t\t// we do this to prevent holding the lock for very long and blocking\n\t\t// operator creation and shutdown\n\t\ttry {\n\t\t\tfor (Map.Entry<Gauge<?>, String> entry : gauges.entrySet()) {\n\t\t\t\tif (closed) {\n\t\t\t\t\treturn;\n\t\t\t\t}\n\t\t\t\treportGauge(entry.getValue(), entry.getKey());\n\t\t\t}\n\n\t\t\tfor (Map.Entry<Counter, String> entry : counters.entrySet()) {\n\t\t\t\tif (closed) {\n\t\t\t\t\treturn;\n\t\t\t\t}\n\t\t\t\treportCounter(entry.getValue(), entry.getKey());\n\t\t\t}\n\n\t\t\tfor (Map.Entry<Histogram, String> entry : histograms.entrySet()) {\n\t\t\t\treportHistogram(entry.getValue(), entry.getKey());\n\t\t\t}\n\n\t\t\tfor (Map.Entry<Meter, String> entry : meters.entrySet()) {\n\t\t\t\treportMeter(entry.getValue(), entry.getKey());\n\t\t\t}\n\t\t}\n\t\tcatch (ConcurrentModificationException | NoSuchElementException e) {\n\t\t\t// ignore - may happen when metrics are concurrently added or removed\n\t\t\t// report next time\n\t\t}\n\t}"}, {"instruction": "Load Keras (Functional API) Model saved using model.save_model(...).\n\n@param modelHdf5Filename     path to HDF5 archive storing Keras Model\n@param inputShape            optional input shape for models that come without such (e.g. notop = false models)\n@param enforceTrainingConfig whether to enforce training configuration options\n@return ComputationGraph\n@throws IOException                            IO exception\n@throws InvalidKerasConfigurationException     Invalid Keras config\n@throws UnsupportedKerasConfigurationException Unsupported Keras config\n@see ComputationGraph", "input": "Java", "output": "public static ComputationGraph importKerasModelAndWeights(String modelHdf5Filename, int[] inputShape,\n                                                              boolean enforceTrainingConfig)\n            throws IOException, UnsupportedKerasConfigurationException, InvalidKerasConfigurationException {\n        KerasModel kerasModel = new KerasModel().modelBuilder.modelHdf5Filename(modelHdf5Filename)\n                .enforceTrainingConfig(enforceTrainingConfig).inputShape(inputShape).buildModel();\n        return kerasModel.getComputationGraph();\n    }"}, {"instruction": "\u6267\u884cWebservice\u8bf7\u6c42\uff0c\u65e2\u53d1\u9001SOAP\u5185\u5bb9\n\n@return \u8fd4\u56de\u7ed3\u679c", "input": "Java", "output": "public SOAPMessage sendForMessage() {\r\n\t\tfinal HttpResponse res = sendForResponse();\r\n\t\tfinal MimeHeaders headers = new MimeHeaders();\r\n\t\tfor (Entry<String, List<String>> entry : res.headers().entrySet()) {\r\n\t\t\tif(StrUtil.isNotEmpty(entry.getKey())) {\r\n\t\t\t\theaders.setHeader(entry.getKey(), CollUtil.get(entry.getValue(), 0));\r\n\t\t\t}\r\n\t\t}\r\n\t\ttry {\r\n\t\t\treturn this.factory.createMessage(headers, res.bodyStream());\r\n\t\t} catch (IOException | SOAPException e) {\r\n\t\t\tthrow new SoapRuntimeException(e);\r\n\t\t}\r\n\t}"}, {"instruction": "Load a graph into memory, using a given EdgeLineProcessor.\nAssume one edge per line\n@param path Path to the file containing the edges, one per line\n@param lineProcessor EdgeLineProcessor used to convert lines of text into a graph (or null for comment lines etc)\n@param vertexFactory Used to create vertices\n@param numVertices number of vertices in the graph\n@param allowMultipleEdges whether the graph should allow multiple edges between a given pair of vertices or not\n@return IGraph", "input": "Java", "output": "public static <V, E> Graph<V, E> loadGraph(String path, EdgeLineProcessor<E> lineProcessor,\n                    VertexFactory<V> vertexFactory, int numVertices, boolean allowMultipleEdges) throws IOException {\n        Graph<V, E> graph = new Graph<>(numVertices, allowMultipleEdges, vertexFactory);\n\n        try (BufferedReader br = new BufferedReader(new FileReader(new File(path)))) {\n            String line;\n            while ((line = br.readLine()) != null) {\n                Edge<E> edge = lineProcessor.processLine(line);\n                if (edge != null) {\n                    graph.addEdge(edge);\n                }\n            }\n        }\n\n        return graph;\n    }"}, {"instruction": "\u4ececontent\u4e2d\u5339\u914d\u51fa\u591a\u4e2a\u503c\u5e76\u6839\u636etemplate\u751f\u6210\u65b0\u7684\u5b57\u7b26\u4e32<br>\n\u4f8b\u5982\uff1a<br>\ncontent 2013\u5e745\u6708 pattern (.*?)\u5e74(.*?)\u6708 template\uff1a $1-$2 return 2013-5\n\n@param pattern \u5339\u914d\u6b63\u5219\n@param content \u88ab\u5339\u914d\u7684\u5185\u5bb9\n@param template \u751f\u6210\u5185\u5bb9\u6a21\u677f\uff0c\u53d8\u91cf $1 \u8868\u793agroup1\u7684\u5185\u5bb9\uff0c\u4ee5\u6b64\u7c7b\u63a8\n@return \u65b0\u5b57\u7b26\u4e32", "input": "Java", "output": "public static String extractMulti(Pattern pattern, CharSequence content, String template) {\r\n\t\tif (null == content || null == pattern || null == template) {\r\n\t\t\treturn null;\r\n\t\t}\r\n\r\n\t\t//\u63d0\u53d6\u6a21\u677f\u4e2d\u7684\u7f16\u53f7\r\n\t\tfinal TreeSet<Integer> varNums = new TreeSet<>(new Comparator<Integer>() {\r\n\t\t\t@Override\r\n\t\t\tpublic int compare(Integer o1, Integer o2) {\r\n\t\t\t\treturn ObjectUtil.compare(o2, o1);\r\n\t\t\t}\r\n\t\t});\r\n\t\tfinal Matcher matcherForTemplate = PatternPool.GROUP_VAR.matcher(template);\r\n\t\twhile (matcherForTemplate.find()) {\r\n\t\t\tvarNums.add(Integer.parseInt(matcherForTemplate.group(1)));\r\n\t\t}\r\n\r\n\t\tfinal Matcher matcher = pattern.matcher(content);\r\n\t\tif (matcher.find()) {\r\n\t\t\tfor (Integer group : varNums) {\r\n\t\t\t\ttemplate = template.replace(\"$\" + group, matcher.group(group));\r\n\t\t\t}\r\n\t\t\treturn template;\r\n\t\t}\r\n\t\treturn null;\r\n\t}"}, {"instruction": "Put ticket granting ticket in request and flow scopes.\n\n@param context     the context\n@param ticketValue the ticket value", "input": "Java", "output": "public static void putTicketGrantingTicketInScopes(final RequestContext context, final String ticketValue) {\n        putTicketGrantingTicketIntoMap(context.getRequestScope(), ticketValue);\n        putTicketGrantingTicketIntoMap(context.getFlowScope(), ticketValue);\n\n        var session = context.getFlowExecutionContext().getActiveSession().getParent();\n        while (session != null) {\n            putTicketGrantingTicketIntoMap(session.getScope(), ticketValue);\n            session = session.getParent();\n        }\n    }"}, {"instruction": "Identical to calling {@code #releaseExceptionally(Exception); #reset()} except it is atomic.\n@param e The exception to fail all waiting futures with.", "input": "Java", "output": "public void releaseExceptionallyAndReset(Throwable e) {\n        ArrayList<CompletableFuture<T>> toComplete = null;\n        synchronized (lock) {\n            if (!waitingFutures.isEmpty()) {\n                toComplete = new ArrayList<>(waitingFutures);\n                waitingFutures.clear();\n            }\n            released = false;\n            this.e = null;\n            result = null;\n            runningThreadId = null;\n        }\n        if (toComplete != null) {\n            for (CompletableFuture<T> f : toComplete) {\n                f.completeExceptionally(e);\n            }\n        }\n    }"}, {"instruction": "Returns the name(s) of the outputs for the given function\n\n@param function the function to get the outputs for\n@return the outputs ids for a given function", "input": "Java", "output": "public String[] getOutputsForFunction(DifferentialFunction function) {\n        if (!ops.containsKey(function.getOwnName()))\n            throw new ND4JIllegalStateException(\"Illegal function instance id found \" + function.getOwnName());\n        List<String> outputs = ops.get(function.getOwnName()).getOutputsOfOp();\n        return outputs == null ? null : outputs.toArray(new String[outputs.size()]);\n    }"}, {"instruction": "/* package", "input": "Java", "output": "void addExecutedCommand(HystrixInvokableInfo<?> command) {\n        if (!allExecutedCommands.offer(command)) {\n            // see RequestLog: Reduce Chance of Memory Leak https://github.com/Netflix/Hystrix/issues/53\n            logger.warn(\"RequestLog ignoring command after reaching limit of \" + MAX_STORAGE + \". See https://github.com/Netflix/Hystrix/issues/53 for more information.\");\n        }\n\n        // TODO remove this when deprecation completed\n        if (command instanceof HystrixCommand) {\n            @SuppressWarnings(\"rawtypes\")\n            HystrixCommand<?> _c = (HystrixCommand) command;\n            if (!executedCommands.offer(_c)) {\n                // see RequestLog: Reduce Chance of Memory Leak https://github.com/Netflix/Hystrix/issues/53\n                logger.warn(\"RequestLog ignoring command after reaching limit of \" + MAX_STORAGE + \". See https://github.com/Netflix/Hystrix/issues/53 for more information.\");\n            }\n        }\n    }"}, {"instruction": "US-ASCII characters excluding CTLs, whitespace, DQUOTE, comma, semicolon, and backslash", "input": "Java", "output": "private static BitSet validCookieValueOctets() {\n        BitSet bits = new BitSet(8);\n        for (int i = 35; i < 127; i++) {\n            // US-ASCII characters excluding CTLs (%x00-1F / %x7F)\n            bits.set(i);\n        }\n        bits.set('\"', false);  // exclude DQUOTE = %x22\n        bits.set(',', false);  // exclude comma = %x2C\n        bits.set(';', false);  // exclude semicolon = %x3B\n        bits.set('\\\\', false); // exclude backslash = %x5C\n        return bits;\n    }"}, {"instruction": "region AutoCloseable Implementation", "input": "Java", "output": "@Override\n    public void close() {\n        if (!this.closed.getAndSet(true)) {\n            // Close all containers that are still open.\n            ArrayList<CompletableFuture<Void>> results = new ArrayList<>();\n            synchronized (this.handles) {\n                ArrayList<ContainerHandle> toClose = new ArrayList<>(this.handles.values());\n                for (ContainerHandle handle : toClose) {\n                    results.add(this.registry.stopContainer(handle, CLOSE_TIMEOUT_PER_CONTAINER));\n                }\n            }\n\n            // Wait for all the containers to be closed.\n            Futures.await(Futures.allOf(results), CLOSE_TIMEOUT_PER_CONTAINER.toMillis());\n        }\n    }"}, {"instruction": "\u8bbe\u7f6e\u54cd\u5e94\u7684Header\n\n@param response \u54cd\u5e94\u5bf9\u8c61{@link HttpServletResponse}\n@param name \u540d\n@param value \u503c\uff0c\u53ef\u4ee5\u662fString\uff0cDate\uff0c int", "input": "Java", "output": "public static void setHeader(HttpServletResponse response, String name, Object value) {\r\n\t\tif (value instanceof String) {\r\n\t\t\tresponse.setHeader(name, (String) value);\r\n\t\t} else if (Date.class.isAssignableFrom(value.getClass())) {\r\n\t\t\tresponse.setDateHeader(name, ((Date) value).getTime());\r\n\t\t} else if (value instanceof Integer || \"int\".equals(value.getClass().getSimpleName().toLowerCase())) {\r\n\t\t\tresponse.setIntHeader(name, (Integer) value);\r\n\t\t} else {\r\n\t\t\tresponse.setHeader(name, value.toString());\r\n\t\t}\r\n\t}"}, {"instruction": "This method initializes logPanel\n\n@return org.parosproxy.paros.extension.history.LogPanel", "input": "Java", "output": "private LogPanel getLogPanel() {\r\n\t\tif (logPanel == null) {\r\n\t\t\tlogPanel = new LogPanel(getView());\r\n\t\t\tlogPanel.setName(Constant.messages.getString(\"history.panel.title\"));\t// ZAP: i18n\r\n\t\t\t// ZAP: Added History (calendar) icon\r\n\t\t\tlogPanel.setIcon(new ImageIcon(ExtensionHistory.class.getResource(\"/resource/icon/16/025.png\")));\t// 'calendar' icon\r\n\t\t\t// Dont allow this tab to be hidden\r\n\t\t\tlogPanel.setHideable(false);\r\n\r\n            logPanel.setExtension(this);\r\n            logPanel.setModel(historyTableModel);\r\n\t\t}\r\n\t\treturn logPanel;\r\n\t}"}, {"instruction": "\u4e24\u4e2a\u96c6\u5408\u7684\u5dee\u96c6<br>\n\u9488\u5bf9\u4e00\u4e2a\u96c6\u5408\u4e2d\u5b58\u5728\u591a\u4e2a\u76f8\u540c\u5143\u7d20\u7684\u60c5\u51b5\uff0c\u8ba1\u7b97\u4e24\u4e2a\u96c6\u5408\u4e2d\u6b64\u5143\u7d20\u7684\u4e2a\u6570\uff0c\u4fdd\u7559\u4e24\u4e2a\u96c6\u5408\u4e2d\u6b64\u5143\u7d20\u4e2a\u6570\u5dee\u7684\u4e2a\u6570<br>\n\u4f8b\u5982\uff1a\u96c6\u54081\uff1a[a, b, c, c, c]\uff0c\u96c6\u54082\uff1a[a, b, c, c]<br>\n\u7ed3\u679c\uff1a[c]\uff0c\u6b64\u7ed3\u679c\u4e2d\u53ea\u4fdd\u7559\u4e86\u4e00\u4e2a<br>\n\u4efb\u610f\u4e00\u4e2a\u96c6\u5408\u4e3a\u7a7a\uff0c\u8fd4\u56de\u53e6\u4e00\u4e2a\u96c6\u5408<br>\n\u4e24\u4e2a\u96c6\u5408\u65e0\u4ea4\u96c6\u5219\u8fd4\u56de\u4e24\u4e2a\u96c6\u5408\u7684\u7ec4\u5408\n\n@param <T> \u96c6\u5408\u5143\u7d20\u7c7b\u578b\n@param coll1 \u96c6\u54081\n@param coll2 \u96c6\u54082\n@return \u5dee\u96c6\u7684\u96c6\u5408\uff0c\u8fd4\u56de {@link ArrayList}", "input": "Java", "output": "public static <T> Collection<T> disjunction(Collection<T> coll1, Collection<T> coll2) {\r\n\t\tif (isEmpty(coll1)) {\r\n\t\t\treturn coll2;\r\n\t\t}\r\n\t\tif (isEmpty(coll2)) {\r\n\t\t\treturn coll1;\r\n\t\t}\r\n\r\n\t\tfinal ArrayList<T> result = new ArrayList<>();\r\n\t\tfinal Map<T, Integer> map1 = countMap(coll1);\r\n\t\tfinal Map<T, Integer> map2 = countMap(coll2);\r\n\t\tfinal Set<T> elts = newHashSet(coll2);\r\n\t\telts.addAll(coll1);\r\n\t\tint m;\r\n\t\tfor (T t : elts) {\r\n\t\t\tm = Math.abs(Convert.toInt(map1.get(t), 0) - Convert.toInt(map2.get(t), 0));\r\n\t\t\tfor (int i = 0; i < m; i++) {\r\n\t\t\t\tresult.add(t);\r\n\t\t\t}\r\n\t\t}\r\n\t\treturn result;\r\n\t}"}, {"instruction": "Create a new SAML ECP response object.\n\n@param assertionConsumerUrl the assertion consumer url\n@return the response", "input": "Java", "output": "public org.opensaml.saml.saml2.ecp.Response newEcpResponse(final String assertionConsumerUrl) {\n        val samlResponse = newSamlObject(org.opensaml.saml.saml2.ecp.Response.class);\n        samlResponse.setSOAP11MustUnderstand(Boolean.TRUE);\n        samlResponse.setSOAP11Actor(ActorBearing.SOAP11_ACTOR_NEXT);\n        samlResponse.setAssertionConsumerServiceURL(assertionConsumerUrl);\n        return samlResponse;\n    }"}, {"instruction": "Saves the client cert settings if the flag is set explicitly.\nOnly works for the CLI currently.", "input": "Java", "output": "private void saveClientCertSettings(){\r\n\r\n        if (getBoolean(PERSIST_CLIENT_CERT, false)){\r\n            logger.warn(\"Saving Client Certificate settings: password will be found in config\");\r\n            setUseClientCert(getBoolean(USE_CLIENT_CERT, false));\r\n            setClientCertLocation(getString(CLIENT_CERT_LOCATION, \"\"));\r\n            setClientCertPassword(getString(CLIENT_CERT_PASSWORD, \"\"));\r\n            setClientCertIndex(getInt(CLIENT_CERT_INDEX, 0));\r\n\r\n        } else {\r\n            // Default to clear settings\r\n            setUseClientCert(false);\r\n            setClientCertLocation(\"\");\r\n            setClientCertPassword(\"\");\r\n            setClientCertIndex(0);\r\n        }\r\n    }"}, {"instruction": "Read bytes into the given {@link ByteBuf} and return the amount.", "input": "Java", "output": "protected final int doReadBytes(ByteBuf byteBuf) throws Exception {\n        int writerIndex = byteBuf.writerIndex();\n        int localReadAmount;\n        unsafe().recvBufAllocHandle().attemptedBytesRead(byteBuf.writableBytes());\n        if (byteBuf.hasMemoryAddress()) {\n            localReadAmount = socket.readAddress(byteBuf.memoryAddress(), writerIndex, byteBuf.capacity());\n        } else {\n            ByteBuffer buf = byteBuf.internalNioBuffer(writerIndex, byteBuf.writableBytes());\n            localReadAmount = socket.read(buf, buf.position(), buf.limit());\n        }\n        if (localReadAmount > 0) {\n            byteBuf.writerIndex(writerIndex + localReadAmount);\n        }\n        return localReadAmount;\n    }"}, {"instruction": "Get a sum of bucket counts from either the start of a histogram's range or end, up to a specified cutoff value.\n\nFor example, if I have the following histogram with a range of 0-40, with 4 buckets and\nper-bucket counts of 5, 2, 10, and 7:\n\n|   5   |   2   |   24   |   7   |\n0       10      20       30      40\n\nCalling this function with a cutoff of 25 and fromStart = true would:\n- Sum the first two bucket counts 5 + 2\n- Since the cutoff falls in the third bucket, multiply the third bucket's count by the fraction of the bucket range\ncovered by the cutoff, in this case the fraction is ((25 - 20) / 10) = 0.5\n- The total count returned is 5 + 2 + 12\n\n@param cutoff Cutoff point within the histogram's range\n@param fromStart If true, sum the bucket counts starting from the beginning of the histogram range.\nIf false, sum from the other direction, starting from the end of the histogram range.\n@return Sum of bucket counts up to the cutoff point", "input": "Java", "output": "private double getCumulativeCount(double cutoff, boolean fromStart)\n  {\n    int cutoffBucket = (int) ((cutoff - lowerLimit) / bucketSize);\n    double count = 0;\n\n    if (fromStart) {\n      for (int i = 0; i <= cutoffBucket; i++) {\n        if (i == cutoffBucket) {\n          double bucketStart = i * bucketSize + lowerLimit;\n          double partialCount = ((cutoff - bucketStart) / bucketSize) * histogram[i];\n          count += partialCount;\n        } else {\n          count += histogram[i];\n        }\n      }\n    } else {\n      for (int i = cutoffBucket; i < histogram.length; i++) {\n        if (i == cutoffBucket) {\n          double bucketEnd = ((i + 1) * bucketSize) + lowerLimit;\n          double partialCount = ((bucketEnd - cutoff) / bucketSize) * histogram[i];\n          count += partialCount;\n        } else {\n          count += histogram[i];\n        }\n      }\n    }\n    return count;\n  }"}, {"instruction": "Get layer output type.\n\n@param inputType Array of InputTypes\n@return output type as InputType\n@throws InvalidKerasConfigurationException Invalid Keras config", "input": "Java", "output": "public InputType getOutputType(InputType... inputType) throws InvalidKerasConfigurationException {\n        if (inputType.length > 1)\n            throw new InvalidKerasConfigurationException(\n                    \"Keras PReLU layer accepts only one input (received \" + inputType.length + \")\");\n        InputType inType = inputType[0];\n\n        // Dynamically infer input shape of PReLU layer from input type\n        PReLULayer shapedLayer = (PReLULayer) this.layer;\n        shapedLayer.setInputShape(inType.getShape());\n        this.layer = shapedLayer;\n\n        return this.getPReLULayer().getOutputType(-1, inputType[0]);\n    }"}, {"instruction": "Creates HostAndPort instance from string.\nString must be in ( host + \":\" + port ) format.\nPort is mandatory. Can convert host part.\n@see #convertHost(String)\n@param from String to parse\n@return HostAndPort instance", "input": "Java", "output": "public static HostAndPort parseString(String from){\n    // NOTE: redis answers with\n    // '99aa9999aa9a99aa099aaa990aa99a09aa9a9999 9a09:9a9:a090:9a::99a slave 8c88888888cc08088cc8c8c888c88c8888c88cc8 0 1468251272993 37 connected'\n    // for CLUSTER NODES, ASK and MOVED scenarios. That's why there is no possibility to parse address in 'correct' way.\n    // Redis should switch to 'bracketized' (RFC 3986) IPv6 address.\n    try {\n      String[] parts = extractParts(from);\n      String host = parts[0];\n      int port = Integer.parseInt(parts[1]);\n      return new HostAndPort(convertHost(host), port);\n    } catch (NumberFormatException ex) {\n      throw new IllegalArgumentException(ex);\n    }\n  }"}, {"instruction": "\u5904\u7406\u6392\u5e8f\n\n@param entityTable\n@param field\n@param entityColumn", "input": "Java", "output": "protected void processOrderBy(EntityTable entityTable, EntityField field, EntityColumn entityColumn) {\n        String orderBy = \"\";\n        if(field.isAnnotationPresent(OrderBy.class)){\n            orderBy = field.getAnnotation(OrderBy.class).value();\n            if (\"\".equals(orderBy)) {\n                orderBy = \"ASC\";\n            }\n            log.warn(OrderBy.class + \" is outdated, use \" + Order.class + \" instead!\");\n        }\n        if (field.isAnnotationPresent(Order.class)) {\n            Order order = field.getAnnotation(Order.class);\n            if (\"\".equals(order.value()) && \"\".equals(orderBy)) {\n                orderBy = \"ASC\";\n            } else {\n                orderBy = order.value();\n            }\n            entityColumn.setOrderPriority(order.priority());\n        }\n        if (StringUtil.isNotEmpty(orderBy)) {\n            entityColumn.setOrderBy(orderBy);\n        }\n    }"}, {"instruction": "reads, plus computes rows-per-chunk, min/max/mean, etc.", "input": "Java", "output": "public Vec close(int rowLayout, Futures fs) {\n    // Compute #chunks\n    int nchunk = _tmp_espc.length;\n    DKV.remove(chunkKey(nchunk),fs); // remove potential trailing key\n    while( nchunk > 1 && _tmp_espc[nchunk-1] == 0 ) {\n      nchunk--;\n      DKV.remove(chunkKey(nchunk),fs); // remove potential trailing key\n    }\n\n    // Replacement plain Vec for AppendableVec.\n    Vec vec = new Vec(_key, rowLayout, domain(), _type);\n    DKV.put(_key,vec,fs);       // Inject the header into the K/V store\n    return vec;\n  }"}, {"instruction": "Merge this configuration with the specified {@link GridHubConfiguration}\n@param other", "input": "Java", "output": "public void merge(GridHubConfiguration other) {\n    if (other == null) {\n      return;\n    }\n    super.merge(other);\n\n    if (isMergeAble(CapabilityMatcher.class, other.capabilityMatcher, capabilityMatcher)) {\n      capabilityMatcher = other.capabilityMatcher;\n    }\n    if (isMergeAble(Integer.class, other.newSessionWaitTimeout, newSessionWaitTimeout)) {\n      newSessionWaitTimeout = other.newSessionWaitTimeout;\n    }\n    if (isMergeAble(Prioritizer.class, other.prioritizer, prioritizer)) {\n      prioritizer = other.prioritizer;\n    }\n    if (isMergeAble(Boolean.class, other.throwOnCapabilityNotPresent, throwOnCapabilityNotPresent)) {\n      throwOnCapabilityNotPresent = other.throwOnCapabilityNotPresent;\n    }\n    if (isMergeAble(String.class, other.registry, registry)) {\n      registry = other.registry;\n    }\n  }"}, {"instruction": "Wraps the given {@link Throwable} in a {@link StatusRuntimeException}. If it contains an\nembedded {@link StatusException} or {@link StatusRuntimeException}, the returned exception will\ncontain the embedded trailers and status, with the given exception as the cause. Otherwise, an\nexception will be generated from an {@link Status#UNKNOWN} status.", "input": "Java", "output": "private static StatusRuntimeException toStatusRuntimeException(Throwable t) {\n    Throwable cause = checkNotNull(t, \"t\");\n    while (cause != null) {\n      // If we have an embedded status, use it and replace the cause\n      if (cause instanceof StatusException) {\n        StatusException se = (StatusException) cause;\n        return new StatusRuntimeException(se.getStatus(), se.getTrailers());\n      } else if (cause instanceof StatusRuntimeException) {\n        StatusRuntimeException se = (StatusRuntimeException) cause;\n        return new StatusRuntimeException(se.getStatus(), se.getTrailers());\n      }\n      cause = cause.getCause();\n    }\n    return Status.UNKNOWN.withDescription(\"unexpected exception\").withCause(t)\n        .asRuntimeException();\n  }"}, {"instruction": "Returns the type information factory for a type using the factory registry or annotations.", "input": "Java", "output": "@Internal\n\tpublic static <OUT> TypeInfoFactory<OUT> getTypeInfoFactory(Type t) {\n\t\tfinal Class<?> factoryClass;\n\t\tif (registeredTypeInfoFactories.containsKey(t)) {\n\t\t\tfactoryClass = registeredTypeInfoFactories.get(t);\n\t\t}\n\t\telse {\n\t\t\tif (!isClassType(t) || !typeToClass(t).isAnnotationPresent(TypeInfo.class)) {\n\t\t\t\treturn null;\n\t\t\t}\n\t\t\tfinal TypeInfo typeInfoAnnotation = typeToClass(t).getAnnotation(TypeInfo.class);\n\t\t\tfactoryClass = typeInfoAnnotation.value();\n\t\t\t// check for valid factory class\n\t\t\tif (!TypeInfoFactory.class.isAssignableFrom(factoryClass)) {\n\t\t\t\tthrow new InvalidTypesException(\"TypeInfo annotation does not specify a valid TypeInfoFactory.\");\n\t\t\t}\n\t\t}\n\n\t\t// instantiate\n\t\treturn (TypeInfoFactory<OUT>) InstantiationUtil.instantiate(factoryClass);\n\t}"}, {"instruction": "Executes the webflow action.\n\n@param context the context\n@return the event", "input": "Java", "output": "@Override\n    protected Event doExecute(final RequestContext context) {\n        try {\n            val request = WebUtils.getHttpServletRequestFromExternalWebflowContext(context);\n            val wa = request.getParameter(WA);\n            if (StringUtils.isNotBlank(wa) && wa.equalsIgnoreCase(WSIGNIN)) {\n                wsFederationResponseValidator.validateWsFederationAuthenticationRequest(context);\n                return super.doExecute(context);\n            }\n            return wsFederationRequestBuilder.buildAuthenticationRequestEvent(context);\n        } catch (final Exception ex) {\n            LOGGER.error(ex.getMessage(), ex);\n            throw new UnauthorizedServiceException(UnauthorizedServiceException.CODE_UNAUTHZ_SERVICE, ex.getMessage());\n        }\n    }"}, {"instruction": "find the leftmost node of a polygon ring", "input": "JavaScript", "output": "function getLeftmost(start) {\n        var p = start, leftmost = start;\n        do {\n            if (p.x < leftmost.x)\n                leftmost = p;\n            p = p.next;\n        } while (p !== start);\n        return leftmost;\n    }"}, {"instruction": "Copyright (c) 2006-2015, JGraph Ltd\nCopyright (c) 2006-2015, Gaudenz Alder\n \n\nClass: mxMorphing\n\nImplements animation for morphing cells. Here is an example of\nusing this class for animating the result of a layout algorithm:\n\n(code)\ngraph.getModel().beginUpdate();\ntry\n{\nvar circleLayout = new mxCircleLayout(graph);\ncircleLayout.execute(graph.getDefaultParent());\n}\nfinally\n{\nvar morph = new mxMorphing(graph);\nmorph.addListener(mxEvent.DONE, function()\n{\ngraph.getModel().endUpdate();\n});\n\nmorph.startAnimation();\n}\n(end)\n\nConstructor: mxMorphing\n\nConstructs an animation.\n\nParameters:\n\ngraph - Reference to the enclosing <mxGraph>.\nsteps - Optional number of steps in the morphing animation. Default is 6.\nease - Optional easing constant for the animation. Default is 1.5.\ndelay - Optional delay between the animation steps. Passed to <mxAnimation>.", "input": "JavaScript", "output": "function mxMorphing(graph, steps, ease, delay)\n{\n\tmxAnimation.call(this, delay);\n\tthis.graph = graph;\n\tthis.steps = (steps != null) ? steps : 6;\n\tthis.ease = (ease != null) ? ease : 1.5;\n}"}, {"instruction": "Built-in hook to upload PIXI.Graphics to the GPU.\n\n@private\n@param {PIXI.Renderer} renderer - instance of the webgl renderer\n@param {PIXI.DisplayObject} item - Item to check\n@return {boolean} If item was uploaded.", "input": "JavaScript", "output": "function uploadGraphics(renderer, item)\n{\n    if (item instanceof Graphics)\n    {\n        // if the item is not dirty and already has webgl data, then it got prepared or rendered\n        // before now and we shouldn't waste time updating it again\n        if (item.dirty || item.clearDirty || !item._webGL[renderer.plugins.graphics.CONTEXT_UID])\n        {\n            renderer.plugins.graphics.updateGraphics(item);\n        }\n\n        return true;\n    }\n\n    return false;\n}"}, {"instruction": "VML override to create a regular HTML image\n@param {String} src\n@param {Number} x\n@param {Number} y\n@param {Number} width\n@param {Number} height", "input": "JavaScript", "output": "function (src, x, y, width, height) {\r\n\t\tvar obj = this.createElement('img')\r\n\t\t\t.attr({ src: src });\r\n\r\n\t\tif (arguments.length > 1) {\r\n\t\t\tobj.css({\r\n\t\t\t\tleft: x,\r\n\t\t\t\ttop: y,\r\n\t\t\t\twidth: width,\r\n\t\t\t\theight: height\r\n\t\t\t});\r\n\t\t}\r\n\t\treturn obj;\r\n\t}"}, {"instruction": "Closes the row stream.\n\n@param {Error} [err]", "input": "JavaScript", "output": "function(err)\n  {\n    // if we have a result stream, stop listening to events on it\n    if (resultStream)\n    {\n      resultStream.removeListener('data', onResultStreamData);\n      resultStream.removeListener('close', onResultStreamClose);\n    }\n\n    // we're done, so time to clean up\n    rowBuffer = null;\n    rowIndex = 0;\n    resultStream = null;\n    numResultStreamInterrupts = 0;\n\n    if (err)\n    {\n      emitError(err);\n    }\n    else\n    {\n      self.push(null);\n    }\n  }"}, {"instruction": "Get the real native browser event from a jQuery event object", "input": "JavaScript", "output": "function(oEvent) {\n\t\t\twhile (oEvent && oEvent.originalEvent && oEvent !== oEvent.originalEvent) {\n\t\t\t\toEvent = oEvent.originalEvent;\n\t\t\t}\n\t\t\treturn oEvent;\n\t\t}"}, {"instruction": "lets you call a css selector with a guard", "input": "JavaScript", "output": "function (args, env) {\n        var lastSelector = this.selectors[this.selectors.length-1];\n        if (!lastSelector.evaldCondition) {\n            return false;\n        }\n        if (lastSelector.condition &&\n            !lastSelector.condition.eval(\n                new(tree.evalEnv)(env,\n                    env.frames))) {\n            return false;\n        }\n        return true;\n    }"}, {"instruction": "Crop connection if connection cropping is provided.\n\n@param {Connection} connection\n@param {Array<Point>} newWaypoints\n\n@return {Array<Point>} cropped connection waypoints", "input": "JavaScript", "output": "function cropConnection(connection, newWaypoints) {\n\n    // crop connection, if docking service is provided only\n    if (!connectionDocking) {\n      return newWaypoints;\n    }\n\n    var oldWaypoints = connection.waypoints,\n        croppedWaypoints;\n\n    // temporary set new waypoints\n    connection.waypoints = newWaypoints;\n\n    croppedWaypoints = connectionDocking.getCroppedWaypoints(connection);\n\n    // restore old waypoints\n    connection.waypoints = oldWaypoints;\n\n    return croppedWaypoints;\n  }"}, {"instruction": "Locate last brace in `str` within the key.\n\n@param {String} str\n@return {Number}\n@api private", "input": "JavaScript", "output": "function lastBraceInKey(str) {\n  var len = str.length\n    , brace\n    , c;\n  for (var i = 0; i < len; ++i) {\n    c = str[i];\n    if (']' == c) brace = false;\n    if ('[' == c) brace = true;\n    if ('=' == c && !brace) return i;\n  }\n}"}, {"instruction": "Converts a SAT.Polygon into a SVG path string.", "input": "JavaScript", "output": "function poly2path(polygon) {\n  var pos = polygon.pos;\n  var points = polygon.calcPoints;\n  var result = 'M' + pos.x + ' ' + pos.y;\n  result += 'M' + (pos.x + points[0].x) + ' ' + (pos.y + points[0].y);\n  for (var i = 1; i < points.length; i++) {\n    var point = points[i];\n    result += 'L' + (pos.x + point.x) + ' ' + (pos.y + point.y);\n  }\n  result += 'Z';\n  return result;\n}"}, {"instruction": "@\n#Crafty.map.remove\n@comp Crafty.map\n@kind Method\n\n@sign public void Crafty.map.remove(Entry entry)\n@param entry - An entry to remove from the hashmap\n\nRemove an entry from the broad phase map.\n\n@example\n~~~\nCrafty.map.remove(e);\n~~~", "input": "JavaScript", "output": "function(entry) {\n        var keys = entry.keys;\n        var obj = entry.obj;\n        var i = 0,\n            j,\n            hash;\n\n        //search in all x buckets\n        for (i = keys.x1; i <= keys.x2; i++) {\n            //insert into all y buckets\n            for (j = keys.y1; j <= keys.y2; j++) {\n                hash = (i << 16) ^ j;\n\n                if (this.map[hash]) {\n                    var cell = this.map[hash],\n                        m,\n                        n = cell.length;\n                    //loop over objs in cell and delete\n                    for (m = 0; m < n; m++)\n                        if (cell[m] && cell[m][0] === obj[0]) cell.splice(m, 1);\n                }\n            }\n        }\n\n        //mark map boundaries as dirty\n        this.boundsDirty = true;\n    }"}, {"instruction": "Trigger update to all registered materials.", "input": "JavaScript", "output": "function (material) {\n    var materials = this.materials;\n    Object.keys(materials).forEach(function (uuid) {\n      materials[uuid].needsUpdate = true;\n    });\n  }"}, {"instruction": "/*\nCreates new sap.ui.layout.ResponsiveFlowLayoutData with the given parameters\n@param {int} iWeight the weight for the layout data\n@param {boolean} bLinebreak Whether the layout data has a linebreak\n@param {boolean} bLinebreakable Whether the layout data is linebreakable\n@returns {sap.ui.layout.ResponsiveFlowLayoutData} The newly created ResponsiveFlowLayoutData\n@private", "input": "JavaScript", "output": "function _createRFLayoutData(iWeight, bLinebreak, bLinebreakable, iMinWidth) {\n\n\t\tvar oLayout = new ResponsiveFlowLayoutData({weight:iWeight,linebreak:bLinebreak === true,linebreakable: bLinebreakable === true});\n\t\tif (iMinWidth) {\n\t\t\toLayout.setMinWidth(iMinWidth);\n\t\t}\n\t\tthis._aLayouts.push(oLayout.getId());\n\t\treturn oLayout;\n\n\t}"}, {"instruction": "Map a function over a list", "input": "JavaScript", "output": "function map(fn, list) {\n\n   return list\n            ? cons(fn(head(list)), map(fn,tail(list)))\n            : emptyList\n            ;\n}"}, {"instruction": "Creates token that can be fed to <code>EditElement</code>\n@param {Number} start\n@param {String} value\n@param {String} type\n@returns", "input": "JavaScript", "output": "function(start, value, type) {\n\t\t\tvar obj = {\n\t\t\t\tstart: start || 0,\n\t\t\t\tvalue: value || '',\n\t\t\t\ttype: type\n\t\t\t};\n\t\t\t\n\t\t\tobj.end = obj.start + obj.value.length;\n\t\t\treturn obj;\n\t\t}"}, {"instruction": "Exports a [SurfacePolyline]{@link SurfacePolyline} in WKT format of type LineString.\n@param {SurfacePolyline} renderable The SurfacePolyline object.\n@throws {ArgumentError} If the specified argument is null or undefined.\n@returns {String} WKT format.", "input": "JavaScript", "output": "function (renderable) {\n                if (!(renderable instanceof WorldWind.SurfacePolyline)) {\n                    throw new ArgumentError(\n                        Logger.logMessage(Logger.LEVEL_SEVERE, \"WktExporter\", \"exportSurfacePolyline\",\n                            \"invalidTypeOfRenderable\"));\n                }\n\n                var sb = WktType.SupportedGeometries.LINE_STRING + '(';\n                for (var i = 0; i < renderable.boundaries.length; i++) {\n                    sb = sb + renderable.boundaries[i].longitude + ' ' +\n                        renderable.boundaries[i].latitude;\n                    sb = sb + ', ';\n                }\n                sb = sb.substring(0, sb.length - 2);\n                sb = sb + ')';\n                return sb;\n            }"}, {"instruction": "Convert CH y/x to WGS lat", "input": "JavaScript", "output": "function CHtoWGSlat(y, x) {\n\n  // Converts militar to civil and  to unit = 1000km\n  // Axiliary values (% Bern)\n  const y_aux = (y - 600000) / 1000000;\n  const x_aux = (x - 200000) / 1000000;\n\n  // Process lat\n  let lat = 16.9023892 +\n      3.238272 * x_aux -\n      0.270978 * Math.pow(y_aux, 2) -\n      0.002528 * Math.pow(x_aux, 2) -\n      0.0447 * Math.pow(y_aux, 2) * x_aux -\n      0.0140 * Math.pow(x_aux, 3);\n\n  // Unit 10000\" to 1 \" and converts seconds to degrees (dec)\n  lat = lat * 100 / 36;\n\n  return lat;\n\n}"}, {"instruction": "\u5728 trailing edge \u4e14\u65f6\u95f4\u7b26\u5408\u6761\u4ef6\u65f6\uff0c\u8c03\u7528 trailingEdge\u51fd\u6570\uff0c\u5426\u5219\u91cd\u542f\u5b9a\u65f6\u5668", "input": "JavaScript", "output": "function timerExpired() {\n    let time = new Date()\n      .getTime();\n    if (shouldInvoke(time)) {\n      return trailingEdge(time);\n    }\n    // \u91cd\u542f\u5b9a\u65f6\u5668\n    timerId = setTimeout(timerExpired, remainingWait(time));\n  }"}, {"instruction": "Tag this match with type and return it for chaining\n\n@param {!RegExp.match} match  RegExp Match object with steps function parameters\nin array position 1 (and optionally 2).\n@param {number} type Either BEZIER or STEP\n@return {RegExp.match} Same object that was passed in.", "input": "JavaScript", "output": "function _tagMatch(match, type) {\n        switch (type) {\n        case BEZIER:\n            match.isBezier = true;\n            break;\n        case STEP:\n            match.isStep = true;\n            break;\n        }\n\n        return match;\n    }"}, {"instruction": "Sets which annotations types are shown in the overview ruler.  Annotations are visible by default.\n\n@param {Object} types a hash table mapping annotation type to visibility (i.e. AnnotationType.ANNOTATION_INFO -> true).\n@since 14.0", "input": "JavaScript", "output": "function(types) {\n\t\t\tif (textUtil.compare(this._overviewAnnotationTypesVisible, types)) return;\n\t\t\tthis._overviewAnnotationTypesVisible = types;\n\t\t\tif (!this._overviewRuler || !this._textView || !this._overviewRulerVisible) { return; }\n\t\t\tthis._overviewRuler.setAnnotationTypeVisible(types);\n\t\t\tthis._textView.redrawLines(0, undefined, this._overviewRuler);\n\t\t}"}, {"instruction": "\u6dfb\u52a0change\u4e8b\u4ef6\n@method change\n@param {Function} fn \u56de\u8c03\u51fd\u6570\n@param {Boolean} after \u65f6\u5019\u7ed1\u5b9aafter\u4e8b\u4ef6\n@chainable", "input": "JavaScript", "output": "function (fn, after) {\n    if (typeof fn === FUNCTION) {\n      this._subs[after ? AFTER : ON].push(fn)\n    }\n\n    return this\n  }"}, {"instruction": "W3C MessageEvent\n\n@see http://www.w3.org/TR/webmessaging/#event-definitions\n@api private", "input": "JavaScript", "output": "function MessageEvent (type, eventInitDict) {\n  Object.defineProperty(this, 'type', { writable: false, value: type, enumerable: true })\n  for (var f in eventInitDict) {\n    if (eventInitDict.hasOwnProperty(f)) {\n      Object.defineProperty(this, f, { writable: false, value: eventInitDict[f], enumerable: true })\n    }\n  }\n}"}, {"instruction": "Define the star object\n\n@param {number} x\n@param {number} y\n@param {number} starSize\n@param {context} ctx\n@param {canvas} fgCanvas\n@param {analyser} analyser\n@param {Uint8Array} streamData", "input": "JavaScript", "output": "function Star( x, y, starSize, ctx, fgCanvas, analyser, streamData ){\n\tthis.x = x;\n\tthis.y = y;\n\tthis.angle = Math.atan( Math.abs(y) / Math.abs(x) );\n\tthis.starSize = starSize;\n\tthis.ctx = ctx;\n\tthis.high = 0;\n\tthis.fgCanvas = fgCanvas;\n\tthis.analyser = analyser;\n\tthis.streamData = streamData;\n}"}, {"instruction": "Check if is running inside Now.sh and apply variables and secrets to `process.env`", "input": "JavaScript", "output": "function config() {\n  // only run this if it's not running inside Now.sh\n  if (Boolean(process.env.NOW_REGION || process.env.NOW)) return\n\n  const secrets = loadSecrets()\n  const required = loadRequired()\n\n  // load environment variables from now.json\n  loadNowJSON(secrets, required)\n}"}, {"instruction": "-- effects ---------------------------------------------- \nRuns all effects of a given type for the given set of property changes\non an instance.\n\n@param {!PropertyEffectsType} inst The instance with effects to run\n@param {Object} effects Object map of property-to-Array of effects\n@param {Object} props Bag of current property changes\n@param {Object=} oldProps Bag of previous values for changed properties\n@param {boolean=} hasPaths True with `props` contains one or more paths\n@param {*=} extraArgs Additional metadata to pass to effect function\n@return {boolean} True if an effect ran for this property\n@private", "input": "JavaScript", "output": "function runEffects(inst, effects, props, oldProps, hasPaths, extraArgs) {\n  if (effects) {\n    let ran = false;\n    let id = dedupeId++;\n    for (let prop in props) {\n      if (runEffectsForProperty(inst, effects, id, prop, props, oldProps, hasPaths, extraArgs)) {\n        ran = true;\n      }\n    }\n    return ran;\n  }\n  return false;\n}"}, {"instruction": "Create a directory\n@param {String} dirPath The path to create\n@param {UserOptions=} options Options for the request\n@memberof ClientInterface\n@returns {Promise} A promise that resolves when the remote path has been created\n@example\nawait client.createDirectory(\"/my/directory\");", "input": "JavaScript", "output": "function createDirectory(dirPath, options) {\n            const createOptions = merge(baseOptions, options || {});\n            return createDir.createDirectory(dirPath, createOptions);\n        }"}, {"instruction": "Special handler for mouse move that will hide the tooltip when the mouse leaves the plotarea.", "input": "JavaScript", "output": "function hideTooltipOnMouseMove(e) {\r\n\t\t\tvar pageX = defined(e.pageX) ? e.pageX : e.page.x, // In mootools the event is wrapped and the page x/y position is named e.page.x\r\n\t\t\t\tpageY = defined(e.pageX) ? e.pageY : e.page.y; // Ref: http://mootools.net/docs/core/Types/DOMEvent\r\n\r\n\t\t\tif (chartPosition &&\r\n\t\t\t\t\t!isInsidePlot(pageX - chartPosition.left - plotLeft,\r\n\t\t\t\t\t\tpageY - chartPosition.top - plotTop)) {\r\n\t\t\t\tresetTracker();\r\n\t\t\t}\r\n\t\t}"}, {"instruction": "/*\nChange findNodes ==> filterNodes\n \n@name filterNodes\n@type {function}\n@description\nFunction to find certain nodes based on a filter passed.\n@param {object} ast   AST node\n@param {function} filter  Filter function to find nodes\n@return {object[]} Array of all the nodes found", "input": "JavaScript", "output": "function(ast, filter) {\n  checkASTandFunction(ast, 'ast', filter, 'filter');\n\n  let result = [];\n  walkNodes(ast, node => {\n    if (filter(node)) result.push(Object.assign({}, node));\n  });\n  return result;\n}"}, {"instruction": "@private\n\nInjects LiveReload script into HTML\n\n@param {Object} ctx\n@param {number} port - server port number", "input": "JavaScript", "output": "function _injectLiveReload(ctx, port) {\n  const { hostname } = ctx.request;\n  const wsPort = _websocketPort(ctx, port);\n  const script = `<script src=\"/dashboard/javascripts/livereload.js?snipver=1&port=${wsPort}&host=${hostname}\"></script>`;\n\n  ctx.body = ctx.body.replace(/(<\\/body>(?![\\s\\S]*<\\/body>[\\s\\S]*$))/i, `${script}\\n$1`);\n}"}, {"instruction": "todos unmarked count", "input": "JavaScript", "output": "function setBadge(todos) {\n  if (chrome.browserAction) {\n    const count = todos.filter(todo => !todo.marked).length;\n    chrome.browserAction.setBadgeText({ text: count > 0 ? count.toString() : '' });\n  }\n}"}, {"instruction": "get a valid repeat mode from an md-mode attribute string.", "input": "JavaScript", "output": "function getRepeatMode(modeStr) {\n    if (!modeStr) { return REPEAT_VIRTUAL; }\n    modeStr = modeStr.toLowerCase();\n    return  REPEAT_MODES.indexOf(modeStr) > -1 ? modeStr : REPEAT_VIRTUAL;\n  }"}, {"instruction": "Join lists with \"i\" and \"i+1\"", "input": "JavaScript", "output": "function addToList(i, L, R) {\n    R[L[i + 1]] = R[i];\n    L[R[i]] = L[i + 1];\n    R[i] = i + 1;\n    L[i + 1] = i;\n}"}, {"instruction": "Add new layers to both existing maps\n@param {Object} splitMaps\n@param {Object|Array<Object>} layers\n@returns {Array<Object>} new splitMaps", "input": "JavaScript", "output": "function addNewLayersToSplitMap(splitMaps, layers) {\n  const newLayers = Array.isArray(layers) ? layers : [layers];\n\n  if (!splitMaps || !splitMaps.length || !newLayers.length) {\n    return splitMaps;\n  }\n\n  // add new layer to both maps,\n  //  don't override, if layer.id is already in splitMaps.settings.layers\n  return splitMaps.map(settings => ({\n    ...settings,\n    layers: {\n      ...settings.layers,\n      ...newLayers.reduce(\n        (accu, newLayer) =>\n          newLayer.config.isVisible\n            ? {\n                ...accu,\n                [newLayer.id]: settings.layers[newLayer.id]\n                  ? settings.layers[newLayer.id]\n                  : generateLayerMetaForSplitViews(newLayer)\n              }\n            : accu,\n        {}\n      )\n    }\n  }));\n}"}, {"instruction": "Checks if the image of a given enabled element fitted the window\nbefore the resize\n\n@param {EnabledElement} enabledElement The Cornerstone Enabled Element\n@param {number} oldCanvasWidth The width of the canvas before the resize\n@param {number} oldCanvasHeight The height of the canvas before the resize\n@return {Boolean} true if it fitted the windows, false otherwise", "input": "JavaScript", "output": "function wasFitToWindow (enabledElement, oldCanvasWidth, oldCanvasHeight) {\r\n  const scale = enabledElement.viewport.scale;\r\n  const imageSize = getImageSize(enabledElement.image, enabledElement.viewport.rotation);\r\n  const imageWidth = Math.round(imageSize.width * scale);\r\n  const imageHeight = Math.round(imageSize.height * scale);\r\n  const x = enabledElement.viewport.translation.x;\r\n  const y = enabledElement.viewport.translation.y;\r\n\r\n  return (imageWidth === oldCanvasWidth && imageHeight <= oldCanvasHeight) ||\r\n    (imageWidth <= oldCanvasWidth && imageHeight === oldCanvasHeight) &&\r\n    (x === 0 && y === 0);\r\n}"}, {"instruction": "Pushs a variable scope (Program or Function) information to the stack.\n\nThis is used in order to check whether or not `this` binding is a\nreference to the global object.\n\n@param {ASTNode} node - A node of the scope. This is one of Program,\nFunctionDeclaration, FunctionExpression, and ArrowFunctionExpression.\n@returns {void}", "input": "JavaScript", "output": "function enterVarScope(node) {\n            const strict = context.getScope().isStrict;\n\n            funcInfo = {\n                upper: funcInfo,\n                node,\n                strict,\n                defaultThis: false,\n                initialized: strict\n            };\n        }"}, {"instruction": "parse JSDoc from leading comments\n@param  {...[type]} comments [description]\n@return {{doc: object}}", "input": "JavaScript", "output": "function captureJsDoc(comments) {\n  let doc\n\n  // capture XSDoc\n  comments.forEach(comment => {\n    // skip non-block comments\n    if (comment.type !== 'Block') return\n    try {\n      doc = doctrine.parse(comment.value, { unwrap: true })\n    } catch (err) {\n      /* don't care, for now? maybe add to `errors?` */\n    }\n  })\n\n  return doc\n}"}, {"instruction": "Properties of a FieldOptions.\n@memberof google.protobuf\n@interface IFieldOptions\n@property {google.protobuf.FieldOptions.CType|null} [ctype] FieldOptions ctype\n@property {boolean|null} [packed] FieldOptions packed\n@property {google.protobuf.FieldOptions.JSType|null} [jstype] FieldOptions jstype\n@property {boolean|null} [lazy] FieldOptions lazy\n@property {boolean|null} [deprecated] FieldOptions deprecated\n@property {boolean|null} [weak] FieldOptions weak\n@property {Array.<google.protobuf.IUninterpretedOption>|null} [uninterpretedOption] FieldOptions uninterpretedOption\n \nConstructs a new FieldOptions.\n@memberof google.protobuf\n@classdesc Represents a FieldOptions.\n@implements IFieldOptions\n@constructor\n@param {google.protobuf.IFieldOptions=} [properties] Properties to set", "input": "JavaScript", "output": "function FieldOptions(properties) {\n                this.uninterpretedOption = [];\n                if (properties)\n                    for (var keys = Object.keys(properties), i = 0; i < keys.length; ++i)\n                        if (properties[keys[i]] != null)\n                            this[keys[i]] = properties[keys[i]];\n            }"}, {"instruction": "Return a page by its path\n@param {String} filePath\n@return {Object|undefined}", "input": "JavaScript", "output": "function getPageByPath(filePath) {\n        var page = output.getPage(filePath);\n        if (!page) return undefined;\n\n        return JSONUtils.encodePage(page, summary);\n    }"}, {"instruction": "Gets the scroll value of the given element in the given side (top and left)\n@method\n@memberof Popper.Utils\n@argument {Element} element\n@argument {String} side `top` or `left`\n@returns {number} amount of scrolled pixels", "input": "JavaScript", "output": "function getScroll(element, side = 'top') {\n  const upperSide = side === 'top' ? 'scrollTop' : 'scrollLeft';\n  const nodeName = element.nodeName;\n\n  if (nodeName === 'BODY' || nodeName === 'HTML') {\n    const html = element.ownerDocument.documentElement;\n    const scrollingElement = element.ownerDocument.scrollingElement || html;\n    return scrollingElement[upperSide];\n  }\n\n  return element[upperSide];\n}"}, {"instruction": "\u8fd4\u56de\u524d\u51e0\u5468\u6216\u540e\u51e0\u5468\u7684\u661f\u671f\u51e0\n\n@param {Date} date \u65e5\u671f\n@param {Number} week \u5468(\u9ed8\u8ba4\u5f53\u524d\u5468)\u3001\u524d\u51e0\u5468\u3001\u540e\u51e0\u5468\n@param {Number} day \u661f\u671f\u5929(\u9ed8\u8ba40)\u3001\u661f\u671f\u4e00(1)\u3001\u661f\u671f\u4e8c(2)\u3001\u661f\u671f\u4e09(3)\u3001\u661f\u671f\u56db(4)\u3001\u661f\u671f\u4e94(5)\u3001\u661f\u671f\u516d(6)\n@return {Date}", "input": "JavaScript", "output": "function getWhatWeek (date, week, day) {\n  var time, whatDayTime, currentDay, customDay\n  date = toStringDate(date)\n  if (baseExports.isDate(date)) {\n    customDay = Number(/^[0-7]$/.test(day) ? day : date.getDay())\n    currentDay = date.getDay()\n    time = getDateTime(date)\n    whatDayTime = time + ((customDay === 0 ? 7 : customDay) - (currentDay === 0 ? 7 : currentDay)) * DAY_TIME\n    if (week && !isNaN(week)) {\n      whatDayTime += week * WEEK_TIME\n    }\n    return new Date(whatDayTime)\n  }\n  return date\n}"}, {"instruction": "Documents are considered to be out-of-sync if they are dirty and\ndo not have \"update while editing\" support\n@param {Document} doc\n@return {boolean}", "input": "JavaScript", "output": "function _docIsOutOfSync(doc) {\n        var liveDoc = _server && _server.get(doc.file.fullPath),\n            isLiveEditingEnabled = liveDoc && liveDoc.isLiveEditingEnabled();\n\n        return doc.isDirty && !isLiveEditingEnabled;\n    }"}, {"instruction": "Reorder tracks in a playlist.\n@param {string} playlistId The playlist's ID\n@param {int} rangeStart The position of the first track to be reordered.\n@param {int} insertBefore The position where the tracks should be inserted.\n@param {Object} options Optional parameters, i.e. range_length and snapshot_id.\n@param {requestCallback} [callback] Optional callback method to be called instead of the promise.\n@returns {Promise|undefined} A promise that if successful returns an object containing a snapshot_id. If rejected,\nit contains an error object. Not returned if a callback is given.", "input": "JavaScript", "output": "function(\n    playlistId,\n    rangeStart,\n    insertBefore,\n    options,\n    callback\n  ) {\n    return WebApiRequest.builder(this.getAccessToken())\n      .withPath('/v1/playlists/' + playlistId + '/tracks')\n      .withHeaders({ 'Content-Type': 'application/json' })\n      .withBodyParameters(\n        {\n          range_start: rangeStart,\n          insert_before: insertBefore\n        },\n        options\n      )\n      .build()\n      .execute(HttpManager.put, callback);\n  }"}, {"instruction": "Create a string representing the current set of meshes for a given set of tiles, based on their created timestamp. Used to determine when tiles should be re-collided.", "input": "JavaScript", "output": "function meshSetString (tiles) {\n    return JSON.stringify(\n        Object.entries(tiles).map(([,t]) => {\n            return Object.entries(t.meshes).map(([,s]) => {\n                return s.map(m => m.created_at);\n            });\n        })\n    );\n}"}, {"instruction": "Repeats a string n times with given separator\n@param str string to repeat\n@param n number of times\n@param sep separator\n@returns {*}", "input": "JavaScript", "output": "function strRepeat(str, n, sep) {\n  if(!n) {\n    return str;\n  }\n  return str + sep + strRepeat(str, --n, sep);\n}"}, {"instruction": "The relative require() itself.", "input": "JavaScript", "output": "function localRequire(path) {\n    var resolved = localRequire.resolve(path);\n    return require(resolved, parent, path);\n  }"}, {"instruction": "actually log the message", "input": "JavaScript", "output": "function logMessage(message)\n    {\n        // post the message\n        var msg = WebInspector.ConsoleMessage.create(\n            WebInspector.ConsoleMessage.MessageSource.Other,\n            messageLevel || WebInspector.ConsoleMessage.MessageLevel.Debug,\n            message);\n\n        self.console.addMessage(msg);\n        if (showConsole)\n            WebInspector.showConsole();\n    }"}, {"instruction": "Log docker to AWS.\n@param region\n@param accountId\n@param username\n@param password\n@returns {Promise}", "input": "JavaScript", "output": "function loginToAws(region, accountId, username, password) {\n    const commandLine = `docker login --username AWS --password ${password} https://${accountId}.dkr.ecr.${region}.amazonaws.com`;\n    return new Promise(\n        (resolve, reject) =>\n            command(commandLine, (err, stdout) => {\n                if (err) {\n                    reject(err);\n                }\n                resolve(stdout);\n            }),\n        { silent: true }\n    );\n}"}, {"instruction": "Sum the width of all tabs.\n\n@param elements\n@returns {number}", "input": "JavaScript", "output": "function getTotalTabsWidth(elements) {\n    var sum = 0, i, tab;\n\n    for (i = 0; i < elements.tabs.length; i++) {\n      tab = elements.tabs[i];\n      sum += tab.offsetWidth;\n    }\n\n    return sum;\n  }"}, {"instruction": "return path to found, project first", "input": "JavaScript", "output": "function projectOrHereRequire(id,root) {\n\ttry {\n\t\treturn resolve.sync(id, {\n\t\t\tpackage: path.join(root,'package.json'),\n\t\t\tpaths: [root],\n\t\t\tbasedir:root\n\t\t});\n\t} catch(ex) {\n\t\t// console.error(ex);\n\t}\n\n\tvar here = path.join(__dirname,'..','..');\n\ttry {\n\t\tvar p = resolve.sync(id, {\n\t\t\tpackage: path.join(here,'package.json'),\n\t\t\tpaths: [here],\n\t\t\tbasedir:here\n\t\t});\n\t\treturn p;\n\t} catch(ex) {\n\t\t// console.error(ex);\n\t}\n}"}, {"instruction": "Calls the provided DOM property descriptor and returns its result. If the\ndescriptor is not available, use fallbackPropertyName to get the property\nvalue in a clobber-vulnerable way, and use fallbackTest to check if the\nproperty was clobbered, throwing an exception if so.\n@param {?Function} fn\n@param {*} object\n@param {string} fallbackPropertyName\n@param {function(*):boolean} fallbackTest\n@return {?}", "input": "JavaScript", "output": "function genericPropertyGet(fn, object, fallbackPropertyName, fallbackTest) {\n  if (fn) {\n    return fn.apply(object);\n  }\n  var propertyValue = object[fallbackPropertyName];\n  if (!fallbackTest(propertyValue)) {\n    throw new Error('Clobbering detected');\n  }\n  return propertyValue;\n}"}, {"instruction": "Mini-implementation of stream.PassThrough We are far from having need for the full implementation, and we can make assumptions like \"many writes, then only one final read\" and we can ignore encoding specifics", "input": "JavaScript", "output": "function PassThrough() {\n  return {\n    buf: '',\n\n    write: function(b) {\n      this.buf += b;\n    },\n\n    end: function(b) {\n      this.buf += b;\n    },\n\n    read: function() {\n      return this.buf;\n    }\n  };\n}"}, {"instruction": "Prefixes the given path with the API's basePath, if that option is enabled and the API has a basePath.\n\n@param   {string}    path\n@returns {string}", "input": "JavaScript", "output": "function getConfiguredPath (path) {\n    if (options.useBasePath && context.api && context.api.basePath) {\n      return context.api.basePath + path;\n    }\n    else {\n      return path;\n    }\n  }"}, {"instruction": "Pick the next auth stage\n\n@private\n@return {string?} login type\n@throws {NoAuthFlowFoundError} If no suitable authentication flow can be found", "input": "JavaScript", "output": "function() {\n        const flow = this._chooseFlow();\n        console.log(\"Active flow => %s\", JSON.stringify(flow));\n        const nextStage = this._firstUncompletedStage(flow);\n        console.log(\"Next stage: %s\", nextStage);\n        return nextStage;\n    }"}, {"instruction": "Get style\n\n@param {Object} options\n@api private", "input": "JavaScript", "output": "function getStyle(options) {\n  var styles = {\n    nested: 0,\n    expanded: 1,\n    compact: 2,\n    compressed: 3\n  };\n\n  return styles[options.outputStyle] || 0;\n}"}, {"instruction": "Given a source file with @NgModule class(es), find the name of the first @NgModule class.\n\n@param source source file containing one or more @NgModule\n@returns the name of the first @NgModule, or `undefined` if none is found", "input": "JavaScript", "output": "function getFirstNgModuleName(source) {\n    // First, find the @NgModule decorators.\n    const ngModulesMetadata = getDecoratorMetadata(source, 'NgModule', '@angular/core');\n    if (ngModulesMetadata.length === 0) {\n        return undefined;\n    }\n    // Then walk parent pointers up the AST, looking for the ClassDeclaration parent of the NgModule\n    // metadata.\n    const moduleClass = findClassDeclarationParent(ngModulesMetadata[0]);\n    if (!moduleClass || !moduleClass.name) {\n        return undefined;\n    }\n    // Get the class name of the module ClassDeclaration.\n    return moduleClass.name.text;\n}"}, {"instruction": "/*------------------------------------*\\\nStyles & DOM\n\\*------------------------------------", "input": "JavaScript", "output": "function createDragger(el){\n            var state = getState(el);\n\n            var dragger = document.createElement('div');\n            var draggerStyler = document.createElement('div');\n\n            dragger.className = state.config.draggerClass;\n\n            dragger.style.position = 'absolute';\n\n            if (!state.draggerEnabled) {\n                dragger.style.display = 'none';\n            }\n\n            draggerStyler.className = state.config.draggerStylerClass;\n\n            dragger.appendChild(draggerStyler);\n            state.el1.appendChild(dragger);\n\n            return dragger;\n        }"}, {"instruction": "PURE_IMPORTS_START _defer,_empty PURE_IMPORTS_END", "input": "JavaScript", "output": "function iif(condition, trueResult, falseResult) {\n    if (trueResult === void 0) {\n        trueResult = _empty__WEBPACK_IMPORTED_MODULE_1__[\"EMPTY\"];\n    }\n    if (falseResult === void 0) {\n        falseResult = _empty__WEBPACK_IMPORTED_MODULE_1__[\"EMPTY\"];\n    }\n    return Object(_defer__WEBPACK_IMPORTED_MODULE_0__[\"defer\"])(function () { return condition() ? trueResult : falseResult; });\n}"}, {"instruction": "Objects", "input": "JavaScript", "output": "function cloneObject(o) {\n  var clone = {};\n  for (var p in o) { clone[p] = o[p]; }\n  return clone;\n}"}, {"instruction": "Copy the values from one mat3 to another\n\n@param {mat3} out the receiving matrix\n@param {mat3} a the source matrix\n@returns {mat3} out", "input": "JavaScript", "output": "function copy(out, a) {\n  out[0] = a[0];\n  out[1] = a[1];\n  out[2] = a[2];\n  out[3] = a[3];\n  out[4] = a[4];\n  out[5] = a[5];\n  out[6] = a[6];\n  out[7] = a[7];\n  out[8] = a[8];\n  return out;\n}"}, {"instruction": "/* jshint ignore:start \nInitialize the BulkExports version of Preview\n\n@constructor Twilio.Preview.BulkExports\n\n@property {Twilio.Preview.BulkExports.ExportList} exports - exports resource\n@property {Twilio.Preview.BulkExports.ExportConfigurationList} exportConfiguration -\nexportConfiguration resource\n\n@param {Twilio.Preview} domain - The twilio domain\n /* jshint ignore:end", "input": "JavaScript", "output": "function BulkExports(domain) {\n  Version.prototype.constructor.call(this, domain, 'BulkExports');\n\n  // Resources\n  this._exports = undefined;\n  this._exportConfiguration = undefined;\n}"}, {"instruction": "Find dimension by property name\n\n@param {string}\nsName Property name\n@returns {sap.ui.model.analytics.odata4analytics.Dimension} The dimension object to\nwhich the given property name is related, because the property\nholds the dimension key, its text, or is an attribute of this\ndimension. If no such dimension exists, null is returned.\n@public\n@function\n@name sap.ui.model.analytics.odata4analytics.QueryResult#findDimensionByPropertyName", "input": "JavaScript", "output": "function(sName) {\n\t\t\tif (this._oDimensionSet[sName]) { // the easy case\n\t\t\t\treturn this._oDimensionSet[sName];\n\t\t\t}\n\n\t\t\tfor ( var sDimensionName in this._oDimensionSet) {\n\t\t\t\tvar oDimension = this._oDimensionSet[sDimensionName];\n\t\t\t\tvar oTextProperty = oDimension.getTextProperty();\n\t\t\t\tif (oTextProperty && oTextProperty.name == sName) {\n\t\t\t\t\treturn oDimension;\n\t\t\t\t}\n\t\t\t\tif (oDimension.findAttributeByName(sName)) {\n\t\t\t\t\treturn oDimension;\n\t\t\t\t}\n\t\t\t}\n\t\t\treturn null;\n\t\t}"}, {"instruction": "Creates an SVG Cursor for the target element\n\n@param {MouseCursor} svgCursor - The cursor.", "input": "JavaScript", "output": "function setToolCursor(element, svgCursor) {\n  if (!globalConfiguration.state.showSVGCursors) {\n    return;\n  }\n  // TODO: (state vs options) Exit if cursor wasn't updated\n  // TODO: Exit if invalid options to create cursor\n\n  // Note: Max size of an SVG cursor is 128x128, default is 32x32.\n  const cursorBlob = svgCursor.getIconWithPointerSVG();\n  const mousePoint = svgCursor.mousePoint;\n\n  const svgCursorUrl = window.URL.createObjectURL(cursorBlob);\n  element.style.cursor = `url('${svgCursorUrl}') ${mousePoint}, auto`;\n\n  state.svgCursorUrl = svgCursorUrl;\n}"}, {"instruction": "Reports an error with the given message and details and throws it.\n\n@param {sap.ui.model.odata.v4.ODataMetaModel} oMetaModel\nThe OData metadata model\n@param {string} sMessage\nError message\n@param {string} sDetails\nError details\n@throws {Error}", "input": "JavaScript", "output": "function reportAndThrowError(oMetaModel, sMessage, sDetails) {\n\t\tvar oError = new Error(sDetails + \": \" + sMessage);\n\n\t\toMetaModel.oModel.reportError(sMessage, sODataMetaModel, oError);\n\t\tthrow oError;\n\t}"}, {"instruction": "Log the progress of a request object.", "input": "JavaScript", "output": "function _logProgress(req) {\n  req.on('response', function(resp) {\n    var len = parseInt(resp.headers['content-length'], 10),\n      bar = new ProgressBar('[:bar]', {\n        complete: '=',\n        incomplete: ' ',\n        total: len,\n        width: 100, // just use 100\n      });\n\n    req.on('data', function(chunk) {\n      bar.tick(chunk.length);\n    });\n  });\n\n  req.on('error', function(err) {\n    console.log(err);\n    _log('error', 'failed to download node sources,');\n    process.exit(1);\n  });\n\n  return req;\n}"}, {"instruction": "Creates a request for an operation on a given service with\na set of input parameters.\n\n@param config [TableStore.Config] the config to perform the operation on\n@param operation [String] the operation to perform on the service\n@param params [Object] parameters to send to the operation.\nSee the operation's documentation for the format of the\nparameters.", "input": "JavaScript", "output": "function Request(config, operation, params) {\n    var endpoint = new TableStore.Endpoint(config.endpoint);\n    var region = config.region;\n    this.config = config;\n    if (config.maxRetries !== undefined) {\n      TableStore.DefaultRetryPolicy.maxRetryTimes = config.maxRetries;\n    }\n    //\u5982\u679c\u5728sdk\u5916\u90e8\u5305\u88c5\u4e86\u4e00\u5c42domain\uff0c\u5c31\u628a\u5b83\u4f20\u5230this.domain\n    this.domain = domain && domain.active;\n    this.operation = operation;\n    this.params = params || {};\n    this.httpRequest = new TableStore.HttpRequest(endpoint, region);\n    this.startTime = TableStore.util.date.getDate();\n\n    this.response = new TableStore.Response(this);\n    this.restartCount = 0;\n    this._asm = new AcceptorStateMachine(fsm.states, 'build');\n\n    TableStore.SequentialExecutor.call(this);\n    this.emit = this.emitEvent;\n  }"}, {"instruction": "return the distance between this vector and the passed one\n@name distance\n@memberOf me.Vector3d\n@function\n@param {me.Vector2d|me.Vector3d} v\n@return {Number}", "input": "JavaScript", "output": "function (v) {\n            var dx = this.x - v.x, dy = this.y - v.y, dz = this.z - (v.z || 0);\n            return Math.sqrt(dx * dx + dy * dy + dz * dz);\n        }"}, {"instruction": "/*\nThis function calculates the absolute 'left' value for a html node", "input": "JavaScript", "output": "function calculateOffsetLeft(obj)\n{\n\tvar curleft = 0;\n\tif (obj.offsetParent) {\n\t\tcurleft = obj.offsetLeft\n\t\twhile (obj = obj.offsetParent) \n\t\t\tcurleft += obj.offsetLeft;\n\t} else if (obj.x)\n\t\tcurleft += obj.x;\n\treturn curleft;\n}"}, {"instruction": "Remove node from scene", "input": "JavaScript", "output": "function (node) {\n        var idx;\n        if (node instanceof Camera) {\n            idx = this._cameraList.indexOf(node);\n            if (idx >= 0) {\n                this._cameraList.splice(idx, 1);\n            }\n        }\n        else if (node instanceof Light) {\n            idx = this.lights.indexOf(node);\n            if (idx >= 0) {\n                this.lights.splice(idx, 1);\n            }\n        }\n        if (node.name) {\n            delete this._nodeRepository[node.name];\n        }\n    }"}, {"instruction": "Password signup handler", "input": "JavaScript", "output": "function createUser (req, res, next) {\n    User.insert(req.body, { private: true }, function (err, user) {\n      if (err) {\n        res.render('signup', {\n          params: qs.stringify(req.body),\n          request: req.body,\n          providers: settings.providers,\n          error: err.message\n        })\n      } else {\n        authenticator.dispatch('password', req, res, next, function (err, user, info) {\n          if (err) { return next(err) }\n          if (!user) {\n          } else {\n            authenticator.login(req, user)\n            req.sendVerificationEmail =\n              req.provider.emailVerification.enable\n            req.flash('isNewUser', true)\n            next()\n          }\n        })\n      }\n    })\n  }"}, {"instruction": "@private\n\nReturns the full path to the welcome project, which we open on first launch.\n\n@param {string} sampleUrl URL for getting started project\n@param {string} initialPath Path to Brackets directory (see {@link FileUtils::#getNativeBracketsDirectoryPath})\n@return {!string} fullPath reference", "input": "JavaScript", "output": "function _getWelcomeProjectPath(sampleUrl, initialPath) {\n        if (sampleUrl) {\n            // Back up one more folder. The samples folder is assumed to be at the same level as\n            // the src folder, and the sampleUrl is relative to the samples folder.\n            initialPath = initialPath.substr(0, initialPath.lastIndexOf(\"/\")) + \"/samples/\" + sampleUrl;\n        }\n\n        return _ensureTrailingSlash(initialPath); // paths above weren't canonical\n    }"}, {"instruction": "Parse length(L) field of BER TLV\n@param s {type.Stream}\n@returns {integer}", "input": "JavaScript", "output": "function decodeLength(s) {\n\tvar size = new type.UInt8().read(s).value;\n\tif(size & 0x80) {\n\t\tsize &= ~0x80;\n\t\tif(size === 1) {\n\t\t\tsize = new type.UInt8().read(s).value;\n\t\t}\n\t\telse if(size === 2) {\n\t\t\tsize = new type.UInt16Be().read(s).value;\n\t\t}\n\t\telse{\n\t\t\tthrow new error.ProtocolError('NODE_RDP_ASN1_BER_INVALID_LENGTH');\n\t\t}\n\t}\n\treturn size;\n}"}, {"instruction": "We don't need to publish all of a doc's data to the app, that will add many kilobytes of loading overhead.", "input": "JavaScript", "output": "function publicDocData(doc, extraData) {\n  const options = _.assign(extraData || {}, { hasDemo: (doc.docType === 'directive') });\n\n  // This RegEx always retrieves the last source descriptor.\n  // For example it retrieves from `/opt/material/src/core/services/ripple/ripple.js` the following\n  // source descriptor: `src/core/`.\n  // This is needed because components are not only located in `src/components`.\n  let descriptor = doc.fileInfo.filePath.toString().match(/src\\/.*?\\//g).pop();\n  if (descriptor) {\n    descriptor = descriptor.substring(descriptor.indexOf('/') + 1, descriptor.lastIndexOf('/'));\n  }\n\n  return buildDocData(doc, options, descriptor || 'components');\n}"}, {"instruction": "('(' supports condition ')')\n| feature", "input": "JavaScript", "output": "function(){\n    this.skipSpacesAndComments();\n    if ('(' == this.peek().type) {\n      var la = this.lookahead(2).type;\n\n      if ('ident' == la || '{' == la) {\n        return this.feature();\n      } else {\n        this.expect('(');\n        var node = new nodes.Expression;\n        node.push(new nodes.Literal('('));\n        node.push(this.supportsCondition());\n        this.expect(')')\n        node.push(new nodes.Literal(')'));\n        this.skipSpacesAndComments();\n        return node;\n      }\n    }\n  }"}, {"instruction": "$NON-NLS-0$", "input": "JavaScript", "output": "function(){\n\t\t\tvar compareParams = PageUtil.matchResourceParameters();\n\t\t\tvar compareTreeExplorer = new mCompareTreeExplorer.CompareTreeExplorer(serviceRegistry, \"compare-tree-results\", commandRegistry, fileClient); //$NON-NLS-0$\n\t\t\tcompareTreeExplorer.startup(compareParams);\n\t\t\tmGlobalCommands.setPageTarget({\n\t\t\t\ttask: messages.compareTreeTitle\n\t\t\t});\n\t\t}"}, {"instruction": "##### BEGIN: MODIFIED BY SAP polyfill for document.createAttributeNS which was removed from Chrome 34 but will be added back in, see: http://datajs.codeplex.com/workitem/1272 https://code.google.com/p/chromium/issues/detail?id=347506 https://codereview.chromium.org/243333003", "input": "JavaScript", "output": "function(namespaceURI, qualifiedName) {\n        var dummy = document.createElement('dummy');\n        dummy.setAttributeNS(namespaceURI, qualifiedName, '');\n        var attr = dummy.attributes[0];\n        dummy.removeAttributeNode(attr);\n        return attr;\n    }"}, {"instruction": "@\n#.shift\n@comp Crafty.polygon\n@kind Method\n\n@sign public void .shift(Number x, Number y)\n@param x - Amount to shift the `x` axis\n@param y - Amount to shift the `y` axis\n\nShifts every single point in the polygon by the specified amount.\n\n@example\n~~~\nvar poly = new Crafty.polygon([50, 0, 100, 100, 0, 100]);\npoly.shift(5,5);\n//[[55, 5, 105, 5, 5, 105];\n~~~", "input": "JavaScript", "output": "function(x, y) {\n        var i = 0,\n            p = this.points,\n            l = p.length;\n        for (; i < l; i += 2) {\n            p[i] += x;\n            p[i + 1] += y;\n        }\n    }"}, {"instruction": "Doc version select", "input": "JavaScript", "output": "function initVersionSelect () {\n    // version select\n    var versionSelect = document.querySelector('.version-select')\n    versionSelect && versionSelect.addEventListener('change', function (e) {\n      var version = e.target.value\n      var section = window.location.pathname.match(/\\/v\\d\\/(\\w+?)\\//)[1]\n      if (version === 'SELF') return\n      window.location.assign(\n        'http://' +\n        version +\n        (version && '.') +\n        'vuejs.org/' + section + '/'\n      )\n    })\n  }"}, {"instruction": "Returns dx/dt given t, x1, and x2, or dy/dt given t, y1, and y2.", "input": "JavaScript", "output": "function GetSlope(aT, aA1, aA2) {\n\t\t\treturn 3.0 * A(aA1, aA2) * aT * aT + 2.0 * B(aA1, aA2) * aT + C(aA1);\n\t\t}"}, {"instruction": "create an touch point\n@constructor\n@param target\n@param identifier\n@param pos\n@param deltaX\n@param deltaY\n@returns {Object} touchPoint", "input": "JavaScript", "output": "function Touch(target, identifier, pos, deltaX, deltaY) {\n        deltaX = deltaX || 0;\n        deltaY = deltaY || 0;\n\n        this.identifier = identifier;\n        this.target = target;\n        this.clientX = pos.clientX + deltaX;\n        this.clientY = pos.clientY + deltaY;\n        this.screenX = pos.screenX + deltaX;\n        this.screenY = pos.screenY + deltaY;\n        this.pageX = pos.pageX + deltaX;\n        this.pageY = pos.pageY + deltaY;\n    }"}, {"instruction": "Helper functions", "input": "JavaScript", "output": "function requireSecondPassword (options) {\n  return function (wallet) {\n    if (wallet.isDoubleEncrypted && !wallet.validateSecondPassword(options.second_password)) {\n      throw 'ERR_SECPASS'\n    }\n    return wallet\n  }\n}"}, {"instruction": "A BufferGeometry where a 'prefab' geometry is repeated a number of times.\n\n@param {Geometry|BufferGeometry} prefab The Geometry instance to repeat.\n@param {Number} count The number of times to repeat the geometry.\n@constructor", "input": "JavaScript", "output": "function PrefabBufferGeometry(prefab, count) {\n  three.BufferGeometry.call(this);\n\n  /**\n   * A reference to the prefab geometry used to create this instance.\n   * @type {Geometry|BufferGeometry}\n   */\n  this.prefabGeometry = prefab;\n  this.isPrefabBufferGeometry = prefab.isBufferGeometry;\n\n  /**\n   * Number of prefabs.\n   * @type {Number}\n   */\n  this.prefabCount = count;\n\n  /**\n   * Number of vertices of the prefab.\n   * @type {Number}\n   */\n  if (this.isPrefabBufferGeometry) {\n    this.prefabVertexCount = prefab.attributes.position.count;\n  } else {\n    this.prefabVertexCount = prefab.vertices.length;\n  }\n\n  this.bufferIndices();\n  this.bufferPositions();\n}"}, {"instruction": "High-resolution timer", "input": "JavaScript", "output": "function hrtimer() {\n    const start = process.hrtime();\n\n    return () => {\n      const durationComponents = process.hrtime(start);\n      const seconds = durationComponents[0];\n      const nanoseconds = durationComponents[1];\n      const duration = (seconds * 1000) + (nanoseconds / 1E6);\n      return duration;\n    };\n  }"}, {"instruction": "---\ncategory: utilities/a11y\n---\nReturns `true` if any of the children are not wrapped with [ScreenReaderContent](#ScreenReaderContent).\n@param {ReactChildren} children - A react component's children prop\n@return {boolean} whether any of the children are visible", "input": "JavaScript", "output": "function hasVisibleChildren (children) {\n  let visible = false\n\n  React.Children.forEach(children, (child) => {\n    if (child && !matchComponentTypes(child, [ScreenReaderContent])) {\n      visible = true\n    }\n  })\n\n  return visible\n}"}, {"instruction": "run the script", "input": "JavaScript", "output": "function() {\n\tvar scriptJs = Ext.getCmp('scriptAreaId').getValue();\n\tvar serverId = Ext.getCmp('serverComId').getValue();\n\n\tif (!serverId) {\n\t\talert('serverId is required!');\n\t\treturn;\n\t}\n\n\twindow.parent.client.request('scripts', {\n\t\tcommand: 'run',\n\t\tserverId: serverId,\n\t\tscript: scriptJs\n\t}, function(err, msg) {\n\t\tif (err) {\n\t\t\talert(err);\n\t\t\treturn;\n\t\t}\n\t\tExt.getCmp('tesultTextId').setValue(msg);\n\t});\n}"}, {"instruction": "Exports JDL a application to a JDL file in the current directory.\n@param application, the JDL application to export.\n@return the exported application in its final form.", "input": "JavaScript", "output": "function exportApplication(application) {\n  checkForErrors(application);\n  const formattedApplication = setUpApplicationStructure(application);\n  writeConfigFile(formattedApplication);\n  return formattedApplication;\n}"}, {"instruction": "createDispatchers\n\nCreate action dispatcher wrappers with bound playerID and credentials", "input": "JavaScript", "output": "function createDispatchers(\n  storeActionType,\n  innerActionNames,\n  store,\n  playerID,\n  credentials,\n  multiplayer\n) {\n  return innerActionNames.reduce((dispatchers, name) => {\n    dispatchers[name] = function(...args) {\n      let assumedPlayerID = playerID;\n\n      // In singleplayer mode, if the client does not have a playerID\n      // associated with it, we attach the currentPlayer as playerID.\n      if (!multiplayer && (playerID === null || playerID === undefined)) {\n        const state = store.getState();\n        assumedPlayerID = state.ctx.currentPlayer;\n      }\n\n      store.dispatch(\n        ActionCreators[storeActionType](\n          name,\n          args,\n          assumedPlayerID,\n          credentials\n        )\n      );\n    };\n    return dispatchers;\n  }, {});\n}"}, {"instruction": "Returns an array of the IDs of providers registered for a specific language\n\n@param {!string} languageId\n@return {Array.<string>} Names of registered providers.", "input": "JavaScript", "output": "function getProviderIDsForLanguage(languageId) {\n        if (!_providers[languageId]) {\n            return [];\n        }\n        return _providers[languageId].map(function (provider) {\n            return provider.name;\n        });\n    }"}, {"instruction": "routeChat handler. Receives a stream of message/location pairs, and responds\nwith a stream of all previous messages at each of those locations.\n@param {Duplex} call The stream for incoming and outgoing messages", "input": "JavaScript", "output": "function routeChat(call) {\n  call.on('data', function(note) {\n    var key = pointKey(note.getLocation());\n    /* For each note sent, respond with all previous notes that correspond to\n     * the same point */\n    if (route_notes.hasOwnProperty(key)) {\n      _.each(route_notes[key], function(note) {\n        call.write(note);\n      });\n    } else {\n      route_notes[key] = [];\n    }\n    // Then add the new note to the list\n    route_notes[key].push(note);\n  });\n  call.on('end', function() {\n    call.end();\n  });\n}"}, {"instruction": "Recursively cleans the payload of all contained requests via {@link #.cleanPayload}.\nModifies the array in-place.\n\n@param {object[]} aRequests\nThe requests\n@returns {object[]}\nThe cleaned requests\n\n@private", "input": "JavaScript", "output": "function (aRequests) {\n\t\t\taRequests.forEach(function (oRequest) {\n\t\t\t\tif (Array.isArray(oRequest)) {\n\t\t\t\t\t_Requestor.cleanBatch(oRequest);\n\t\t\t\t} else {\n\t\t\t\t\toRequest.body = _Requestor.cleanPayload(oRequest.body);\n\t\t\t\t}\n\t\t\t});\n\t\t\treturn aRequests;\n\t\t}"}, {"instruction": "register a list of ng filters to ngVue", "input": "JavaScript", "output": "function registerFilters (filters) {\n  if (isArray(filters)) {\n    lazyStringFilters = lazyStringFilters.concat(filters)\n  } else if (isObject(filters)) {\n    Object.keys(filters).forEach(name => {\n      addFilter(name, filters[name])\n    })\n  }\n}"}, {"instruction": "// RunServer listens on the given port if listener is not given,\n// then spawns a go-routine continuously serving until the stopCh is closed.\n// It returns a stoppedCh that is closed when all non-hijacked active requests\n// have been processed.\n// This function does not block\n// TODO: make private when insecure serving is gone from the kube-apiserver", "input": "go language", "output": "func RunServer(\n\tserver *http.Server,\n\tln net.Listener,\n\tshutDownTimeout time.Duration,\n\tstopCh <-chan struct{},\n) (<-chan struct{}, error) {\n\tif ln == nil {\n\t\treturn nil, fmt.Errorf(\"listener must not be nil\")\n\t}\n\n\t// Shutdown server gracefully.\n\tstoppedCh := make(chan struct{})\n\tgo func() {\n\t\tdefer close(stoppedCh)\n\t\t<-stopCh\n\t\tctx, cancel := context.WithTimeout(context.Background(), shutDownTimeout)\n\t\tserver.Shutdown(ctx)\n\t\tcancel()\n\t}()\n\n\tgo func() {\n\t\tdefer utilruntime.HandleCrash()\n\n\t\tvar listener net.Listener\n\t\tlistener = tcpKeepAliveListener{ln.(*net.TCPListener)}\n\t\tif server.TLSConfig != nil {\n\t\t\tlistener = tls.NewListener(listener, server.TLSConfig)\n\t\t}\n\n\t\terr := server.Serve(listener)\n\n\t\tmsg := fmt.Sprintf(\"Stopped listening on %s\", ln.Addr().String())\n\t\tselect {\n\t\tcase <-stopCh:\n\t\t\tklog.Info(msg)\n\t\tdefault:\n\t\t\tpanic(fmt.Sprintf(\"%s due to error: %v\", msg, err))\n\t\t}\n\t}()\n\n\treturn stoppedCh, nil\n}"}, {"instruction": "// Intersect tries to return a pair of coordinates which are compatible with the\n// current set and a given set. We employ some special knowledge about network\n// segments to avoid doing a full intersection, since this is in several hot\n// paths. This might return nil for either coordinate in the output pair if an\n// intersection cannot be found. The ComputeDistance function above is designed\n// to deal with that.", "input": "go language", "output": "func (cs CoordinateSet) Intersect(other CoordinateSet) (*coordinate.Coordinate, *coordinate.Coordinate) {\n\t// Use the empty segment by default.\n\tsegment := \"\"\n\n\t// If we have a single segment, then let our segment take priority since\n\t// we are possibly a client. Any node with more than one segment can only\n\t// be a server, which means it should be in all segments.\n\tif len(cs) == 1 {\n\t\tfor s := range cs {\n\t\t\tsegment = s\n\t\t}\n\t}\n\n\t// Likewise for the other set.\n\tif len(other) == 1 {\n\t\tfor s := range other {\n\t\t\tsegment = s\n\t\t}\n\t}\n\n\treturn cs[segment], other[segment]\n}"}, {"instruction": "// NewFilteredJobInformer constructs a new informer for Job type.\n// Always prefer using an informer factory to get a shared informer instead of getting an independent\n// one. This reduces memory footprint and number of connections to the server.", "input": "go language", "output": "func NewFilteredJobInformer(client kubernetes.Interface, namespace string, resyncPeriod time.Duration, indexers cache.Indexers, tweakListOptions internalinterfaces.TweakListOptionsFunc) cache.SharedIndexInformer {\n\treturn cache.NewSharedIndexInformer(\n\t\t&cache.ListWatch{\n\t\t\tListFunc: func(options metav1.ListOptions) (runtime.Object, error) {\n\t\t\t\tif tweakListOptions != nil {\n\t\t\t\t\ttweakListOptions(&options)\n\t\t\t\t}\n\t\t\t\treturn client.BatchV1().Jobs(namespace).List(options)\n\t\t\t},\n\t\t\tWatchFunc: func(options metav1.ListOptions) (watch.Interface, error) {\n\t\t\t\tif tweakListOptions != nil {\n\t\t\t\t\ttweakListOptions(&options)\n\t\t\t\t}\n\t\t\t\treturn client.BatchV1().Jobs(namespace).Watch(options)\n\t\t\t},\n\t\t},\n\t\t&batchv1.Job{},\n\t\tresyncPeriod,\n\t\tindexers,\n\t)\n}"}, {"instruction": "// Next implements the Executor Next interface.", "input": "go language", "output": "func (e *CheckTableExec) Next(ctx context.Context, req *chunk.RecordBatch) error {\n\tif e.done {\n\t\treturn nil\n\t}\n\tdefer func() { e.done = true }()\n\tfor _, t := range e.tables {\n\t\tdbName := t.DBInfo.Name\n\t\ttb, err := e.is.TableByName(dbName, t.Name)\n\t\tif err != nil {\n\t\t\treturn err\n\t\t}\n\t\tif tb.Meta().GetPartitionInfo() != nil {\n\t\t\terr = e.doCheckPartitionedTable(tb.(table.PartitionedTable))\n\t\t} else {\n\t\t\terr = e.doCheckTable(tb)\n\t\t}\n\t\tif err != nil {\n\t\t\tlogutil.Logger(ctx).Warn(\"check table failed\", zap.String(\"tableName\", t.Name.O), zap.Error(err))\n\t\t\tif admin.ErrDataInConsistent.Equal(err) {\n\t\t\t\treturn ErrAdminCheckTable.GenWithStack(\"%v err:%v\", t.Name, err)\n\t\t\t}\n\n\t\t\treturn errors.Errorf(\"%v err:%v\", t.Name, err)\n\t\t}\n\t}\n\treturn nil\n}"}, {"instruction": "// SetCapabilities sets the provided capabilities on the spec\n// All capabilities are added if privileged is true", "input": "go language", "output": "func SetCapabilities(s *specs.Spec, caplist []string) error {\n\ts.Process.Capabilities.Effective = caplist\n\ts.Process.Capabilities.Bounding = caplist\n\ts.Process.Capabilities.Permitted = caplist\n\ts.Process.Capabilities.Inheritable = caplist\n\t// setUser has already been executed here\n\t// if non root drop capabilities in the way execve does\n\tif s.Process.User.UID != 0 {\n\t\ts.Process.Capabilities.Effective = []string{}\n\t\ts.Process.Capabilities.Permitted = []string{}\n\t}\n\treturn nil\n}"}, {"instruction": "// New initialises a ReleaseHandler.", "input": "go language", "output": "func New(version string, skipPublish, try bool) *ReleaseHandler {\n\t// When triggered from CI release branch\n\tversion = strings.TrimPrefix(version, \"release-\")\n\tversion = strings.TrimPrefix(version, \"v\")\n\trh := &ReleaseHandler{cliVersion: version, skipPublish: skipPublish, try: try}\n\n\tif try {\n\t\trh.git = func(args ...string) (string, error) {\n\t\t\tfmt.Println(\"git\", strings.Join(args, \" \"))\n\t\t\treturn \"\", nil\n\t\t}\n\t} else {\n\t\trh.git = git\n\t}\n\n\treturn rh\n}"}, {"instruction": "// Growing Persistent volumes is only allowed for PVCs for which their StorageClass\n// explicitly allows it.", "input": "go language", "output": "func (pvcr *persistentVolumeClaimResize) allowResize(pvc, oldPvc *api.PersistentVolumeClaim) bool {\n\tpvcStorageClass := apihelper.GetPersistentVolumeClaimClass(pvc)\n\toldPvcStorageClass := apihelper.GetPersistentVolumeClaimClass(oldPvc)\n\tif pvcStorageClass == \"\" || oldPvcStorageClass == \"\" || pvcStorageClass != oldPvcStorageClass {\n\t\treturn false\n\t}\n\tsc, err := pvcr.scLister.Get(pvcStorageClass)\n\tif err != nil {\n\t\treturn false\n\t}\n\tif sc.AllowVolumeExpansion != nil {\n\t\treturn *sc.AllowVolumeExpansion\n\t}\n\treturn false\n}"}, {"instruction": "// getNewConn is used to return a new connection", "input": "go language", "output": "func (p *ConnPool) getNewConn(dc string, addr net.Addr, version int, useTLS bool) (*Conn, error) {\n\t// Get a new, raw connection.\n\tconn, _, err := p.DialTimeout(dc, addr, defaultDialTimeout, useTLS)\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\n\t// Switch the multiplexing based on version\n\tvar session muxSession\n\tif version < 2 {\n\t\tconn.Close()\n\t\treturn nil, fmt.Errorf(\"cannot make client connection, unsupported protocol version %d\", version)\n\t}\n\n\t// Write the Consul multiplex byte to set the mode\n\tif _, err := conn.Write([]byte{byte(RPCMultiplexV2)}); err != nil {\n\t\tconn.Close()\n\t\treturn nil, err\n\t}\n\n\t// Setup the logger\n\tconf := yamux.DefaultConfig()\n\tconf.LogOutput = p.LogOutput\n\n\t// Create a multiplexed session\n\tsession, _ = yamux.Client(conn, conf)\n\n\t// Wrap the connection\n\tc := &Conn{\n\t\trefCount: 1,\n\t\taddr:     addr,\n\t\tsession:  session,\n\t\tclients:  list.New(),\n\t\tlastUsed: time.Now(),\n\t\tversion:  version,\n\t\tpool:     p,\n\t}\n\treturn c, nil\n}"}, {"instruction": "// newTikvHandlerTool checks and prepares for tikv handler.\n// It would panic when any error happens.", "input": "go language", "output": "func (s *Server) newTikvHandlerTool() *tikvHandlerTool {\n\tvar tikvStore tikv.Storage\n\tstore, ok := s.driver.(*TiDBDriver)\n\tif !ok {\n\t\tpanic(\"Invalid KvStore with illegal driver\")\n\t}\n\n\tif tikvStore, ok = store.store.(tikv.Storage); !ok {\n\t\tpanic(\"Invalid KvStore with illegal store\")\n\t}\n\n\tregionCache := tikvStore.GetRegionCache()\n\n\treturn &tikvHandlerTool{\n\t\thelper.Helper{\n\t\t\tRegionCache: regionCache,\n\t\t\tStore:       tikvStore,\n\t\t},\n\t}\n}"}, {"instruction": "// DeepCopyInto is an autogenerated deepcopy function, copying the receiver, writing into out. in must be non-nil.", "input": "go language", "output": "func (in *NetworkPolicyPeer) DeepCopyInto(out *NetworkPolicyPeer) {\n\t*out = *in\n\tif in.PodSelector != nil {\n\t\tin, out := &in.PodSelector, &out.PodSelector\n\t\t*out = new(metav1.LabelSelector)\n\t\t(*in).DeepCopyInto(*out)\n\t}\n\tif in.NamespaceSelector != nil {\n\t\tin, out := &in.NamespaceSelector, &out.NamespaceSelector\n\t\t*out = new(metav1.LabelSelector)\n\t\t(*in).DeepCopyInto(*out)\n\t}\n\tif in.IPBlock != nil {\n\t\tin, out := &in.IPBlock, &out.IPBlock\n\t\t*out = new(IPBlock)\n\t\t(*in).DeepCopyInto(*out)\n\t}\n\treturn\n}"}, {"instruction": "// DeriveStats implement LogicalPlan DeriveStats interface.", "input": "go language", "output": "func (p *baseLogicalPlan) DeriveStats(childStats []*property.StatsInfo) (*property.StatsInfo, error) {\n\tif len(childStats) == 1 {\n\t\tp.stats = childStats[0]\n\t\treturn p.stats, nil\n\t}\n\tif len(childStats) > 1 {\n\t\terr := ErrInternal.GenWithStack(\"LogicalPlans with more than one child should implement their own DeriveStats().\")\n\t\treturn nil, err\n\t}\n\tprofile := &property.StatsInfo{\n\t\tRowCount:    float64(1),\n\t\tCardinality: make([]float64, p.self.Schema().Len()),\n\t}\n\tfor i := range profile.Cardinality {\n\t\tprofile.Cardinality[i] = float64(1)\n\t}\n\tp.stats = profile\n\treturn profile, nil\n}"}, {"instruction": "// signpackageCmd returns the cobra command for signing a package", "input": "go language", "output": "func signpackageCmd(cf *ChaincodeCmdFactory) *cobra.Command {\n\tspCmd := &cobra.Command{\n\t\tUse:       \"signpackage\",\n\t\tShort:     \"Sign the specified chaincode package\",\n\t\tLong:      \"Sign the specified chaincode package\",\n\t\tValidArgs: []string{\"2\"},\n\t\tRunE: func(cmd *cobra.Command, args []string) error {\n\t\t\tif len(args) < 2 {\n\t\t\t\treturn fmt.Errorf(\"peer chaincode signpackage <inputpackage> <outputpackage>\")\n\t\t\t}\n\t\t\treturn signpackage(cmd, args[0], args[1], cf)\n\t\t},\n\t}\n\n\treturn spCmd\n}"}, {"instruction": "// NewDefinitionNamer constructs a new DefinitionNamer to be used to customize OpenAPI spec.", "input": "go language", "output": "func NewDefinitionNamer(schemes ...*runtime.Scheme) *DefinitionNamer {\n\tret := &DefinitionNamer{\n\t\ttypeGroupVersionKinds: map[string]groupVersionKinds{},\n\t}\n\tfor _, s := range schemes {\n\t\tfor gvk, rtype := range s.AllKnownTypes() {\n\t\t\tnewGVK := gvkConvert(gvk)\n\t\t\texists := false\n\t\t\tfor _, existingGVK := range ret.typeGroupVersionKinds[typeName(rtype)] {\n\t\t\t\tif newGVK == existingGVK {\n\t\t\t\t\texists = true\n\t\t\t\t\tbreak\n\t\t\t\t}\n\t\t\t}\n\t\t\tif !exists {\n\t\t\t\tret.typeGroupVersionKinds[typeName(rtype)] = append(ret.typeGroupVersionKinds[typeName(rtype)], newGVK)\n\t\t\t}\n\t\t}\n\t}\n\tfor _, gvk := range ret.typeGroupVersionKinds {\n\t\tsort.Sort(gvk)\n\t}\n\treturn ret\n}"}, {"instruction": "// GetCSIAttachLimitKey returns limit key used for CSI volumes", "input": "go language", "output": "func GetCSIAttachLimitKey(driverName string) string {\n\tcsiPrefixLength := len(CSIAttachLimitPrefix)\n\ttotalkeyLength := csiPrefixLength + len(driverName)\n\tif totalkeyLength >= ResourceNameLengthLimit {\n\t\tcharsFromDriverName := driverName[:23]\n\t\thash := sha1.New()\n\t\thash.Write([]byte(driverName))\n\t\thashed := hex.EncodeToString(hash.Sum(nil))\n\t\thashed = hashed[:16]\n\t\treturn CSIAttachLimitPrefix + charsFromDriverName + hashed\n\t}\n\treturn CSIAttachLimitPrefix + driverName\n}"}, {"instruction": "// NewWatchingConfigMapManager creates a manager that keeps a cache of all configmaps\n// necessary for registered pods.\n// It implements the following logic:\n// - whenever a pod is created or updated, we start inidvidual watches for all\n//   referenced objects that aren't referenced from other registered pods\n// - every GetObject() returns a value from local cache propagated via watches", "input": "go language", "output": "func NewWatchingConfigMapManager(kubeClient clientset.Interface) Manager {\n\tlistConfigMap := func(namespace string, opts metav1.ListOptions) (runtime.Object, error) {\n\t\treturn kubeClient.CoreV1().ConfigMaps(namespace).List(opts)\n\t}\n\twatchConfigMap := func(namespace string, opts metav1.ListOptions) (watch.Interface, error) {\n\t\treturn kubeClient.CoreV1().ConfigMaps(namespace).Watch(opts)\n\t}\n\tnewConfigMap := func() runtime.Object {\n\t\treturn &v1.ConfigMap{}\n\t}\n\tgr := corev1.Resource(\"configmap\")\n\treturn &configMapManager{\n\t\tmanager: manager.NewWatchBasedManager(listConfigMap, watchConfigMap, newConfigMap, gr, getConfigMapNames),\n\t}\n}"}, {"instruction": "// NewServerTimeout returns an error indicating the requested action could not be completed due to a\n// transient error, and the client should try again.", "input": "go language", "output": "func NewServerTimeout(qualifiedResource schema.GroupResource, operation string, retryAfterSeconds int) *StatusError {\n\treturn &StatusError{metav1.Status{\n\t\tStatus: metav1.StatusFailure,\n\t\tCode:   http.StatusInternalServerError,\n\t\tReason: metav1.StatusReasonServerTimeout,\n\t\tDetails: &metav1.StatusDetails{\n\t\t\tGroup:             qualifiedResource.Group,\n\t\t\tKind:              qualifiedResource.Resource,\n\t\t\tName:              operation,\n\t\t\tRetryAfterSeconds: int32(retryAfterSeconds),\n\t\t},\n\t\tMessage: fmt.Sprintf(\"The %s operation against %s could not be completed at this time, please try again.\", operation, qualifiedResource.String()),\n\t}}\n}"}, {"instruction": "// writeActionSymbol writes a symbol to represent the given action, followed\n// by a space.\n//\n// It only supports the actions that can be represented with a single character:\n// Create, Delete, Update and NoAction.", "input": "go language", "output": "func (p *blockBodyDiffPrinter) writeActionSymbol(action plans.Action) {\n\tswitch action {\n\tcase plans.Create:\n\t\tp.buf.WriteString(p.color.Color(\"[green]+[reset] \"))\n\tcase plans.Delete:\n\t\tp.buf.WriteString(p.color.Color(\"[red]-[reset] \"))\n\tcase plans.Update:\n\t\tp.buf.WriteString(p.color.Color(\"[yellow]~[reset] \"))\n\tcase plans.NoOp:\n\t\tp.buf.WriteString(\"  \")\n\tdefault:\n\t\t// Should never happen\n\t\tp.buf.WriteString(p.color.Color(\"? \"))\n\t}\n}"}, {"instruction": "// gcUnscheduledTerminating deletes pods that are terminating and haven't been scheduled to a particular node.", "input": "go language", "output": "func (gcc *PodGCController) gcUnscheduledTerminating(pods []*v1.Pod) {\n\tklog.V(4).Infof(\"GC'ing unscheduled pods which are terminating.\")\n\n\tfor _, pod := range pods {\n\t\tif pod.DeletionTimestamp == nil || len(pod.Spec.NodeName) > 0 {\n\t\t\tcontinue\n\t\t}\n\n\t\tklog.V(2).Infof(\"Found unscheduled terminating Pod %v/%v not assigned to any Node. Deleting.\", pod.Namespace, pod.Name)\n\t\tif err := gcc.deletePod(pod.Namespace, pod.Name); err != nil {\n\t\t\tutilruntime.HandleError(err)\n\t\t} else {\n\t\t\tklog.V(0).Infof(\"Forced deletion of unscheduled terminating Pod %v/%v succeeded\", pod.Namespace, pod.Name)\n\t\t}\n\t}\n}"}, {"instruction": "// TranslateCSIPVToInTree takes a PV with CSIPersistentVolumeSource set and\n// translates the GCE PD CSI source to a GCEPersistentDisk source.", "input": "go language", "output": "func (g *gcePersistentDiskCSITranslator) TranslateCSIPVToInTree(pv *v1.PersistentVolume) (*v1.PersistentVolume, error) {\n\tif pv == nil || pv.Spec.CSI == nil {\n\t\treturn nil, fmt.Errorf(\"pv is nil or CSI source not defined on pv\")\n\t}\n\tcsiSource := pv.Spec.CSI\n\n\tpdName, err := pdNameFromVolumeID(csiSource.VolumeHandle)\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\n\tgceSource := &v1.GCEPersistentDiskVolumeSource{\n\t\tPDName:   pdName,\n\t\tFSType:   csiSource.FSType,\n\t\tReadOnly: csiSource.ReadOnly,\n\t}\n\tif partition, ok := csiSource.VolumeAttributes[\"partition\"]; ok && partition != \"\" {\n\t\tpartInt, err := strconv.Atoi(partition)\n\t\tif err != nil {\n\t\t\treturn nil, fmt.Errorf(\"Failed to convert partition %v to integer: %v\", partition, err)\n\t\t}\n\t\tgceSource.Partition = int32(partInt)\n\t}\n\n\t// TODO: Take the zone/regional information and stick it into the label.\n\n\tpv.Spec.CSI = nil\n\tpv.Spec.GCEPersistentDisk = gceSource\n\n\treturn pv, nil\n}"}, {"instruction": "// UnmarshalJSON implements the json.Unmarshaler interface by decoding the json\n// string values into the config fields", "input": "go language", "output": "func (n *NodeConfig) UnmarshalJSON(data []byte) error {\n\tvar confJSON nodeConfigJSON\n\tif err := json.Unmarshal(data, &confJSON); err != nil {\n\t\treturn err\n\t}\n\n\tif confJSON.ID != \"\" {\n\t\tif err := n.ID.UnmarshalText([]byte(confJSON.ID)); err != nil {\n\t\t\treturn err\n\t\t}\n\t}\n\n\tif confJSON.PrivateKey != \"\" {\n\t\tkey, err := hex.DecodeString(confJSON.PrivateKey)\n\t\tif err != nil {\n\t\t\treturn err\n\t\t}\n\t\tprivKey, err := crypto.ToECDSA(key)\n\t\tif err != nil {\n\t\t\treturn err\n\t\t}\n\t\tn.PrivateKey = privKey\n\t}\n\n\tn.Name = confJSON.Name\n\tn.Services = confJSON.Services\n\tn.Port = confJSON.Port\n\tn.EnableMsgEvents = confJSON.EnableMsgEvents\n\n\treturn nil\n}"}, {"instruction": "// CompactRules combines rules that contain a single APIGroup/Resource, differ only by verb, and contain no other attributes.\n// this is a fast check, and works well with the decomposed \"missing rules\" list from a Covers check.", "input": "go language", "output": "func CompactRules(rules []rbacv1.PolicyRule) ([]rbacv1.PolicyRule, error) {\n\tcompacted := make([]rbacv1.PolicyRule, 0, len(rules))\n\n\tsimpleRules := map[simpleResource]*rbacv1.PolicyRule{}\n\tfor _, rule := range rules {\n\t\tif resource, isSimple := isSimpleResourceRule(&rule); isSimple {\n\t\t\tif existingRule, ok := simpleRules[resource]; ok {\n\t\t\t\t// Add the new verbs to the existing simple resource rule\n\t\t\t\tif existingRule.Verbs == nil {\n\t\t\t\t\texistingRule.Verbs = []string{}\n\t\t\t\t}\n\t\t\t\texistingRule.Verbs = append(existingRule.Verbs, rule.Verbs...)\n\t\t\t} else {\n\t\t\t\t// Copy the rule to accumulate matching simple resource rules into\n\t\t\t\tsimpleRules[resource] = rule.DeepCopy()\n\t\t\t}\n\t\t} else {\n\t\t\tcompacted = append(compacted, rule)\n\t\t}\n\t}\n\n\t// Once we've consolidated the simple resource rules, add them to the compacted list\n\tfor _, simpleRule := range simpleRules {\n\t\tcompacted = append(compacted, *simpleRule)\n\t}\n\n\treturn compacted, nil\n}"}, {"instruction": "// ParseBitStr parses bit string.\n// The string format can be b'val', B'val' or 0bval, val must be 0 or 1.\n// See https://dev.mysql.com/doc/refman/5.7/en/bit-value-literals.html", "input": "go language", "output": "func ParseBitStr(s string) (BinaryLiteral, error) {\n\tif len(s) == 0 {\n\t\treturn nil, errors.Errorf(\"invalid empty string for parsing bit type\")\n\t}\n\n\tif s[0] == 'b' || s[0] == 'B' {\n\t\t// format is b'val' or B'val'\n\t\ts = strings.Trim(s[1:], \"'\")\n\t} else if strings.HasPrefix(s, \"0b\") {\n\t\ts = s[2:]\n\t} else {\n\t\t// here means format is not b'val', B'val' or 0bval.\n\t\treturn nil, errors.Errorf(\"invalid bit type format %s\", s)\n\t}\n\n\tif len(s) == 0 {\n\t\treturn ZeroBinaryLiteral, nil\n\t}\n\n\talignedLength := (len(s) + 7) &^ 7\n\ts = (\"00000000\" + s)[len(s)+8-alignedLength:] // Pad with zero (slice from `-alignedLength`)\n\tbyteLength := len(s) >> 3\n\tbuf := make([]byte, byteLength)\n\n\tfor i := 0; i < byteLength; i++ {\n\t\tstrPosition := i << 3\n\t\tval, err := strconv.ParseUint(s[strPosition:strPosition+8], 2, 8)\n\t\tif err != nil {\n\t\t\treturn nil, errors.Trace(err)\n\t\t}\n\t\tbuf[i] = byte(val)\n\t}\n\n\treturn buf, nil\n}"}, {"instruction": "// RegisterDefaults adds defaulters functions to the given scheme.\n// Public to allow building arbitrary schemes.\n// All generated defaulters are covering - they call all nested defaulters.", "input": "go language", "output": "func RegisterDefaults(scheme *runtime.Scheme) error {\n\tscheme.AddTypeDefaultingFunc(&ClusterConfiguration{}, func(obj interface{}) { SetObjectDefaults_ClusterConfiguration(obj.(*ClusterConfiguration)) })\n\tscheme.AddTypeDefaultingFunc(&ClusterStatus{}, func(obj interface{}) { SetObjectDefaults_ClusterStatus(obj.(*ClusterStatus)) })\n\tscheme.AddTypeDefaultingFunc(&InitConfiguration{}, func(obj interface{}) { SetObjectDefaults_InitConfiguration(obj.(*InitConfiguration)) })\n\tscheme.AddTypeDefaultingFunc(&JoinConfiguration{}, func(obj interface{}) { SetObjectDefaults_JoinConfiguration(obj.(*JoinConfiguration)) })\n\treturn nil\n}"}, {"instruction": "// ReadDockerConfigJSONFile attempts to read a docker config.json file from the given paths.\n// if searchPaths is empty, the default paths are used.", "input": "go language", "output": "func ReadDockerConfigJSONFile(searchPaths []string) (cfg DockerConfig, err error) {\n\tif len(searchPaths) == 0 {\n\t\tsearchPaths = DefaultDockerConfigJSONPaths()\n\t}\n\tfor _, configPath := range searchPaths {\n\t\tabsDockerConfigFileLocation, err := filepath.Abs(filepath.Join(configPath, configJsonFileName))\n\t\tif err != nil {\n\t\t\tklog.Errorf(\"while trying to canonicalize %s: %v\", configPath, err)\n\t\t\tcontinue\n\t\t}\n\t\tklog.V(4).Infof(\"looking for %s at %s\", configJsonFileName, absDockerConfigFileLocation)\n\t\tcfg, err = ReadSpecificDockerConfigJsonFile(absDockerConfigFileLocation)\n\t\tif err != nil {\n\t\t\tif !os.IsNotExist(err) {\n\t\t\t\tklog.V(4).Infof(\"while trying to read %s: %v\", absDockerConfigFileLocation, err)\n\t\t\t}\n\t\t\tcontinue\n\t\t}\n\t\tklog.V(4).Infof(\"found valid %s at %s\", configJsonFileName, absDockerConfigFileLocation)\n\t\treturn cfg, nil\n\t}\n\treturn nil, fmt.Errorf(\"couldn't find valid %s after checking in %v\", configJsonFileName, searchPaths)\n\n}"}, {"instruction": "// newEndpointImpl creates a new endpoint for the given resourceName.\n// This is to be used during normal device plugin registration.", "input": "go language", "output": "func newEndpointImpl(socketPath, resourceName string, callback monitorCallback) (*endpointImpl, error) {\n\tclient, c, err := dial(socketPath)\n\tif err != nil {\n\t\tklog.Errorf(\"Can't create new endpoint with path %s err %v\", socketPath, err)\n\t\treturn nil, err\n\t}\n\n\treturn &endpointImpl{\n\t\tclient:     client,\n\t\tclientConn: c,\n\n\t\tsocketPath:   socketPath,\n\t\tresourceName: resourceName,\n\n\t\tcb: callback,\n\t}, nil\n}"}, {"instruction": "// Get pods which should be resynchronized. Currently, the following pod should be resynchronized:\n//   * pod whose work is ready.\n//   * internal modules that request sync of a pod.", "input": "go language", "output": "func (kl *Kubelet) getPodsToSync() []*v1.Pod {\n\tallPods := kl.podManager.GetPods()\n\tpodUIDs := kl.workQueue.GetWork()\n\tpodUIDSet := sets.NewString()\n\tfor _, podUID := range podUIDs {\n\t\tpodUIDSet.Insert(string(podUID))\n\t}\n\tvar podsToSync []*v1.Pod\n\tfor _, pod := range allPods {\n\t\tif podUIDSet.Has(string(pod.UID)) {\n\t\t\t// The work of the pod is ready\n\t\t\tpodsToSync = append(podsToSync, pod)\n\t\t\tcontinue\n\t\t}\n\t\tfor _, podSyncLoopHandler := range kl.PodSyncLoopHandlers {\n\t\t\tif podSyncLoopHandler.ShouldSync(pod) {\n\t\t\t\tpodsToSync = append(podsToSync, pod)\n\t\t\t\tbreak\n\t\t\t}\n\t\t}\n\t}\n\treturn podsToSync\n}"}, {"instruction": "// bitsetEncodeBytes compresses the input byte slice according to the sparse\n// bitset representation algorithm.", "input": "go language", "output": "func bitsetEncodeBytes(data []byte) []byte {\n\t// Empty slices get compressed to nil\n\tif len(data) == 0 {\n\t\treturn nil\n\t}\n\t// One byte slices compress to nil or retain the single byte\n\tif len(data) == 1 {\n\t\tif data[0] == 0 {\n\t\t\treturn nil\n\t\t}\n\t\treturn data\n\t}\n\t// Calculate the bitset of set bytes, and gather the non-zero bytes\n\tnonZeroBitset := make([]byte, (len(data)+7)/8)\n\tnonZeroBytes := make([]byte, 0, len(data))\n\n\tfor i, b := range data {\n\t\tif b != 0 {\n\t\t\tnonZeroBytes = append(nonZeroBytes, b)\n\t\t\tnonZeroBitset[i/8] |= 1 << byte(7-i%8)\n\t\t}\n\t}\n\tif len(nonZeroBytes) == 0 {\n\t\treturn nil\n\t}\n\treturn append(bitsetEncodeBytes(nonZeroBitset), nonZeroBytes...)\n}"}, {"instruction": "// ReadAtMost reads at most max bytes from the end of the file identified by path or\n// returns an error. It returns true if the file was longer than max. It will\n// allocate up to max bytes.", "input": "go language", "output": "func ReadAtMost(path string, max int64) ([]byte, bool, error) {\n\tf, err := os.Open(path)\n\tif err != nil {\n\t\treturn nil, false, err\n\t}\n\tdefer f.Close()\n\tfi, err := f.Stat()\n\tif err != nil {\n\t\treturn nil, false, err\n\t}\n\tsize := fi.Size()\n\tif size == 0 {\n\t\treturn nil, false, nil\n\t}\n\tif size < max {\n\t\tmax = size\n\t}\n\toffset, err := f.Seek(-max, io.SeekEnd)\n\tif err != nil {\n\t\treturn nil, false, err\n\t}\n\tdata, err := ioutil.ReadAll(f)\n\treturn data, offset > 0, err\n}"}, {"instruction": "// Validate makes sure there is no discrepency in provided option values", "input": "go language", "output": "func (o *CreateRoleOptions) Validate() error {\n\tif o.Name == \"\" {\n\t\treturn fmt.Errorf(\"name must be specified\")\n\t}\n\n\t// validate verbs.\n\tif len(o.Verbs) == 0 {\n\t\treturn fmt.Errorf(\"at least one verb must be specified\")\n\t}\n\n\tfor _, v := range o.Verbs {\n\t\tif !arrayContains(validResourceVerbs, v) {\n\t\t\treturn fmt.Errorf(\"invalid verb: '%s'\", v)\n\t\t}\n\t}\n\n\t// validate resources.\n\tif len(o.Resources) == 0 {\n\t\treturn fmt.Errorf(\"at least one resource must be specified\")\n\t}\n\n\treturn o.validateResource()\n}"}, {"instruction": "// Visit walks the provided node, transforming field initializations of the form\n// m.Field = &OptionalType{} -> m.Field = OptionalType{}", "input": "go language", "output": "func (v optionalAssignmentVisitor) Visit(n ast.Node) ast.Visitor {\n\tswitch t := n.(type) {\n\tcase *ast.AssignStmt:\n\t\tif len(t.Lhs) == 1 && len(t.Rhs) == 1 {\n\t\t\tif !isFieldSelector(t.Lhs[0], \"m\", \"\") {\n\t\t\t\treturn nil\n\t\t\t}\n\t\t\tunary, ok := t.Rhs[0].(*ast.UnaryExpr)\n\t\t\tif !ok || unary.Op != token.AND {\n\t\t\t\treturn nil\n\t\t\t}\n\t\t\tcomposite, ok := unary.X.(*ast.CompositeLit)\n\t\t\tif !ok || composite.Type == nil || len(composite.Elts) != 0 {\n\t\t\t\treturn nil\n\t\t\t}\n\t\t\tif ident, ok := composite.Type.(*ast.Ident); ok && v.fn(ident.Name) {\n\t\t\t\tt.Rhs[0] = composite\n\t\t\t}\n\t\t}\n\t\treturn nil\n\t}\n\treturn v\n}"}, {"instruction": "// AddFlags adds flags related to NamespaceController for controller manager to the specified FlagSet.", "input": "go language", "output": "func (o *NamespaceControllerOptions) AddFlags(fs *pflag.FlagSet) {\n\tif o == nil {\n\t\treturn\n\t}\n\n\tfs.DurationVar(&o.NamespaceSyncPeriod.Duration, \"namespace-sync-period\", o.NamespaceSyncPeriod.Duration, \"The period for syncing namespace life-cycle updates\")\n\tfs.Int32Var(&o.ConcurrentNamespaceSyncs, \"concurrent-namespace-syncs\", o.ConcurrentNamespaceSyncs, \"The number of namespace objects that are allowed to sync concurrently. Larger number = more responsive namespace termination, but more CPU (and network) load\")\n}"}, {"instruction": "// applyPackageSpecOverride applies the package spec overrides for the given\n// osDistro to the packageSpecs and returns the applied result.", "input": "go language", "output": "func applyPackageSpecOverride(packageSpecs []PackageSpec, overrides []PackageSpecOverride, osDistro string) []PackageSpec {\n\tvar override *PackageSpecOverride\n\tfor _, o := range overrides {\n\t\tif o.OSDistro == osDistro {\n\t\t\toverride = &o\n\t\t\tbreak\n\t\t}\n\t}\n\tif override == nil {\n\t\treturn packageSpecs\n\t}\n\n\t// Remove packages in the spec that matches the overrides in\n\t// Subtractions.\n\tvar out []PackageSpec\n\tsubtractions := make(map[string]bool)\n\tfor _, spec := range override.Subtractions {\n\t\tsubtractions[spec.Name] = true\n\t}\n\tfor _, spec := range packageSpecs {\n\t\tif _, ok := subtractions[spec.Name]; !ok {\n\t\t\tout = append(out, spec)\n\t\t}\n\t}\n\n\t// Add packages in the spec that matches the overrides in Additions.\n\treturn append(out, override.Additions...)\n}"}, {"instruction": "// AsCanonicalBytes accepts a buffer to write the base-10 string value of this field to, and returns\n// either that buffer or a larger buffer and the current exponent of the value. The value is adjusted\n// until the exponent is a multiple of 3 - i.e. 1.1e5 would return \"110\", 3.", "input": "go language", "output": "func (a int64Amount) AsCanonicalBytes(out []byte) (result []byte, exponent int32) {\n\tmantissa := a.value\n\texponent = int32(a.scale)\n\n\tamount, times := removeInt64Factors(mantissa, 10)\n\texponent += int32(times)\n\n\t// make sure exponent is a multiple of 3\n\tvar ok bool\n\tswitch exponent % 3 {\n\tcase 1, -2:\n\t\tamount, ok = int64MultiplyScale10(amount)\n\t\tif !ok {\n\t\t\treturn infDecAmount{a.AsDec()}.AsCanonicalBytes(out)\n\t\t}\n\t\texponent = exponent - 1\n\tcase 2, -1:\n\t\tamount, ok = int64MultiplyScale100(amount)\n\t\tif !ok {\n\t\t\treturn infDecAmount{a.AsDec()}.AsCanonicalBytes(out)\n\t\t}\n\t\texponent = exponent - 2\n\t}\n\treturn strconv.AppendInt(out, amount, 10), exponent\n}"}, {"instruction": "// LookupRuntimeHandler returns the RuntimeHandler string associated with the given RuntimeClass\n// name (or the default of \"\" for nil). If the RuntimeClass is not found, it returns an\n// errors.NotFound error.", "input": "go language", "output": "func (m *Manager) LookupRuntimeHandler(runtimeClassName *string) (string, error) {\n\tif runtimeClassName == nil || *runtimeClassName == \"\" {\n\t\t// The default RuntimeClass always resolves to the empty runtime handler.\n\t\treturn \"\", nil\n\t}\n\n\tname := *runtimeClassName\n\n\trc, err := m.lister.Get(name)\n\tif err != nil {\n\t\tif errors.IsNotFound(err) {\n\t\t\treturn \"\", err\n\t\t}\n\t\treturn \"\", fmt.Errorf(\"Failed to lookup RuntimeClass %s: %v\", name, err)\n\t}\n\n\treturn rc.Handler, nil\n}"}, {"instruction": "// BatchDelete deletes key-value pairs from TiKV", "input": "go language", "output": "func (c *RawKVClient) BatchDelete(keys [][]byte) error {\n\tstart := time.Now()\n\tdefer func() {\n\t\ttikvRawkvCmdHistogramWithBatchDelete.Observe(time.Since(start).Seconds())\n\t}()\n\n\tbo := NewBackoffer(context.Background(), rawkvMaxBackoff)\n\tresp, err := c.sendBatchReq(bo, keys, tikvrpc.CmdRawBatchDelete)\n\tif err != nil {\n\t\treturn errors.Trace(err)\n\t}\n\tcmdResp := resp.RawBatchDelete\n\tif cmdResp == nil {\n\t\treturn errors.Trace(ErrBodyMissing)\n\t}\n\tif cmdResp.GetError() != \"\" {\n\t\treturn errors.New(cmdResp.GetError())\n\t}\n\treturn nil\n}"}, {"instruction": "// initTableIndices initializes the indices of the tableCommon.", "input": "go language", "output": "func initTableIndices(t *tableCommon) error {\n\ttblInfo := t.meta\n\tfor _, idxInfo := range tblInfo.Indices {\n\t\tif idxInfo.State == model.StateNone {\n\t\t\treturn table.ErrIndexStateCantNone.GenWithStack(\"index %s can't be in none state\", idxInfo.Name)\n\t\t}\n\n\t\t// Use partition ID for index, because tableCommon may be table or partition.\n\t\tidx := NewIndex(t.physicalTableID, tblInfo, idxInfo)\n\t\tt.indices = append(t.indices, idx)\n\t}\n\treturn nil\n}"}, {"instruction": "// start starts the progressReporter", "input": "go language", "output": "func (p *progressReporter) start() {\n\tgo func() {\n\t\tticker := time.NewTicker(defaultImagePullingProgressReportInterval)\n\t\tdefer ticker.Stop()\n\t\tfor {\n\t\t\t// TODO(random-liu): Report as events.\n\t\t\tselect {\n\t\t\tcase <-ticker.C:\n\t\t\t\tprogress, timestamp := p.progress.get()\n\t\t\t\t// If there is no progress for p.imagePullProgressDeadline, cancel the operation.\n\t\t\t\tif time.Since(timestamp) > p.imagePullProgressDeadline {\n\t\t\t\t\tklog.Errorf(\"Cancel pulling image %q because of no progress for %v, latest progress: %q\", p.image, p.imagePullProgressDeadline, progress)\n\t\t\t\t\tp.cancel()\n\t\t\t\t\treturn\n\t\t\t\t}\n\t\t\t\tklog.V(2).Infof(\"Pulling image %q: %q\", p.image, progress)\n\t\t\tcase <-p.stopCh:\n\t\t\t\tprogress, _ := p.progress.get()\n\t\t\t\tklog.V(2).Infof(\"Stop pulling image %q: %q\", p.image, progress)\n\t\t\t\treturn\n\t\t\t}\n\t\t}\n\t}()\n}"}, {"instruction": "// Get returns a single intention by ID.", "input": "go language", "output": "func (s *Intention) Get(\n\targs *structs.IntentionQueryRequest,\n\treply *structs.IndexedIntentions) error {\n\t// Forward if necessary\n\tif done, err := s.srv.forward(\"Intention.Get\", args, args, reply); done {\n\t\treturn err\n\t}\n\n\treturn s.srv.blockingQuery(\n\t\t&args.QueryOptions,\n\t\t&reply.QueryMeta,\n\t\tfunc(ws memdb.WatchSet, state *state.Store) error {\n\t\t\tindex, ixn, err := state.IntentionGet(ws, args.IntentionID)\n\t\t\tif err != nil {\n\t\t\t\treturn err\n\t\t\t}\n\t\t\tif ixn == nil {\n\t\t\t\treturn ErrIntentionNotFound\n\t\t\t}\n\n\t\t\treply.Index = index\n\t\t\treply.Intentions = structs.Intentions{ixn}\n\n\t\t\t// Filter\n\t\t\tif err := s.srv.filterACL(args.Token, reply); err != nil {\n\t\t\t\treturn err\n\t\t\t}\n\n\t\t\t// If ACLs prevented any responses, error\n\t\t\tif len(reply.Intentions) == 0 {\n\t\t\t\ts.srv.logger.Printf(\"[WARN] consul.intention: Request to get intention '%s' denied due to ACLs\", args.IntentionID)\n\t\t\t\treturn acl.ErrPermissionDenied\n\t\t\t}\n\n\t\t\treturn nil\n\t\t},\n\t)\n}"}, {"instruction": "// New builds a new IPWhiteLister given a list of CIDR-Strings to whitelist", "input": "go language", "output": "func New(ctx context.Context, next http.Handler, config config.IPWhiteList, name string) (http.Handler, error) {\n\tlogger := middlewares.GetLogger(ctx, name, typeName)\n\tlogger.Debug(\"Creating middleware\")\n\n\tif len(config.SourceRange) == 0 {\n\t\treturn nil, errors.New(\"sourceRange is empty, IPWhiteLister not created\")\n\t}\n\n\tchecker, err := ip.NewChecker(config.SourceRange)\n\tif err != nil {\n\t\treturn nil, fmt.Errorf(\"cannot parse CIDR whitelist %s: %v\", config.SourceRange, err)\n\t}\n\n\tstrategy, err := config.IPStrategy.Get()\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\n\tlogger.Debugf(\"Setting up IPWhiteLister with sourceRange: %s\", config.SourceRange)\n\treturn &ipWhiteLister{\n\t\tstrategy:    strategy,\n\t\twhiteLister: checker,\n\t\tnext:        next,\n\t\tname:        name,\n\t}, nil\n}"}, {"instruction": "// Create creates a WAL ready for appending records. The given metadata is\n// recorded at the head of each WAL file, and can be retrieved with ReadAll.", "input": "go language", "output": "func Create(dirpath string, metadata []byte) (*WAL, error) {\n\tif Exist(dirpath) {\n\t\treturn nil, os.ErrExist\n\t}\n\n\tif err := os.MkdirAll(dirpath, privateDirMode); err != nil {\n\t\treturn nil, err\n\t}\n\n\tp := path.Join(dirpath, walName(0, 0))\n\tf, err := os.OpenFile(p, os.O_WRONLY|os.O_APPEND|os.O_CREATE, 0600)\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\tl, err := fileutil.NewLock(f.Name())\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\tif err = l.Lock(); err != nil {\n\t\treturn nil, err\n\t}\n\n\tw := &WAL{\n\t\tdir:      dirpath,\n\t\tmetadata: metadata,\n\t\tseq:      0,\n\t\tf:        f,\n\t\tencoder:  newEncoder(f, 0),\n\t}\n\tw.locks = append(w.locks, l)\n\tif err := w.saveCrc(0); err != nil {\n\t\treturn nil, err\n\t}\n\tif err := w.encoder.encode(&walpb.Record{Type: metadataType, Data: metadata}); err != nil {\n\t\treturn nil, err\n\t}\n\tif err := w.SaveSnapshot(walpb.Snapshot{}); err != nil {\n\t\treturn nil, err\n\t}\n\treturn w, nil\n}"}, {"instruction": "// sendSummary send the summary events for a single folder", "input": "go language", "output": "func (c *folderSummaryService) sendSummary(folder string) {\n\t// The folder summary contains how many bytes, files etc\n\t// are in the folder and how in sync we are.\n\tdata, err := c.Summary(folder)\n\tif err != nil {\n\t\treturn\n\t}\n\tevents.Default.Log(events.FolderSummary, map[string]interface{}{\n\t\t\"folder\":  folder,\n\t\t\"summary\": data,\n\t})\n\n\tfor _, devCfg := range c.cfg.Folders()[folder].Devices {\n\t\tif devCfg.DeviceID.Equals(c.id) {\n\t\t\t// We already know about ourselves.\n\t\t\tcontinue\n\t\t}\n\t\tif _, ok := c.model.Connection(devCfg.DeviceID); !ok {\n\t\t\t// We're not interested in disconnected devices.\n\t\t\tcontinue\n\t\t}\n\n\t\t// Get completion percentage of this folder for the\n\t\t// remote device.\n\t\tcomp := c.model.Completion(devCfg.DeviceID, folder).Map()\n\t\tcomp[\"folder\"] = folder\n\t\tcomp[\"device\"] = devCfg.DeviceID.String()\n\t\tevents.Default.Log(events.FolderCompletion, comp)\n\t}\n}"}, {"instruction": "// buildGlobalHashTable builds a global hash table for the inner relation.\n// key of hash table: hash value of key columns\n// value of hash table: RowPtr of the corresponded row", "input": "go language", "output": "func (e *HashJoinExec) buildGlobalHashTable() error {\n\te.globalHashTable = mvmap.NewMVMap()\n\tvar (\n\t\thasNull bool\n\t\terr     error\n\t\tkeyBuf  = make([]byte, 0, 64)\n\t\tvalBuf  = make([]byte, 8)\n\t)\n\n\tfor chkIdx := 0; chkIdx < e.innerResult.NumChunks(); chkIdx++ {\n\t\tif e.finished.Load().(bool) {\n\t\t\treturn nil\n\t\t}\n\t\tchk := e.innerResult.GetChunk(chkIdx)\n\t\tfor j, numRows := 0, chk.NumRows(); j < numRows; j++ {\n\t\t\thasNull, keyBuf, err = e.getJoinKeyFromChkRow(false, chk.GetRow(j), keyBuf)\n\t\t\tif err != nil {\n\t\t\t\treturn err\n\t\t\t}\n\t\t\tif hasNull {\n\t\t\t\tcontinue\n\t\t\t}\n\t\t\trowPtr := chunk.RowPtr{ChkIdx: uint32(chkIdx), RowIdx: uint32(j)}\n\t\t\t*(*chunk.RowPtr)(unsafe.Pointer(&valBuf[0])) = rowPtr\n\t\t\te.globalHashTable.Put(keyBuf, valBuf)\n\t\t}\n\t}\n\treturn nil\n}"}, {"instruction": "// createLogGroup creates a log group for the instance of the awslogs logging driver", "input": "go language", "output": "func (l *logStream) createLogGroup() error {\n\tif _, err := l.client.CreateLogGroup(&cloudwatchlogs.CreateLogGroupInput{\n\t\tLogGroupName: aws.String(l.logGroupName),\n\t}); err != nil {\n\t\tif awsErr, ok := err.(awserr.Error); ok {\n\t\t\tfields := logrus.Fields{\n\t\t\t\t\"errorCode\":      awsErr.Code(),\n\t\t\t\t\"message\":        awsErr.Message(),\n\t\t\t\t\"origError\":      awsErr.OrigErr(),\n\t\t\t\t\"logGroupName\":   l.logGroupName,\n\t\t\t\t\"logCreateGroup\": l.logCreateGroup,\n\t\t\t}\n\t\t\tif awsErr.Code() == resourceAlreadyExistsCode {\n\t\t\t\t// Allow creation to succeed\n\t\t\t\tlogrus.WithFields(fields).Info(\"Log group already exists\")\n\t\t\t\treturn nil\n\t\t\t}\n\t\t\tlogrus.WithFields(fields).Error(\"Failed to create log group\")\n\t\t}\n\t\treturn err\n\t}\n\treturn nil\n}"}, {"instruction": "// CreateDirIfMissing creates a dir for dirPath if not already exists. If the dir is empty it returns true", "input": "go language", "output": "func CreateDirIfMissing(dirPath string) (bool, error) {\n\t// if dirPath does not end with a path separator, it leaves out the last segment while creating directories\n\tif !strings.HasSuffix(dirPath, \"/\") {\n\t\tdirPath = dirPath + \"/\"\n\t}\n\tlogger.Debugf(\"CreateDirIfMissing [%s]\", dirPath)\n\tlogDirStatus(\"Before creating dir\", dirPath)\n\terr := os.MkdirAll(path.Dir(dirPath), 0755)\n\tif err != nil {\n\t\tlogger.Debugf(\"Error creating dir [%s]\", dirPath)\n\t\treturn false, errors.Wrapf(err, \"error creating dir [%s]\", dirPath)\n\t}\n\tlogDirStatus(\"After creating dir\", dirPath)\n\treturn DirEmpty(dirPath)\n}"}, {"instruction": "// ContainerStatus returns the container status.", "input": "go language", "output": "func (r *RemoteRuntimeService) ContainerStatus(containerID string) (*runtimeapi.ContainerStatus, error) {\n\tctx, cancel := getContextWithTimeout(r.timeout)\n\tdefer cancel()\n\n\tresp, err := r.runtimeClient.ContainerStatus(ctx, &runtimeapi.ContainerStatusRequest{\n\t\tContainerId: containerID,\n\t})\n\tif err != nil {\n\t\t// Don't spam the log with endless messages about the same failure.\n\t\tif r.logReduction.ShouldMessageBePrinted(err.Error(), containerID) {\n\t\t\tklog.Errorf(\"ContainerStatus %q from runtime service failed: %v\", containerID, err)\n\t\t}\n\t\treturn nil, err\n\t}\n\tr.logReduction.ClearID(containerID)\n\n\tif resp.Status != nil {\n\t\tif err := verifyContainerStatus(resp.Status); err != nil {\n\t\t\tklog.Errorf(\"ContainerStatus of %q failed: %v\", containerID, err)\n\t\t\treturn nil, err\n\t\t}\n\t}\n\n\treturn resp.Status, nil\n}"}, {"instruction": "// semverCompare returns whether the client's version is older, equal or newer than the given image's version.", "input": "go language", "output": "func semverCompare(image string) int {\n\tsplit := strings.Split(image, \":\")\n\tif len(split) < 2 {\n\t\t// If we don't know the version, we consider the client version newer.\n\t\treturn 1\n\t}\n\ttillerVersion, err := semver.NewVersion(split[1])\n\tif err != nil {\n\t\t// same thing with unparsable tiller versions (e.g. canary releases).\n\t\treturn 1\n\t}\n\tclientVersion, err := semver.NewVersion(version.Version)\n\tif err != nil {\n\t\t// aaaaaand same thing with unparsable helm versions (e.g. canary releases).\n\t\treturn 1\n\t}\n\treturn clientVersion.Compare(tillerVersion)\n}"}, {"instruction": "// RunCompletion checks given arguments and executes command", "input": "go language", "output": "func RunCompletion(out io.Writer, boilerPlate string, cmd *cobra.Command, args []string) error {\n\tif len(args) == 0 {\n\t\treturn cmdutil.UsageErrorf(cmd, \"Shell not specified.\")\n\t}\n\tif len(args) > 1 {\n\t\treturn cmdutil.UsageErrorf(cmd, \"Too many arguments. Expected only the shell type.\")\n\t}\n\trun, found := completionShells[args[0]]\n\tif !found {\n\t\treturn cmdutil.UsageErrorf(cmd, \"Unsupported shell type %q.\", args[0])\n\t}\n\n\treturn run(out, boilerPlate, cmd.Parent())\n}"}, {"instruction": "// CreateOrRetainConfigMap creates a ConfigMap if the target resource doesn't exist. If the resource exists already, this function will retain the resource instead.", "input": "go language", "output": "func CreateOrRetainConfigMap(client clientset.Interface, cm *v1.ConfigMap, configMapName string) error {\n\tif _, err := client.CoreV1().ConfigMaps(cm.ObjectMeta.Namespace).Get(configMapName, metav1.GetOptions{}); err != nil {\n\t\tif !apierrors.IsNotFound(err) {\n\t\t\treturn nil\n\t\t}\n\t\tif _, err := client.CoreV1().ConfigMaps(cm.ObjectMeta.Namespace).Create(cm); err != nil {\n\t\t\tif !apierrors.IsAlreadyExists(err) {\n\t\t\t\treturn errors.Wrap(err, \"unable to create configmap\")\n\t\t\t}\n\t\t}\n\t}\n\treturn nil\n}"}, {"instruction": "// UploadDirectory uploads a directory tree to swarm and either adds the files\n// to an existing manifest (if the manifest argument is non-empty) or creates a\n// new manifest, returning the resulting manifest hash (files from the\n// directory will then be available at bzz:/<hash>/path/to/file), with\n// the file specified in defaultPath being uploaded to the root of the manifest\n// (i.e. bzz:/<hash>/)", "input": "go language", "output": "func (c *Client) UploadDirectory(dir, defaultPath, manifest string, toEncrypt bool) (string, error) {\n\tstat, err := os.Stat(dir)\n\tif err != nil {\n\t\treturn \"\", err\n\t} else if !stat.IsDir() {\n\t\treturn \"\", fmt.Errorf(\"not a directory: %s\", dir)\n\t}\n\tif defaultPath != \"\" {\n\t\tif _, err := os.Stat(filepath.Join(dir, defaultPath)); err != nil {\n\t\t\tif os.IsNotExist(err) {\n\t\t\t\treturn \"\", fmt.Errorf(\"the default path %q was not found in the upload directory %q\", defaultPath, dir)\n\t\t\t}\n\t\t\treturn \"\", fmt.Errorf(\"default path: %v\", err)\n\t\t}\n\t}\n\treturn c.TarUpload(manifest, &DirectoryUploader{dir}, defaultPath, toEncrypt)\n}"}, {"instruction": "// StoreResult stores the retrieved data in local database", "input": "go language", "output": "func (req *BloomRequest) StoreResult(db ethdb.Database) {\n\tfor i, sectionIdx := range req.SectionIndexList {\n\t\tsectionHead := rawdb.ReadCanonicalHash(db, (sectionIdx+1)*req.Config.BloomTrieSize-1)\n\t\t// if we don't have the canonical hash stored for this section head number, we'll still store it under\n\t\t// a key with a zero sectionHead. GetBloomBits will look there too if we still don't have the canonical\n\t\t// hash. In the unlikely case we've retrieved the section head hash since then, we'll just retrieve the\n\t\t// bit vector again from the network.\n\t\trawdb.WriteBloomBits(db, req.BitIdx, sectionIdx, sectionHead, req.BloomBits[i])\n\t}\n}"}, {"instruction": "// ComputeHash returns a hash value calculated from pod template and\n// a collisionCount to avoid hash collision. The hash will be safe encoded to\n// avoid bad words.", "input": "go language", "output": "func ComputeHash(template *v1.PodTemplateSpec, collisionCount *int32) string {\n\tpodTemplateSpecHasher := fnv.New32a()\n\thashutil.DeepHashObject(podTemplateSpecHasher, *template)\n\n\t// Add collisionCount in the hash if it exists.\n\tif collisionCount != nil {\n\t\tcollisionCountBytes := make([]byte, 8)\n\t\tbinary.LittleEndian.PutUint32(collisionCountBytes, uint32(*collisionCount))\n\t\tpodTemplateSpecHasher.Write(collisionCountBytes)\n\t}\n\n\treturn rand.SafeEncodeString(fmt.Sprint(podTemplateSpecHasher.Sum32()))\n}"}, {"instruction": "// VisitKind prints a Kind type. It prints each key in the kind, with\n// the type, the required flag, and the description.", "input": "go language", "output": "func (f *regularFieldsPrinter) VisitKind(k *proto.Kind) {\n\tfor _, key := range k.Keys() {\n\t\tv := k.Fields[key]\n\t\trequired := \"\"\n\t\tif k.IsRequired(key) {\n\t\t\trequired = \" -required-\"\n\t\t}\n\n\t\tif err := f.Writer.Write(\"%s\\t<%s>%s\", key, GetTypeName(v), required); err != nil {\n\t\t\tf.Error = err\n\t\t\treturn\n\t\t}\n\t\tif err := f.Writer.Indent(indentDesc).WriteWrapped(\"%s\", v.GetDescription()); err != nil {\n\t\t\tf.Error = err\n\t\t\treturn\n\t\t}\n\t\tif err := f.Writer.Write(\"\"); err != nil {\n\t\t\tf.Error = err\n\t\t\treturn\n\t\t}\n\t}\n}"}, {"instruction": "// ScanIndexData scans the index handles and values in a limited number, according to the index information.\n// It returns data and the next startVals until it doesn't have data, then returns data is nil and\n// the next startVals is the values which can't get data. If startVals = nil and limit = -1,\n// it returns the index data of the whole.", "input": "go language", "output": "func ScanIndexData(sc *stmtctx.StatementContext, txn kv.Transaction, kvIndex table.Index, startVals []types.Datum, limit int64) (\n\t[]*RecordData, []types.Datum, error) {\n\tit, _, err := kvIndex.Seek(sc, txn, startVals)\n\tif err != nil {\n\t\treturn nil, nil, errors.Trace(err)\n\t}\n\tdefer it.Close()\n\n\tvar idxRows []*RecordData\n\tvar curVals []types.Datum\n\tfor limit != 0 {\n\t\tval, h, err1 := it.Next()\n\t\tif terror.ErrorEqual(err1, io.EOF) {\n\t\t\treturn idxRows, nextIndexVals(curVals), nil\n\t\t} else if err1 != nil {\n\t\t\treturn nil, nil, errors.Trace(err1)\n\t\t}\n\t\tidxRows = append(idxRows, &RecordData{Handle: h, Values: val})\n\t\tlimit--\n\t\tcurVals = val\n\t}\n\n\tnextVals, _, err := it.Next()\n\tif terror.ErrorEqual(err, io.EOF) {\n\t\treturn idxRows, nextIndexVals(curVals), nil\n\t} else if err != nil {\n\t\treturn nil, nil, errors.Trace(err)\n\t}\n\n\treturn idxRows, nextVals, nil\n}"}, {"instruction": "// unpackAtomic unpacks ( hexdata -> go ) a single value", "input": "go language", "output": "func (arguments Arguments) unpackAtomic(v interface{}, marshalledValues interface{}) error {\n\tif arguments.LengthNonIndexed() == 0 {\n\t\treturn nil\n\t}\n\targument := arguments.NonIndexed()[0]\n\telem := reflect.ValueOf(v).Elem()\n\n\tif elem.Kind() == reflect.Struct {\n\t\tfieldmap, err := mapArgNamesToStructFields([]string{argument.Name}, elem)\n\t\tif err != nil {\n\t\t\treturn err\n\t\t}\n\t\tfield := elem.FieldByName(fieldmap[argument.Name])\n\t\tif !field.IsValid() {\n\t\t\treturn fmt.Errorf(\"abi: field %s can't be found in the given value\", argument.Name)\n\t\t}\n\t\treturn unpack(&argument.Type, field.Addr().Interface(), marshalledValues)\n\t}\n\treturn unpack(&argument.Type, elem.Addr().Interface(), marshalledValues)\n}"}, {"instruction": "// controlledHistories returns all ControllerRevisions in namespace that selected by selector and owned by accessor\n// TODO: Rename this to controllerHistory when other controllers have been upgraded", "input": "go language", "output": "func controlledHistoryV1(\n\tapps clientappsv1.AppsV1Interface,\n\tnamespace string,\n\tselector labels.Selector,\n\taccessor metav1.Object) ([]*appsv1.ControllerRevision, error) {\n\tvar result []*appsv1.ControllerRevision\n\thistoryList, err := apps.ControllerRevisions(namespace).List(metav1.ListOptions{LabelSelector: selector.String()})\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\tfor i := range historyList.Items {\n\t\thistory := historyList.Items[i]\n\t\t// Only add history that belongs to the API object\n\t\tif metav1.IsControlledBy(&history, accessor) {\n\t\t\tresult = append(result, &history)\n\t\t}\n\t}\n\treturn result, nil\n}"}, {"instruction": "// readLoop runs in its own goroutine. it handles incoming UDP packets.", "input": "go language", "output": "func (t *UDPv4) readLoop(unhandled chan<- ReadPacket) {\n\tdefer t.wg.Done()\n\tif unhandled != nil {\n\t\tdefer close(unhandled)\n\t}\n\n\tbuf := make([]byte, maxPacketSize)\n\tfor {\n\t\tnbytes, from, err := t.conn.ReadFromUDP(buf)\n\t\tif netutil.IsTemporaryError(err) {\n\t\t\t// Ignore temporary read errors.\n\t\t\tt.log.Debug(\"Temporary UDP read error\", \"err\", err)\n\t\t\tcontinue\n\t\t} else if err != nil {\n\t\t\t// Shut down the loop for permament errors.\n\t\t\tif err != io.EOF {\n\t\t\t\tt.log.Debug(\"UDP read error\", \"err\", err)\n\t\t\t}\n\t\t\treturn\n\t\t}\n\t\tif t.handlePacket(from, buf[:nbytes]) != nil && unhandled != nil {\n\t\t\tselect {\n\t\t\tcase unhandled <- ReadPacket{buf[:nbytes], from}:\n\t\t\tdefault:\n\t\t\t}\n\t\t}\n\t}\n}"}, {"instruction": "// Resize perform resize of file system", "input": "go language", "output": "func (resizefs *ResizeFs) Resize(devicePath string, deviceMountPath string) (bool, error) {\n\tformat, err := resizefs.mounter.GetDiskFormat(devicePath)\n\n\tif err != nil {\n\t\tformatErr := fmt.Errorf(\"ResizeFS.Resize - error checking format for device %s: %v\", devicePath, err)\n\t\treturn false, formatErr\n\t}\n\n\t// If disk has no format, there is no need to resize the disk because mkfs.*\n\t// by default will use whole disk anyways.\n\tif format == \"\" {\n\t\treturn false, nil\n\t}\n\n\tklog.V(3).Infof(\"ResizeFS.Resize - Expanding mounted volume %s\", devicePath)\n\tswitch format {\n\tcase \"ext3\", \"ext4\":\n\t\treturn resizefs.extResize(devicePath)\n\tcase \"xfs\":\n\t\treturn resizefs.xfsResize(deviceMountPath)\n\t}\n\treturn false, fmt.Errorf(\"ResizeFS.Resize - resize of format %s is not supported for device %s mounted at %s\", format, devicePath, deviceMountPath)\n}"}, {"instruction": "// Create makes a new prepared query. The ID of the new query is returned.", "input": "go language", "output": "func (c *PreparedQuery) Create(query *PreparedQueryDefinition, q *WriteOptions) (string, *WriteMeta, error) {\n\tr := c.c.newRequest(\"POST\", \"/v1/query\")\n\tr.setWriteOptions(q)\n\tr.obj = query\n\trtt, resp, err := requireOK(c.c.doRequest(r))\n\tif err != nil {\n\t\treturn \"\", nil, err\n\t}\n\tdefer resp.Body.Close()\n\n\twm := &WriteMeta{}\n\twm.RequestTime = rtt\n\n\tvar out struct{ ID string }\n\tif err := decodeBody(resp, &out); err != nil {\n\t\treturn \"\", nil, err\n\t}\n\treturn out.ID, wm, nil\n}"}, {"instruction": "// RunInContainer runs a command in a container, returns the combined stdout, stderr as an array of bytes", "input": "go language", "output": "func (kl *Kubelet) RunInContainer(podFullName string, podUID types.UID, containerName string, cmd []string) ([]byte, error) {\n\tcontainer, err := kl.findContainer(podFullName, podUID, containerName)\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\tif container == nil {\n\t\treturn nil, fmt.Errorf(\"container not found (%q)\", containerName)\n\t}\n\t// TODO(tallclair): Pass a proper timeout value.\n\treturn kl.runner.RunInContainer(container.ID, cmd, 0)\n}"}, {"instruction": "// ctr mutex must be held when calling this function.", "input": "go language", "output": "func (c *client) terminateContainer(ctr *container) error {\n\tconst terminateTimeout = time.Minute * 5\n\tctr.terminateInvoked = true\n\terr := ctr.hcsContainer.Terminate()\n\n\tif hcsshim.IsPending(err) {\n\t\terr = ctr.hcsContainer.WaitTimeout(terminateTimeout)\n\t} else if hcsshim.IsAlreadyStopped(err) {\n\t\terr = nil\n\t}\n\n\tif err != nil {\n\t\tc.logger.WithError(err).WithField(\"container\", ctr.id).\n\t\t\tDebug(\"failed to terminate container\")\n\t\treturn err\n\t}\n\n\treturn nil\n}"}, {"instruction": "// NewControllerRevision returns a ControllerRevision with a ControllerRef pointing to parent and indicating that\n// parent is of parentKind. The ControllerRevision has labels matching template labels, contains Data equal to data, and\n// has a Revision equal to revision. The collisionCount is used when creating the name of the ControllerRevision\n// so the name is likely unique. If the returned error is nil, the returned ControllerRevision is valid. If the\n// returned error is not nil, the returned ControllerRevision is invalid for use.", "input": "go language", "output": "func NewControllerRevision(parent metav1.Object,\n\tparentKind schema.GroupVersionKind,\n\ttemplateLabels map[string]string,\n\tdata runtime.RawExtension,\n\trevision int64,\n\tcollisionCount *int32) (*apps.ControllerRevision, error) {\n\tlabelMap := make(map[string]string)\n\tfor k, v := range templateLabels {\n\t\tlabelMap[k] = v\n\t}\n\tcr := &apps.ControllerRevision{\n\t\tObjectMeta: metav1.ObjectMeta{\n\t\t\tLabels:          labelMap,\n\t\t\tOwnerReferences: []metav1.OwnerReference{*metav1.NewControllerRef(parent, parentKind)},\n\t\t},\n\t\tData:     data,\n\t\tRevision: revision,\n\t}\n\thash := HashControllerRevision(cr, collisionCount)\n\tcr.Name = ControllerRevisionName(parent.GetName(), hash)\n\tcr.Labels[ControllerRevisionHashLabel] = hash\n\treturn cr, nil\n}"}, {"instruction": "// TODO: test", "input": "go language", "output": "func (n *EvalDeposeState) Eval(ctx EvalContext) (interface{}, error) {\n\tabsAddr := n.Addr.Absolute(ctx.Path())\n\tstate := ctx.State()\n\n\tvar key states.DeposedKey\n\tif n.ForceKey == states.NotDeposed {\n\t\tkey = state.DeposeResourceInstanceObject(absAddr)\n\t} else {\n\t\tkey = n.ForceKey\n\t\tstate.DeposeResourceInstanceObjectForceKey(absAddr, key)\n\t}\n\tlog.Printf(\"[TRACE] EvalDeposeState: prior object for %s now deposed with key %s\", absAddr, key)\n\n\tif n.OutputKey != nil {\n\t\t*n.OutputKey = key\n\t}\n\n\treturn nil, nil\n}"}, {"instruction": "// List takes label and field selectors, and returns the list of ReplicationControllers that match those selectors.", "input": "go language", "output": "func (c *FakeReplicationControllers) List(opts v1.ListOptions) (result *corev1.ReplicationControllerList, err error) {\n\tobj, err := c.Fake.\n\t\tInvokes(testing.NewListAction(replicationcontrollersResource, replicationcontrollersKind, c.ns, opts), &corev1.ReplicationControllerList{})\n\n\tif obj == nil {\n\t\treturn nil, err\n\t}\n\n\tlabel, _, _ := testing.ExtractFromListOptions(opts)\n\tif label == nil {\n\t\tlabel = labels.Everything()\n\t}\n\tlist := &corev1.ReplicationControllerList{ListMeta: obj.(*corev1.ReplicationControllerList).ListMeta}\n\tfor _, item := range obj.(*corev1.ReplicationControllerList).Items {\n\t\tif label.Matches(labels.Set(item.Labels)) {\n\t\t\tlist.Items = append(list.Items, item)\n\t\t}\n\t}\n\treturn list, err\n}"}, {"instruction": "// NegotiateFormat returns an acceptable Accept format.", "input": "go language", "output": "func (c *Context) NegotiateFormat(offered ...string) string {\n\tassert1(len(offered) > 0, \"you must provide at least one offer\")\n\n\tif c.Accepted == nil {\n\t\tc.Accepted = parseAccept(c.requestHeader(\"Accept\"))\n\t}\n\tif len(c.Accepted) == 0 {\n\t\treturn offered[0]\n\t}\n\tfor _, accepted := range c.Accepted {\n\t\tfor _, offert := range offered {\n\t\t\t// According to RFC 2616 and RFC 2396, non-ASCII characters are not allowed in headers,\n\t\t\t// therefore we can just iterate over the string without casting it into []rune\n\t\t\ti := 0\n\t\t\tfor ; i < len(accepted); i++ {\n\t\t\t\tif accepted[i] == '*' || offert[i] == '*' {\n\t\t\t\t\treturn offert\n\t\t\t\t}\n\t\t\t\tif accepted[i] != offert[i] {\n\t\t\t\t\tbreak\n\t\t\t\t}\n\t\t\t}\n\t\t\tif i == len(accepted) {\n\t\t\t\treturn offert\n\t\t\t}\n\t\t}\n\t}\n\treturn \"\"\n}"}, {"instruction": "// Open implements the Executor Open interface.", "input": "go language", "output": "func (e *CleanupIndexExec) Open(ctx context.Context) error {\n\tif err := e.baseExecutor.Open(ctx); err != nil {\n\t\treturn err\n\t}\n\te.idxChunk = chunk.New(e.getIdxColTypes(), e.initCap, e.maxChunkSize)\n\te.idxValues = make(map[int64][][]types.Datum, e.batchSize)\n\te.batchKeys = make([]kv.Key, 0, e.batchSize)\n\te.idxValsBufs = make([][]types.Datum, e.batchSize)\n\tsc := e.ctx.GetSessionVars().StmtCtx\n\tidxKey, _, err := e.index.GenIndexKey(sc, []types.Datum{{}}, math.MinInt64, nil)\n\tif err != nil {\n\t\treturn err\n\t}\n\te.lastIdxKey = idxKey\n\treturn nil\n}"}, {"instruction": "// fetchAllInners reads all data from the inner table and stores them in a List.", "input": "go language", "output": "func (e *NestedLoopApplyExec) fetchAllInners(ctx context.Context) error {\n\terr := e.innerExec.Open(ctx)\n\tdefer terror.Call(e.innerExec.Close)\n\tif err != nil {\n\t\treturn err\n\t}\n\te.innerList.Reset()\n\tinnerIter := chunk.NewIterator4Chunk(e.innerChunk)\n\tfor {\n\t\terr := e.innerExec.Next(ctx, chunk.NewRecordBatch(e.innerChunk))\n\t\tif err != nil {\n\t\t\treturn err\n\t\t}\n\t\tif e.innerChunk.NumRows() == 0 {\n\t\t\treturn nil\n\t\t}\n\n\t\te.innerSelected, err = expression.VectorizedFilter(e.ctx, e.innerFilter, innerIter, e.innerSelected)\n\t\tif err != nil {\n\t\t\treturn err\n\t\t}\n\t\tfor row := innerIter.Begin(); row != innerIter.End(); row = innerIter.Next() {\n\t\t\tif e.innerSelected[row.Idx()] {\n\t\t\t\te.innerList.AppendRow(row)\n\t\t\t}\n\t\t}\n\t}\n}"}, {"instruction": "// AddSymKeyFromPassword generates the key from password, stores it, and returns its id.", "input": "go language", "output": "func (whisper *Whisper) AddSymKeyFromPassword(password string) (string, error) {\n\tid, err := GenerateRandomID()\n\tif err != nil {\n\t\treturn \"\", fmt.Errorf(\"failed to generate ID: %s\", err)\n\t}\n\tif whisper.HasSymKey(id) {\n\t\treturn \"\", fmt.Errorf(\"failed to generate unique ID\")\n\t}\n\n\t// kdf should run no less than 0.1 seconds on an average computer,\n\t// because it's an once in a session experience\n\tderived := pbkdf2.Key([]byte(password), nil, 65356, aesKeyLength, sha256.New)\n\tif err != nil {\n\t\treturn \"\", err\n\t}\n\n\twhisper.keyMu.Lock()\n\tdefer whisper.keyMu.Unlock()\n\n\t// double check is necessary, because deriveKeyMaterial() is very slow\n\tif whisper.symKeys[id] != nil {\n\t\treturn \"\", fmt.Errorf(\"critical error: failed to generate unique ID\")\n\t}\n\twhisper.symKeys[id] = derived\n\treturn id, nil\n}"}, {"instruction": "// Cleanup stops active swarm node. This is run before daemon shutdown.", "input": "go language", "output": "func (c *Cluster) Cleanup() {\n\tc.controlMutex.Lock()\n\tdefer c.controlMutex.Unlock()\n\n\tc.mu.Lock()\n\tnode := c.nr\n\tif node == nil {\n\t\tc.mu.Unlock()\n\t\treturn\n\t}\n\tstate := c.currentNodeState()\n\tc.mu.Unlock()\n\n\tif state.IsActiveManager() {\n\t\tactive, reachable, unreachable, err := managerStats(state.controlClient, state.NodeID())\n\t\tif err == nil {\n\t\t\tsinglenode := active && isLastManager(reachable, unreachable)\n\t\t\tif active && !singlenode && removingManagerCausesLossOfQuorum(reachable, unreachable) {\n\t\t\t\tlogrus.Errorf(\"Leaving cluster with %v managers left out of %v. Raft quorum will be lost.\", reachable-1, reachable+unreachable)\n\t\t\t}\n\t\t}\n\t}\n\n\tif err := node.Stop(); err != nil {\n\t\tlogrus.Errorf(\"failed to shut down cluster node: %v\", err)\n\t\tsignal.DumpStacks(\"\")\n\t}\n\n\tc.mu.Lock()\n\tc.nr = nil\n\tc.mu.Unlock()\n}"}, {"instruction": "// DeepCopyInto is an autogenerated deepcopy function, copying the receiver, writing into out. in must be non-nil.", "input": "go language", "output": "func (in *SubjectAccessReviewSpec) DeepCopyInto(out *SubjectAccessReviewSpec) {\n\t*out = *in\n\tif in.ResourceAttributes != nil {\n\t\tin, out := &in.ResourceAttributes, &out.ResourceAttributes\n\t\t*out = new(ResourceAttributes)\n\t\t**out = **in\n\t}\n\tif in.NonResourceAttributes != nil {\n\t\tin, out := &in.NonResourceAttributes, &out.NonResourceAttributes\n\t\t*out = new(NonResourceAttributes)\n\t\t**out = **in\n\t}\n\tif in.Groups != nil {\n\t\tin, out := &in.Groups, &out.Groups\n\t\t*out = make([]string, len(*in))\n\t\tcopy(*out, *in)\n\t}\n\tif in.Extra != nil {\n\t\tin, out := &in.Extra, &out.Extra\n\t\t*out = make(map[string]ExtraValue, len(*in))\n\t\tfor key, val := range *in {\n\t\t\tvar outVal []string\n\t\t\tif val == nil {\n\t\t\t\t(*out)[key] = nil\n\t\t\t} else {\n\t\t\t\tin, out := &val, &outVal\n\t\t\t\t*out = make(ExtraValue, len(*in))\n\t\t\t\tcopy(*out, *in)\n\t\t\t}\n\t\t\t(*out)[key] = outVal\n\t\t}\n\t}\n\treturn\n}"}, {"instruction": "// HEALTHCHECK foo\n//\n// Set the default healthcheck command to run in the container (which may be empty).\n// Argument handling is the same as RUN.\n//", "input": "go language", "output": "func dispatchHealthcheck(d dispatchRequest, c *instructions.HealthCheckCommand) error {\n\trunConfig := d.state.runConfig\n\tif runConfig.Healthcheck != nil {\n\t\toldCmd := runConfig.Healthcheck.Test\n\t\tif len(oldCmd) > 0 && oldCmd[0] != \"NONE\" {\n\t\t\tfmt.Fprintf(d.builder.Stdout, \"Note: overriding previous HEALTHCHECK: %v\\n\", oldCmd)\n\t\t}\n\t}\n\trunConfig.Healthcheck = c.Health\n\treturn d.builder.commit(d.state, fmt.Sprintf(\"HEALTHCHECK %q\", runConfig.Healthcheck))\n}"}, {"instruction": "// LstatIfPossible returns the os.FileInfo structure describing a given file.\n// It attempts to use Lstat if supported or defers to the os.  In addition to\n// the FileInfo, a boolean is returned telling whether Lstat was called.", "input": "go language", "output": "func (fs *LanguageFs) LstatIfPossible(name string) (os.FileInfo, bool, error) {\n\tname, err := fs.realName(name)\n\tif err != nil {\n\t\treturn nil, false, err\n\t}\n\n\tvar fi os.FileInfo\n\tvar b bool\n\n\tif lif, ok := fs.Fs.(afero.Lstater); ok {\n\t\tfi, b, err = lif.LstatIfPossible(name)\n\t} else {\n\t\tfi, err = fs.Fs.Stat(name)\n\t}\n\n\tif err != nil {\n\t\treturn nil, b, err\n\t}\n\n\tlfi, err := fs.newLanguageFileInfo(name, fi)\n\n\treturn lfi, b, err\n}"}, {"instruction": "// GetNetworks returns a list of all networks", "input": "go language", "output": "func (daemon *Daemon) GetNetworks(filter filters.Args, config types.NetworkListConfig) ([]types.NetworkResource, error) {\n\tnetworks := daemon.getAllNetworks()\n\n\tlist := make([]types.NetworkResource, 0, len(networks))\n\tvar idx map[string]libnetwork.Network\n\tif config.Detailed {\n\t\tidx = make(map[string]libnetwork.Network)\n\t}\n\n\tfor _, n := range networks {\n\t\tnr := buildNetworkResource(n)\n\t\tlist = append(list, nr)\n\t\tif config.Detailed {\n\t\t\tidx[nr.ID] = n\n\t\t}\n\t}\n\n\tvar err error\n\tlist, err = internalnetwork.FilterNetworks(list, filter)\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\n\tif config.Detailed {\n\t\tfor i, n := range list {\n\t\t\tnp := &n\n\t\t\tbuildDetailedNetworkResources(np, idx[n.ID], config.Verbose)\n\t\t\tlist[i] = *np\n\t\t}\n\t}\n\n\treturn list, nil\n}"}, {"instruction": "// PrintFs prints the given filesystem to the given writer starting from the given path.\n// This is useful for debugging.", "input": "go language", "output": "func PrintFs(fs afero.Fs, path string, w io.Writer) {\n\tif fs == nil {\n\t\treturn\n\t}\n\tafero.Walk(fs, path, func(path string, info os.FileInfo, err error) error {\n\t\tif info != nil && !info.IsDir() {\n\t\t\ts := path\n\t\t\tif lang, ok := info.(hugofs.LanguageAnnouncer); ok {\n\t\t\t\ts = s + \"\\tLANG: \" + lang.Lang()\n\t\t\t}\n\t\t\tif fp, ok := info.(hugofs.FilePather); ok {\n\t\t\t\ts = s + \"\\tRF: \" + fp.Filename() + \"\\tBP: \" + fp.BaseDir()\n\t\t\t}\n\t\t\tfmt.Fprintln(w, \"    \", s)\n\t\t}\n\t\treturn nil\n\t})\n}"}, {"instruction": "// NewCmdAlpha returns \"kubeadm alpha\" command.", "input": "go language", "output": "func NewCmdAlpha(in io.Reader, out io.Writer) *cobra.Command {\n\tcmd := &cobra.Command{\n\t\tUse:   \"alpha\",\n\t\tShort: \"Kubeadm experimental sub-commands\",\n\t}\n\n\tcmd.AddCommand(newCmdCertsUtility())\n\tcmd.AddCommand(newCmdKubeletUtility())\n\tcmd.AddCommand(newCmdKubeConfigUtility(out))\n\tcmd.AddCommand(NewCmdSelfhosting(in))\n\n\t// TODO: This command should be removed as soon as the kubeadm init phase refactoring is completed.\n\t//\t\t current phases implemented as cobra.Commands should become workflow.Phases, while other utilities\n\t// \t\t hosted under kubeadm alpha phases command should found a new home under kubeadm alpha (without phases)\n\tcmd.AddCommand(newCmdPhase(out))\n\n\treturn cmd\n}"}, {"instruction": "// Patch applies the patch and returns the patched validatingWebhookConfiguration.", "input": "go language", "output": "func (c *FakeValidatingWebhookConfigurations) Patch(name string, pt types.PatchType, data []byte, subresources ...string) (result *v1beta1.ValidatingWebhookConfiguration, err error) {\n\tobj, err := c.Fake.\n\t\tInvokes(testing.NewRootPatchSubresourceAction(validatingwebhookconfigurationsResource, name, pt, data, subresources...), &v1beta1.ValidatingWebhookConfiguration{})\n\tif obj == nil {\n\t\treturn nil, err\n\t}\n\treturn obj.(*v1beta1.ValidatingWebhookConfiguration), err\n}"}, {"instruction": "// Backup creates a backup of an etcd2 data directory at the given backupDir.", "input": "go language", "output": "func (e *CombinedEtcdClient) Backup(version *EtcdVersion, backupDir string) error {\n\t// We cannot use etcd/client (v2) to make this call. It is implemented in the etcdctl client code.\n\tif version.Major != 2 {\n\t\treturn fmt.Errorf(\"etcd 2.x required but got version '%s'\", version)\n\t}\n\treturn e.runEtcdctlCommand(version,\n\t\t\"--debug\",\n\t\t\"backup\",\n\t\t\"--data-dir\", e.cfg.dataDirectory,\n\t\t\"--backup-dir\", backupDir,\n\t)\n}"}, {"instruction": "// DecodeFormats takes a list of output format configurations and merges those,\n// in the order given, with the Hugo defaults as the last resort.", "input": "go language", "output": "func DecodeFormats(mediaTypes media.Types, maps ...map[string]interface{}) (Formats, error) {\n\tf := make(Formats, len(DefaultFormats))\n\tcopy(f, DefaultFormats)\n\n\tfor _, m := range maps {\n\t\tfor k, v := range m {\n\t\t\tfound := false\n\t\t\tfor i, vv := range f {\n\t\t\t\tif strings.EqualFold(k, vv.Name) {\n\t\t\t\t\t// Merge it with the existing\n\t\t\t\t\tif err := decode(mediaTypes, v, &f[i]); err != nil {\n\t\t\t\t\t\treturn f, err\n\t\t\t\t\t}\n\t\t\t\t\tfound = true\n\t\t\t\t}\n\t\t\t}\n\t\t\tif !found {\n\t\t\t\tvar newOutFormat Format\n\t\t\t\tnewOutFormat.Name = k\n\t\t\t\tif err := decode(mediaTypes, v, &newOutFormat); err != nil {\n\t\t\t\t\treturn f, err\n\t\t\t\t}\n\n\t\t\t\t// We need values for these\n\t\t\t\tif newOutFormat.BaseName == \"\" {\n\t\t\t\t\tnewOutFormat.BaseName = \"index\"\n\t\t\t\t}\n\t\t\t\tif newOutFormat.Rel == \"\" {\n\t\t\t\t\tnewOutFormat.Rel = \"alternate\"\n\t\t\t\t}\n\n\t\t\t\tf = append(f, newOutFormat)\n\t\t\t}\n\t\t}\n\t}\n\n\tsort.Sort(f)\n\n\treturn f, nil\n}"}, {"instruction": "// UserEvents is used to return a slice of the most recent\n// user events.", "input": "go language", "output": "func (a *Agent) UserEvents() []*UserEvent {\n\tn := len(a.eventBuf)\n\tout := make([]*UserEvent, n)\n\ta.eventLock.RLock()\n\tdefer a.eventLock.RUnlock()\n\n\t// Check if the buffer is full\n\tif a.eventBuf[a.eventIndex] != nil {\n\t\tif a.eventIndex == 0 {\n\t\t\tcopy(out, a.eventBuf)\n\t\t} else {\n\t\t\tcopy(out, a.eventBuf[a.eventIndex:])\n\t\t\tcopy(out[n-a.eventIndex:], a.eventBuf[:a.eventIndex])\n\t\t}\n\t} else {\n\t\t// We haven't filled the buffer yet\n\t\tcopy(out, a.eventBuf[:a.eventIndex])\n\t\tout = out[:a.eventIndex]\n\t}\n\treturn out\n}"}, {"instruction": "// DropTable will proceed even if some table in the list does not exists.", "input": "go language", "output": "func (d *ddl) DropTable(ctx sessionctx.Context, ti ast.Ident) (err error) {\n\tschema, tb, err := d.getSchemaAndTableByIdent(ctx, ti)\n\tif err != nil {\n\t\treturn errors.Trace(err)\n\t}\n\n\tjob := &model.Job{\n\t\tSchemaID:   schema.ID,\n\t\tTableID:    tb.Meta().ID,\n\t\tType:       model.ActionDropTable,\n\t\tBinlogInfo: &model.HistoryInfo{},\n\t}\n\n\terr = d.doDDLJob(ctx, job)\n\terr = d.callHookOnChanged(err)\n\treturn errors.Trace(err)\n}"}, {"instruction": "// Modify modifies a JSON object by insert, replace or set.\n// All path expressions cannot contain * or ** wildcard.\n// If any error occurs, the input won't be changed.", "input": "go language", "output": "func (bj BinaryJSON) Modify(pathExprList []PathExpression, values []BinaryJSON, mt ModifyType) (retj BinaryJSON, err error) {\n\tif len(pathExprList) != len(values) {\n\t\t// TODO: should return 1582(42000)\n\t\treturn retj, errors.New(\"Incorrect parameter count\")\n\t}\n\tfor _, pathExpr := range pathExprList {\n\t\tif pathExpr.flags.containsAnyAsterisk() {\n\t\t\t// TODO: should return 3149(42000)\n\t\t\treturn retj, errors.New(\"Invalid path expression\")\n\t\t}\n\t}\n\tfor i := 0; i < len(pathExprList); i++ {\n\t\tpathExpr, value := pathExprList[i], values[i]\n\t\tmodifier := &binaryModifier{bj: bj}\n\t\tswitch mt {\n\t\tcase ModifyInsert:\n\t\t\tbj = modifier.insert(pathExpr, value)\n\t\tcase ModifyReplace:\n\t\t\tbj = modifier.replace(pathExpr, value)\n\t\tcase ModifySet:\n\t\t\tbj = modifier.set(pathExpr, value)\n\t\t}\n\t}\n\treturn bj, nil\n}"}, {"instruction": "// CreateDisk creates a new Persistent Disk, with the specified name &\n// size, in the specified zone. It stores specified tags encoded in\n// JSON in Description field.", "input": "go language", "output": "func (g *Cloud) CreateDisk(\n\tname string, diskType string, zone string, sizeGb int64, tags map[string]string) error {\n\t// Do not allow creation of PDs in zones that are do not have nodes. Such PDs\n\t// are not currently usable.\n\tcurZones, err := g.GetAllCurrentZones()\n\tif err != nil {\n\t\treturn err\n\t}\n\tif !curZones.Has(zone) {\n\t\treturn fmt.Errorf(\"kubernetes does not have a node in zone %q\", zone)\n\t}\n\n\ttagsStr, err := g.encodeDiskTags(tags)\n\tif err != nil {\n\t\treturn err\n\t}\n\n\tdiskType, err = getDiskType(diskType)\n\tif err != nil {\n\t\treturn err\n\t}\n\n\tmc := newDiskMetricContextZonal(\"create\", g.region, zone)\n\n\terr = g.manager.CreateDiskOnCloudProvider(\n\t\tname, sizeGb, tagsStr, diskType, zone)\n\n\tmc.Observe(err)\n\tif isGCEError(err, \"alreadyExists\") {\n\t\tklog.Warningf(\"GCE PD %q already exists, reusing\", name)\n\t\treturn nil\n\t}\n\treturn err\n}"}, {"instruction": "// receive reads the result from a watcher, restarting it if necessary.", "input": "go language", "output": "func (rw *RetryWatcher) receive() {\n\tdefer close(rw.doneChan)\n\tdefer close(rw.resultChan)\n\n\tklog.V(4).Info(\"Starting RetryWatcher.\")\n\tdefer klog.V(4).Info(\"Stopping RetryWatcher.\")\n\n\tctx, cancel := context.WithCancel(context.Background())\n\tdefer cancel()\n\tgo func() {\n\t\tselect {\n\t\tcase <-rw.stopChan:\n\t\t\tcancel()\n\t\t\treturn\n\t\tcase <-ctx.Done():\n\t\t\treturn\n\t\t}\n\t}()\n\n\t// We use non sliding until so we don't introduce delays on happy path when WATCH call\n\t// timeouts or gets closed and we need to reestablish it while also avoiding hot loops.\n\twait.NonSlidingUntilWithContext(ctx, func(ctx context.Context) {\n\t\tdone, retryAfter := rw.doReceive()\n\t\tif done {\n\t\t\tcancel()\n\t\t\treturn\n\t\t}\n\n\t\ttime.Sleep(retryAfter)\n\n\t\tklog.V(4).Infof(\"Restarting RetryWatcher at RV=%q\", rw.lastResourceVersion)\n\t}, rw.minRestartDelay)\n}"}, {"instruction": "// ValidateIgnorePreflightErrors validates duplicates in ignore-preflight-errors flag.", "input": "go language", "output": "func ValidateIgnorePreflightErrors(ignorePreflightErrors []string) (sets.String, error) {\n\tignoreErrors := sets.NewString()\n\tallErrs := field.ErrorList{}\n\n\tfor _, item := range ignorePreflightErrors {\n\t\tignoreErrors.Insert(strings.ToLower(item)) // parameters are case insensitive\n\t}\n\n\tif ignoreErrors.Has(\"all\") && ignoreErrors.Len() > 1 {\n\t\tallErrs = append(allErrs, field.Invalid(field.NewPath(\"ignore-preflight-errors\"), strings.Join(ignoreErrors.List(), \",\"), \"don't specify individual checks if 'all' is used\"))\n\t}\n\n\treturn ignoreErrors, allErrs.ToAggregate()\n}"}, {"instruction": "// Shutdown blocks until stopCh passed to the Run method is closed and all\n// events added prior to that moment are batched and sent to the delegate backend.", "input": "go language", "output": "func (b *bufferedBackend) Shutdown() {\n\t// Wait until the routine spawned in Run method exits.\n\t<-b.shutdownCh\n\n\t// Wait until all sending routines exit.\n\t//\n\t// - When b.shutdownCh is closed, we know that the goroutine in Run has terminated.\n\t// - This means that processIncomingEvents has terminated.\n\t// - Which means that b.buffer is closed and cannot accept any new events anymore.\n\t// - Because processEvents is called synchronously from the Run goroutine, the waitgroup has its final value.\n\t// Hence wg.Wait will not miss any more outgoing batches.\n\tb.wg.Wait()\n\n\tb.delegateBackend.Shutdown()\n}"}, {"instruction": "// TranslatePullError is used to convert an error from a registry pull\n// operation to an error representing the entire pull operation. Any error\n// information which is not used by the returned error gets output to\n// log at info level.", "input": "go language", "output": "func TranslatePullError(err error, ref reference.Named) error {\n\tswitch v := err.(type) {\n\tcase errcode.Errors:\n\t\tif len(v) != 0 {\n\t\t\tfor _, extra := range v[1:] {\n\t\t\t\tlogrus.Infof(\"Ignoring extra error returned from registry: %v\", extra)\n\t\t\t}\n\t\t\treturn TranslatePullError(v[0], ref)\n\t\t}\n\tcase errcode.Error:\n\t\tswitch v.Code {\n\t\tcase errcode.ErrorCodeDenied, v2.ErrorCodeManifestUnknown, v2.ErrorCodeNameUnknown:\n\t\t\treturn notFoundError{v, ref}\n\t\t}\n\tcase xfer.DoNotRetry:\n\t\treturn TranslatePullError(v.Err, ref)\n\t}\n\n\treturn errdefs.Unknown(err)\n}"}, {"instruction": "// Next moves the cursor to next block and returns true iff the iterator is not exhausted", "input": "go language", "output": "func (itr *blocksItr) Next() (ledger.QueryResult, error) {\n\tif itr.maxBlockNumAvailable < itr.blockNumToRetrieve {\n\t\titr.maxBlockNumAvailable = itr.waitForBlock(itr.blockNumToRetrieve)\n\t}\n\titr.closeMarkerLock.Lock()\n\tdefer itr.closeMarkerLock.Unlock()\n\tif itr.closeMarker {\n\t\treturn nil, nil\n\t}\n\tif itr.stream == nil {\n\t\tlogger.Debugf(\"Initializing block stream for iterator. itr.maxBlockNumAvailable=%d\", itr.maxBlockNumAvailable)\n\t\tif err := itr.initStream(); err != nil {\n\t\t\treturn nil, err\n\t\t}\n\t}\n\tnextBlockBytes, err := itr.stream.nextBlockBytes()\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\titr.blockNumToRetrieve++\n\treturn deserializeBlock(nextBlockBytes)\n}"}, {"instruction": "// SetLocalSystemVar sets values of the local variables which in \"server\" scope.", "input": "go language", "output": "func SetLocalSystemVar(name string, val string) {\n\tswitch name {\n\tcase TiDBDDLReorgWorkerCount:\n\t\tSetDDLReorgWorkerCounter(int32(tidbOptPositiveInt32(val, DefTiDBDDLReorgWorkerCount)))\n\tcase TiDBDDLReorgBatchSize:\n\t\tSetDDLReorgBatchSize(int32(tidbOptPositiveInt32(val, DefTiDBDDLReorgBatchSize)))\n\tcase TiDBDDLErrorCountLimit:\n\t\tSetDDLErrorCountLimit(tidbOptInt64(val, DefTiDBDDLErrorCountLimit))\n\t}\n}"}, {"instruction": "// handshake works like TCP handshake, but in a higher level, it first writes initial packet to client,\n// during handshake, client and server negotiate compatible features and do authentication.\n// After handshake, client can send sql query to server.", "input": "go language", "output": "func (cc *clientConn) handshake(ctx context.Context) error {\n\tif err := cc.writeInitialHandshake(); err != nil {\n\t\treturn err\n\t}\n\tif err := cc.readOptionalSSLRequestAndHandshakeResponse(ctx); err != nil {\n\t\terr1 := cc.writeError(err)\n\t\tif err1 != nil {\n\t\t\tlogutil.Logger(ctx).Debug(\"writeError failed\", zap.Error(err1))\n\t\t}\n\t\treturn err\n\t}\n\tdata := cc.alloc.AllocWithLen(4, 32)\n\tdata = append(data, mysql.OKHeader)\n\tdata = append(data, 0, 0)\n\tif cc.capability&mysql.ClientProtocol41 > 0 {\n\t\tdata = dumpUint16(data, mysql.ServerStatusAutocommit)\n\t\tdata = append(data, 0, 0)\n\t}\n\n\terr := cc.writePacket(data)\n\tcc.pkt.sequence = 0\n\tif err != nil {\n\t\treturn err\n\t}\n\n\treturn cc.flush()\n}"}, {"instruction": "// update takes a hash that forms the next leaf level (level-1) node in the merkle tree.\n// Also, complete the merkle tree as much as possible with the addition of this new leaf node -\n// i.e. recursively build the higher level nodes and delete the underlying sub-tree.", "input": "go language", "output": "func (m *merkleTree) update(nextLeafLevelHash Hash) error {\n\tlogger.Debugf(\"Before update() = %s\", m)\n\tdefer logger.Debugf(\"After update() = %s\", m)\n\tm.tree[leafLevel] = append(m.tree[leafLevel], nextLeafLevelHash)\n\tcurrentLevel := leafLevel\n\tfor {\n\t\tcurrentLevelHashes := m.tree[currentLevel]\n\t\tif uint32(len(currentLevelHashes)) <= m.maxDegree {\n\t\t\treturn nil\n\t\t}\n\t\tnextLevelHash, err := computeCombinedHash(currentLevelHashes)\n\t\tif err != nil {\n\t\t\treturn err\n\t\t}\n\t\tdelete(m.tree, currentLevel)\n\t\tnextLevel := currentLevel + 1\n\t\tm.tree[nextLevel] = append(m.tree[nextLevel], nextLevelHash)\n\t\tif nextLevel > m.maxLevel {\n\t\t\tm.maxLevel = nextLevel\n\t\t}\n\t\tcurrentLevel = nextLevel\n\t}\n}"}, {"instruction": "//\n// Setting, updating & deleting state object methods.\n//\n// updateStateObject writes the given object to the trie.", "input": "go language", "output": "func (s *StateDB) updateStateObject(stateObject *stateObject) {\n\t// Track the amount of time wasted on updating the account from the trie\n\tif metrics.EnabledExpensive {\n\t\tdefer func(start time.Time) { s.AccountUpdates += time.Since(start) }(time.Now())\n\t}\n\t// Encode the account and update the account trie\n\taddr := stateObject.Address()\n\n\tdata, err := rlp.EncodeToBytes(stateObject)\n\tif err != nil {\n\t\tpanic(fmt.Errorf(\"can't encode object at %x: %v\", addr[:], err))\n\t}\n\ts.setError(s.trie.TryUpdate(addr[:], data))\n}"}, {"instruction": "// Run is used to run a watch plan", "input": "go language", "output": "func (p *Plan) RunWithConfig(address string, conf *consulapi.Config) error {\n\t// Setup the client\n\tp.address = address\n\tif conf == nil {\n\t\tconf = consulapi.DefaultConfig()\n\t}\n\tconf.Address = address\n\tconf.Datacenter = p.Datacenter\n\tconf.Token = p.Token\n\tclient, err := consulapi.NewClient(conf)\n\tif err != nil {\n\t\treturn fmt.Errorf(\"Failed to connect to agent: %v\", err)\n\t}\n\n\t// Create the logger\n\toutput := p.LogOutput\n\tif output == nil {\n\t\toutput = os.Stderr\n\t}\n\tlogger := log.New(output, \"\", log.LstdFlags)\n\n\treturn p.RunWithClientAndLogger(client, logger)\n}"}, {"instruction": "// evalString evals a builtinCurrentUserSig.\n// See https://dev.mysql.com/doc/refman/5.7/en/information-functions.html#function_current-user", "input": "go language", "output": "func (b *builtinCurrentRoleSig) evalString(row chunk.Row) (string, bool, error) {\n\tdata := b.ctx.GetSessionVars()\n\tif data == nil || data.ActiveRoles == nil {\n\t\treturn \"\", true, errors.Errorf(\"Missing session variable when eval builtin\")\n\t}\n\tif len(data.ActiveRoles) == 0 {\n\t\treturn \"\", false, nil\n\t}\n\tres := \"\"\n\tfor i, r := range data.ActiveRoles {\n\t\tres += r.String()\n\t\tif i != len(data.ActiveRoles)-1 {\n\t\t\tres += \",\"\n\t\t}\n\t}\n\treturn res, false, nil\n}"}, {"instruction": "// verifySandboxStatus verified whether all required fields are set in PodSandboxStatus.", "input": "go language", "output": "func verifySandboxStatus(status *runtimeapi.PodSandboxStatus) error {\n\tif status.Id == \"\" {\n\t\treturn fmt.Errorf(\"Id is not set\")\n\t}\n\n\tif status.Metadata == nil {\n\t\treturn fmt.Errorf(\"Metadata is not set\")\n\t}\n\n\tmetadata := status.Metadata\n\tif metadata.Name == \"\" || metadata.Namespace == \"\" || metadata.Uid == \"\" {\n\t\treturn fmt.Errorf(\"Name, Namespace or Uid is not in metadata %q\", metadata)\n\t}\n\n\tif status.CreatedAt == 0 {\n\t\treturn fmt.Errorf(\"CreatedAt is not set\")\n\t}\n\n\treturn nil\n}"}, {"instruction": "// NewStateSync create a new state trie download scheduler.", "input": "go language", "output": "func NewStateSync(root common.Hash, database ethdb.Reader) *trie.Sync {\n\tvar syncer *trie.Sync\n\tcallback := func(leaf []byte, parent common.Hash) error {\n\t\tvar obj Account\n\t\tif err := rlp.Decode(bytes.NewReader(leaf), &obj); err != nil {\n\t\t\treturn err\n\t\t}\n\t\tsyncer.AddSubTrie(obj.Root, 64, parent, nil)\n\t\tsyncer.AddRawEntry(common.BytesToHash(obj.CodeHash), 64, parent)\n\t\treturn nil\n\t}\n\tsyncer = trie.NewSync(root, database, callback)\n\treturn syncer\n}"}, {"instruction": "// InstanceType returns the type of the specified instance.\n// Note that if the instance does not exist or is no longer running, we must return (\"\", cloudprovider.InstanceNotFound)\n// (Implementer Note): This is used by kubelet. Kubelet will label the node. Real log from kubelet:\n//       Adding node label from cloud provider: beta.kubernetes.io/instance-type=[value]", "input": "go language", "output": "func (az *Cloud) InstanceType(ctx context.Context, name types.NodeName) (string, error) {\n\t// Returns \"\" for unmanaged nodes because azure cloud provider couldn't fetch information for them.\n\tunmanaged, err := az.IsNodeUnmanaged(string(name))\n\tif err != nil {\n\t\treturn \"\", err\n\t}\n\tif unmanaged {\n\t\tklog.V(4).Infof(\"InstanceType: omitting unmanaged node %q\", name)\n\t\treturn \"\", nil\n\t}\n\n\tif az.UseInstanceMetadata {\n\t\tmetadata, err := az.metadata.GetMetadata()\n\t\tif err != nil {\n\t\t\treturn \"\", err\n\t\t}\n\n\t\tif metadata.Compute == nil {\n\t\t\treturn \"\", fmt.Errorf(\"failure of getting instance metadata\")\n\t\t}\n\n\t\tisLocalInstance, err := az.isCurrentInstance(name, metadata.Compute.Name)\n\t\tif err != nil {\n\t\t\treturn \"\", err\n\t\t}\n\t\tif isLocalInstance {\n\t\t\tif metadata.Compute.VMSize != \"\" {\n\t\t\t\treturn metadata.Compute.VMSize, nil\n\t\t\t}\n\t\t}\n\t}\n\n\treturn az.vmSet.GetInstanceTypeByNodeName(string(name))\n}"}, {"instruction": "// CreateConfiguration creates a provider configuration from content using templating.", "input": "go language", "output": "func (p *Provider) CreateConfiguration(tmplContent string, funcMap template.FuncMap, templateObjects interface{}) (*config.Configuration, error) {\n\tvar defaultFuncMap = sprig.TxtFuncMap()\n\tdefaultFuncMap[\"normalize\"] = provider.Normalize\n\tdefaultFuncMap[\"split\"] = strings.Split\n\tfor funcID, funcElement := range funcMap {\n\t\tdefaultFuncMap[funcID] = funcElement\n\t}\n\n\ttmpl := template.New(p.Filename).Funcs(defaultFuncMap)\n\n\t_, err := tmpl.Parse(tmplContent)\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\n\tvar buffer bytes.Buffer\n\terr = tmpl.Execute(&buffer, templateObjects)\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\n\tvar renderedTemplate = buffer.String()\n\tif p.DebugLogGeneratedTemplate {\n\t\tlog.Debugf(\"Template content: %s\", tmplContent)\n\t\tlog.Debugf(\"Rendering results: %s\", renderedTemplate)\n\t}\n\treturn p.DecodeConfiguration(renderedTemplate)\n}"}, {"instruction": "// monitorResizeEvents spawns a goroutine that periodically gets the terminal size and tries to send\n// it to the resizeEvents channel if the size has changed. The goroutine stops when the stop channel\n// is closed.", "input": "go language", "output": "func monitorResizeEvents(fd uintptr, resizeEvents chan<- remotecommand.TerminalSize, stop chan struct{}) {\n\tgo func() {\n\t\tdefer runtime.HandleCrash()\n\n\t\tsize := GetSize(fd)\n\t\tif size == nil {\n\t\t\treturn\n\t\t}\n\t\tlastSize := *size\n\n\t\tfor {\n\t\t\t// see if we need to stop running\n\t\t\tselect {\n\t\t\tcase <-stop:\n\t\t\t\treturn\n\t\t\tdefault:\n\t\t\t}\n\n\t\t\tsize := GetSize(fd)\n\t\t\tif size == nil {\n\t\t\t\treturn\n\t\t\t}\n\n\t\t\tif size.Height != lastSize.Height || size.Width != lastSize.Width {\n\t\t\t\tlastSize.Height = size.Height\n\t\t\t\tlastSize.Width = size.Width\n\t\t\t\tresizeEvents <- *size\n\t\t\t}\n\n\t\t\t// sleep to avoid hot looping\n\t\t\ttime.Sleep(250 * time.Millisecond)\n\t\t}\n\t}()\n}"}, {"instruction": "// Remove disassociates a metadata entry from a layer DiffID.", "input": "go language", "output": "func (serv *v2MetadataService) Remove(metadata V2Metadata) error {\n\tif serv.store == nil {\n\t\t// Support a service which has no backend storage, in this case\n\t\t// an remove becomes a no-op.\n\t\t// TODO: implement in memory storage\n\t\treturn nil\n\t}\n\tdiffID, err := serv.GetDiffID(metadata.Digest)\n\tif err != nil {\n\t\treturn err\n\t}\n\toldMetadata, err := serv.GetMetadata(diffID)\n\tif err != nil {\n\t\toldMetadata = nil\n\t}\n\tnewMetadata := make([]V2Metadata, 0, len(oldMetadata))\n\n\t// Copy all other metadata to new slice\n\tfor _, oldMeta := range oldMetadata {\n\t\tif oldMeta != metadata {\n\t\t\tnewMetadata = append(newMetadata, oldMeta)\n\t\t}\n\t}\n\n\tif len(newMetadata) == 0 {\n\t\treturn serv.store.Delete(serv.diffIDNamespace(), serv.diffIDKey(diffID))\n\t}\n\n\tjsonBytes, err := json.Marshal(newMetadata)\n\tif err != nil {\n\t\treturn err\n\t}\n\n\treturn serv.store.Set(serv.diffIDNamespace(), serv.diffIDKey(diffID), jsonBytes)\n}"}, {"instruction": "// DeepCopyInto is an autogenerated deepcopy function, copying the receiver, writing into out. in must be non-nil.", "input": "go language", "output": "func (in *HorizontalPodAutoscalerSpec) DeepCopyInto(out *HorizontalPodAutoscalerSpec) {\n\t*out = *in\n\tout.ScaleTargetRef = in.ScaleTargetRef\n\tif in.MinReplicas != nil {\n\t\tin, out := &in.MinReplicas, &out.MinReplicas\n\t\t*out = new(int32)\n\t\t**out = **in\n\t}\n\tif in.Metrics != nil {\n\t\tin, out := &in.Metrics, &out.Metrics\n\t\t*out = make([]MetricSpec, len(*in))\n\t\tfor i := range *in {\n\t\t\t(*in)[i].DeepCopyInto(&(*out)[i])\n\t\t}\n\t}\n\treturn\n}"}, {"instruction": "// Next implements the Executor Next interface.", "input": "go language", "output": "func (e *DeallocateExec) Next(ctx context.Context, req *chunk.RecordBatch) error {\n\tvars := e.ctx.GetSessionVars()\n\tid, ok := vars.PreparedStmtNameToID[e.Name]\n\tif !ok {\n\t\treturn errors.Trace(plannercore.ErrStmtNotFound)\n\t}\n\tdelete(vars.PreparedStmtNameToID, e.Name)\n\tif plannercore.PreparedPlanCacheEnabled() {\n\t\te.ctx.PreparedPlanCache().Delete(plannercore.NewPSTMTPlanCacheKey(\n\t\t\tvars, id, vars.PreparedStmts[id].SchemaVersion,\n\t\t))\n\t}\n\tvars.RemovePreparedStmt(id)\n\treturn nil\n}"}, {"instruction": "// getResourceHandler is an HTTP handler function for get requests. It delegates to the\n// passed-in getterFunc to perform the actual get.", "input": "go language", "output": "func getResourceHandler(scope *RequestScope, getter getterFunc) http.HandlerFunc {\n\treturn func(w http.ResponseWriter, req *http.Request) {\n\t\ttrace := utiltrace.New(\"Get \" + req.URL.Path)\n\t\tdefer trace.LogIfLong(500 * time.Millisecond)\n\n\t\tnamespace, name, err := scope.Namer.Name(req)\n\t\tif err != nil {\n\t\t\tscope.err(err, w, req)\n\t\t\treturn\n\t\t}\n\t\tctx := req.Context()\n\t\tctx = request.WithNamespace(ctx, namespace)\n\n\t\toutputMediaType, _, err := negotiation.NegotiateOutputMediaType(req, scope.Serializer, scope)\n\t\tif err != nil {\n\t\t\tscope.err(err, w, req)\n\t\t\treturn\n\t\t}\n\n\t\tresult, err := getter(ctx, name, req, trace)\n\t\tif err != nil {\n\t\t\tscope.err(err, w, req)\n\t\t\treturn\n\t\t}\n\n\t\ttrace.Step(\"About to write a response\")\n\t\ttransformResponseObject(ctx, scope, trace, req, w, http.StatusOK, outputMediaType, result)\n\t\ttrace.Step(\"Transformed response object\")\n\t}\n}"}, {"instruction": "// eventLoop runs a loop until the event mux closes. It will install and uninstall new\n// sync subscriptions and broadcasts sync status updates to the installed sync subscriptions.", "input": "go language", "output": "func (api *PublicDownloaderAPI) eventLoop() {\n\tvar (\n\t\tsub               = api.mux.Subscribe(StartEvent{}, DoneEvent{}, FailedEvent{})\n\t\tsyncSubscriptions = make(map[chan interface{}]struct{})\n\t)\n\n\tfor {\n\t\tselect {\n\t\tcase i := <-api.installSyncSubscription:\n\t\t\tsyncSubscriptions[i] = struct{}{}\n\t\tcase u := <-api.uninstallSyncSubscription:\n\t\t\tdelete(syncSubscriptions, u.c)\n\t\t\tclose(u.uninstalled)\n\t\tcase event := <-sub.Chan():\n\t\t\tif event == nil {\n\t\t\t\treturn\n\t\t\t}\n\n\t\t\tvar notification interface{}\n\t\t\tswitch event.Data.(type) {\n\t\t\tcase StartEvent:\n\t\t\t\tnotification = &SyncingResult{\n\t\t\t\t\tSyncing: true,\n\t\t\t\t\tStatus:  api.d.Progress(),\n\t\t\t\t}\n\t\t\tcase DoneEvent, FailedEvent:\n\t\t\t\tnotification = false\n\t\t\t}\n\t\t\t// broadcast\n\t\t\tfor c := range syncSubscriptions {\n\t\t\t\tc <- notification\n\t\t\t}\n\t\t}\n\t}\n}"}, {"instruction": "// ChannelCreationBlockToGenesisBlock converts a channel creation block to a genesis block", "input": "go language", "output": "func ChannelCreationBlockToGenesisBlock(block *common.Block) (*common.Block, error) {\n\tif block == nil {\n\t\treturn nil, errors.New(\"nil block\")\n\t}\n\tenv, err := utils.ExtractEnvelope(block, 0)\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\tpayload, err := utils.ExtractPayload(env)\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\tblock.Data.Data = [][]byte{payload.Data}\n\tblock.Header.DataHash = block.Data.Hash()\n\tblock.Header.Number = 0\n\tblock.Header.PreviousHash = nil\n\tmetadata := &common.BlockMetadata{\n\t\tMetadata: make([][]byte, 4),\n\t}\n\tblock.Metadata = metadata\n\tmetadata.Metadata[common.BlockMetadataIndex_LAST_CONFIG] = utils.MarshalOrPanic(&common.Metadata{\n\t\tValue: utils.MarshalOrPanic(&common.LastConfig{Index: 0}),\n\t\t// This is a genesis block, peer never verify this signature because we can't bootstrap\n\t\t// trust from an earlier block, hence there are no signatures here.\n\t})\n\treturn block, nil\n}"}, {"instruction": "// Checksum sends a checksum request.", "input": "go language", "output": "func Checksum(ctx context.Context, client kv.Client, kvReq *kv.Request, vars *kv.Variables) (SelectResult, error) {\n\tresp := client.Send(ctx, kvReq, vars)\n\tif resp == nil {\n\t\treturn nil, errors.New(\"client returns nil response\")\n\t}\n\tresult := &selectResult{\n\t\tlabel:    \"checksum\",\n\t\tresp:     resp,\n\t\tresults:  make(chan resultWithErr, kvReq.Concurrency),\n\t\tclosed:   make(chan struct{}),\n\t\tfeedback: statistics.NewQueryFeedback(0, nil, 0, false),\n\t\tsqlType:  metrics.LblGeneral,\n\t}\n\treturn result, nil\n}"}, {"instruction": "// Validate if the remote version is one Minor release newer than the client version.\n// This is done to conform with \"stable-X\" and only allow remote versions from\n// the same Patch level release.", "input": "go language", "output": "func validateStableVersion(remoteVersion, clientVersion string) (string, error) {\n\tverRemote, err := versionutil.ParseGeneric(remoteVersion)\n\tif err != nil {\n\t\treturn \"\", errors.Wrap(err, \"remote version error\")\n\t}\n\tverClient, err := versionutil.ParseGeneric(clientVersion)\n\tif err != nil {\n\t\treturn \"\", errors.Wrap(err, \"client version error\")\n\t}\n\t// If the remote Major version is bigger or if the Major versions are the same,\n\t// but the remote Minor is bigger use the client version release. This handles Major bumps too.\n\tif verClient.Major() < verRemote.Major() ||\n\t\t(verClient.Major() == verRemote.Major()) && verClient.Minor() < verRemote.Minor() {\n\t\testimatedRelease := fmt.Sprintf(\"stable-%d.%d\", verClient.Major(), verClient.Minor())\n\t\tklog.Infof(\"remote version is much newer: %s; falling back to: %s\", remoteVersion, estimatedRelease)\n\t\treturn estimatedRelease, nil\n\t}\n\treturn remoteVersion, nil\n}"}, {"instruction": "// NodeUnpublishVolume implements csi method", "input": "go language", "output": "func (f *NodeClient) NodeUnpublishVolume(ctx context.Context, req *csipb.NodeUnpublishVolumeRequest, opts ...grpc.CallOption) (*csipb.NodeUnpublishVolumeResponse, error) {\n\tif f.nextErr != nil {\n\t\treturn nil, f.nextErr\n\t}\n\n\tif req.GetVolumeId() == \"\" {\n\t\treturn nil, errors.New(\"missing volume id\")\n\t}\n\tif req.GetTargetPath() == \"\" {\n\t\treturn nil, errors.New(\"missing target path\")\n\t}\n\tdelete(f.nodePublishedVolumes, req.GetVolumeId())\n\treturn &csipb.NodeUnpublishVolumeResponse{}, nil\n}"}, {"instruction": "// MatchDomain return true if a domain match the cert domain", "input": "go language", "output": "func MatchDomain(domain string, certDomain string) bool {\n\tif domain == certDomain {\n\t\treturn true\n\t}\n\n\tfor len(certDomain) > 0 && certDomain[len(certDomain)-1] == '.' {\n\t\tcertDomain = certDomain[:len(certDomain)-1]\n\t}\n\n\tlabels := strings.Split(domain, \".\")\n\tfor i := range labels {\n\t\tlabels[i] = \"*\"\n\t\tcandidate := strings.Join(labels, \".\")\n\t\tif certDomain == candidate {\n\t\t\treturn true\n\t\t}\n\t}\n\treturn false\n}"}, {"instruction": "// sniffJSONStateTerraformVersion attempts to sniff the Terraform version\n// specification from the given state file source code. The result is either\n// a version string or an empty string if no version number could be extracted.\n//\n// This is a best-effort function intended to produce nicer error messages. It\n// should not be used for any real processing.", "input": "go language", "output": "func sniffJSONStateTerraformVersion(src []byte) string {\n\ttype VersionSniff struct {\n\t\tVersion string `json:\"terraform_version\"`\n\t}\n\tvar sniff VersionSniff\n\n\terr := json.Unmarshal(src, &sniff)\n\tif err != nil {\n\t\treturn \"\"\n\t}\n\n\t// Attempt to parse the string as a version so we won't report garbage\n\t// as a version number.\n\t_, err = version.NewVersion(sniff.Version)\n\tif err != nil {\n\t\treturn \"\"\n\t}\n\n\treturn sniff.Version\n}"}, {"instruction": "// IsComputed returns whether the given key is computed or not.", "input": "go language", "output": "func (c *ResourceConfig) IsComputed(k string) bool {\n\t// The next thing we do is check the config if we get a computed\n\t// value out of it.\n\tv, ok := c.get(k, c.Config)\n\tif !ok {\n\t\treturn false\n\t}\n\n\t// If value is nil, then it isn't computed\n\tif v == nil {\n\t\treturn false\n\t}\n\n\t// Test if the value contains an unknown value\n\tvar w unknownCheckWalker\n\tif err := reflectwalk.Walk(v, &w); err != nil {\n\t\tpanic(err)\n\t}\n\n\treturn w.Unknown\n}"}, {"instruction": "// NewChildImage creates a new Image as a child of this image.", "input": "go language", "output": "func NewChildImage(img *Image, child ChildConfig, os string) *Image {\n\tisEmptyLayer := layer.IsEmpty(child.DiffID)\n\tvar rootFS *RootFS\n\tif img.RootFS != nil {\n\t\trootFS = img.RootFS.Clone()\n\t} else {\n\t\trootFS = NewRootFS()\n\t}\n\n\tif !isEmptyLayer {\n\t\trootFS.Append(child.DiffID)\n\t}\n\timgHistory := NewHistory(\n\t\tchild.Author,\n\t\tchild.Comment,\n\t\tstrings.Join(child.ContainerConfig.Cmd, \" \"),\n\t\tisEmptyLayer)\n\n\treturn &Image{\n\t\tV1Image: V1Image{\n\t\t\tDockerVersion:   dockerversion.Version,\n\t\t\tConfig:          child.Config,\n\t\t\tArchitecture:    img.BaseImgArch(),\n\t\t\tOS:              os,\n\t\t\tContainer:       child.ContainerID,\n\t\t\tContainerConfig: *child.ContainerConfig,\n\t\t\tAuthor:          child.Author,\n\t\t\tCreated:         imgHistory.Created,\n\t\t},\n\t\tRootFS:     rootFS,\n\t\tHistory:    append(img.History, imgHistory),\n\t\tOSFeatures: img.OSFeatures,\n\t\tOSVersion:  img.OSVersion,\n\t}\n}"}, {"instruction": "// ValidateFinalizers tests if the finalizers name are valid, and if there are conflicting finalizers.", "input": "go language", "output": "func ValidateFinalizers(finalizers []string, fldPath *field.Path) field.ErrorList {\n\tallErrs := field.ErrorList{}\n\thasFinalizerOrphanDependents := false\n\thasFinalizerDeleteDependents := false\n\tfor _, finalizer := range finalizers {\n\t\tallErrs = append(allErrs, ValidateFinalizerName(finalizer, fldPath)...)\n\t\tif finalizer == metav1.FinalizerOrphanDependents {\n\t\t\thasFinalizerOrphanDependents = true\n\t\t}\n\t\tif finalizer == metav1.FinalizerDeleteDependents {\n\t\t\thasFinalizerDeleteDependents = true\n\t\t}\n\t}\n\tif hasFinalizerDeleteDependents && hasFinalizerOrphanDependents {\n\t\tallErrs = append(allErrs, field.Invalid(fldPath, finalizers, fmt.Sprintf(\"finalizer %s and %s cannot be both set\", metav1.FinalizerOrphanDependents, metav1.FinalizerDeleteDependents)))\n\t}\n\treturn allErrs\n}"}, {"instruction": "// parseInitVersion parses a Tini version string, and extracts the \"version\"\n// and \"git commit\" from the output.\n//\n// Output example from `docker-init --version`:\n//\n//     tini version 0.18.0 - git.fec3683", "input": "go language", "output": "func parseInitVersion(v string) (version string, commit string, err error) {\n\tparts := strings.Split(strings.TrimSpace(v), \" - \")\n\n\tif len(parts) >= 2 {\n\t\tgitParts := strings.Split(parts[1], \".\")\n\t\tif len(gitParts) == 2 && gitParts[0] == \"git\" {\n\t\t\tcommit = gitParts[1]\n\t\t}\n\t}\n\tif strings.HasPrefix(parts[0], \"tini version \") {\n\t\tversion = strings.TrimPrefix(parts[0], \"tini version \")\n\t}\n\tif version == \"\" && commit == \"\" {\n\t\terr = errors.Errorf(\"unknown output format: %s\", v)\n\t}\n\treturn version, commit, err\n}"}, {"instruction": "// Validate checks if a given ipset is valid or not.", "input": "go language", "output": "func (set *IPSet) Validate() bool {\n\t// Check if protocol is valid for `HashIPPort`, `HashIPPortIP` and `HashIPPortNet` type set.\n\tif set.SetType == HashIPPort || set.SetType == HashIPPortIP || set.SetType == HashIPPortNet {\n\t\tif valid := validateHashFamily(set.HashFamily); !valid {\n\t\t\treturn false\n\t\t}\n\t}\n\t// check set type\n\tif valid := validateIPSetType(set.SetType); !valid {\n\t\treturn false\n\t}\n\t// check port range for bitmap type set\n\tif set.SetType == BitmapPort {\n\t\tif valid := validatePortRange(set.PortRange); !valid {\n\t\t\treturn false\n\t\t}\n\t}\n\t// check hash size value of ipset\n\tif set.HashSize <= 0 {\n\t\tklog.Errorf(\"Invalid hashsize value %d, should be >0\", set.HashSize)\n\t\treturn false\n\t}\n\t// check max elem value of ipset\n\tif set.MaxElem <= 0 {\n\t\tklog.Errorf(\"Invalid maxelem value %d, should be >0\", set.MaxElem)\n\t\treturn false\n\t}\n\n\treturn true\n}"}, {"instruction": "// DeepCopyInto is an autogenerated deepcopy function, copying the receiver, writing into out. in must be non-nil.", "input": "go language", "output": "func (in *PolicyRule) DeepCopyInto(out *PolicyRule) {\n\t*out = *in\n\tif in.Verbs != nil {\n\t\tin, out := &in.Verbs, &out.Verbs\n\t\t*out = make([]string, len(*in))\n\t\tcopy(*out, *in)\n\t}\n\tif in.APIGroups != nil {\n\t\tin, out := &in.APIGroups, &out.APIGroups\n\t\t*out = make([]string, len(*in))\n\t\tcopy(*out, *in)\n\t}\n\tif in.Resources != nil {\n\t\tin, out := &in.Resources, &out.Resources\n\t\t*out = make([]string, len(*in))\n\t\tcopy(*out, *in)\n\t}\n\tif in.ResourceNames != nil {\n\t\tin, out := &in.ResourceNames, &out.ResourceNames\n\t\t*out = make([]string, len(*in))\n\t\tcopy(*out, *in)\n\t}\n\tif in.NonResourceURLs != nil {\n\t\tin, out := &in.NonResourceURLs, &out.NonResourceURLs\n\t\t*out = make([]string, len(*in))\n\t\tcopy(*out, *in)\n\t}\n\treturn\n}"}, {"instruction": "// Transformer returns a func that can be used in the transformer publishing chain.\n// TODO(bep) minify config etc", "input": "go language", "output": "func (m Client) Transformer(mediatype media.Type) transform.Transformer {\n\t_, params, min := m.m.Match(mediatype.Type())\n\tif min == nil {\n\t\t// No minifier for this MIME type\n\t\treturn nil\n\t}\n\n\treturn func(ft transform.FromTo) error {\n\t\t// Note that the source io.Reader will already be buffered, but it implements\n\t\t// the Bytes() method, which is recognized by the Minify library.\n\t\treturn min.Minify(m.m, ft.To(), ft.From(), params)\n\t}\n}"}, {"instruction": "// ParseSetValue creates a Set with special number.", "input": "go language", "output": "func ParseSetValue(elems []string, number uint64) (Set, error) {\n\tif number == 0 {\n\t\treturn zeroSet, nil\n\t}\n\n\tvalue := number\n\tvar items []string\n\tfor i := 0; i < len(elems); i++ {\n\t\tif number&setIndexValue[i] > 0 {\n\t\t\titems = append(items, elems[i])\n\t\t\tnumber &= setIndexInvertValue[i]\n\t\t}\n\t}\n\n\tif number != 0 {\n\t\treturn Set{}, errors.Errorf(\"invalid number %d for Set %v\", number, elems)\n\t}\n\n\treturn Set{Name: strings.Join(items, \",\"), Value: value}, nil\n}"}, {"instruction": "// TarResourceRebase is like TarResource but renames the first path element of\n// items in the resulting tar archive to match the given rebaseName if not \"\".", "input": "go language", "output": "func TarResourceRebase(sourcePath, rebaseName string) (content io.ReadCloser, err error) {\n\tsourcePath = normalizePath(sourcePath)\n\tif _, err = os.Lstat(sourcePath); err != nil {\n\t\t// Catches the case where the source does not exist or is not a\n\t\t// directory if asserted to be a directory, as this also causes an\n\t\t// error.\n\t\treturn\n\t}\n\n\t// Separate the source path between its directory and\n\t// the entry in that directory which we are archiving.\n\tsourceDir, sourceBase := SplitPathDirEntry(sourcePath)\n\topts := TarResourceRebaseOpts(sourceBase, rebaseName)\n\n\tlogrus.Debugf(\"copying %q from %q\", sourceBase, sourceDir)\n\treturn TarWithOptions(sourceDir, opts)\n}"}, {"instruction": "// isVolumeUsed returns list of pods that use given PV.", "input": "go language", "output": "func (ctrl *PersistentVolumeController) isVolumeUsed(pv *v1.PersistentVolume) ([]string, bool, error) {\n\tif pv.Spec.ClaimRef == nil {\n\t\treturn nil, false, nil\n\t}\n\tclaimName := pv.Spec.ClaimRef.Name\n\n\tpodNames := sets.NewString()\n\tpods, err := ctrl.podLister.Pods(pv.Spec.ClaimRef.Namespace).List(labels.Everything())\n\tif err != nil {\n\t\treturn nil, false, fmt.Errorf(\"error listing pods: %s\", err)\n\t}\n\tfor _, pod := range pods {\n\t\tif util.IsPodTerminated(pod, pod.Status) {\n\t\t\tcontinue\n\t\t}\n\t\tfor i := range pod.Spec.Volumes {\n\t\t\tusedPV := &pod.Spec.Volumes[i]\n\t\t\tif usedPV.PersistentVolumeClaim != nil && usedPV.PersistentVolumeClaim.ClaimName == claimName {\n\t\t\t\tpodNames.Insert(pod.Namespace + \"/\" + pod.Name)\n\t\t\t}\n\t\t}\n\t}\n\treturn podNames.List(), podNames.Len() != 0, nil\n}"}, {"instruction": "// Derive attempts to explicitly derive a hierarchical deterministic account at\n// the specified derivation path. If requested, the derived account will be added\n// to the wallet's tracked account list.", "input": "go language", "output": "func (w *Wallet) Derive(path accounts.DerivationPath, pin bool) (accounts.Account, error) {\n\tw.lock.Lock()\n\tdefer w.lock.Unlock()\n\n\taccount, err := w.session.derive(path)\n\tif err != nil {\n\t\treturn accounts.Account{}, err\n\t}\n\n\tif pin {\n\t\tpairing := w.Hub.pairing(w)\n\t\tpairing.Accounts[account.Address] = path\n\t\tif err := w.Hub.setPairing(w, pairing); err != nil {\n\t\t\treturn accounts.Account{}, err\n\t\t}\n\t}\n\n\treturn account, nil\n}"}, {"instruction": "// elgEnabledCollNames returns the names of the collections for which the peer is not eligible as per 'existingPkg' and is eligible as per 'postCommitPkg'", "input": "go language", "output": "func (n *collElgNotifier) elgEnabledCollNames(ledgerID string,\n\texistingPkg, postCommitPkg *common.CollectionConfigPackage) ([]string, error) {\n\n\tcollectionNames := []string{}\n\texisingConfs := retrieveCollConfs(existingPkg)\n\tpostCommitConfs := retrieveCollConfs(postCommitPkg)\n\texistingConfMap := map[string]*common.StaticCollectionConfig{}\n\tfor _, existingConf := range exisingConfs {\n\t\texistingConfMap[existingConf.Name] = existingConf\n\t}\n\n\tfor _, postCommitConf := range postCommitConfs {\n\t\tcollName := postCommitConf.Name\n\t\texistingConf, ok := existingConfMap[collName]\n\t\tif !ok { // brand new collection\n\t\t\tcontinue\n\t\t}\n\t\tmembershipEnabled, err := n.elgEnabled(ledgerID, existingConf.MemberOrgsPolicy, postCommitConf.MemberOrgsPolicy)\n\t\tif err != nil {\n\t\t\treturn nil, err\n\t\t}\n\t\tif !membershipEnabled {\n\t\t\tcontinue\n\t\t}\n\t\t// not an existing member and added now\n\t\tcollectionNames = append(collectionNames, collName)\n\t}\n\treturn collectionNames, nil\n}"}, {"instruction": "// itemFromIterator returns the Item from the current iterator position.\n// If the complete encoded key does not start with totalPrefix,\n// leveldb.ErrNotFound is returned. Value for totalPrefix must start with\n// Index prefix.", "input": "go language", "output": "func (f Index) itemFromIterator(it iterator.Iterator, totalPrefix []byte) (i Item, err error) {\n\tkey := it.Key()\n\tif !bytes.HasPrefix(key, totalPrefix) {\n\t\treturn i, leveldb.ErrNotFound\n\t}\n\t// create a copy of key byte slice not to share leveldb underlaying slice array\n\tkeyItem, err := f.decodeKeyFunc(append([]byte(nil), key...))\n\tif err != nil {\n\t\treturn i, err\n\t}\n\t// create a copy of value byte slice not to share leveldb underlaying slice array\n\tvalueItem, err := f.decodeValueFunc(keyItem, append([]byte(nil), it.Value()...))\n\tif err != nil {\n\t\treturn i, err\n\t}\n\treturn keyItem.Merge(valueItem), it.Error()\n}"}, {"instruction": "// GetCmpFunction get the compare function according to two arguments.", "input": "go language", "output": "func GetCmpFunction(lhs, rhs Expression) CompareFunc {\n\tswitch GetAccurateCmpType(lhs, rhs) {\n\tcase types.ETInt:\n\t\treturn CompareInt\n\tcase types.ETReal:\n\t\treturn CompareReal\n\tcase types.ETDecimal:\n\t\treturn CompareDecimal\n\tcase types.ETString:\n\t\treturn CompareString\n\tcase types.ETDuration:\n\t\treturn CompareDuration\n\tcase types.ETDatetime, types.ETTimestamp:\n\t\treturn CompareTime\n\tcase types.ETJson:\n\t\treturn CompareJSON\n\t}\n\treturn nil\n}"}, {"instruction": "// Validate checks ServerRunOptions and return a slice of found errs.", "input": "go language", "output": "func (s *ServerRunOptions) Validate() []error {\n\tvar errs []error\n\tif s.MasterCount <= 0 {\n\t\terrs = append(errs, fmt.Errorf(\"--apiserver-count should be a positive number, but value '%d' provided\", s.MasterCount))\n\t}\n\terrs = append(errs, s.Etcd.Validate()...)\n\terrs = append(errs, validateClusterIPFlags(s)...)\n\terrs = append(errs, validateServiceNodePort(s)...)\n\terrs = append(errs, s.SecureServing.Validate()...)\n\terrs = append(errs, s.Authentication.Validate()...)\n\terrs = append(errs, s.Authorization.Validate()...)\n\terrs = append(errs, s.Audit.Validate()...)\n\terrs = append(errs, s.Admission.Validate()...)\n\terrs = append(errs, s.InsecureServing.Validate()...)\n\terrs = append(errs, s.APIEnablement.Validate(legacyscheme.Scheme, apiextensionsapiserver.Scheme, aggregatorscheme.Scheme)...)\n\terrs = append(errs, validateTokenRequest(s)...)\n\n\treturn errs\n}"}, {"instruction": "//genChaincodeDeploymentSpec creates ChaincodeDeploymentSpec as the package to install", "input": "go language", "output": "func genChaincodeDeploymentSpec(cmd *cobra.Command, chaincodeName, chaincodeVersion string) (*pb.ChaincodeDeploymentSpec, error) {\n\tif existed, _ := ccprovider.ChaincodePackageExists(chaincodeName, chaincodeVersion); existed {\n\t\treturn nil, fmt.Errorf(\"chaincode %s:%s already exists\", chaincodeName, chaincodeVersion)\n\t}\n\n\tspec, err := getChaincodeSpec(cmd)\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\n\tcds, err := getChaincodeDeploymentSpec(spec, true)\n\tif err != nil {\n\t\treturn nil, fmt.Errorf(\"error getting chaincode code %s: %s\", chaincodeName, err)\n\t}\n\n\treturn cds, nil\n}"}, {"instruction": "// isForbidden determines if an import is forbidden,\n// which is true when the import is:\n//   - of a package under the rootPackage\n//   - is not of the base import path or a sub-package of it\n//   - is not of an allowed path or a sub-package of one", "input": "go language", "output": "func (i *ImportRestriction) isForbidden(imp string) bool {\n\timportsBelowRoot := strings.HasPrefix(imp, rootPackage)\n\timportsBelowBase := strings.HasPrefix(imp, i.BaseDir)\n\timportsAllowed := false\n\tfor _, allowed := range i.AllowedImports {\n\t\texactlyImportsAllowed := imp == allowed\n\t\timportsBelowAllowed := strings.HasPrefix(imp, fmt.Sprintf(\"%s/\", allowed))\n\t\timportsAllowed = importsAllowed || (importsBelowAllowed || exactlyImportsAllowed)\n\t}\n\n\treturn importsBelowRoot && !importsBelowBase && !importsAllowed\n}"}, {"instruction": "// setWS creates the WebSocket RPC listener interface string from the set\n// command line flags, returning empty if the HTTP endpoint is disabled.", "input": "go language", "output": "func setWS(ctx *cli.Context, cfg *node.Config) {\n\tif ctx.GlobalBool(WSEnabledFlag.Name) && cfg.WSHost == \"\" {\n\t\tcfg.WSHost = \"127.0.0.1\"\n\t\tif ctx.GlobalIsSet(WSListenAddrFlag.Name) {\n\t\t\tcfg.WSHost = ctx.GlobalString(WSListenAddrFlag.Name)\n\t\t}\n\t}\n\n\tif ctx.GlobalIsSet(WSPortFlag.Name) {\n\t\tcfg.WSPort = ctx.GlobalInt(WSPortFlag.Name)\n\t}\n\tif ctx.GlobalIsSet(WSAllowedOriginsFlag.Name) {\n\t\tcfg.WSOrigins = splitAndTrim(ctx.GlobalString(WSAllowedOriginsFlag.Name))\n\t}\n\tif ctx.GlobalIsSet(WSApiFlag.Name) {\n\t\tcfg.WSModules = splitAndTrim(ctx.GlobalString(WSApiFlag.Name))\n\t}\n}"}, {"instruction": "// dataForPseudoProfiling returns pseudo data for table profiling when system variable `profiling` is set to `ON`.", "input": "go language", "output": "func dataForPseudoProfiling() [][]types.Datum {\n\tvar rows [][]types.Datum\n\trow := types.MakeDatums(\n\t\t0,                      // QUERY_ID\n\t\t0,                      // SEQ\n\t\t\"\",                     // STATE\n\t\ttypes.NewDecFromInt(0), // DURATION\n\t\ttypes.NewDecFromInt(0), // CPU_USER\n\t\ttypes.NewDecFromInt(0), // CPU_SYSTEM\n\t\t0,                      // CONTEXT_VOLUNTARY\n\t\t0,                      // CONTEXT_INVOLUNTARY\n\t\t0,                      // BLOCK_OPS_IN\n\t\t0,                      // BLOCK_OPS_OUT\n\t\t0,                      // MESSAGES_SENT\n\t\t0,                      // MESSAGES_RECEIVED\n\t\t0,                      // PAGE_FAULTS_MAJOR\n\t\t0,                      // PAGE_FAULTS_MINOR\n\t\t0,                      // SWAPS\n\t\t\"\",                     // SOURCE_FUNCTION\n\t\t\"\",                     // SOURCE_FILE\n\t\t0,                      // SOURCE_LINE\n\t)\n\trows = append(rows, row)\n\treturn rows\n}"}, {"instruction": "// Handle filesystem notify event.\n// Files names:\n// - MUST NOT start with a '.'", "input": "go language", "output": "func (w *Watcher) handleCreateEvent(event fsnotify.Event) error {\n\tklog.V(6).Infof(\"Handling create event: %v\", event)\n\n\tif w.containsBlacklistedDir(event.Name) {\n\t\treturn nil\n\t}\n\n\tfi, err := os.Stat(event.Name)\n\tif err != nil {\n\t\treturn fmt.Errorf(\"stat file %s failed: %v\", event.Name, err)\n\t}\n\n\tif strings.HasPrefix(fi.Name(), \".\") {\n\t\tklog.V(5).Infof(\"Ignoring file (starts with '.'): %s\", fi.Name())\n\t\treturn nil\n\t}\n\n\tif !fi.IsDir() {\n\t\tif fi.Mode()&os.ModeSocket == 0 {\n\t\t\tklog.V(5).Infof(\"Ignoring non socket file %s\", fi.Name())\n\t\t\treturn nil\n\t\t}\n\n\t\treturn w.handlePluginRegistration(event.Name)\n\t}\n\n\treturn w.traversePluginDir(event.Name)\n}"}, {"instruction": "// Compare compares the nodes and pods of NodeLister with Cache.Snapshot.", "input": "go language", "output": "func (c *CacheComparer) Compare() error {\n\tklog.V(3).Info(\"cache comparer started\")\n\tdefer klog.V(3).Info(\"cache comparer finished\")\n\n\tnodes, err := c.NodeLister.List(labels.Everything())\n\tif err != nil {\n\t\treturn err\n\t}\n\n\tpods, err := c.PodLister.List(labels.Everything())\n\tif err != nil {\n\t\treturn err\n\t}\n\n\tsnapshot := c.Cache.Snapshot()\n\n\tpendingPods := c.PodQueue.PendingPods()\n\n\tif missed, redundant := c.CompareNodes(nodes, snapshot.Nodes); len(missed)+len(redundant) != 0 {\n\t\tklog.Warningf(\"cache mismatch: missed nodes: %s; redundant nodes: %s\", missed, redundant)\n\t}\n\n\tif missed, redundant := c.ComparePods(pods, pendingPods, snapshot.Nodes); len(missed)+len(redundant) != 0 {\n\t\tklog.Warningf(\"cache mismatch: missed pods: %s; redundant pods: %s\", missed, redundant)\n\t}\n\n\treturn nil\n}"}, {"instruction": "// Duration converts the given number to a time.Duration.\n// Unit is one of nanosecond/ns, microsecond/us/\u00b5s, millisecond/ms, second/s, minute/m or hour/h.", "input": "go language", "output": "func (ns *Namespace) Duration(unit interface{}, number interface{}) (_time.Duration, error) {\n\tunitStr, err := cast.ToStringE(unit)\n\tif err != nil {\n\t\treturn 0, err\n\t}\n\tunitDuration, found := durationUnits[unitStr]\n\tif !found {\n\t\treturn 0, fmt.Errorf(\"%q is not a valid duration unit\", unit)\n\t}\n\tn, err := cast.ToInt64E(number)\n\tif err != nil {\n\t\treturn 0, err\n\t}\n\treturn _time.Duration(n) * unitDuration, nil\n}"}, {"instruction": "// NewREST returns a RESTStorage object that will work against priority classes.", "input": "go language", "output": "func NewREST(optsGetter generic.RESTOptionsGetter) *REST {\n\tstore := &genericregistry.Store{\n\t\tNewFunc:                  func() runtime.Object { return &scheduling.PriorityClass{} },\n\t\tNewListFunc:              func() runtime.Object { return &scheduling.PriorityClassList{} },\n\t\tDefaultQualifiedResource: scheduling.Resource(\"priorityclasses\"),\n\n\t\tCreateStrategy: priorityclass.Strategy,\n\t\tUpdateStrategy: priorityclass.Strategy,\n\t\tDeleteStrategy: priorityclass.Strategy,\n\n\t\tTableConvertor: printerstorage.TableConvertor{TableGenerator: printers.NewTableGenerator().With(printersinternal.AddHandlers)},\n\t}\n\toptions := &generic.StoreOptions{RESTOptions: optsGetter}\n\tif err := store.CompleteWithOptions(options); err != nil {\n\t\tpanic(err) // TODO: Propagate error up\n\t}\n\n\treturn &REST{store}\n}"}, {"instruction": "// updateAllServiceIndexesOfNode updates the Raft index of all the services associated with this node", "input": "go language", "output": "func (s *Store) updateAllServiceIndexesOfNode(tx *memdb.Txn, idx uint64, nodeID string) error {\n\tservices, err := tx.Get(\"services\", \"node\", nodeID)\n\tif err != nil {\n\t\treturn fmt.Errorf(\"failed updating services for node %s: %s\", nodeID, err)\n\t}\n\tfor service := services.Next(); service != nil; service = services.Next() {\n\t\tsvc := service.(*structs.ServiceNode).ToNodeService()\n\t\tif err := tx.Insert(\"index\", &IndexEntry{serviceIndexName(svc.Service), idx}); err != nil {\n\t\t\treturn fmt.Errorf(\"failed updating index: %s\", err)\n\t\t}\n\t}\n\treturn nil\n}"}, {"instruction": "// makeListenerFromUserConfig returns the listener config decoded from an\n// arbitrary proto3 json format string or an error if it's invalid.\n//\n// For now we only support embedding in JSON strings because of the hcl parsing\n// pain (see config.go comment above call to patchSliceOfMaps). Until we\n// refactor config parser a _lot_ user's opaque config that contains arrays will\n// be mangled. We could actually fix that up in mapstructure which knows the\n// type of the target so could resolve the slices to singletons unambiguously\n// and it would work for us here... but we still have the problem that the\n// config would render incorrectly in general in our HTTP API responses so we\n// really need to fix it \"properly\".\n//\n// When we do that we can support just nesting the config directly into the\n// JSON/hcl naturally but this is a stop-gap that gets us an escape hatch\n// immediately. It's also probably not a bad thing to support long-term since\n// any config generated by other systems will likely be in canonical protobuf\n// from rather than our slight variant in JSON/hcl.", "input": "go language", "output": "func makeListenerFromUserConfig(configJSON string) (*envoy.Listener, error) {\n\t// Figure out if there is an @type field. We don't require is since we know\n\t// this will be a listener but unmarshalling into types.Any fails if it's not\n\t// there and unmarshalling into listener directly fails if it is...\n\tvar jsonFields map[string]*json.RawMessage\n\tif err := json.Unmarshal([]byte(configJSON), &jsonFields); err != nil {\n\t\treturn nil, err\n\t}\n\n\tvar l envoy.Listener\n\n\tif _, ok := jsonFields[\"@type\"]; ok {\n\t\t// Type field is present so decode it as a types.Any\n\t\tvar any types.Any\n\t\terr := jsonpb.UnmarshalString(configJSON, &any)\n\t\tif err != nil {\n\t\t\treturn nil, err\n\t\t}\n\t\t// And then unmarshal the listener again...\n\t\terr = proto.Unmarshal(any.Value, &l)\n\t\tif err != nil {\n\t\t\treturn nil, err\n\t\t}\n\t\treturn &l, err\n\t}\n\n\t// No @type so try decoding as a straight listener.\n\terr := jsonpb.UnmarshalString(configJSON, &l)\n\treturn &l, err\n}"}, {"instruction": "// init registers the windows graph drivers to the register.", "input": "go language", "output": "func init() {\n\tgraphdriver.Register(\"windowsfilter\", InitFilter)\n\t// DOCKER_WINDOWSFILTER_NOREEXEC allows for inline processing which makes\n\t// debugging issues in the re-exec codepath significantly easier.\n\tif os.Getenv(\"DOCKER_WINDOWSFILTER_NOREEXEC\") != \"\" {\n\t\tlogrus.Warnf(\"WindowsGraphDriver is set to not re-exec. This is intended for debugging purposes only.\")\n\t\tnoreexec = true\n\t} else {\n\t\treexec.Register(\"docker-windows-write-layer\", writeLayerReexec)\n\t}\n}"}, {"instruction": "// parseWhereArgs parses the end arguments to the where function.  Return a\n// match value and an operator, if one is defined.", "input": "go language", "output": "func parseWhereArgs(args ...interface{}) (mv reflect.Value, op string, err error) {\n\tswitch len(args) {\n\tcase 1:\n\t\tmv = reflect.ValueOf(args[0])\n\tcase 2:\n\t\tvar ok bool\n\t\tif op, ok = args[0].(string); !ok {\n\t\t\terr = errors.New(\"operator argument must be string type\")\n\t\t\treturn\n\t\t}\n\t\top = strings.TrimSpace(strings.ToLower(op))\n\t\tmv = reflect.ValueOf(args[1])\n\tdefault:\n\t\terr = errors.New(\"can't evaluate the array by no match argument or more than or equal to two arguments\")\n\t}\n\treturn\n}"}, {"instruction": "// Init takes two arguments, a string and int. These are stored in the key/value pair in the state", "input": "go language", "output": "func (t *SimpleChaincode) Init(stub shim.ChaincodeStubInterface) pb.Response {\n\tvar event string // Indicates whether event has happened. Initially 0\n\tvar eventVal int // State of event\n\tvar err error\n\t_, args := stub.GetFunctionAndParameters()\n\tif len(args) != 2 {\n\t\treturn shim.Error(\"Incorrect number of arguments. Expecting 2\")\n\t}\n\n\t// Initialize the chaincode\n\tevent = args[0]\n\teventVal, err = strconv.Atoi(args[1])\n\tif err != nil {\n\t\treturn shim.Error(\"Expecting integer value for event status\")\n\t}\n\tfmt.Printf(\"eventVal = %d\\n\", eventVal)\n\n\terr = stub.PutState(event, []byte(strconv.Itoa(eventVal)))\n\tif err != nil {\n\t\treturn shim.Error(err.Error())\n\t}\n\n\treturn shim.Success(nil)\n}"}, {"instruction": "// Evaluate uses the PolicyChecker to determine if a request should be allowed.\n// The decision is cached until the identity expires or the chain configuration\n// changes.", "input": "go language", "output": "func (ac *SessionAccessControl) Evaluate() error {\n\tif !ac.sessionEndTime.IsZero() && time.Now().After(ac.sessionEndTime) {\n\t\treturn errors.Errorf(\"client identity expired %v before\", time.Since(ac.sessionEndTime))\n\t}\n\n\tpolicyCheckNeeded := !ac.usedAtLeastOnce\n\n\tif currentConfigSequence := ac.sequencer.Sequence(); currentConfigSequence > ac.lastConfigSequence {\n\t\tac.lastConfigSequence = currentConfigSequence\n\t\tpolicyCheckNeeded = true\n\t}\n\n\tif !policyCheckNeeded {\n\t\treturn nil\n\t}\n\n\tac.usedAtLeastOnce = true\n\treturn ac.policyChecker.CheckPolicy(ac.envelope, ac.channelID)\n}"}, {"instruction": "// SelectPeers returns a slice of peers that match the routing filter", "input": "go language", "output": "func SelectPeers(k int, peerPool []discovery.NetworkMember, filter RoutingFilter) []*comm.RemotePeer {\n\tvar res []*comm.RemotePeer\n\trand.Seed(int64(util.RandomUInt64()))\n\t// Iterate over the possible candidates in random order\n\tfor _, index := range rand.Perm(len(peerPool)) {\n\t\t// If we collected K peers, we can stop the iteration.\n\t\tif len(res) == k {\n\t\t\tbreak\n\t\t}\n\t\tpeer := peerPool[index]\n\t\t// For each one, check if it is a worthy candidate to be selected\n\t\tif !filter(peer) {\n\t\t\tcontinue\n\t\t}\n\t\tp := &comm.RemotePeer{PKIID: peer.PKIid, Endpoint: peer.PreferredEndpoint()}\n\t\tres = append(res, p)\n\t}\n\treturn res\n}"}, {"instruction": "// ValidateTopologySelectorTerm tests that the specified topology selector term has valid data,\n// and constructs a map representing the term in raw form.", "input": "go language", "output": "func ValidateTopologySelectorTerm(term core.TopologySelectorTerm, fldPath *field.Path) (map[string]sets.String, field.ErrorList) {\n\tallErrs := field.ErrorList{}\n\texprMap := make(map[string]sets.String)\n\texprPath := fldPath.Child(\"matchLabelExpressions\")\n\n\t// Allow empty MatchLabelExpressions, in case this field becomes optional in the future.\n\tfor i, req := range term.MatchLabelExpressions {\n\t\tidxPath := exprPath.Index(i)\n\t\tvalueSet, exprErrs := validateTopologySelectorLabelRequirement(req, idxPath)\n\t\tallErrs = append(allErrs, exprErrs...)\n\n\t\t// Validate no duplicate keys exist.\n\t\tif _, exists := exprMap[req.Key]; exists {\n\t\t\tallErrs = append(allErrs, field.Duplicate(idxPath.Child(\"key\"), req.Key))\n\t\t}\n\t\texprMap[req.Key] = valueSet\n\t}\n\n\treturn exprMap, allErrs\n}"}, {"instruction": "// GetProposalHash1 gets the proposal hash bytes after sanitizing the\n// chaincode proposal payload according to the rules of visibility", "input": "go language", "output": "func GetProposalHash1(header *common.Header, ccPropPayl []byte, visibility []byte) ([]byte, error) {\n\t// check for nil argument\n\tif header == nil ||\n\t\theader.ChannelHeader == nil ||\n\t\theader.SignatureHeader == nil ||\n\t\tccPropPayl == nil {\n\t\treturn nil, errors.New(\"nil arguments\")\n\t}\n\n\t// unmarshal the chaincode proposal payload\n\tcpp, err := GetChaincodeProposalPayload(ccPropPayl)\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\n\tppBytes, err := GetBytesProposalPayloadForTx(cpp, visibility)\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\n\thash2, err := factory.GetDefault().GetHash(&bccsp.SHA256Opts{})\n\tif err != nil {\n\t\treturn nil, errors.WithMessage(err, \"error instantiating hash function\")\n\t}\n\t// hash the serialized Channel Header object\n\thash2.Write(header.ChannelHeader)\n\t// hash the serialized Signature Header object\n\thash2.Write(header.SignatureHeader)\n\t// hash of the part of the chaincode proposal payload that will go to the tx\n\thash2.Write(ppBytes)\n\treturn hash2.Sum(nil), nil\n}"}, {"instruction": "// Insert the given request into the cache and returns the token used for fetching it out.", "input": "go language", "output": "func (c *requestCache) Insert(req request) (token string, err error) {\n\tc.lock.Lock()\n\tdefer c.lock.Unlock()\n\n\t// Remove expired entries.\n\tc.gc()\n\t// If the cache is full, reject the request.\n\tif c.ll.Len() == maxInFlight {\n\t\treturn \"\", NewErrorTooManyInFlight()\n\t}\n\ttoken, err = c.uniqueToken()\n\tif err != nil {\n\t\treturn \"\", err\n\t}\n\tele := c.ll.PushFront(&cacheEntry{token, req, c.clock.Now().Add(cacheTTL)})\n\n\tc.tokens[token] = ele\n\treturn token, nil\n}"}, {"instruction": "// PreparedQueryList returns all the prepared queries.", "input": "go language", "output": "func (s *Store) PreparedQueryList(ws memdb.WatchSet) (uint64, structs.PreparedQueries, error) {\n\ttx := s.db.Txn(false)\n\tdefer tx.Abort()\n\n\t// Get the table index.\n\tidx := maxIndexTxn(tx, \"prepared-queries\")\n\n\t// Query all of the prepared queries in the state store.\n\tqueries, err := tx.Get(\"prepared-queries\", \"id\")\n\tif err != nil {\n\t\treturn 0, nil, fmt.Errorf(\"failed prepared query lookup: %s\", err)\n\t}\n\tws.Add(queries.WatchCh())\n\n\t// Go over all of the queries and build the response.\n\tvar result structs.PreparedQueries\n\tfor wrapped := queries.Next(); wrapped != nil; wrapped = queries.Next() {\n\t\tresult = append(result, toPreparedQuery(wrapped))\n\t}\n\treturn idx, result, nil\n}"}, {"instruction": "// WeakDecode behaves in the same way as mapstructure.WeakDecode but has a\n// DecodeHook which defeats the backward compatibility mode of mapstructure\n// which WeakDecodes []interface{}{} into an empty map[string]interface{}. This\n// allows us to use WeakDecode (desirable), but not fail on empty lists.", "input": "go language", "output": "func WeakDecode(m interface{}, rawVal interface{}) error {\n\tconfig := &mapstructure.DecoderConfig{\n\t\tDecodeHook: func(source reflect.Type, target reflect.Type, val interface{}) (interface{}, error) {\n\t\t\tsliceType := reflect.TypeOf(hilMapstructureDecodeHookEmptySlice)\n\t\t\tstringSliceType := reflect.TypeOf(hilMapstructureDecodeHookStringSlice)\n\t\t\tmapType := reflect.TypeOf(hilMapstructureDecodeHookEmptyMap)\n\n\t\t\tif (source == sliceType || source == stringSliceType) && target == mapType {\n\t\t\t\treturn nil, fmt.Errorf(\"Cannot convert a []interface{} into a map[string]interface{}\")\n\t\t\t}\n\n\t\t\treturn val, nil\n\t\t},\n\t\tWeaklyTypedInput: true,\n\t\tResult:           rawVal,\n\t}\n\n\tdecoder, err := mapstructure.NewDecoder(config)\n\tif err != nil {\n\t\treturn err\n\t}\n\n\treturn decoder.Decode(m)\n}"}, {"instruction": "// Will insert if needed any new key/value pars and return ids", "input": "go language", "output": "func (r *SqlAnnotationRepo) ensureTagsExist(sess *DBSession, tags []*models.Tag) ([]*models.Tag, error) {\n\tfor _, tag := range tags {\n\t\tvar existingTag models.Tag\n\n\t\t// check if it exists\n\t\tif exists, err := sess.Table(\"tag\").Where(dialect.Quote(\"key\")+\"=? AND \"+dialect.Quote(\"value\")+\"=?\", tag.Key, tag.Value).Get(&existingTag); err != nil {\n\t\t\treturn nil, err\n\t\t} else if exists {\n\t\t\ttag.Id = existingTag.Id\n\t\t} else {\n\t\t\tif _, err := sess.Table(\"tag\").Insert(tag); err != nil {\n\t\t\t\treturn nil, err\n\t\t\t}\n\t\t}\n\t}\n\n\treturn tags, nil\n}"}, {"instruction": "// Unlock network operations for a specific pod.  The reference count for the\n// pod will be decreased.  If the reference count reaches zero, the pod will be\n// removed from the pod map.", "input": "go language", "output": "func (pm *PluginManager) podUnlock(fullPodName string) {\n\tpm.podsLock.Lock()\n\tdefer pm.podsLock.Unlock()\n\n\tlock, ok := pm.pods[fullPodName]\n\tif !ok {\n\t\tklog.Warningf(\"Unbalanced pod lock unref for %s\", fullPodName)\n\t\treturn\n\t} else if lock.refcount == 0 {\n\t\t// This should never ever happen, but handle it anyway\n\t\tdelete(pm.pods, fullPodName)\n\t\tklog.Warningf(\"Pod lock for %s still in map with zero refcount\", fullPodName)\n\t\treturn\n\t}\n\tlock.refcount--\n\tlock.mu.Unlock()\n\tif lock.refcount == 0 {\n\t\tdelete(pm.pods, fullPodName)\n\t}\n}"}, {"instruction": "// Extract extracts compressed archives\n//\n// Implements Extractor.", "input": "go language", "output": "func (g *TarGzExtractor) Extract(buffer *bytes.Buffer, targetDir string) error {\n\tuncompressedStream, err := gzip.NewReader(buffer)\n\tif err != nil {\n\t\treturn err\n\t}\n\n\ttarReader := tar.NewReader(uncompressedStream)\n\n\tos.MkdirAll(targetDir, 0755)\n\n\tfor true {\n\t\theader, err := tarReader.Next()\n\n\t\tif err == io.EOF {\n\t\t\tbreak\n\t\t}\n\n\t\tif err != nil {\n\t\t\treturn err\n\t\t}\n\n\t\tpath, err := fp.SecureJoin(targetDir, header.Name)\n\t\tif err != nil {\n\t\t\treturn err\n\t\t}\n\n\t\tswitch header.Typeflag {\n\t\tcase tar.TypeDir:\n\t\t\tif err := os.Mkdir(path, 0755); err != nil {\n\t\t\t\treturn err\n\t\t\t}\n\t\tcase tar.TypeReg:\n\t\t\toutFile, err := os.Create(path)\n\t\t\tif err != nil {\n\t\t\t\treturn err\n\t\t\t}\n\t\t\tif _, err := io.Copy(outFile, tarReader); err != nil {\n\t\t\t\toutFile.Close()\n\t\t\t\treturn err\n\t\t\t}\n\t\t\toutFile.Close()\n\t\tdefault:\n\t\t\treturn fmt.Errorf(\"unknown type: %b in %s\", header.Typeflag, header.Name)\n\t\t}\n\t}\n\n\treturn nil\n\n}"}, {"instruction": "// DumpFeedbackToKV dumps the given feedback to physical kv layer.", "input": "go language", "output": "func (h *Handle) DumpFeedbackToKV(fb *statistics.QueryFeedback) error {\n\tvals, err := statistics.EncodeFeedback(fb)\n\tif err != nil {\n\t\tlogutil.Logger(context.Background()).Debug(\"error occurred when encoding feedback\", zap.Error(err))\n\t\treturn nil\n\t}\n\tvar isIndex int64\n\tif fb.Tp == statistics.IndexType {\n\t\tisIndex = 1\n\t}\n\tsql := fmt.Sprintf(\"insert into mysql.stats_feedback (table_id, hist_id, is_index, feedback) values \"+\n\t\t\"(%d, %d, %d, X'%X')\", fb.PhysicalID, fb.Hist.ID, isIndex, vals)\n\th.mu.Lock()\n\t_, err = h.mu.ctx.(sqlexec.SQLExecutor).Execute(context.TODO(), sql)\n\th.mu.Unlock()\n\tif err != nil {\n\t\tmetrics.DumpFeedbackCounter.WithLabelValues(metrics.LblError).Inc()\n\t} else {\n\t\tmetrics.DumpFeedbackCounter.WithLabelValues(metrics.LblOK).Inc()\n\t}\n\treturn errors.Trace(err)\n}"}, {"instruction": "// Provide implements DockerConfigProvider", "input": "go language", "output": "func (g *containerRegistryProvider) Provide(image string) credentialprovider.DockerConfig {\n\tcfg := credentialprovider.DockerConfig{}\n\n\ttokenJsonBlob, err := credentialprovider.ReadUrl(metadataToken, g.Client, metadataHeader)\n\tif err != nil {\n\t\tklog.Errorf(\"while reading access token endpoint: %v\", err)\n\t\treturn cfg\n\t}\n\n\temail, err := credentialprovider.ReadUrl(metadataEmail, g.Client, metadataHeader)\n\tif err != nil {\n\t\tklog.Errorf(\"while reading email endpoint: %v\", err)\n\t\treturn cfg\n\t}\n\n\tvar parsedBlob tokenBlob\n\tif err := json.Unmarshal([]byte(tokenJsonBlob), &parsedBlob); err != nil {\n\t\tklog.Errorf(\"while parsing json blob %s: %v\", tokenJsonBlob, err)\n\t\treturn cfg\n\t}\n\n\tentry := credentialprovider.DockerConfigEntry{\n\t\tUsername: \"_token\",\n\t\tPassword: parsedBlob.AccessToken,\n\t\tEmail:    string(email),\n\t}\n\n\t// Add our entry for each of the supported container registry URLs\n\tfor _, k := range containerRegistryUrls {\n\t\tcfg[k] = entry\n\t}\n\treturn cfg\n}"}, {"instruction": "// GetMemberID returns the member ID of the given peer URL", "input": "go language", "output": "func (c *Client) GetMemberID(peerURL string) (uint64, error) {\n\tcli, err := clientv3.New(clientv3.Config{\n\t\tEndpoints:   c.Endpoints,\n\t\tDialTimeout: 30 * time.Second,\n\t\tTLS:         c.TLS,\n\t})\n\tif err != nil {\n\t\treturn 0, err\n\t}\n\tdefer cli.Close()\n\n\tctx, cancel := context.WithTimeout(context.Background(), 10*time.Second)\n\tresp, err := cli.MemberList(ctx)\n\tcancel()\n\tif err != nil {\n\t\treturn 0, err\n\t}\n\n\tfor _, member := range resp.Members {\n\t\tif member.GetPeerURLs()[0] == peerURL {\n\t\t\treturn member.GetID(), nil\n\t\t}\n\t}\n\treturn 0, nil\n}"}, {"instruction": "// Gets the current load balancer state", "input": "go language", "output": "func (c *Cloud) describeLoadBalancer(name string) (*elb.LoadBalancerDescription, error) {\n\trequest := &elb.DescribeLoadBalancersInput{}\n\trequest.LoadBalancerNames = []*string{&name}\n\n\tresponse, err := c.elb.DescribeLoadBalancers(request)\n\tif err != nil {\n\t\tif awsError, ok := err.(awserr.Error); ok {\n\t\t\tif awsError.Code() == \"LoadBalancerNotFound\" {\n\t\t\t\treturn nil, nil\n\t\t\t}\n\t\t}\n\t\treturn nil, err\n\t}\n\n\tvar ret *elb.LoadBalancerDescription\n\tfor _, loadBalancer := range response.LoadBalancerDescriptions {\n\t\tif ret != nil {\n\t\t\tklog.Errorf(\"Found multiple load balancers with name: %s\", name)\n\t\t}\n\t\tret = loadBalancer\n\t}\n\treturn ret, nil\n}"}, {"instruction": "// SigningIdentityForRequest provides a mock function with given fields: _a0", "input": "go language", "output": "func (_m *SigningIdentityFetcher) SigningIdentityForRequest(_a0 *peer.SignedProposal) (endorsement.SigningIdentity, error) {\n\tret := _m.Called(_a0)\n\n\tvar r0 endorsement.SigningIdentity\n\tif rf, ok := ret.Get(0).(func(*peer.SignedProposal) endorsement.SigningIdentity); ok {\n\t\tr0 = rf(_a0)\n\t} else {\n\t\tif ret.Get(0) != nil {\n\t\t\tr0 = ret.Get(0).(endorsement.SigningIdentity)\n\t\t}\n\t}\n\n\tvar r1 error\n\tif rf, ok := ret.Get(1).(func(*peer.SignedProposal) error); ok {\n\t\tr1 = rf(_a0)\n\t} else {\n\t\tr1 = ret.Error(1)\n\t}\n\n\treturn r0, r1\n}"}, {"instruction": "// processed updates the client buffer according to actual request cost after\n// serving has been finished.\n//\n// Note: processed should always be called for all accepted requests", "input": "go language", "output": "func (cm *ClientManager) processed(node *ClientNode, maxCost, realCost uint64, now mclock.AbsTime) {\n\tcm.lock.Lock()\n\tdefer cm.lock.Unlock()\n\n\tif realCost > maxCost {\n\t\trealCost = maxCost\n\t}\n\tcm.updateNodeRc(node, int64(maxCost-realCost), &node.params, now)\n\tif uint64(node.corrBufValue) > node.bufValue {\n\t\tif node.log != nil {\n\t\t\tnode.log.add(now, fmt.Sprintf(\"corrected  bv=%d  oldBv=%d\", node.corrBufValue, node.bufValue))\n\t\t}\n\t\tnode.bufValue = uint64(node.corrBufValue)\n\t}\n}"}, {"instruction": "// verifyIntegrity is a debug method to iterate over the entire trie stored in\n// memory and check whether every node is reachable from the meta root. The goal\n// is to find any errors that might cause memory leaks and or trie nodes to go\n// missing.\n//\n// This method is extremely CPU and memory intensive, only use when must.", "input": "go language", "output": "func (db *Database) verifyIntegrity() {\n\t// Iterate over all the cached nodes and accumulate them into a set\n\treachable := map[common.Hash]struct{}{{}: {}}\n\n\tfor child := range db.dirties[common.Hash{}].children {\n\t\tdb.accumulate(child, reachable)\n\t}\n\t// Find any unreachable but cached nodes\n\tvar unreachable []string\n\tfor hash, node := range db.dirties {\n\t\tif _, ok := reachable[hash]; !ok {\n\t\t\tunreachable = append(unreachable, fmt.Sprintf(\"%x: {Node: %v, Parents: %d, Prev: %x, Next: %x}\",\n\t\t\t\thash, node.node, node.parents, node.flushPrev, node.flushNext))\n\t\t}\n\t}\n\tif len(unreachable) != 0 {\n\t\tpanic(fmt.Sprintf(\"trie cache memory leak: %v\", unreachable))\n\t}\n}"}, {"instruction": "// composeGlobalPrivUpdate composes update stmt assignment list string for global scope privilege update.", "input": "go language", "output": "func composeGlobalPrivUpdate(priv mysql.PrivilegeType, value string) (string, error) {\n\tif priv == mysql.AllPriv {\n\t\tstrs := make([]string, 0, len(mysql.Priv2UserCol))\n\t\tfor _, v := range mysql.Priv2UserCol {\n\t\t\tstrs = append(strs, fmt.Sprintf(`%s='%s'`, v, value))\n\t\t}\n\t\treturn strings.Join(strs, \", \"), nil\n\t}\n\tcol, ok := mysql.Priv2UserCol[priv]\n\tif !ok {\n\t\treturn \"\", errors.Errorf(\"Unknown priv: %v\", priv)\n\t}\n\treturn fmt.Sprintf(`%s='%s'`, col, value), nil\n}"}, {"instruction": "// IPRange returns a SchemaValidateFunc which tests if the provided value\n// is of type string, and in valid IP range notation", "input": "go language", "output": "func IPRange() schema.SchemaValidateFunc {\n\treturn func(i interface{}, k string) (s []string, es []error) {\n\t\tv, ok := i.(string)\n\t\tif !ok {\n\t\t\tes = append(es, fmt.Errorf(\"expected type of %s to be string\", k))\n\t\t\treturn\n\t\t}\n\n\t\tips := strings.Split(v, \"-\")\n\t\tif len(ips) != 2 {\n\t\t\tes = append(es, fmt.Errorf(\n\t\t\t\t\"expected %s to contain a valid IP range, got: %s\", k, v))\n\t\t\treturn\n\t\t}\n\t\tip1 := net.ParseIP(ips[0])\n\t\tip2 := net.ParseIP(ips[1])\n\t\tif ip1 == nil || ip2 == nil || bytes.Compare(ip1, ip2) > 0 {\n\t\t\tes = append(es, fmt.Errorf(\n\t\t\t\t\"expected %s to contain a valid IP range, got: %s\", k, v))\n\t\t}\n\t\treturn\n\t}\n}"}, {"instruction": "// DeepCopyInto is an autogenerated deepcopy function, copying the receiver, writing into out. in must be non-nil.", "input": "go language", "output": "func (in *HPAControllerConfiguration) DeepCopyInto(out *HPAControllerConfiguration) {\n\t*out = *in\n\tout.HorizontalPodAutoscalerSyncPeriod = in.HorizontalPodAutoscalerSyncPeriod\n\tout.HorizontalPodAutoscalerUpscaleForbiddenWindow = in.HorizontalPodAutoscalerUpscaleForbiddenWindow\n\tout.HorizontalPodAutoscalerDownscaleForbiddenWindow = in.HorizontalPodAutoscalerDownscaleForbiddenWindow\n\tout.HorizontalPodAutoscalerDownscaleStabilizationWindow = in.HorizontalPodAutoscalerDownscaleStabilizationWindow\n\tout.HorizontalPodAutoscalerCPUInitializationPeriod = in.HorizontalPodAutoscalerCPUInitializationPeriod\n\tout.HorizontalPodAutoscalerInitialReadinessDelay = in.HorizontalPodAutoscalerInitialReadinessDelay\n\treturn\n}"}, {"instruction": "// create creates log group and log stream for the instance of the awslogs logging driver", "input": "go language", "output": "func (l *logStream) create() error {\n\tif err := l.createLogStream(); err != nil {\n\t\tif l.logCreateGroup {\n\t\t\tif awsErr, ok := err.(awserr.Error); ok && awsErr.Code() == resourceNotFoundCode {\n\t\t\t\tif err := l.createLogGroup(); err != nil {\n\t\t\t\t\treturn errors.Wrap(err, \"failed to create Cloudwatch log group\")\n\t\t\t\t}\n\t\t\t\terr := l.createLogStream()\n\t\t\t\tif err != nil {\n\t\t\t\t\treturn errors.Wrap(err, \"failed to create Cloudwatch log stream\")\n\t\t\t\t}\n\t\t\t\treturn nil\n\t\t\t}\n\t\t}\n\t\tif err != nil {\n\t\t\treturn errors.Wrap(err, \"failed to create Cloudwatch log stream\")\n\t\t}\n\t}\n\n\treturn nil\n}"}, {"instruction": "// getRun handles requests to run a command inside a container.", "input": "go language", "output": "func (s *Server) getRun(request *restful.Request, response *restful.Response) {\n\tparams := getExecRequestParams(request)\n\tpod, ok := s.host.GetPodByName(params.podNamespace, params.podName)\n\tif !ok {\n\t\tresponse.WriteError(http.StatusNotFound, fmt.Errorf(\"pod does not exist\"))\n\t\treturn\n\t}\n\n\t// For legacy reasons, run uses different query param than exec.\n\tparams.cmd = strings.Split(request.QueryParameter(\"cmd\"), \" \")\n\tdata, err := s.host.RunInContainer(kubecontainer.GetPodFullName(pod), params.podUID, params.containerName, params.cmd)\n\tif err != nil {\n\t\tresponse.WriteError(http.StatusInternalServerError, err)\n\t\treturn\n\t}\n\twriteJSONResponse(response, data)\n}"}, {"instruction": "// AuthenticateRequest authenticates the request using a chain of authenticator.Request objects.", "input": "go language", "output": "func (authHandler *unionAuthRequestHandler) AuthenticateRequest(req *http.Request) (*authenticator.Response, bool, error) {\n\tvar errlist []error\n\tfor _, currAuthRequestHandler := range authHandler.Handlers {\n\t\tresp, ok, err := currAuthRequestHandler.AuthenticateRequest(req)\n\t\tif err != nil {\n\t\t\tif authHandler.FailOnError {\n\t\t\t\treturn resp, ok, err\n\t\t\t}\n\t\t\terrlist = append(errlist, err)\n\t\t\tcontinue\n\t\t}\n\n\t\tif ok {\n\t\t\treturn resp, ok, err\n\t\t}\n\t}\n\n\treturn nil, false, utilerrors.NewAggregate(errlist)\n}"}, {"instruction": "// RecoverPubkey returns the public key of the signer.\n// msg must be the 32-byte hash of the message to be signed.\n// sig must be a 65-byte compact ECDSA signature containing the\n// recovery id as the last element.", "input": "go language", "output": "func RecoverPubkey(msg []byte, sig []byte) ([]byte, error) {\n\tif len(msg) != 32 {\n\t\treturn nil, ErrInvalidMsgLen\n\t}\n\tif err := checkSignature(sig); err != nil {\n\t\treturn nil, err\n\t}\n\n\tvar (\n\t\tpubkey  = make([]byte, 65)\n\t\tsigdata = (*C.uchar)(unsafe.Pointer(&sig[0]))\n\t\tmsgdata = (*C.uchar)(unsafe.Pointer(&msg[0]))\n\t)\n\tif C.secp256k1_ext_ecdsa_recover(context, (*C.uchar)(unsafe.Pointer(&pubkey[0])), sigdata, msgdata) == 0 {\n\t\treturn nil, ErrRecoverFailed\n\t}\n\treturn pubkey, nil\n}"}, {"instruction": "// inferTypeFromDefault contains the logic for the old method of inferring\n// variable types - we can also use this for validating that the declared\n// type matches the type of the default value", "input": "go language", "output": "func (v *Variable) inferTypeFromDefault() VariableType {\n\tif v.Default == nil {\n\t\treturn VariableTypeString\n\t}\n\n\tvar s string\n\tif err := hilmapstructure.WeakDecode(v.Default, &s); err == nil {\n\t\tv.Default = s\n\t\treturn VariableTypeString\n\t}\n\n\tvar m map[string]interface{}\n\tif err := hilmapstructure.WeakDecode(v.Default, &m); err == nil {\n\t\tv.Default = m\n\t\treturn VariableTypeMap\n\t}\n\n\tvar l []interface{}\n\tif err := hilmapstructure.WeakDecode(v.Default, &l); err == nil {\n\t\tv.Default = l\n\t\treturn VariableTypeList\n\t}\n\n\treturn VariableTypeUnknown\n}"}, {"instruction": "// build a marshalGraph structure from a *Graph", "input": "go language", "output": "func newMarshalGraph(name string, g *Graph) *marshalGraph {\n\tmg := &marshalGraph{\n\t\tType:  \"Graph\",\n\t\tName:  name,\n\t\tAttrs: make(map[string]string),\n\t}\n\n\tfor _, v := range g.Vertices() {\n\t\tid := marshalVertexID(v)\n\t\tif sg, ok := marshalSubgrapher(v); ok {\n\t\t\tsmg := newMarshalGraph(VertexName(v), sg)\n\t\t\tsmg.ID = id\n\t\t\tmg.Subgraphs = append(mg.Subgraphs, smg)\n\t\t}\n\n\t\tmv := newMarshalVertex(v)\n\t\tmg.Vertices = append(mg.Vertices, mv)\n\t}\n\n\tsort.Sort(vertices(mg.Vertices))\n\n\tfor _, e := range g.Edges() {\n\t\tmg.Edges = append(mg.Edges, newMarshalEdge(e))\n\t}\n\n\tsort.Sort(edges(mg.Edges))\n\n\tfor _, c := range (&AcyclicGraph{*g}).Cycles() {\n\t\tvar cycle []*marshalVertex\n\t\tfor _, v := range c {\n\t\t\tmv := newMarshalVertex(v)\n\t\t\tcycle = append(cycle, mv)\n\t\t}\n\t\tmg.Cycles = append(mg.Cycles, cycle)\n\t}\n\n\treturn mg\n}"}, {"instruction": "// DeepCopyInto is an autogenerated deepcopy function, copying the receiver, writing into out. in must be non-nil.", "input": "go language", "output": "func (in *ISCSIVolumeSource) DeepCopyInto(out *ISCSIVolumeSource) {\n\t*out = *in\n\tif in.Portals != nil {\n\t\tin, out := &in.Portals, &out.Portals\n\t\t*out = make([]string, len(*in))\n\t\tcopy(*out, *in)\n\t}\n\tif in.SecretRef != nil {\n\t\tin, out := &in.SecretRef, &out.SecretRef\n\t\t*out = new(LocalObjectReference)\n\t\t**out = **in\n\t}\n\tif in.InitiatorName != nil {\n\t\tin, out := &in.InitiatorName, &out.InitiatorName\n\t\t*out = new(string)\n\t\t**out = **in\n\t}\n\treturn\n}"}, {"instruction": "// ValidateObjectMetaUpdate validates an object's metadata when updated", "input": "go language", "output": "func ValidateObjectMetaUpdate(newMeta, oldMeta *metav1.ObjectMeta, fldPath *field.Path) field.ErrorList {\n\tallErrs := apimachineryvalidation.ValidateObjectMetaUpdate(newMeta, oldMeta, fldPath)\n\t// run additional checks for the finalizer name\n\tfor i := range newMeta.Finalizers {\n\t\tallErrs = append(allErrs, validateKubeFinalizerName(string(newMeta.Finalizers[i]), fldPath.Child(\"finalizers\").Index(i))...)\n\t}\n\n\treturn allErrs\n}"}, {"instruction": "// parseRSAPrivateKey parses a single RSA private key from the provided data", "input": "go language", "output": "func parseRSAPrivateKey(data []byte) (*rsa.PrivateKey, error) {\n\tvar err error\n\n\t// Parse the key\n\tvar parsedKey interface{}\n\tif parsedKey, err = x509.ParsePKCS1PrivateKey(data); err != nil {\n\t\tif parsedKey, err = x509.ParsePKCS8PrivateKey(data); err != nil {\n\t\t\treturn nil, err\n\t\t}\n\t}\n\n\t// Test if parsed key is an RSA Private Key\n\tvar privKey *rsa.PrivateKey\n\tvar ok bool\n\tif privKey, ok = parsedKey.(*rsa.PrivateKey); !ok {\n\t\treturn nil, fmt.Errorf(\"data doesn't contain valid RSA Private Key\")\n\t}\n\n\treturn privKey, nil\n}"}, {"instruction": "// Validate checks to the AnnotateOptions to see if there is sufficient information run the command.", "input": "go language", "output": "func (o AnnotateOptions) Validate() error {\n\tif o.all && len(o.selector) > 0 {\n\t\treturn fmt.Errorf(\"cannot set --all and --selector at the same time\")\n\t}\n\tif o.all && len(o.fieldSelector) > 0 {\n\t\treturn fmt.Errorf(\"cannot set --all and --field-selector at the same time\")\n\t}\n\tif len(o.resources) < 1 && cmdutil.IsFilenameSliceEmpty(o.Filenames, o.Kustomize) {\n\t\treturn fmt.Errorf(\"one or more resources must be specified as <resource> <name> or <resource>/<name>\")\n\t}\n\tif len(o.newAnnotations) < 1 && len(o.removeAnnotations) < 1 {\n\t\treturn fmt.Errorf(\"at least one annotation update is required\")\n\t}\n\treturn validateAnnotations(o.removeAnnotations, o.newAnnotations)\n}"}, {"instruction": "// DefaultAttachFunc is the default AttachFunc used", "input": "go language", "output": "func DefaultAttachFunc(o *AttachOptions, containerToAttach *corev1.Container, raw bool, sizeQueue remotecommand.TerminalSizeQueue) func() error {\n\treturn func() error {\n\t\trestClient, err := restclient.RESTClientFor(o.Config)\n\t\tif err != nil {\n\t\t\treturn err\n\t\t}\n\t\treq := restClient.Post().\n\t\t\tResource(\"pods\").\n\t\t\tName(o.Pod.Name).\n\t\t\tNamespace(o.Pod.Namespace).\n\t\t\tSubResource(\"attach\")\n\t\treq.VersionedParams(&corev1.PodAttachOptions{\n\t\t\tContainer: containerToAttach.Name,\n\t\t\tStdin:     o.Stdin,\n\t\t\tStdout:    o.Out != nil,\n\t\t\tStderr:    !o.DisableStderr,\n\t\t\tTTY:       raw,\n\t\t}, scheme.ParameterCodec)\n\n\t\treturn o.Attach.Attach(\"POST\", req.URL(), o.Config, o.In, o.Out, o.ErrOut, raw, sizeQueue)\n\t}\n}"}, {"instruction": "// PruneCache removes all cached build sources", "input": "go language", "output": "func (b *Backend) PruneCache(ctx context.Context, opts types.BuildCachePruneOptions) (*types.BuildCachePruneReport, error) {\n\teg, ctx := errgroup.WithContext(ctx)\n\n\tvar fsCacheSize uint64\n\teg.Go(func() error {\n\t\tvar err error\n\t\tfsCacheSize, err = b.fsCache.Prune(ctx)\n\t\tif err != nil {\n\t\t\treturn errors.Wrap(err, \"failed to prune fscache\")\n\t\t}\n\t\treturn nil\n\t})\n\n\tvar buildCacheSize int64\n\tvar cacheIDs []string\n\teg.Go(func() error {\n\t\tvar err error\n\t\tbuildCacheSize, cacheIDs, err = b.buildkit.Prune(ctx, opts)\n\t\tif err != nil {\n\t\t\treturn errors.Wrap(err, \"failed to prune build cache\")\n\t\t}\n\t\treturn nil\n\t})\n\n\tif err := eg.Wait(); err != nil {\n\t\treturn nil, err\n\t}\n\n\treturn &types.BuildCachePruneReport{SpaceReclaimed: fsCacheSize + uint64(buildCacheSize), CachesDeleted: cacheIDs}, nil\n}"}, {"instruction": "// locatePartition returns the partition ID of the input record.", "input": "go language", "output": "func (t *partitionedTable) locatePartition(ctx sessionctx.Context, pi *model.PartitionInfo, r []types.Datum) (int64, error) {\n\tvar err error\n\tvar idx int\n\tswitch t.meta.Partition.Type {\n\tcase model.PartitionTypeRange:\n\t\tidx, err = t.locateRangePartition(ctx, pi, r)\n\tcase model.PartitionTypeHash:\n\t\tidx, err = t.locateHashPartition(ctx, pi, r)\n\t}\n\tif err != nil {\n\t\treturn 0, errors.Trace(err)\n\t}\n\treturn pi.Definitions[idx].ID, nil\n}"}, {"instruction": "// getGroupVersionKind returns the GroupVersionKind of the object", "input": "go language", "output": "func getGroupVersionKind(config map[string]interface{}) (schema.GroupVersionKind, error) {\n\tgvk := schema.GroupVersionKind{}\n\tif gv, found := config[\"apiVersion\"]; found {\n\t\tcasted, ok := gv.(string)\n\t\tif !ok {\n\t\t\treturn gvk, fmt.Errorf(\"Expected string for apiVersion, found %T\", gv)\n\t\t}\n\t\ts := strings.Split(casted, \"/\")\n\t\tif len(s) != 1 {\n\t\t\tgvk.Group = s[0]\n\t\t}\n\t\tgvk.Version = s[len(s)-1]\n\t} else {\n\t\treturn gvk, fmt.Errorf(\"Missing apiVersion in Kind %v\", config)\n\t}\n\tif k, found := config[\"kind\"]; found {\n\t\tcasted, ok := k.(string)\n\t\tif !ok {\n\t\t\treturn gvk, fmt.Errorf(\"Expected string for kind, found %T\", k)\n\t\t}\n\t\tgvk.Kind = casted\n\t} else {\n\t\treturn gvk, fmt.Errorf(\"Missing kind in Kind %v\", config)\n\t}\n\treturn gvk, nil\n}"}, {"instruction": "// ReadDataSource mocks base method", "input": "go language", "output": "func (m *MockProviderClient) ReadDataSource(arg0 context.Context, arg1 *tfplugin5.ReadDataSource_Request, arg2 ...grpc.CallOption) (*tfplugin5.ReadDataSource_Response, error) {\n\tm.ctrl.T.Helper()\n\tvarargs := []interface{}{arg0, arg1}\n\tfor _, a := range arg2 {\n\t\tvarargs = append(varargs, a)\n\t}\n\tret := m.ctrl.Call(m, \"ReadDataSource\", varargs...)\n\tret0, _ := ret[0].(*tfplugin5.ReadDataSource_Response)\n\tret1, _ := ret[1].(error)\n\treturn ret0, ret1\n}"}, {"instruction": "// CryptBlocks implements BlockMode.CryptBlocks interface.", "input": "go language", "output": "func (x *ecbDecrypter) CryptBlocks(dst, src []byte) {\n\tif len(src)%x.blockSize != 0 {\n\t\tpanic(\"ECBDecrypter: input not full blocks\")\n\t}\n\tif len(dst) < len(src) {\n\t\tpanic(\"ECBDecrypter: output smaller than input\")\n\t}\n\t// See https://en.wikipedia.org/wiki/Block_cipher_mode_of_operation#Electronic_Codebook_.28ECB.29\n\tfor len(src) > 0 {\n\t\tx.b.Decrypt(dst, src[:x.blockSize])\n\t\tsrc = src[x.blockSize:]\n\t\tdst = dst[x.blockSize:]\n\t}\n}"}, {"instruction": "// SameLineage returns true only if the state given in argument belongs\n// to the same \"lineage\" of states as the receiver.", "input": "go language", "output": "func (s *State) SameLineage(other *State) bool {\n\ts.Lock()\n\tdefer s.Unlock()\n\n\t// If one of the states has no lineage then it is assumed to predate\n\t// this concept, and so we'll accept it as belonging to any lineage\n\t// so that a lineage string can be assigned to newer versions\n\t// without breaking compatibility with older versions.\n\tif s.Lineage == \"\" || other.Lineage == \"\" {\n\t\treturn true\n\t}\n\n\treturn s.Lineage == other.Lineage\n}"}, {"instruction": "// NewContainerManager creates windows container manager.", "input": "go language", "output": "func NewContainerManager(mountUtil mount.Interface, cadvisorInterface cadvisor.Interface, nodeConfig NodeConfig, failSwapOn bool, devicePluginEnabled bool, recorder record.EventRecorder) (ContainerManager, error) {\n\tvar capacity = v1.ResourceList{}\n\t// It is safe to invoke `MachineInfo` on cAdvisor before logically initializing cAdvisor here because\n\t// machine info is computed and cached once as part of cAdvisor object creation.\n\t// But `RootFsInfo` and `ImagesFsInfo` are not available at this moment so they will be called later during manager starts\n\tmachineInfo, err := cadvisorInterface.MachineInfo()\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\tcapacity = cadvisor.CapacityFromMachineInfo(machineInfo)\n\n\treturn &containerManagerImpl{\n\t\tcapacity:          capacity,\n\t\tnodeConfig:        nodeConfig,\n\t\tcadvisorInterface: cadvisorInterface,\n\t}, nil\n}"}, {"instruction": "// AuthenticateToken authenticates the token using a chain of authenticator.Token objects.", "input": "go language", "output": "func (authHandler *unionAuthTokenHandler) AuthenticateToken(ctx context.Context, token string) (*authenticator.Response, bool, error) {\n\tvar errlist []error\n\tfor _, currAuthRequestHandler := range authHandler.Handlers {\n\t\tinfo, ok, err := currAuthRequestHandler.AuthenticateToken(ctx, token)\n\t\tif err != nil {\n\t\t\tif authHandler.FailOnError {\n\t\t\t\treturn info, ok, err\n\t\t\t}\n\t\t\terrlist = append(errlist, err)\n\t\t\tcontinue\n\t\t}\n\n\t\tif ok {\n\t\t\treturn info, ok, err\n\t\t}\n\t}\n\n\treturn nil, false, utilerrors.NewAggregate(errlist)\n}"}, {"instruction": "// IsFullyQualifiedName checks if the name is fully qualified.", "input": "go language", "output": "func IsFullyQualifiedName(fldPath *field.Path, name string) field.ErrorList {\n\tvar allErrors field.ErrorList\n\tif len(name) == 0 {\n\t\treturn append(allErrors, field.Required(fldPath, \"\"))\n\t}\n\tif errs := IsDNS1123Subdomain(name); len(errs) > 0 {\n\t\treturn append(allErrors, field.Invalid(fldPath, name, strings.Join(errs, \",\")))\n\t}\n\tif len(strings.Split(name, \".\")) < 3 {\n\t\treturn append(allErrors, field.Invalid(fldPath, name, \"should be a domain with at least three segments separated by dots\"))\n\t}\n\treturn allErrors\n}"}, {"instruction": "// NewLeaderElectionService returns a new LeaderElectionService", "input": "go language", "output": "func NewLeaderElectionService(adapter LeaderElectionAdapter, id string, callback leadershipCallback, config ElectionConfig) LeaderElectionService {\n\tif len(id) == 0 {\n\t\tpanic(\"Empty id\")\n\t}\n\tle := &leaderElectionSvcImpl{\n\t\tid:            peerID(id),\n\t\tproposals:     util.NewSet(),\n\t\tadapter:       adapter,\n\t\tstopChan:      make(chan struct{}, 1),\n\t\tinterruptChan: make(chan struct{}, 1),\n\t\tlogger:        util.GetLogger(util.ElectionLogger, \"\"),\n\t\tcallback:      noopCallback,\n\t\tconfig:        config,\n\t}\n\n\tif callback != nil {\n\t\tle.callback = callback\n\t}\n\n\tgo le.start()\n\treturn le\n}"}, {"instruction": "// EnsurePortProxyRule checks if the specified redirect exists, if not creates it.", "input": "go language", "output": "func (runner *runner) EnsurePortProxyRule(args []string) (bool, error) {\n\tklog.V(4).Infof(\"running netsh interface portproxy add v4tov4 %v\", args)\n\tout, err := runner.exec.Command(cmdNetsh, args...).CombinedOutput()\n\n\tif err == nil {\n\t\treturn true, nil\n\t}\n\tif ee, ok := err.(utilexec.ExitError); ok {\n\t\t// netsh uses exit(0) to indicate a success of the operation,\n\t\t// as compared to a malformed commandline, for example.\n\t\tif ee.Exited() && ee.ExitStatus() != 0 {\n\t\t\treturn false, nil\n\t\t}\n\t}\n\treturn false, fmt.Errorf(\"error checking portproxy rule: %v: %s\", err, out)\n\n}"}, {"instruction": "// Equal normalizes two URLs and then compares for equality.", "input": "go language", "output": "func Equal(a, b string) bool {\n\tau, err := url.Parse(a)\n\tif err != nil {\n\t\ta = filepath.Clean(a)\n\t\tb = filepath.Clean(b)\n\t\t// If urls are paths, return true only if they are an exact match\n\t\treturn a == b\n\t}\n\tbu, err := url.Parse(b)\n\tif err != nil {\n\t\treturn false\n\t}\n\n\tfor _, u := range []*url.URL{au, bu} {\n\t\tif u.Path == \"\" {\n\t\t\tu.Path = \"/\"\n\t\t}\n\t\tu.Path = filepath.Clean(u.Path)\n\t}\n\treturn au.String() == bu.String()\n}"}, {"instruction": "// Unmount runs umount(8) in the host's mount namespace.", "input": "go language", "output": "func (n *Mounter) Unmount(target string) error {\n\targs := []string{target}\n\t// No need to execute systemd-run here, it's enough that unmount is executed\n\t// in the host's mount namespace. It will finish appropriate fuse daemon(s)\n\t// running in any scope.\n\tklog.V(5).Infof(\"nsenter unmount args: %v\", args)\n\toutputBytes, err := n.ne.Exec(\"umount\", args).CombinedOutput()\n\tif len(outputBytes) != 0 {\n\t\tklog.V(5).Infof(\"Output of unmounting %s: %v\", target, string(outputBytes))\n\t}\n\treturn err\n}"}, {"instruction": "// Validate checks to the LabelOptions to see if there is sufficient information run the command.", "input": "go language", "output": "func (o *LabelOptions) Validate() error {\n\tif o.all && len(o.selector) > 0 {\n\t\treturn fmt.Errorf(\"cannot set --all and --selector at the same time\")\n\t}\n\tif o.all && len(o.fieldSelector) > 0 {\n\t\treturn fmt.Errorf(\"cannot set --all and --field-selector at the same time\")\n\t}\n\tif len(o.resources) < 1 && cmdutil.IsFilenameSliceEmpty(o.FilenameOptions.Filenames, o.FilenameOptions.Kustomize) {\n\t\treturn fmt.Errorf(\"one or more resources must be specified as <resource> <name> or <resource>/<name>\")\n\t}\n\tif len(o.newLabels) < 1 && len(o.removeLabels) < 1 && !o.list {\n\t\treturn fmt.Errorf(\"at least one label update is required\")\n\t}\n\treturn nil\n}"}, {"instruction": "// Update updates internal state of what has been downloaded into the temporary\n// files by the remote device for this specific folder.", "input": "go language", "output": "func (p *deviceFolderDownloadState) Update(updates []protocol.FileDownloadProgressUpdate) {\n\tp.mut.Lock()\n\tdefer p.mut.Unlock()\n\n\tfor _, update := range updates {\n\t\tlocal, ok := p.files[update.Name]\n\t\tif update.UpdateType == protocol.UpdateTypeForget && ok && local.version.Equal(update.Version) {\n\t\t\tdelete(p.files, update.Name)\n\t\t} else if update.UpdateType == protocol.UpdateTypeAppend {\n\t\t\tif !ok {\n\t\t\t\tlocal = deviceFolderFileDownloadState{\n\t\t\t\t\tblockIndexes: update.BlockIndexes,\n\t\t\t\t\tversion:      update.Version,\n\t\t\t\t}\n\t\t\t} else if !local.version.Equal(update.Version) {\n\t\t\t\tlocal.blockIndexes = append(local.blockIndexes[:0], update.BlockIndexes...)\n\t\t\t\tlocal.version = update.Version\n\t\t\t} else {\n\t\t\t\tlocal.blockIndexes = append(local.blockIndexes, update.BlockIndexes...)\n\t\t\t}\n\t\t\tp.files[update.Name] = local\n\t\t}\n\t}\n}"}, {"instruction": "// execute fetches Chunks from src and update each aggregate function for each row in Chunk.", "input": "go language", "output": "func (e *HashAggExec) execute(ctx context.Context) (err error) {\n\tinputIter := chunk.NewIterator4Chunk(e.childResult)\n\tfor {\n\t\terr := e.children[0].Next(ctx, chunk.NewRecordBatch(e.childResult))\n\t\tif err != nil {\n\t\t\treturn err\n\t\t}\n\n\t\tfailpoint.Inject(\"unparallelHashAggError\", func(val failpoint.Value) {\n\t\t\tif val.(bool) {\n\t\t\t\tfailpoint.Return(errors.New(\"HashAggExec.unparallelExec error\"))\n\t\t\t}\n\t\t})\n\n\t\t// no more data.\n\t\tif e.childResult.NumRows() == 0 {\n\t\t\treturn nil\n\t\t}\n\t\tfor row := inputIter.Begin(); row != inputIter.End(); row = inputIter.Next() {\n\t\t\tgroupKey, err := e.getGroupKey(row)\n\t\t\tif err != nil {\n\t\t\t\treturn err\n\t\t\t}\n\t\t\tif !e.groupSet.Exist(groupKey) {\n\t\t\t\te.groupSet.Insert(groupKey)\n\t\t\t\te.groupKeys = append(e.groupKeys, groupKey)\n\t\t\t}\n\t\t\tpartialResults := e.getPartialResults(groupKey)\n\t\t\tfor i, af := range e.PartialAggFuncs {\n\t\t\t\terr = af.UpdatePartialResult(e.ctx, []chunk.Row{row}, partialResults[i])\n\t\t\t\tif err != nil {\n\t\t\t\t\treturn err\n\t\t\t\t}\n\t\t\t}\n\t\t}\n\t}\n}"}, {"instruction": "// New creates a new table convertor for the provided CRD column definition. If the printer definition cannot be parsed,\n// error will be returned along with a default table convertor.", "input": "go language", "output": "func New(crdColumns []apiextensions.CustomResourceColumnDefinition) (rest.TableConvertor, error) {\n\theaders := []metav1beta1.TableColumnDefinition{\n\t\t{Name: \"Name\", Type: \"string\", Format: \"name\", Description: swaggerMetadataDescriptions[\"name\"]},\n\t}\n\tc := &convertor{\n\t\theaders: headers,\n\t}\n\n\tfor _, col := range crdColumns {\n\t\tpath := jsonpath.New(col.Name)\n\t\tif err := path.Parse(fmt.Sprintf(\"{%s}\", col.JSONPath)); err != nil {\n\t\t\treturn c, fmt.Errorf(\"unrecognized column definition %q\", col.JSONPath)\n\t\t}\n\t\tpath.AllowMissingKeys(true)\n\n\t\tdesc := fmt.Sprintf(\"Custom resource definition column (in JSONPath format): %s\", col.JSONPath)\n\t\tif len(col.Description) > 0 {\n\t\t\tdesc = col.Description\n\t\t}\n\n\t\tc.additionalColumns = append(c.additionalColumns, path)\n\t\tc.headers = append(c.headers, metav1beta1.TableColumnDefinition{\n\t\t\tName:        col.Name,\n\t\t\tType:        col.Type,\n\t\t\tFormat:      col.Format,\n\t\t\tDescription: desc,\n\t\t\tPriority:    col.Priority,\n\t\t})\n\t}\n\n\treturn c, nil\n}"}, {"instruction": "// Endorsers provides a mock function with given fields: invocationChain, f", "input": "go language", "output": "func (_m *ChannelResponse) Endorsers(invocationChain client.InvocationChain, f client.Filter) (client.Endorsers, error) {\n\tret := _m.Called(invocationChain, f)\n\n\tvar r0 client.Endorsers\n\tif rf, ok := ret.Get(0).(func(client.InvocationChain, client.Filter) client.Endorsers); ok {\n\t\tr0 = rf(invocationChain, f)\n\t} else {\n\t\tif ret.Get(0) != nil {\n\t\t\tr0 = ret.Get(0).(client.Endorsers)\n\t\t}\n\t}\n\n\tvar r1 error\n\tif rf, ok := ret.Get(1).(func(client.InvocationChain, client.Filter) error); ok {\n\t\tr1 = rf(invocationChain, f)\n\t} else {\n\t\tr1 = ret.Error(1)\n\t}\n\n\treturn r0, r1\n}"}, {"instruction": "// Need to hold lock on m.fmut when calling this.", "input": "go language", "output": "func (m *model) tearDownFolderLocked(cfg config.FolderConfiguration, err error) {\n\t// Stop the services running for this folder and wait for them to finish\n\t// stopping to prevent races on restart.\n\ttokens := m.folderRunnerTokens[cfg.ID]\n\n\tm.fmut.Unlock()\n\n\t// Close connections to affected devices\n\t// Must happen before stopping the folder service to abort ongoing\n\t// transmissions and thus allow timely service termination.\n\tm.closeConns(cfg.DeviceIDs(), err)\n\n\tfor _, id := range tokens {\n\t\tm.RemoveAndWait(id, 0)\n\t}\n\n\tm.fmut.Lock()\n\n\t// Clean up our config maps\n\tdelete(m.folderCfgs, cfg.ID)\n\tdelete(m.folderFiles, cfg.ID)\n\tdelete(m.folderIgnores, cfg.ID)\n\tdelete(m.folderRunners, cfg.ID)\n\tdelete(m.folderRunnerTokens, cfg.ID)\n}"}, {"instruction": "// Recv blocks until a response is received from the stream or the\n// timeout expires.", "input": "go language", "output": "func (stream *ImpatientStream) Recv() (*orderer.DeliverResponse, error) {\n\t// Initialize a timeout to cancel the stream when it expires\n\ttimeout := time.NewTimer(stream.waitTimeout)\n\tdefer timeout.Stop()\n\n\tresponseChan := make(chan errorAndResponse, 1)\n\n\t// receive waitGroup ensures the goroutine below exits before\n\t// this function exits.\n\tvar receive sync.WaitGroup\n\treceive.Add(1)\n\tdefer receive.Wait()\n\n\tgo func() {\n\t\tdefer receive.Done()\n\t\tresp, err := stream.AtomicBroadcast_DeliverClient.Recv()\n\t\tresponseChan <- errorAndResponse{err: err, resp: resp}\n\t}()\n\n\tselect {\n\tcase <-timeout.C:\n\t\tstream.cancelFunc()\n\t\treturn nil, errors.Errorf(\"didn't receive a response within %v\", stream.waitTimeout)\n\tcase respAndErr := <-responseChan:\n\t\treturn respAndErr.resp, respAndErr.err\n\t}\n}"}, {"instruction": "// DeepCopyInto is an autogenerated deepcopy function, copying the receiver, writing into out. in must be non-nil.", "input": "go language", "output": "func (in *LocalEtcd) DeepCopyInto(out *LocalEtcd) {\n\t*out = *in\n\tout.ImageMeta = in.ImageMeta\n\tif in.ExtraArgs != nil {\n\t\tin, out := &in.ExtraArgs, &out.ExtraArgs\n\t\t*out = make(map[string]string, len(*in))\n\t\tfor key, val := range *in {\n\t\t\t(*out)[key] = val\n\t\t}\n\t}\n\tif in.ServerCertSANs != nil {\n\t\tin, out := &in.ServerCertSANs, &out.ServerCertSANs\n\t\t*out = make([]string, len(*in))\n\t\tcopy(*out, *in)\n\t}\n\tif in.PeerCertSANs != nil {\n\t\tin, out := &in.PeerCertSANs, &out.PeerCertSANs\n\t\t*out = make([]string, len(*in))\n\t\tcopy(*out, *in)\n\t}\n\treturn\n}"}, {"instruction": "// pushRepository pushes layers that do not already exist on the registry.", "input": "go language", "output": "func (p *v1Pusher) pushRepository(ctx context.Context) error {\n\timgList, tags, referencedLayers, err := p.getImageList()\n\tdefer func() {\n\t\tfor _, l := range referencedLayers {\n\t\t\tl.Release()\n\t\t}\n\t}()\n\tif err != nil {\n\t\treturn err\n\t}\n\n\timageIndex := createImageIndex(imgList, tags)\n\tfor _, data := range imageIndex {\n\t\tlogrus.Debugf(\"Pushing ID: %s with Tag: %s\", data.ID, data.Tag)\n\t}\n\n\t// Register all the images in a repository with the registry\n\t// If an image is not in this list it will not be associated with the repository\n\trepoData, err := p.session.PushImageJSONIndex(p.repoInfo.Name, imageIndex, false, nil)\n\tif err != nil {\n\t\treturn err\n\t}\n\t// push the repository to each of the endpoints only if it does not exist.\n\tfor _, endpoint := range repoData.Endpoints {\n\t\tif err := p.pushImageToEndpoint(ctx, endpoint, imgList, tags, repoData); err != nil {\n\t\t\treturn err\n\t\t}\n\t}\n\t_, err = p.session.PushImageJSONIndex(p.repoInfo.Name, imageIndex, true, repoData.Endpoints)\n\treturn err\n}"}, {"instruction": "// CopyMatchingTag copies fields tagged tag:\"value\" from \"from\" struct onto \"to\" struct.", "input": "go language", "output": "func CopyMatchingTag(from interface{}, to interface{}, tag string, shouldCopy func(value string) bool) {\n\tfromStruct := reflect.ValueOf(from).Elem()\n\tfromType := fromStruct.Type()\n\n\ttoStruct := reflect.ValueOf(to).Elem()\n\ttoType := toStruct.Type()\n\n\tif fromType != toType {\n\t\tpanic(fmt.Sprintf(\"non equal types: %s != %s\", fromType, toType))\n\t}\n\n\tfor i := 0; i < toStruct.NumField(); i++ {\n\t\tfromField := fromStruct.Field(i)\n\t\ttoField := toStruct.Field(i)\n\n\t\tif !toField.CanSet() {\n\t\t\t// Unexported fields\n\t\t\tcontinue\n\t\t}\n\n\t\tstructTag := toType.Field(i).Tag\n\n\t\tv := structTag.Get(tag)\n\t\tif shouldCopy(v) {\n\t\t\ttoField.Set(fromField)\n\t\t}\n\t}\n}"}, {"instruction": "// CompileExecutePreparedStmt compiles a session Execute command to a stmt.Statement.", "input": "go language", "output": "func CompileExecutePreparedStmt(ctx sessionctx.Context, ID uint32, args ...interface{}) (sqlexec.Statement, error) {\n\texecStmt := &ast.ExecuteStmt{ExecID: ID}\n\tif err := ResetContextOfStmt(ctx, execStmt); err != nil {\n\t\treturn nil, err\n\t}\n\texecStmt.UsingVars = make([]ast.ExprNode, len(args))\n\tfor i, val := range args {\n\t\texecStmt.UsingVars[i] = ast.NewValueExpr(val)\n\t}\n\tis := GetInfoSchema(ctx)\n\texecPlan, err := planner.Optimize(ctx, execStmt, is)\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\n\tstmt := &ExecStmt{\n\t\tInfoSchema: is,\n\t\tPlan:       execPlan,\n\t\tStmtNode:   execStmt,\n\t\tCtx:        ctx,\n\t}\n\tif prepared, ok := ctx.GetSessionVars().PreparedStmts[ID]; ok {\n\t\tstmt.Text = prepared.Stmt.Text()\n\t\tctx.GetSessionVars().StmtCtx.OriginalSQL = stmt.Text\n\t}\n\treturn stmt, nil\n}"}, {"instruction": "// the functions below show some best practices on how\n// to use entities to perform cryptographic operations\n// over the ledger state\n// getStateAndDecrypt retrieves the value associated to key,\n// decrypts it with the supplied entity and returns the result\n// of the decryption", "input": "go language", "output": "func getStateAndDecrypt(stub shim.ChaincodeStubInterface, ent entities.Encrypter, key string) ([]byte, error) {\n\t// at first we retrieve the ciphertext from the ledger\n\tciphertext, err := stub.GetState(key)\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\n\t// GetState will return a nil slice if the key does not exist.\n\t// Note that the chaincode logic may want to distinguish between\n\t// nil slice (key doesn't exist in state db) and empty slice\n\t// (key found in state db but value is empty). We do not\n\t// distinguish the case here\n\tif len(ciphertext) == 0 {\n\t\treturn nil, errors.New(\"no ciphertext to decrypt\")\n\t}\n\n\treturn ent.Decrypt(ciphertext)\n}"}, {"instruction": "// creates a unique path for disks (even if they share the same *.vhd name)", "input": "go language", "output": "func makeGlobalPDPath(host volume.VolumeHost, diskUri string, isManaged bool) (string, error) {\n\tdiskUri = libstrings.ToLower(diskUri) // always lower uri because users may enter it in caps.\n\tuniqueDiskNameTemplate := \"%s%s\"\n\thashedDiskUri := azure.MakeCRC32(diskUri)\n\tprefix := \"b\"\n\tif isManaged {\n\t\tprefix = \"m\"\n\t}\n\t// \"{m for managed b for blob}{hashed diskUri or DiskId depending on disk kind }\"\n\tdiskName := fmt.Sprintf(uniqueDiskNameTemplate, prefix, hashedDiskUri)\n\tpdPath := filepath.Join(host.GetPluginDir(azureDataDiskPluginName), util.MountsInGlobalPDPath, diskName)\n\n\treturn pdPath, nil\n}"}, {"instruction": "// removeRegisterTopic deletes all tickets for the given topic.", "input": "go language", "output": "func (s *ticketStore) removeRegisterTopic(topic Topic) {\n\tlog.Trace(\"Removing discovery topic\", \"topic\", topic)\n\tif s.tickets[topic] == nil {\n\t\tlog.Warn(\"Removing non-existent discovery topic\", \"topic\", topic)\n\t\treturn\n\t}\n\tfor _, list := range s.tickets[topic].buckets {\n\t\tfor _, ref := range list {\n\t\t\tref.t.refCnt--\n\t\t\tif ref.t.refCnt == 0 {\n\t\t\t\tdelete(s.nodes, ref.t.node)\n\t\t\t\tdelete(s.nodeLastReq, ref.t.node)\n\t\t\t}\n\t\t}\n\t}\n\tdelete(s.tickets, topic)\n}"}, {"instruction": "// findProcess for non-Windows. Note that this very likely doesn't\n// work for all non-Windows platforms Go supports and we should expand\n// support as we experience it.", "input": "go language", "output": "func findProcess(pid int) (*os.Process, error) {\n\t// FindProcess never fails on unix-like systems.\n\tp, err := os.FindProcess(pid)\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\n\t// On Unix-like systems, we can verify a process is alive by sending\n\t// a 0 signal. This will do nothing to the process but will still\n\t// return errors if the process is gone.\n\terr = p.Signal(syscall.Signal(0))\n\tif err == nil {\n\t\treturn p, nil\n\t}\n\n\treturn nil, fmt.Errorf(\"process %d is dead or running as another user\", pid)\n}"}, {"instruction": "// ChaincodeInfo implements function in interface ledger.DeployedChaincodeInfoProvider", "input": "go language", "output": "func (p *DeployedCCInfoProvider) ChaincodeInfo(chaincodeName string, qe ledger.SimpleQueryExecutor) (*ledger.DeployedChaincodeInfo, error) {\n\tchaincodeDataBytes, err := qe.GetState(lsccNamespace, chaincodeName)\n\tif err != nil || chaincodeDataBytes == nil {\n\t\treturn nil, err\n\t}\n\tchaincodeData := &ccprovider.ChaincodeData{}\n\tif err := proto.Unmarshal(chaincodeDataBytes, chaincodeData); err != nil {\n\t\treturn nil, errors.Wrap(err, \"error unmarshalling chaincode state data\")\n\t}\n\tcollConfigPkg, err := fetchCollConfigPkg(chaincodeName, qe)\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\treturn &ledger.DeployedChaincodeInfo{\n\t\tName:                chaincodeName,\n\t\tHash:                chaincodeData.Id,\n\t\tVersion:             chaincodeData.Version,\n\t\tCollectionConfigPkg: collConfigPkg,\n\t}, nil\n}"}, {"instruction": "// readDefaultBigInt reads a single line from stdin, trimming if from spaces,\n// enforcing it to parse into a big integer. If an empty line is entered, the\n// default value is returned.", "input": "go language", "output": "func (w *wizard) readDefaultBigInt(def *big.Int) *big.Int {\n\tfor {\n\t\tfmt.Printf(\"> \")\n\t\ttext, err := w.in.ReadString('\\n')\n\t\tif err != nil {\n\t\t\tlog.Crit(\"Failed to read user input\", \"err\", err)\n\t\t}\n\t\tif text = strings.TrimSpace(text); text == \"\" {\n\t\t\treturn def\n\t\t}\n\t\tval, ok := new(big.Int).SetString(text, 0)\n\t\tif !ok {\n\t\t\tlog.Error(\"Invalid input, expected big integer\")\n\t\t\tcontinue\n\t\t}\n\t\treturn val\n\t}\n}"}, {"instruction": "// Uniq takes in a slice or array and returns a slice with subsequent\n// duplicate elements removed.", "input": "go language", "output": "func (ns *Namespace) Uniq(seq interface{}) (interface{}, error) {\n\tif seq == nil {\n\t\treturn make([]interface{}, 0), nil\n\t}\n\n\tv := reflect.ValueOf(seq)\n\tvar slice reflect.Value\n\n\tswitch v.Kind() {\n\tcase reflect.Slice:\n\t\tslice = reflect.MakeSlice(v.Type(), 0, 0)\n\tcase reflect.Array:\n\t\tslice = reflect.MakeSlice(reflect.SliceOf(v.Type().Elem()), 0, 0)\n\tdefault:\n\t\treturn nil, errors.Errorf(\"type %T not supported\", seq)\n\t}\n\n\tseen := make(map[interface{}]bool)\n\tfor i := 0; i < v.Len(); i++ {\n\t\tev, _ := indirectInterface(v.Index(i))\n\t\tif !ev.Type().Comparable() {\n\t\t\treturn nil, errors.New(\"elements must be comparable\")\n\t\t}\n\t\tkey := normalize(ev)\n\t\tif _, found := seen[key]; !found {\n\t\t\tslice = reflect.Append(slice, ev)\n\t\t\tseen[key] = true\n\t\t}\n\t}\n\n\treturn slice.Interface(), nil\n\n}"}, {"instruction": "// GetCCPackage tries each known package implementation one by one\n// till the right package is found", "input": "go language", "output": "func GetCCPackage(buf []byte) (CCPackage, error) {\n\t// try raw CDS\n\tcds := &CDSPackage{}\n\tif ccdata, err := cds.InitFromBuffer(buf); err != nil {\n\t\tcds = nil\n\t} else {\n\t\terr = cds.ValidateCC(ccdata)\n\t\tif err != nil {\n\t\t\tcds = nil\n\t\t}\n\t}\n\n\t// try signed CDS\n\tscds := &SignedCDSPackage{}\n\tif ccdata, err := scds.InitFromBuffer(buf); err != nil {\n\t\tscds = nil\n\t} else {\n\t\terr = scds.ValidateCC(ccdata)\n\t\tif err != nil {\n\t\t\tscds = nil\n\t\t}\n\t}\n\n\tif cds != nil && scds != nil {\n\t\t// Both were unmarshaled successfully, this is exactly why the approach of\n\t\t// hoping proto fails for bad inputs is fatally flawed.\n\t\tccproviderLogger.Errorf(\"Could not determine chaincode package type, guessing SignedCDS\")\n\t\treturn scds, nil\n\t}\n\n\tif cds != nil {\n\t\treturn cds, nil\n\t}\n\n\tif scds != nil {\n\t\treturn scds, nil\n\t}\n\n\treturn nil, errors.New(\"could not unmarshal chaincode package to CDS or SignedCDS\")\n}"}, {"instruction": "// dropDisabledRunAsGroupField removes disabled fields from PodSpec related\n// to RunAsGroup", "input": "go language", "output": "func dropDisabledRunAsGroupField(podSpec, oldPodSpec *api.PodSpec) {\n\tif !utilfeature.DefaultFeatureGate.Enabled(features.RunAsGroup) && !runAsGroupInUse(oldPodSpec) {\n\t\tif podSpec.SecurityContext != nil {\n\t\t\tpodSpec.SecurityContext.RunAsGroup = nil\n\t\t}\n\t\tfor i := range podSpec.Containers {\n\t\t\tif podSpec.Containers[i].SecurityContext != nil {\n\t\t\t\tpodSpec.Containers[i].SecurityContext.RunAsGroup = nil\n\t\t\t}\n\t\t}\n\t\tfor i := range podSpec.InitContainers {\n\t\t\tif podSpec.InitContainers[i].SecurityContext != nil {\n\t\t\t\tpodSpec.InitContainers[i].SecurityContext.RunAsGroup = nil\n\t\t\t}\n\t\t}\n\t}\n}"}, {"instruction": "// setClaimProvisioner saves\n// claim.Annotations[annStorageProvisioner] = class.Provisioner", "input": "go language", "output": "func (ctrl *PersistentVolumeController) setClaimProvisioner(claim *v1.PersistentVolumeClaim, provisionerName string) (*v1.PersistentVolumeClaim, error) {\n\tif val, ok := claim.Annotations[annStorageProvisioner]; ok && val == provisionerName {\n\t\t// annotation is already set, nothing to do\n\t\treturn claim, nil\n\t}\n\n\t// The volume from method args can be pointing to watcher cache. We must not\n\t// modify these, therefore create a copy.\n\tclaimClone := claim.DeepCopy()\n\tmetav1.SetMetaDataAnnotation(&claimClone.ObjectMeta, annStorageProvisioner, provisionerName)\n\tnewClaim, err := ctrl.kubeClient.CoreV1().PersistentVolumeClaims(claim.Namespace).Update(claimClone)\n\tif err != nil {\n\t\treturn newClaim, err\n\t}\n\t_, err = ctrl.storeClaimUpdate(newClaim)\n\tif err != nil {\n\t\treturn newClaim, err\n\t}\n\treturn newClaim, nil\n}"}, {"instruction": "// tar2ext4Actual is the implementation of tar2ext to write a layer from a tar file.\n// It can be called through re-exec (default), or inline for debugging.", "input": "go language", "output": "func tar2ext4Actual(dest string, diff io.Reader) (int64, error) {\n\t// maxDiskSize is not relating to the sandbox size - this is the\n\t// maximum possible size a layer VHD generated can be from an EXT4\n\t// layout perspective.\n\tconst maxDiskSize = 128 * 1024 * 1024 * 1024 // 128GB\n\tout, err := os.Create(dest)\n\tif err != nil {\n\t\treturn 0, err\n\t}\n\tdefer out.Close()\n\tif err := tar2ext4.Convert(\n\t\tdiff,\n\t\tout,\n\t\ttar2ext4.AppendVhdFooter,\n\t\ttar2ext4.ConvertWhiteout,\n\t\ttar2ext4.MaximumDiskSize(maxDiskSize)); err != nil {\n\t\treturn 0, err\n\t}\n\tfi, err := os.Stat(dest)\n\tif err != nil {\n\t\treturn 0, err\n\t}\n\treturn fi.Size(), nil\n}"}, {"instruction": "// getKubeletSandboxes lists all (or just the running) sandboxes managed by kubelet.", "input": "go language", "output": "func (m *kubeGenericRuntimeManager) getKubeletSandboxes(all bool) ([]*runtimeapi.PodSandbox, error) {\n\tvar filter *runtimeapi.PodSandboxFilter\n\tif !all {\n\t\treadyState := runtimeapi.PodSandboxState_SANDBOX_READY\n\t\tfilter = &runtimeapi.PodSandboxFilter{\n\t\t\tState: &runtimeapi.PodSandboxStateValue{\n\t\t\t\tState: readyState,\n\t\t\t},\n\t\t}\n\t}\n\n\tresp, err := m.runtimeService.ListPodSandbox(filter)\n\tif err != nil {\n\t\tklog.Errorf(\"ListPodSandbox failed: %v\", err)\n\t\treturn nil, err\n\t}\n\n\treturn resp, nil\n}"}, {"instruction": "// syncNamespaceFromKey looks for a namespace with the specified key in its store and synchronizes it", "input": "go language", "output": "func (nm *NamespaceController) syncNamespaceFromKey(key string) (err error) {\n\tstartTime := time.Now()\n\tdefer func() {\n\t\tklog.V(4).Infof(\"Finished syncing namespace %q (%v)\", key, time.Since(startTime))\n\t}()\n\n\tnamespace, err := nm.lister.Get(key)\n\tif errors.IsNotFound(err) {\n\t\tklog.Infof(\"Namespace has been deleted %v\", key)\n\t\treturn nil\n\t}\n\tif err != nil {\n\t\tutilruntime.HandleError(fmt.Errorf(\"Unable to retrieve namespace %v from store: %v\", key, err))\n\t\treturn err\n\t}\n\treturn nm.namespacedResourcesDeleter.Delete(namespace.Name)\n}"}, {"instruction": "// MakeChainDatabase open an LevelDB using the flags passed to the client and will hard crash if it fails.", "input": "go language", "output": "func MakeChainDatabase(ctx *cli.Context, stack *node.Node) ethdb.Database {\n\tvar (\n\t\tcache   = ctx.GlobalInt(CacheFlag.Name) * ctx.GlobalInt(CacheDatabaseFlag.Name) / 100\n\t\thandles = makeDatabaseHandles()\n\t)\n\tname := \"chaindata\"\n\tif ctx.GlobalString(SyncModeFlag.Name) == \"light\" {\n\t\tname = \"lightchaindata\"\n\t}\n\tchainDb, err := stack.OpenDatabase(name, cache, handles, \"\")\n\tif err != nil {\n\t\tFatalf(\"Could not open database: %v\", err)\n\t}\n\treturn chainDb\n}"}, {"instruction": "// NodePublishVolume implements CSI NodePublishVolume", "input": "go language", "output": "func (f *NodeClient) NodePublishVolume(ctx context.Context, req *csipb.NodePublishVolumeRequest, opts ...grpc.CallOption) (*csipb.NodePublishVolumeResponse, error) {\n\tif f.nextErr != nil {\n\t\treturn nil, f.nextErr\n\t}\n\n\tif req.GetVolumeId() == \"\" {\n\t\treturn nil, errors.New(\"missing volume id\")\n\t}\n\tif req.GetTargetPath() == \"\" {\n\t\treturn nil, errors.New(\"missing target path\")\n\t}\n\tfsTypes := \"block|ext4|xfs|zfs\"\n\tfsType := req.GetVolumeCapability().GetMount().GetFsType()\n\tif !strings.Contains(fsTypes, fsType) {\n\t\treturn nil, errors.New(\"invalid fstype\")\n\t}\n\tf.nodePublishedVolumes[req.GetVolumeId()] = CSIVolume{\n\t\tVolumeHandle:    req.GetVolumeId(),\n\t\tPath:            req.GetTargetPath(),\n\t\tDeviceMountPath: req.GetStagingTargetPath(),\n\t\tVolumeContext:   req.GetVolumeContext(),\n\t\tFSType:          req.GetVolumeCapability().GetMount().GetFsType(),\n\t\tMountFlags:      req.GetVolumeCapability().GetMount().MountFlags,\n\t}\n\treturn &csipb.NodePublishVolumeResponse{}, nil\n}"}, {"instruction": "// NewTransaction creates a new transaction with the given properties. Contracts\n// can be created by transacting with a nil recipient.", "input": "go language", "output": "func NewTransaction(nonce int64, to *Address, amount *BigInt, gasLimit int64, gasPrice *BigInt, data []byte) *Transaction {\n\tif to == nil {\n\t\treturn &Transaction{types.NewContractCreation(uint64(nonce), amount.bigint, uint64(gasLimit), gasPrice.bigint, common.CopyBytes(data))}\n\t}\n\treturn &Transaction{types.NewTransaction(uint64(nonce), to.address, amount.bigint, uint64(gasLimit), gasPrice.bigint, common.CopyBytes(data))}\n}"}, {"instruction": "// Gets Disk counts per storage account", "input": "go language", "output": "func (c *BlobDiskController) getDiskCount(SAName string) (int, error) {\n\t// if we have it in cache\n\tif c.accounts[SAName].diskCount != -1 {\n\t\treturn int(c.accounts[SAName].diskCount), nil\n\t}\n\n\tvar err error\n\tvar blobSvc azstorage.BlobStorageClient\n\n\tif err = c.ensureDefaultContainer(SAName); err != nil {\n\t\treturn 0, err\n\t}\n\n\tif blobSvc, err = c.getBlobSvcClient(SAName); err != nil {\n\t\treturn 0, err\n\t}\n\tparams := azstorage.ListBlobsParameters{}\n\n\tcontainer := blobSvc.GetContainerReference(vhdContainerName)\n\tresponse, err := container.ListBlobs(params)\n\tif err != nil {\n\t\treturn 0, err\n\t}\n\tklog.V(4).Infof(\"azure-Disk -  refreshed data count for account %s and found %v\", SAName, len(response.Blobs))\n\tc.accounts[SAName].diskCount = int32(len(response.Blobs))\n\n\treturn int(c.accounts[SAName].diskCount), nil\n}"}, {"instruction": "// evalInt evals INSTR(str,substr), case insensitive\n// See https://dev.mysql.com/doc/refman/5.7/en/string-functions.html#function_instr", "input": "go language", "output": "func (b *builtinInstrSig) evalInt(row chunk.Row) (int64, bool, error) {\n\tstr, IsNull, err := b.args[0].EvalString(b.ctx, row)\n\tif IsNull || err != nil {\n\t\treturn 0, true, err\n\t}\n\tstr = strings.ToLower(str)\n\n\tsubstr, IsNull, err := b.args[1].EvalString(b.ctx, row)\n\tif IsNull || err != nil {\n\t\treturn 0, true, err\n\t}\n\tsubstr = strings.ToLower(substr)\n\n\tidx := strings.Index(str, substr)\n\tif idx == -1 {\n\t\treturn 0, false, nil\n\t}\n\treturn int64(utf8.RuneCountInString(str[:idx]) + 1), false, nil\n}"}, {"instruction": "// HandleDates updates all the dates given the current configuration and the\n// supplied front matter params. Note that this requires all lower-case keys\n// in the params map.", "input": "go language", "output": "func (f FrontMatterHandler) HandleDates(d *FrontMatterDescriptor) error {\n\tif d.Dates == nil {\n\t\tpanic(\"missing dates\")\n\t}\n\n\tif f.dateHandler == nil {\n\t\tpanic(\"missing date handler\")\n\t}\n\n\tif _, err := f.dateHandler(d); err != nil {\n\t\treturn err\n\t}\n\n\tif _, err := f.lastModHandler(d); err != nil {\n\t\treturn err\n\t}\n\n\tif _, err := f.publishDateHandler(d); err != nil {\n\t\treturn err\n\t}\n\n\tif _, err := f.expiryDateHandler(d); err != nil {\n\t\treturn err\n\t}\n\n\treturn nil\n}"}, {"instruction": "// ApplyDefaults applies the default values to Options.", "input": "go language", "output": "func (o *Options) ApplyDefaults(in *kubeproxyconfig.KubeProxyConfiguration) (*kubeproxyconfig.KubeProxyConfiguration, error) {\n\texternal, err := o.scheme.ConvertToVersion(in, v1alpha1.SchemeGroupVersion)\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\n\to.scheme.Default(external)\n\n\tinternal, err := o.scheme.ConvertToVersion(external, kubeproxyconfig.SchemeGroupVersion)\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\n\tout := internal.(*kubeproxyconfig.KubeProxyConfiguration)\n\n\treturn out, nil\n}"}, {"instruction": "// EnforcePtr ensures that obj is a pointer of some sort. Returns a reflect.Value\n// of the dereferenced pointer, ensuring that it is settable/addressable.\n// Returns an error if this is not possible.", "input": "go language", "output": "func EnforcePtr(obj interface{}) (reflect.Value, error) {\n\tv := reflect.ValueOf(obj)\n\tif v.Kind() != reflect.Ptr {\n\t\tif v.Kind() == reflect.Invalid {\n\t\t\treturn reflect.Value{}, fmt.Errorf(\"expected pointer, but got invalid kind\")\n\t\t}\n\t\treturn reflect.Value{}, fmt.Errorf(\"expected pointer, but got %v type\", v.Type())\n\t}\n\tif v.IsNil() {\n\t\treturn reflect.Value{}, fmt.Errorf(\"expected pointer, but got nil\")\n\t}\n\treturn v.Elem(), nil\n}"}, {"instruction": "// sweepList will loop over the list, merge each session's local stats into handle\n// and remove closed session's collector.", "input": "go language", "output": "func (h *Handle) sweepList() {\n\tprev := h.listHead\n\tprev.Lock()\n\terrorRateMap := make(errorRateDeltaMap)\n\tfor curr := prev.next; curr != nil; curr = curr.next {\n\t\tcurr.Lock()\n\t\t// Merge the session stats into handle and error rate map.\n\t\th.merge(curr, errorRateMap)\n\t\tif curr.deleted {\n\t\t\tprev.next = curr.next\n\t\t\t// Since the session is already closed, we can safely unlock it here.\n\t\t\tcurr.Unlock()\n\t\t} else {\n\t\t\t// Unlock the previous lock, so we only holds at most two session's lock at the same time.\n\t\t\tprev.Unlock()\n\t\t\tprev = curr\n\t\t}\n\t}\n\tprev.Unlock()\n\th.mu.Lock()\n\th.mu.rateMap.merge(errorRateMap)\n\th.mu.Unlock()\n}"}, {"instruction": "// StrToUint converts a string to an unsigned integer at the best-effortt.", "input": "go language", "output": "func StrToUint(sc *stmtctx.StatementContext, str string) (uint64, error) {\n\tstr = strings.TrimSpace(str)\n\tvalidPrefix, err := getValidIntPrefix(sc, str)\n\tif validPrefix[0] == '+' {\n\t\tvalidPrefix = validPrefix[1:]\n\t}\n\tuVal, err1 := strconv.ParseUint(validPrefix, 10, 64)\n\tif err1 != nil {\n\t\treturn uVal, ErrOverflow.GenWithStackByArgs(\"BIGINT UNSIGNED\", validPrefix)\n\t}\n\treturn uVal, errors.Trace(err)\n}"}, {"instruction": "// nodes returns a paginated list of node addresses. If key is not nil,\n// only nodes that contain that key will be returned.", "input": "go language", "output": "func (s *GlobalStore) nodes(key []byte, startAddr *common.Address, limit int) (nodes mock.Nodes, err error) {\n\titer := s.db.NewIterator(nil, nil)\n\tdefer iter.Release()\n\n\tif limit <= 0 {\n\t\tlimit = mock.DefaultLimit\n\t}\n\n\tprefix := []byte{indexForNodesPrefix}\n\tif key != nil {\n\t\tprefix = indexForNodesWithHashPrefix(key)\n\t}\n\tstartKey := prefix\n\tif startAddr != nil {\n\t\tif key != nil {\n\t\t\tstartKey = indexForNodesWithHash(key, *startAddr)\n\t\t} else {\n\t\t\tstartKey = indexForNodes(*startAddr)\n\t\t}\n\t}\n\n\tok := iter.Seek(startKey)\n\tif !ok {\n\t\treturn nodes, iter.Error()\n\t}\n\tfor ; ok; ok = iter.Next() {\n\t\tk := iter.Key()\n\t\tif !bytes.HasPrefix(k, prefix) {\n\t\t\tbreak\n\t\t}\n\t\taddr := common.BytesToAddress(append([]byte(nil), bytes.TrimPrefix(k, prefix)...))\n\n\t\tif len(nodes.Addrs) >= limit {\n\t\t\tnodes.Next = &addr\n\t\t\tbreak\n\t\t}\n\n\t\tnodes.Addrs = append(nodes.Addrs, addr)\n\t}\n\treturn nodes, iter.Error()\n}"}, {"instruction": "// convertVolPathsToDevicePaths removes cluster or folder path from volPaths and convert to canonicalPath", "input": "go language", "output": "func (vs *VSphere) convertVolPathsToDevicePaths(ctx context.Context, nodeVolumes map[k8stypes.NodeName][]string) (map[k8stypes.NodeName][]string, error) {\n\tvmVolumes := make(map[k8stypes.NodeName][]string)\n\tfor nodeName, volPaths := range nodeVolumes {\n\t\tnodeInfo, err := vs.nodeManager.GetNodeInfo(nodeName)\n\t\tif err != nil {\n\t\t\treturn nil, err\n\t\t}\n\n\t\t_, err = vs.getVSphereInstanceForServer(nodeInfo.vcServer, ctx)\n\t\tif err != nil {\n\t\t\treturn nil, err\n\t\t}\n\n\t\tfor i, volPath := range volPaths {\n\t\t\tdeviceVolPath, err := convertVolPathToDevicePath(ctx, nodeInfo.dataCenter, volPath)\n\t\t\tif err != nil {\n\t\t\t\tklog.Errorf(\"Failed to convert vsphere volume path %s to device path for volume %s. err: %+v\", volPath, deviceVolPath, err)\n\t\t\t\treturn nil, err\n\t\t\t}\n\t\t\tvolPaths[i] = deviceVolPath\n\t\t}\n\t\tvmVolumes[nodeName] = volPaths\n\t}\n\treturn vmVolumes, nil\n}"}, {"instruction": "// deleteDupKeys picks primary/unique key-value pairs from rows and remove them from the dupKVs", "input": "go language", "output": "func (b *batchChecker) deleteDupKeys(ctx sessionctx.Context, t table.Table, rows [][]types.Datum) error {\n\tcleanupRows, err := b.getKeysNeedCheck(ctx, t, rows)\n\tif err != nil {\n\t\treturn err\n\t}\n\tfor _, row := range cleanupRows {\n\t\tif row.handleKey != nil {\n\t\t\tdelete(b.dupKVs, string(row.handleKey.newKV.key))\n\t\t}\n\t\tfor _, uk := range row.uniqueKeys {\n\t\t\tdelete(b.dupKVs, string(uk.newKV.key))\n\t\t}\n\t}\n\treturn nil\n}"}, {"instruction": "// This helper turns a provider configs field into a deterministic\n// string value for comparison in tests.", "input": "go language", "output": "func providerConfigsStr(pcs []*ProviderConfig) string {\n\tresult := \"\"\n\n\tns := make([]string, 0, len(pcs))\n\tm := make(map[string]*ProviderConfig)\n\tfor _, n := range pcs {\n\t\tns = append(ns, n.Name)\n\t\tm[n.Name] = n\n\t}\n\tsort.Strings(ns)\n\n\tfor _, n := range ns {\n\t\tpc := m[n]\n\n\t\tresult += fmt.Sprintf(\"%s\\n\", n)\n\n\t\tkeys := make([]string, 0, len(pc.RawConfig.Raw))\n\t\tfor k, _ := range pc.RawConfig.Raw {\n\t\t\tkeys = append(keys, k)\n\t\t}\n\t\tsort.Strings(keys)\n\n\t\tfor _, k := range keys {\n\t\t\tresult += fmt.Sprintf(\"  %s\\n\", k)\n\t\t}\n\n\t\tif len(pc.RawConfig.Variables) > 0 {\n\t\t\tresult += fmt.Sprintf(\"  vars\\n\")\n\t\t\tfor _, rawV := range pc.RawConfig.Variables {\n\t\t\t\tkind := \"unknown\"\n\t\t\t\tstr := rawV.FullKey()\n\n\t\t\t\tswitch rawV.(type) {\n\t\t\t\tcase *ResourceVariable:\n\t\t\t\t\tkind = \"resource\"\n\t\t\t\tcase *UserVariable:\n\t\t\t\t\tkind = \"user\"\n\t\t\t\t}\n\n\t\t\t\tresult += fmt.Sprintf(\"    %s: %s\\n\", kind, str)\n\t\t\t}\n\t\t}\n\t}\n\n\treturn strings.TrimSpace(result)\n}"}, {"instruction": "// GetTxSimulationResults implements method in interface `ledger.TxSimulator`", "input": "go language", "output": "func (s *lockBasedTxSimulator) GetTxSimulationResults() (*ledger.TxSimulationResults, error) {\n\tif s.simulationResultsComputed {\n\t\treturn nil, errors.New(\"this function should only be called once on a transaction simulator instance\")\n\t}\n\tdefer func() { s.simulationResultsComputed = true }()\n\tlogger.Debugf(\"Simulation completed, getting simulation results\")\n\tif s.helper.err != nil {\n\t\treturn nil, s.helper.err\n\t}\n\ts.helper.addRangeQueryInfo()\n\treturn s.rwsetBuilder.GetTxSimulationResults()\n}"}, {"instruction": "// Set adds or updates the given entry in the record. It panics if the value can't be\n// encoded. If the record is signed, Set increments the sequence number and invalidates\n// the sequence number.", "input": "go language", "output": "func (r *Record) Set(e Entry) {\n\tblob, err := rlp.EncodeToBytes(e)\n\tif err != nil {\n\t\tpanic(fmt.Errorf(\"enr: can't encode %s: %v\", e.ENRKey(), err))\n\t}\n\tr.invalidate()\n\n\tpairs := make([]pair, len(r.pairs))\n\tcopy(pairs, r.pairs)\n\ti := sort.Search(len(pairs), func(i int) bool { return pairs[i].k >= e.ENRKey() })\n\tswitch {\n\tcase i < len(pairs) && pairs[i].k == e.ENRKey():\n\t\t// element is present at r.pairs[i]\n\t\tpairs[i].v = blob\n\tcase i < len(r.pairs):\n\t\t// insert pair before i-th elem\n\t\tel := pair{e.ENRKey(), blob}\n\t\tpairs = append(pairs, pair{})\n\t\tcopy(pairs[i+1:], pairs[i:])\n\t\tpairs[i] = el\n\tdefault:\n\t\t// element should be placed at the end of r.pairs\n\t\tpairs = append(pairs, pair{e.ENRKey(), blob})\n\t}\n\tr.pairs = pairs\n}"}, {"instruction": "// SetClientRootCAs sets the list of authorities used to verify client\n// certificates based on a list of PEM-encoded X509 certificate authorities", "input": "go language", "output": "func (gServer *GRPCServer) SetClientRootCAs(clientRoots [][]byte) error {\n\tgServer.lock.Lock()\n\tdefer gServer.lock.Unlock()\n\n\terrMsg := \"Failed to set client root certificate(s): %s\"\n\n\t//create a new map and CertPool\n\tclientRootCAs := make(map[string]*x509.Certificate)\n\tfor _, clientRoot := range clientRoots {\n\t\tcerts, subjects, err := pemToX509Certs(clientRoot)\n\t\tif err != nil {\n\t\t\treturn fmt.Errorf(errMsg, err.Error())\n\t\t}\n\t\tif len(certs) >= 1 {\n\t\t\tfor i, cert := range certs {\n\t\t\t\t//add it to our clientRootCAs map using subject as key\n\t\t\t\tclientRootCAs[subjects[i]] = cert\n\t\t\t}\n\t\t}\n\t}\n\n\t//create a new CertPool and populate with the new clientRootCAs\n\tcertPool := x509.NewCertPool()\n\tfor _, clientRoot := range clientRootCAs {\n\t\tcertPool.AddCert(clientRoot)\n\t}\n\t//replace the internal map\n\tgServer.clientRootCAs = clientRootCAs\n\t//replace the current ClientCAs pool\n\tgServer.tlsConfig.ClientCAs = certPool\n\treturn nil\n}"}, {"instruction": "// StatusViewerFor returns a StatusViewer for the resource specified by kind.", "input": "go language", "output": "func StatusViewerFor(kind schema.GroupKind) (StatusViewer, error) {\n\tswitch kind {\n\tcase extensionsv1beta1.SchemeGroupVersion.WithKind(\"Deployment\").GroupKind(),\n\t\tappsv1.SchemeGroupVersion.WithKind(\"Deployment\").GroupKind():\n\t\treturn &DeploymentStatusViewer{}, nil\n\tcase extensionsv1beta1.SchemeGroupVersion.WithKind(\"DaemonSet\").GroupKind(),\n\t\tappsv1.SchemeGroupVersion.WithKind(\"DaemonSet\").GroupKind():\n\t\treturn &DaemonSetStatusViewer{}, nil\n\tcase appsv1.SchemeGroupVersion.WithKind(\"StatefulSet\").GroupKind():\n\t\treturn &StatefulSetStatusViewer{}, nil\n\t}\n\treturn nil, fmt.Errorf(\"no status viewer has been implemented for %v\", kind)\n}"}, {"instruction": "// NewBootstrapTokenString converts the given Bootstrap Token as a string\n// to the BootstrapTokenString object used for serialization/deserialization\n// and internal usage. It also automatically validates that the given token\n// is of the right format", "input": "go language", "output": "func NewBootstrapTokenString(token string) (*BootstrapTokenString, error) {\n\tsubstrs := bootstraputil.BootstrapTokenRegexp.FindStringSubmatch(token)\n\t// TODO: Add a constant for the 3 value here, and explain better why it's needed (other than because how the regexp parsin works)\n\tif len(substrs) != 3 {\n\t\treturn nil, errors.Errorf(\"the bootstrap token %q was not of the form %q\", token, bootstrapapi.BootstrapTokenPattern)\n\t}\n\n\treturn &BootstrapTokenString{ID: substrs[1], Secret: substrs[2]}, nil\n}"}, {"instruction": "// WaitForClusterAvailable returns true if all endpoints in the cluster are available after retry attempts, an error is returned otherwise", "input": "go language", "output": "func (c *Client) WaitForClusterAvailable(retries int, retryInterval time.Duration) (bool, error) {\n\tfor i := 0; i < retries; i++ {\n\t\tif i > 0 {\n\t\t\tklog.V(1).Infof(\"[etcd] Waiting %v until next retry\\n\", retryInterval)\n\t\t\ttime.Sleep(retryInterval)\n\t\t}\n\t\tklog.V(2).Infof(\"[etcd] attempting to see if all cluster endpoints (%s) are available %d/%d\", c.Endpoints, i+1, retries)\n\t\tresp, err := c.ClusterAvailable()\n\t\tif err != nil {\n\t\t\tswitch err {\n\t\t\tcase context.DeadlineExceeded:\n\t\t\t\tklog.V(1).Infof(\"[etcd] Attempt timed out\")\n\t\t\tdefault:\n\t\t\t\tklog.V(1).Infof(\"[etcd] Attempt failed with error: %v\\n\", err)\n\t\t\t}\n\t\t\tcontinue\n\t\t}\n\t\treturn resp, nil\n\t}\n\treturn false, errors.New(\"timeout waiting for etcd cluster to be available\")\n}"}, {"instruction": "// setClientCapacity sets the priority capacity assigned to a given client", "input": "go language", "output": "func (v *priorityClientPool) setClientCapacity(id enode.ID, cap uint64) error {\n\tv.lock.Lock()\n\tdefer v.lock.Unlock()\n\n\tc := v.clients[id]\n\tif c.cap == cap {\n\t\treturn nil\n\t}\n\tif c.connected {\n\t\tif v.totalConnectedCap+cap > v.totalCap+c.cap {\n\t\t\treturn ErrTotalCap\n\t\t}\n\t\tif c.cap == 0 {\n\t\t\tif v.child != nil {\n\t\t\t\tv.child.unregisterPeer(c.peer)\n\t\t\t}\n\t\t\tv.priorityCount++\n\t\t}\n\t\tif cap == 0 {\n\t\t\tv.priorityCount--\n\t\t}\n\t\tv.totalConnectedCap += cap - c.cap\n\t\tif v.child != nil {\n\t\t\tv.child.setLimits(v.maxPeers-v.priorityCount, v.totalCap-v.totalConnectedCap)\n\t\t}\n\t\tif cap == 0 {\n\t\t\tif v.child != nil {\n\t\t\t\tv.child.registerPeer(c.peer)\n\t\t\t}\n\t\t\tc.peer.updateCapacity(v.freeClientCap)\n\t\t} else {\n\t\t\tc.peer.updateCapacity(cap)\n\t\t}\n\t}\n\tif cap != 0 || c.connected {\n\t\tc.cap = cap\n\t\tv.clients[id] = c\n\t} else {\n\t\tdelete(v.clients, id)\n\t}\n\treturn nil\n}"}, {"instruction": "// Coordinates queries for all nodes with coordinates.", "input": "go language", "output": "func (s *Store) Coordinates(ws memdb.WatchSet) (uint64, structs.Coordinates, error) {\n\ttx := s.db.Txn(false)\n\tdefer tx.Abort()\n\n\t// Get the table index.\n\tidx := maxIndexTxn(tx, \"coordinates\")\n\n\t// Pull all the coordinates.\n\titer, err := tx.Get(\"coordinates\", \"id\")\n\tif err != nil {\n\t\treturn 0, nil, fmt.Errorf(\"failed coordinate lookup: %s\", err)\n\t}\n\tws.Add(iter.WatchCh())\n\n\tvar results structs.Coordinates\n\tfor coord := iter.Next(); coord != nil; coord = iter.Next() {\n\t\tresults = append(results, coord.(*structs.Coordinate))\n\t}\n\treturn idx, results, nil\n}"}, {"instruction": "// gatherAllHostports returns all hostports that should be presented on node,\n// given the list of pods running on that node and ignoring host network\n// pods (which don't need hostport <-> container port mapping).", "input": "go language", "output": "func gatherAllHostports(activePodPortMappings []*PodPortMapping) (map[*PortMapping]targetPod, error) {\n\tpodHostportMap := make(map[*PortMapping]targetPod)\n\tfor _, pm := range activePodPortMappings {\n\t\tif pm.IP.To4() == nil {\n\t\t\treturn nil, fmt.Errorf(\"Invalid or missing pod %s IP\", getPodFullName(pm))\n\t\t}\n\t\t// should not handle hostports for hostnetwork pods\n\t\tif pm.HostNetwork {\n\t\t\tcontinue\n\t\t}\n\n\t\tfor _, port := range pm.PortMappings {\n\t\t\tif port.HostPort != 0 {\n\t\t\t\tpodHostportMap[port] = targetPod{podFullName: getPodFullName(pm), podIP: pm.IP.String()}\n\t\t\t}\n\t\t}\n\t}\n\treturn podHostportMap, nil\n}"}, {"instruction": "// GetCgroupCPUAndMemoryStats returns the CPU and memory stats of the cgroup with the cgroupName. Note that\n// this function doesn't generate filesystem stats.", "input": "go language", "output": "func (p *StatsProvider) GetCgroupCPUAndMemoryStats(cgroupName string, updateStats bool) (*statsapi.ContainerStats, error) {\n\tinfo, err := getCgroupInfo(p.cadvisor, cgroupName, updateStats)\n\tif err != nil {\n\t\treturn nil, fmt.Errorf(\"failed to get cgroup stats for %q: %v\", cgroupName, err)\n\t}\n\t// Rootfs and imagefs doesn't make sense for raw cgroup.\n\ts := cadvisorInfoToContainerCPUAndMemoryStats(cgroupName, info)\n\treturn s, nil\n}"}, {"instruction": "// resolveGeneratedColumns resolves generated columns with their generation\n// expressions respectively. onDups indicates which columns are in on-duplicate list.", "input": "go language", "output": "func (b *PlanBuilder) resolveGeneratedColumns(columns []*table.Column, onDups map[string]struct{}, mockPlan LogicalPlan) (igc InsertGeneratedColumns, err error) {\n\tfor _, column := range columns {\n\t\tif !column.IsGenerated() {\n\t\t\tcontinue\n\t\t}\n\t\tcolumnName := &ast.ColumnName{Name: column.Name}\n\t\tcolumnName.SetText(column.Name.O)\n\n\t\tcolExpr, _, err := mockPlan.findColumn(columnName)\n\t\tif err != nil {\n\t\t\treturn igc, err\n\t\t}\n\n\t\texpr, _, err := b.rewrite(column.GeneratedExpr, mockPlan, nil, true)\n\t\tif err != nil {\n\t\t\treturn igc, err\n\t\t}\n\t\texpr = expression.BuildCastFunction(b.ctx, expr, colExpr.GetType())\n\n\t\tigc.Columns = append(igc.Columns, columnName)\n\t\tigc.Exprs = append(igc.Exprs, expr)\n\t\tif onDups == nil {\n\t\t\tcontinue\n\t\t}\n\t\tfor dep := range column.Dependences {\n\t\t\tif _, ok := onDups[dep]; ok {\n\t\t\t\tassign := &expression.Assignment{Col: colExpr, Expr: expr}\n\t\t\t\tigc.OnDuplicates = append(igc.OnDuplicates, assign)\n\t\t\t\tbreak\n\t\t\t}\n\t\t}\n\t}\n\treturn igc, nil\n}"}, {"instruction": "// ValidateWebhook validates the webhook", "input": "go language", "output": "func ValidateWebhook(w auditregistration.Webhook, fldPath *field.Path) field.ErrorList {\n\tvar allErrs field.ErrorList\n\tif w.Throttle != nil {\n\t\tallErrs = append(allErrs, ValidateWebhookThrottleConfig(w.Throttle, fldPath.Child(\"throttle\"))...)\n\t}\n\n\tcc := w.ClientConfig\n\tswitch {\n\tcase (cc.URL == nil) == (cc.Service == nil):\n\t\tallErrs = append(allErrs, field.Required(fldPath.Child(\"clientConfig\"), \"exactly one of url or service is required\"))\n\tcase cc.URL != nil:\n\t\tallErrs = append(allErrs, webhook.ValidateWebhookURL(fldPath.Child(\"clientConfig\").Child(\"url\"), *cc.URL, false)...)\n\tcase cc.Service != nil:\n\t\tallErrs = append(allErrs, webhook.ValidateWebhookService(fldPath.Child(\"clientConfig\").Child(\"service\"), cc.Service.Name, cc.Service.Namespace, cc.Service.Path, cc.Service.Port)...)\n\t}\n\treturn allErrs\n}"}, {"instruction": "// RequestVerificationWithUser implements the Manager interface.", "input": "go language", "output": "func (p *UserPrivileges) RequestVerificationWithUser(db, table, column string, priv mysql.PrivilegeType, user *auth.UserIdentity) bool {\n\tif SkipWithGrant {\n\t\treturn true\n\t}\n\n\tif user == nil {\n\t\treturn false\n\t}\n\n\t// Skip check for INFORMATION_SCHEMA database.\n\t// See https://dev.mysql.com/doc/refman/5.7/en/information-schema.html\n\tif strings.EqualFold(db, \"INFORMATION_SCHEMA\") {\n\t\treturn true\n\t}\n\n\tmysqlPriv := p.Handle.Get()\n\treturn mysqlPriv.RequestVerification(nil, user.Username, user.Hostname, db, table, column, priv)\n}"}, {"instruction": "// Command provides a mock function with given fields: name, help, onCommand", "input": "go language", "output": "func (_m *CommandRegistrar) Command(name string, help string, onCommand common.CLICommand) *kingpin.CmdClause {\n\tret := _m.Called(name, help, onCommand)\n\n\tvar r0 *kingpin.CmdClause\n\tif rf, ok := ret.Get(0).(func(string, string, common.CLICommand) *kingpin.CmdClause); ok {\n\t\tr0 = rf(name, help, onCommand)\n\t} else {\n\t\tif ret.Get(0) != nil {\n\t\t\tr0 = ret.Get(0).(*kingpin.CmdClause)\n\t\t}\n\t}\n\n\treturn r0\n}"}, {"instruction": "// newTerminalPrompter creates a liner based user input prompter working off the\n// standard input and output streams.", "input": "go language", "output": "func newTerminalPrompter() *terminalPrompter {\n\tp := new(terminalPrompter)\n\t// Get the original mode before calling NewLiner.\n\t// This is usually regular \"cooked\" mode where characters echo.\n\tnormalMode, _ := liner.TerminalMode()\n\t// Turn on liner. It switches to raw mode.\n\tp.State = liner.NewLiner()\n\trawMode, err := liner.TerminalMode()\n\tif err != nil || !liner.TerminalSupported() {\n\t\tp.supported = false\n\t} else {\n\t\tp.supported = true\n\t\tp.normalMode = normalMode\n\t\tp.rawMode = rawMode\n\t\t// Switch back to normal mode while we're not prompting.\n\t\tnormalMode.ApplyMode()\n\t}\n\tp.SetCtrlCAborts(true)\n\tp.SetTabCompletionStyle(liner.TabPrints)\n\tp.SetMultiLineMode(true)\n\treturn p\n}"}, {"instruction": "// canAdmitPod determines if a pod can be admitted, and gives a reason if it\n// cannot. \"pod\" is new pod, while \"pods\" are all admitted pods\n// The function returns a boolean value indicating whether the pod\n// can be admitted, a brief single-word reason and a message explaining why\n// the pod cannot be admitted.", "input": "go language", "output": "func (kl *Kubelet) canAdmitPod(pods []*v1.Pod, pod *v1.Pod) (bool, string, string) {\n\t// the kubelet will invoke each pod admit handler in sequence\n\t// if any handler rejects, the pod is rejected.\n\t// TODO: move out of disk check into a pod admitter\n\t// TODO: out of resource eviction should have a pod admitter call-out\n\tattrs := &lifecycle.PodAdmitAttributes{Pod: pod, OtherPods: pods}\n\tfor _, podAdmitHandler := range kl.admitHandlers {\n\t\tif result := podAdmitHandler.Admit(attrs); !result.Admit {\n\t\t\treturn false, result.Reason, result.Message\n\t\t}\n\t}\n\n\treturn true, \"\", \"\"\n}"}, {"instruction": "// ParseResponse parses the given response about the given channel", "input": "go language", "output": "func (parser *PeerResponseParser) ParseResponse(channel string, res ServiceResponse) error {\n\tvar listPeers peerLister\n\tif channel == \"\" {\n\t\tlistPeers = res.ForLocal()\n\t} else {\n\t\tlistPeers = &simpleChannelResponse{res.ForChannel(channel)}\n\t}\n\tpeers, err := listPeers.Peers()\n\tif err != nil {\n\t\treturn err\n\t}\n\n\tchannelState := channel != \"\"\n\tb, _ := json.MarshalIndent(assemblePeers(peers, channelState), \"\", \"\\t\")\n\tfmt.Fprintln(parser.Writer, string(b))\n\treturn nil\n}"}, {"instruction": "// Due to the way cgo works this has to be in a separate file, as devmapper.go has\n// definitions in the cgo block, which is incompatible with using \"//export\"\n// DevmapperLogCallback exports the devmapper log callback for cgo. Note that\n// because we are using callbacks, this function will be called for *every* log\n// in libdm (even debug ones because there's no way of setting the verbosity\n// level for an external logging callback).\n//export DevmapperLogCallback", "input": "go language", "output": "func DevmapperLogCallback(level C.int, file *C.char, line, dmErrnoOrClass C.int, message *C.char) {\n\tmsg := C.GoString(message)\n\n\t// Track what errno libdm saw, because the library only gives us 0 or 1.\n\tif level < LogLevelDebug {\n\t\tif strings.Contains(msg, \"busy\") {\n\t\t\tdmSawBusy = true\n\t\t}\n\n\t\tif strings.Contains(msg, \"File exists\") {\n\t\t\tdmSawExist = true\n\t\t}\n\n\t\tif strings.Contains(msg, \"No such device or address\") {\n\t\t\tdmSawEnxio = true\n\t\t}\n\t\tif strings.Contains(msg, \"No data available\") {\n\t\t\tdmSawEnoData = true\n\t\t}\n\t}\n\n\tif dmLogger != nil {\n\t\tdmLogger.DMLog(int(level), C.GoString(file), int(line), int(dmErrnoOrClass), msg)\n\t}\n}"}, {"instruction": "// Update takes the representation of a horizontalPodAutoscaler and updates it. Returns the server's representation of the horizontalPodAutoscaler, and an error, if there is any.", "input": "go language", "output": "func (c *FakeHorizontalPodAutoscalers) Update(horizontalPodAutoscaler *v2beta1.HorizontalPodAutoscaler) (result *v2beta1.HorizontalPodAutoscaler, err error) {\n\tobj, err := c.Fake.\n\t\tInvokes(testing.NewUpdateAction(horizontalpodautoscalersResource, c.ns, horizontalPodAutoscaler), &v2beta1.HorizontalPodAutoscaler{})\n\n\tif obj == nil {\n\t\treturn nil, err\n\t}\n\treturn obj.(*v2beta1.HorizontalPodAutoscaler), err\n}"}, {"instruction": "// DidDisconnect tracks the fact that the \"one\" node disconnected from the\n// \"other\" node", "input": "go language", "output": "func (net *Network) DidDisconnect(one, other enode.ID) error {\n\tnet.lock.Lock()\n\tdefer net.lock.Unlock()\n\tconn := net.getConn(one, other)\n\tif conn == nil {\n\t\treturn fmt.Errorf(\"connection between %v and %v does not exist\", one, other)\n\t}\n\tif !conn.Up {\n\t\treturn fmt.Errorf(\"%v and %v already disconnected\", one, other)\n\t}\n\tconn.Up = false\n\tconn.initiated = time.Now().Add(-DialBanTimeout)\n\tnet.events.Send(NewEvent(conn))\n\treturn nil\n}"}, {"instruction": "// ServerGroups returns the supported groups, with information like supported\n// versions and the preferred version.", "input": "go language", "output": "func (c *FakeDiscovery) ServerGroups() (*metav1.APIGroupList, error) {\n\taction := testing.ActionImpl{\n\t\tVerb:     \"get\",\n\t\tResource: schema.GroupVersionResource{Resource: \"group\"},\n\t}\n\tc.Invokes(action, nil)\n\n\tgroups := map[string]*metav1.APIGroup{}\n\n\tfor _, res := range c.Resources {\n\t\tgv, err := schema.ParseGroupVersion(res.GroupVersion)\n\t\tif err != nil {\n\t\t\treturn nil, err\n\t\t}\n\t\tgroup := groups[gv.Group]\n\t\tif group == nil {\n\t\t\tgroup = &metav1.APIGroup{\n\t\t\t\tName: gv.Group,\n\t\t\t\tPreferredVersion: metav1.GroupVersionForDiscovery{\n\t\t\t\t\tGroupVersion: res.GroupVersion,\n\t\t\t\t\tVersion:      gv.Version,\n\t\t\t\t},\n\t\t\t}\n\t\t\tgroups[gv.Group] = group\n\t\t}\n\n\t\tgroup.Versions = append(group.Versions, metav1.GroupVersionForDiscovery{\n\t\t\tGroupVersion: res.GroupVersion,\n\t\t\tVersion:      gv.Version,\n\t\t})\n\t}\n\n\tlist := &metav1.APIGroupList{}\n\tfor _, apiGroup := range groups {\n\t\tlist.Groups = append(list.Groups, *apiGroup)\n\t}\n\n\treturn list, nil\n\n}"}, {"instruction": "// getCgroupPath gets the file path to the \"devices\" subsystem of the desired cgroup.\n// cgroupPath is the path in the cgroup hierarchy.", "input": "go language", "output": "func getCgroupPath(cgroupPath string) (string, error) {\n\tcgroupPath = libcontainerutils.CleanPath(cgroupPath)\n\n\tmnt, root, err := libcontainercgroups.FindCgroupMountpointAndRoot(\"devices\")\n\t// If we didn't mount the subsystem, there is no point we make the path.\n\tif err != nil {\n\t\treturn \"\", err\n\t}\n\n\t// If the cgroup name/path is absolute do not look relative to the cgroup of the init process.\n\tif filepath.IsAbs(cgroupPath) {\n\t\t// Sometimes subsystems can be mounted together as 'cpu,cpuacct'.\n\t\treturn filepath.Join(root, mnt, cgroupPath), nil\n\t}\n\n\tparentPath, err := getCgroupParentPath(mnt, root)\n\tif err != nil {\n\t\treturn \"\", err\n\t}\n\n\treturn filepath.Join(parentPath, cgroupPath), nil\n}"}, {"instruction": "// servicesWatch is used to watch the list of available services", "input": "go language", "output": "func servicesWatch(params map[string]interface{}) (WatcherFunc, error) {\n\tstale := false\n\tif err := assignValueBool(params, \"stale\", &stale); err != nil {\n\t\treturn nil, err\n\t}\n\n\tfn := func(p *Plan) (BlockingParamVal, interface{}, error) {\n\t\tcatalog := p.client.Catalog()\n\t\topts := makeQueryOptionsWithContext(p, stale)\n\t\tdefer p.cancelFunc()\n\t\tservices, meta, err := catalog.Services(&opts)\n\t\tif err != nil {\n\t\t\treturn nil, nil, err\n\t\t}\n\t\treturn WaitIndexVal(meta.LastIndex), services, err\n\t}\n\treturn fn, nil\n}"}, {"instruction": "// Patch applies the patch and returns the patched controllerRevision.", "input": "go language", "output": "func (c *FakeControllerRevisions) Patch(name string, pt types.PatchType, data []byte, subresources ...string) (result *v1beta2.ControllerRevision, err error) {\n\tobj, err := c.Fake.\n\t\tInvokes(testing.NewPatchSubresourceAction(controllerrevisionsResource, c.ns, name, pt, data, subresources...), &v1beta2.ControllerRevision{})\n\n\tif obj == nil {\n\t\treturn nil, err\n\t}\n\treturn obj.(*v1beta2.ControllerRevision), err\n}"}, {"instruction": "// DeepCopyInto is an autogenerated deepcopy function, copying the receiver, writing into out. in must be non-nil.", "input": "go language", "output": "func (in *HorizontalPodAutoscalerSpec) DeepCopyInto(out *HorizontalPodAutoscalerSpec) {\n\t*out = *in\n\tout.ScaleTargetRef = in.ScaleTargetRef\n\tif in.MinReplicas != nil {\n\t\tin, out := &in.MinReplicas, &out.MinReplicas\n\t\t*out = new(int32)\n\t\t**out = **in\n\t}\n\tif in.TargetCPUUtilizationPercentage != nil {\n\t\tin, out := &in.TargetCPUUtilizationPercentage, &out.TargetCPUUtilizationPercentage\n\t\t*out = new(int32)\n\t\t**out = **in\n\t}\n\treturn\n}"}, {"instruction": "// A Zero date is a signal that the name can not be parsed.\n// This follows the format as outlined in Jekyll, https://jekyllrb.com/docs/posts/:\n// \"Where YEAR is a four-digit number, MONTH and DAY are both two-digit numbers\"", "input": "go language", "output": "func dateAndSlugFromBaseFilename(name string) (time.Time, string) {\n\twithoutExt, _ := helpers.FileAndExt(name)\n\n\tif len(withoutExt) < 10 {\n\t\t// This can not be a date.\n\t\treturn time.Time{}, \"\"\n\t}\n\n\t// Note: Hugo currently have no custom timezone support.\n\t// We will have to revisit this when that is in place.\n\td, err := time.Parse(\"2006-01-02\", withoutExt[:10])\n\tif err != nil {\n\t\treturn time.Time{}, \"\"\n\t}\n\n\t// Be a little lenient with the format here.\n\tslug := strings.Trim(withoutExt[10:], \" -_\")\n\n\treturn d, slug\n}"}, {"instruction": "// AddMethodMapping adds a method to a template function namespace.", "input": "go language", "output": "func (t *TemplateFuncsNamespace) AddMethodMapping(m interface{}, aliases []string, examples [][2]string) {\n\tif t.MethodMappings == nil {\n\t\tt.MethodMappings = make(map[string]TemplateFuncMethodMapping)\n\t}\n\n\tname := methodToName(m)\n\n\t// sanity check\n\tfor _, e := range examples {\n\t\tif e[0] == \"\" {\n\t\t\tpanic(t.Name + \": Empty example for \" + name)\n\t\t}\n\t}\n\tfor _, a := range aliases {\n\t\tif a == \"\" {\n\t\t\tpanic(t.Name + \": Empty alias for \" + name)\n\t\t}\n\t}\n\n\tt.MethodMappings[name] = TemplateFuncMethodMapping{\n\t\tMethod:   m,\n\t\tAliases:  aliases,\n\t\tExamples: examples,\n\t}\n\n}"}, {"instruction": "// ToleratesTaint checks if the toleration tolerates the taint.\n// The matching follows the rules below:\n// (1) Empty toleration.effect means to match all taint effects,\n//     otherwise taint effect must equal to toleration.effect.\n// (2) If toleration.operator is 'Exists', it means to match all taint values.\n// (3) Empty toleration.key means to match all taint keys.\n//     If toleration.key is empty, toleration.operator must be 'Exists';\n//     this combination means to match all taint values and all taint keys.", "input": "go language", "output": "func (t *Toleration) ToleratesTaint(taint *Taint) bool {\n\tif len(t.Effect) > 0 && t.Effect != taint.Effect {\n\t\treturn false\n\t}\n\n\tif len(t.Key) > 0 && t.Key != taint.Key {\n\t\treturn false\n\t}\n\n\t// TODO: Use proper defaulting when Toleration becomes a field of PodSpec\n\tswitch t.Operator {\n\t// empty operator means Equal\n\tcase \"\", TolerationOpEqual:\n\t\treturn t.Value == taint.Value\n\tcase TolerationOpExists:\n\t\treturn true\n\tdefault:\n\t\treturn false\n\t}\n}"}, {"instruction": "// GetResourcesAndPairs retrieves resources and \"KEY=VALUE or KEY-\" pair args from given args", "input": "go language", "output": "func GetResourcesAndPairs(args []string, pairType string) (resources []string, pairArgs []string, err error) {\n\tfoundPair := false\n\tfor _, s := range args {\n\t\tnonResource := (strings.Contains(s, \"=\") && s[0] != '=') || (strings.HasSuffix(s, \"-\") && s != \"-\")\n\t\tswitch {\n\t\tcase !foundPair && nonResource:\n\t\t\tfoundPair = true\n\t\t\tfallthrough\n\t\tcase foundPair && nonResource:\n\t\t\tpairArgs = append(pairArgs, s)\n\t\tcase !foundPair && !nonResource:\n\t\t\tresources = append(resources, s)\n\t\tcase foundPair && !nonResource:\n\t\t\terr = fmt.Errorf(\"all resources must be specified before %s changes: %s\", pairType, s)\n\t\t\treturn\n\t\t}\n\t}\n\treturn\n}"}, {"instruction": "// NewFilteredPriorityClassInformer constructs a new informer for PriorityClass type.\n// Always prefer using an informer factory to get a shared informer instead of getting an independent\n// one. This reduces memory footprint and number of connections to the server.", "input": "go language", "output": "func NewFilteredPriorityClassInformer(client kubernetes.Interface, resyncPeriod time.Duration, indexers cache.Indexers, tweakListOptions internalinterfaces.TweakListOptionsFunc) cache.SharedIndexInformer {\n\treturn cache.NewSharedIndexInformer(\n\t\t&cache.ListWatch{\n\t\t\tListFunc: func(options v1.ListOptions) (runtime.Object, error) {\n\t\t\t\tif tweakListOptions != nil {\n\t\t\t\t\ttweakListOptions(&options)\n\t\t\t\t}\n\t\t\t\treturn client.SchedulingV1alpha1().PriorityClasses().List(options)\n\t\t\t},\n\t\t\tWatchFunc: func(options v1.ListOptions) (watch.Interface, error) {\n\t\t\t\tif tweakListOptions != nil {\n\t\t\t\t\ttweakListOptions(&options)\n\t\t\t\t}\n\t\t\t\treturn client.SchedulingV1alpha1().PriorityClasses().Watch(options)\n\t\t\t},\n\t\t},\n\t\t&schedulingv1alpha1.PriorityClass{},\n\t\tresyncPeriod,\n\t\tindexers,\n\t)\n}"}, {"instruction": "// RecommendedDefaultVolumeConfiguration defaults a pointer to a VolumeConfiguration\n// struct. This will set the recommended default values, but they may be subject to\n// change between API versions. This function is intentionally not registered in the\n// scheme as a \"normal\" `SetDefaults_Foo` function to allow consumers of this type to\n// set whatever defaults for their embedded configs. Forcing consumers to use these\n// defaults would be problematic as defaulting in the scheme is done as part of the\n// conversion, and there would be no easy way to opt-out. Instead, if you want to use\n// this defaulting method run it in your wrapper struct of this type in its `SetDefaults_` method.", "input": "go language", "output": "func RecommendedDefaultVolumeConfiguration(obj *kubectrlmgrconfigv1alpha1.VolumeConfiguration) {\n\tif obj.EnableHostPathProvisioning == nil {\n\t\tobj.EnableHostPathProvisioning = utilpointer.BoolPtr(false)\n\t}\n\tif obj.EnableDynamicProvisioning == nil {\n\t\tobj.EnableDynamicProvisioning = utilpointer.BoolPtr(true)\n\t}\n\tif obj.FlexVolumePluginDir == \"\" {\n\t\tobj.FlexVolumePluginDir = \"/usr/libexec/kubernetes/kubelet-plugins/volume/exec/\"\n\t}\n\t// Use the default PersistentVolumeRecyclerConfiguration options.\n\tRecommendedDefaultPersistentVolumeRecyclerConfiguration(&obj.PersistentVolumeRecyclerConfiguration)\n}"}, {"instruction": "// RegisterEthService adds an Ethereum client to the stack.", "input": "go language", "output": "func RegisterEthService(stack *node.Node, cfg *eth.Config) {\n\tvar err error\n\tif cfg.SyncMode == downloader.LightSync {\n\t\terr = stack.Register(func(ctx *node.ServiceContext) (node.Service, error) {\n\t\t\treturn les.New(ctx, cfg)\n\t\t})\n\t} else {\n\t\terr = stack.Register(func(ctx *node.ServiceContext) (node.Service, error) {\n\t\t\tfullNode, err := eth.New(ctx, cfg)\n\t\t\tif fullNode != nil && cfg.LightServ > 0 {\n\t\t\t\tls, _ := les.NewLesServer(fullNode, cfg)\n\t\t\t\tfullNode.AddLesServer(ls)\n\t\t\t}\n\t\t\treturn fullNode, err\n\t\t})\n\t}\n\tif err != nil {\n\t\tFatalf(\"Failed to register the Ethereum service: %v\", err)\n\t}\n}"}, {"instruction": "// oldPodsRunning returns whether there are old pods running or any of the old ReplicaSets thinks that it runs pods.", "input": "go language", "output": "func oldPodsRunning(newRS *apps.ReplicaSet, oldRSs []*apps.ReplicaSet, podMap map[types.UID]*v1.PodList) bool {\n\tif oldPods := util.GetActualReplicaCountForReplicaSets(oldRSs); oldPods > 0 {\n\t\treturn true\n\t}\n\tfor rsUID, podList := range podMap {\n\t\t// If the pods belong to the new ReplicaSet, ignore.\n\t\tif newRS != nil && newRS.UID == rsUID {\n\t\t\tcontinue\n\t\t}\n\t\tfor _, pod := range podList.Items {\n\t\t\tswitch pod.Status.Phase {\n\t\t\tcase v1.PodFailed, v1.PodSucceeded:\n\t\t\t\t// Don't count pods in terminal state.\n\t\t\t\tcontinue\n\t\t\tcase v1.PodUnknown:\n\t\t\t\t// This happens in situation like when the node is temporarily disconnected from the cluster.\n\t\t\t\t// If we can't be sure that the pod is not running, we have to count it.\n\t\t\t\treturn true\n\t\t\tdefault:\n\t\t\t\t// Pod is not in terminal phase.\n\t\t\t\treturn true\n\t\t\t}\n\t\t}\n\t}\n\treturn false\n}"}, {"instruction": "// validatePSPSupplementalGroup validates the SupplementalGroupsStrategyOptions fields of the PodSecurityPolicy.", "input": "go language", "output": "func validatePSPSupplementalGroup(fldPath *field.Path, groupOptions *policy.SupplementalGroupsStrategyOptions) field.ErrorList {\n\tallErrs := field.ErrorList{}\n\n\tsupportedRules := sets.NewString(\n\t\tstring(policy.SupplementalGroupsStrategyRunAsAny),\n\t\tstring(policy.SupplementalGroupsStrategyMayRunAs),\n\t\tstring(policy.SupplementalGroupsStrategyMustRunAs),\n\t)\n\tif !supportedRules.Has(string(groupOptions.Rule)) {\n\t\tallErrs = append(allErrs, field.NotSupported(fldPath.Child(\"rule\"), groupOptions.Rule, supportedRules.List()))\n\t}\n\n\tfor idx, rng := range groupOptions.Ranges {\n\t\tallErrs = append(allErrs, validateGroupIDRange(fldPath.Child(\"ranges\").Index(idx), rng)...)\n\t}\n\treturn allErrs\n}"}, {"instruction": "// ConfigList returns the list of configs.", "input": "go language", "output": "func (cli *Client) ConfigList(ctx context.Context, options types.ConfigListOptions) ([]swarm.Config, error) {\n\tif err := cli.NewVersionError(\"1.30\", \"config list\"); err != nil {\n\t\treturn nil, err\n\t}\n\tquery := url.Values{}\n\n\tif options.Filters.Len() > 0 {\n\t\tfilterJSON, err := filters.ToJSON(options.Filters)\n\t\tif err != nil {\n\t\t\treturn nil, err\n\t\t}\n\n\t\tquery.Set(\"filters\", filterJSON)\n\t}\n\n\tresp, err := cli.get(ctx, \"/configs\", query, nil)\n\tdefer ensureReaderClosed(resp)\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\n\tvar configs []swarm.Config\n\terr = json.NewDecoder(resp.body).Decode(&configs)\n\treturn configs, err\n}"}, {"instruction": "// DefaultVersionedAPIPathFor constructs the default path for the given group version, assuming the given\n// API path, following the standard conventions of the Kubernetes API.", "input": "go language", "output": "func DefaultVersionedAPIPath(apiPath string, groupVersion schema.GroupVersion) string {\n\tversionedAPIPath := path.Join(\"/\", apiPath)\n\n\t// Add the version to the end of the path\n\tif len(groupVersion.Group) > 0 {\n\t\tversionedAPIPath = path.Join(versionedAPIPath, groupVersion.Group, groupVersion.Version)\n\n\t} else {\n\t\tversionedAPIPath = path.Join(versionedAPIPath, groupVersion.Version)\n\t}\n\n\treturn versionedAPIPath\n}"}, {"instruction": "// Update implements Aggregation interface.", "input": "go language", "output": "func (mmf *maxMinFunction) Update(evalCtx *AggEvaluateContext, sc *stmtctx.StatementContext, row chunk.Row) error {\n\ta := mmf.Args[0]\n\tvalue, err := a.Eval(row)\n\tif err != nil {\n\t\treturn err\n\t}\n\tif evalCtx.Value.IsNull() {\n\t\tevalCtx.Value = *(&value).Copy()\n\t}\n\tif value.IsNull() {\n\t\treturn nil\n\t}\n\tvar c int\n\tc, err = evalCtx.Value.CompareDatum(sc, &value)\n\tif err != nil {\n\t\treturn err\n\t}\n\tif (mmf.isMax && c == -1) || (!mmf.isMax && c == 1) {\n\t\tevalCtx.Value = *(&value).Copy()\n\t}\n\treturn nil\n}"}, {"instruction": "// ReloadGlobalConfig reloads global configuration for this server.", "input": "go language", "output": "func ReloadGlobalConfig() error {\n\tconfReloadLock.Lock()\n\tdefer confReloadLock.Unlock()\n\n\tnc := NewConfig()\n\tif err := nc.Load(reloadConfPath); err != nil {\n\t\treturn err\n\t}\n\tif err := nc.Valid(); err != nil {\n\t\treturn err\n\t}\n\tc := GetGlobalConfig()\n\n\tdiffs := collectsDiff(*nc, *c, \"\")\n\tif len(diffs) == 0 {\n\t\treturn nil\n\t}\n\tvar formattedDiff bytes.Buffer\n\tfor k, vs := range diffs {\n\t\tformattedDiff.WriteString(fmt.Sprintf(\", %v:%v->%v\", k, vs[1], vs[0]))\n\t}\n\tunsupported := make([]string, 0, 2)\n\tfor k := range diffs {\n\t\tif _, ok := supportedReloadConfigs[k]; !ok {\n\t\t\tunsupported = append(unsupported, k)\n\t\t}\n\t}\n\tif len(unsupported) > 0 {\n\t\treturn fmt.Errorf(\"reloading config %v is not supported, only %v are supported now, \"+\n\t\t\t\"your changes%s\", unsupported, supportedReloadConfList, formattedDiff.String())\n\t}\n\n\tconfReloader(nc, c)\n\tglobalConf.Store(nc)\n\tlogutil.Logger(context.Background()).Info(\"reload config changes\" + formattedDiff.String())\n\treturn nil\n}"}, {"instruction": "// buildTargetGroupName will build unique name for targetGroup of service & port.\n// the name is in format k8s-{namespace:8}-{name:8}-{uuid:10} (chosen to benefit most common use cases).\n// Note: targetProtocol & targetType are included since they cannot be modified on existing targetGroup.", "input": "go language", "output": "func (c *Cloud) buildTargetGroupName(serviceName types.NamespacedName, servicePort int64, targetProtocol string, targetType string) string {\n\thasher := sha1.New()\n\t_, _ = hasher.Write([]byte(c.tagging.clusterID()))\n\t_, _ = hasher.Write([]byte(serviceName.Namespace))\n\t_, _ = hasher.Write([]byte(serviceName.Name))\n\t_, _ = hasher.Write([]byte(strconv.FormatInt(servicePort, 10)))\n\t_, _ = hasher.Write([]byte(targetProtocol))\n\t_, _ = hasher.Write([]byte(targetType))\n\ttgUUID := hex.EncodeToString(hasher.Sum(nil))\n\n\tsanitizedNamespace := invalidELBV2NameRegex.ReplaceAllString(serviceName.Namespace, \"\")\n\tsanitizedServiceName := invalidELBV2NameRegex.ReplaceAllString(serviceName.Name, \"\")\n\treturn fmt.Sprintf(\"k8s-%.8s-%.8s-%.10s\", sanitizedNamespace, sanitizedServiceName, tgUUID)\n}"}, {"instruction": "// StmtCommit implements the sessionctx.Context interface.", "input": "go language", "output": "func (s *session) StmtCommit() error {\n\tdefer s.txn.cleanup()\n\tst := &s.txn\n\tvar count int\n\terr := kv.WalkMemBuffer(st.buf, func(k kv.Key, v []byte) error {\n\t\tfailpoint.Inject(\"mockStmtCommitError\", func(val failpoint.Value) {\n\t\t\tif val.(bool) {\n\t\t\t\tcount++\n\t\t\t}\n\t\t})\n\n\t\tif count > 3 {\n\t\t\treturn errors.New(\"mock stmt commit error\")\n\t\t}\n\n\t\tif len(v) == 0 {\n\t\t\treturn st.Transaction.Delete(k)\n\t\t}\n\t\treturn st.Transaction.Set(k, v)\n\t})\n\tif err != nil {\n\t\tst.doNotCommit = err\n\t\treturn err\n\t}\n\n\t// Need to flush binlog.\n\tfor tableID, delta := range st.mutations {\n\t\tmutation := getBinlogMutation(s, tableID)\n\t\tmergeToMutation(mutation, delta)\n\t}\n\n\tif len(st.dirtyTableOP) > 0 {\n\t\tdirtyDB := executor.GetDirtyDB(s)\n\t\tfor _, op := range st.dirtyTableOP {\n\t\t\tmergeToDirtyDB(dirtyDB, op)\n\t\t}\n\t}\n\treturn nil\n}"}, {"instruction": "// DeprecateBackend can be used to wrap a backend to retrun a deprecation\n// warning during validation.", "input": "go language", "output": "func deprecateBackend(b backend.Backend, message string) backend.Backend {\n\t// Since a Backend wrapped by deprecatedBackendShim can no longer be\n\t// asserted as an Enhanced or Local backend, disallow those types here\n\t// entirely.  If something other than a basic backend.Backend needs to be\n\t// deprecated, we can add that functionality to schema.Backend or the\n\t// backend itself.\n\tif _, ok := b.(backend.Enhanced); ok {\n\t\tpanic(\"cannot use DeprecateBackend on an Enhanced Backend\")\n\t}\n\n\tif _, ok := b.(backend.Local); ok {\n\t\tpanic(\"cannot use DeprecateBackend on a Local Backend\")\n\t}\n\n\treturn deprecatedBackendShim{\n\t\tBackend: b,\n\t\tMessage: message,\n\t}\n}"}, {"instruction": "// Collect implements prometheus.Collector\n// Since new containers are frequently created and removed, using the prometheus.Gauge Collector would\n// leak metric collectors for containers or pods that no longer exist.  Instead, implement\n// prometheus.Collector in a way that only collects metrics for active containers.", "input": "go language", "output": "func (rc *resourceMetricCollector) Collect(ch chan<- prometheus.Metric) {\n\trc.errors.Set(0)\n\tdefer rc.errors.Collect(ch)\n\tsummary, err := rc.provider.GetCPUAndMemoryStats()\n\tif err != nil {\n\t\trc.errors.Set(1)\n\t\tklog.Warningf(\"Error getting summary for resourceMetric prometheus endpoint: %v\", err)\n\t\treturn\n\t}\n\n\tfor _, metric := range rc.config.NodeMetrics {\n\t\tif value, timestamp := metric.ValueFn(summary.Node); value != nil {\n\t\t\tch <- prometheus.NewMetricWithTimestamp(timestamp,\n\t\t\t\tprometheus.MustNewConstMetric(metric.desc(), prometheus.GaugeValue, *value))\n\t\t}\n\t}\n\n\tfor _, pod := range summary.Pods {\n\t\tfor _, container := range pod.Containers {\n\t\t\tfor _, metric := range rc.config.ContainerMetrics {\n\t\t\t\tif value, timestamp := metric.ValueFn(container); value != nil {\n\t\t\t\t\tch <- prometheus.NewMetricWithTimestamp(timestamp,\n\t\t\t\t\t\tprometheus.MustNewConstMetric(metric.desc(), prometheus.GaugeValue, *value, container.Name, pod.PodRef.Name, pod.PodRef.Namespace))\n\t\t\t\t}\n\t\t\t}\n\t\t}\n\t}\n}"}, {"instruction": "// CreateLocalEtcdStaticPodManifestFile will write local etcd static pod manifest file.\n// This function is used by init - when the etcd cluster is empty - or by kubeadm\n// upgrade - when the etcd cluster is already up and running (and the --initial-cluster flag have no impact)", "input": "go language", "output": "func CreateLocalEtcdStaticPodManifestFile(manifestDir string, nodeName string, cfg *kubeadmapi.ClusterConfiguration, endpoint *kubeadmapi.APIEndpoint) error {\n\tif cfg.Etcd.External != nil {\n\t\treturn errors.New(\"etcd static pod manifest cannot be generated for cluster using external etcd\")\n\t}\n\t// gets etcd StaticPodSpec\n\tspec := GetEtcdPodSpec(cfg, endpoint, nodeName, []etcdutil.Member{})\n\n\t// writes etcd StaticPod to disk\n\tif err := staticpodutil.WriteStaticPodToDisk(kubeadmconstants.Etcd, manifestDir, spec); err != nil {\n\t\treturn err\n\t}\n\n\tklog.V(1).Infof(\"[etcd] wrote Static Pod manifest for a local etcd member to %q\\n\", kubeadmconstants.GetStaticPodFilepath(kubeadmconstants.Etcd, manifestDir))\n\treturn nil\n}"}, {"instruction": "// FindServer takes out an internal \"read lock\" and searches through the list\n// of servers to find a \"healthy\" server.  If the server is actually\n// unhealthy, we rely on Serf to detect this and remove the node from the\n// server list.  If the server at the front of the list has failed or fails\n// during an RPC call, it is rotated to the end of the list.  If there are no\n// servers available, return nil.", "input": "go language", "output": "func (m *Manager) FindServer() *metadata.Server {\n\tl := m.getServerList()\n\tnumServers := len(l.servers)\n\tif numServers == 0 {\n\t\tm.logger.Printf(\"[WARN] manager: No servers available\")\n\t\treturn nil\n\t}\n\n\t// Return whatever is at the front of the list because it is\n\t// assumed to be the oldest in the server list (unless -\n\t// hypothetically - the server list was rotated right after a\n\t// server was added).\n\treturn l.servers[0]\n}"}, {"instruction": "// CompareKubeAwareVersionStrings compares two kube-like version strings.\n// Kube-like version strings are starting with a v, followed by a major version, optional \"alpha\" or \"beta\" strings\n// followed by a minor version (e.g. v1, v2beta1). Versions will be sorted based on GA/alpha/beta first and then major\n// and minor versions. e.g. v2, v1, v1beta2, v1beta1, v1alpha1.", "input": "go language", "output": "func CompareKubeAwareVersionStrings(v1, v2 string) int {\n\tif v1 == v2 {\n\t\treturn 0\n\t}\n\tv1major, v1type, v1minor, ok1 := parseKubeVersion(v1)\n\tv2major, v2type, v2minor, ok2 := parseKubeVersion(v2)\n\tswitch {\n\tcase !ok1 && !ok2:\n\t\treturn strings.Compare(v2, v1)\n\tcase !ok1 && ok2:\n\t\treturn -1\n\tcase ok1 && !ok2:\n\t\treturn 1\n\t}\n\tif v1type != v2type {\n\t\treturn int(v1type) - int(v2type)\n\t}\n\tif v1major != v2major {\n\t\treturn v1major - v2major\n\t}\n\treturn v1minor - v2minor\n}"}, {"instruction": "// Less compares two values in a byTimestamp slice by Timestamp.  Less is\n// required by the sort.Interface interface.", "input": "go language", "output": "func (slice byTimestamp) Less(i, j int) bool {\n\tiTimestamp, jTimestamp := int64(0), int64(0)\n\tif slice != nil && slice[i].inputLogEvent.Timestamp != nil {\n\t\tiTimestamp = *slice[i].inputLogEvent.Timestamp\n\t}\n\tif slice != nil && slice[j].inputLogEvent.Timestamp != nil {\n\t\tjTimestamp = *slice[j].inputLogEvent.Timestamp\n\t}\n\tif iTimestamp == jTimestamp {\n\t\treturn slice[i].insertOrder < slice[j].insertOrder\n\t}\n\treturn iTimestamp < jTimestamp\n}"}, {"instruction": "// UpstreamResolverFuncFromClient returns a closure that captures a consul\n// client and when called provides a ConsulResolver that can resolve the given\n// UpstreamConfig using the provided api.Client dependency.", "input": "go language", "output": "func UpstreamResolverFuncFromClient(client *api.Client) func(cfg UpstreamConfig) (connect.Resolver, error) {\n\treturn func(cfg UpstreamConfig) (connect.Resolver, error) {\n\t\t// For now default to service as it has the most natural meaning and the error\n\t\t// that the service doesn't exist is probably reasonable if misconfigured. We\n\t\t// should probably handle actual configs that have invalid types at a higher\n\t\t// level anyway (like when parsing).\n\t\ttyp := connect.ConsulResolverTypeService\n\t\tif cfg.DestinationType == \"prepared_query\" {\n\t\t\ttyp = connect.ConsulResolverTypePreparedQuery\n\t\t}\n\t\treturn &connect.ConsulResolver{\n\t\t\tClient:     client,\n\t\t\tNamespace:  cfg.DestinationNamespace,\n\t\t\tName:       cfg.DestinationName,\n\t\t\tType:       typ,\n\t\t\tDatacenter: cfg.Datacenter,\n\t\t}, nil\n\t}\n}"}, {"instruction": "// convertToRuntimeCapabilities converts v1.Capabilities to runtimeapi.Capability.", "input": "go language", "output": "func convertToRuntimeCapabilities(opts *v1.Capabilities) *runtimeapi.Capability {\n\tif opts == nil {\n\t\treturn nil\n\t}\n\n\tcapabilities := &runtimeapi.Capability{\n\t\tAddCapabilities:  make([]string, len(opts.Add)),\n\t\tDropCapabilities: make([]string, len(opts.Drop)),\n\t}\n\tfor index, value := range opts.Add {\n\t\tcapabilities.AddCapabilities[index] = string(value)\n\t}\n\tfor index, value := range opts.Drop {\n\t\tcapabilities.DropCapabilities[index] = string(value)\n\t}\n\n\treturn capabilities\n}"}, {"instruction": "// DeepCopyInto is an autogenerated deepcopy function, copying the receiver, writing into out. in must be non-nil.", "input": "go language", "output": "func (in *Headers) DeepCopyInto(out *Headers) {\n\t*out = *in\n\tif in.CustomRequestHeaders != nil {\n\t\tin, out := &in.CustomRequestHeaders, &out.CustomRequestHeaders\n\t\t*out = make(map[string]string, len(*in))\n\t\tfor key, val := range *in {\n\t\t\t(*out)[key] = val\n\t\t}\n\t}\n\tif in.CustomResponseHeaders != nil {\n\t\tin, out := &in.CustomResponseHeaders, &out.CustomResponseHeaders\n\t\t*out = make(map[string]string, len(*in))\n\t\tfor key, val := range *in {\n\t\t\t(*out)[key] = val\n\t\t}\n\t}\n\tif in.AllowedHosts != nil {\n\t\tin, out := &in.AllowedHosts, &out.AllowedHosts\n\t\t*out = make([]string, len(*in))\n\t\tcopy(*out, *in)\n\t}\n\tif in.HostsProxyHeaders != nil {\n\t\tin, out := &in.HostsProxyHeaders, &out.HostsProxyHeaders\n\t\t*out = make([]string, len(*in))\n\t\tcopy(*out, *in)\n\t}\n\tif in.SSLProxyHeaders != nil {\n\t\tin, out := &in.SSLProxyHeaders, &out.SSLProxyHeaders\n\t\t*out = make(map[string]string, len(*in))\n\t\tfor key, val := range *in {\n\t\t\t(*out)[key] = val\n\t\t}\n\t}\n\treturn\n}"}, {"instruction": "// NewHandle creates a Handle for update stats.", "input": "go language", "output": "func NewHandle(ctx sessionctx.Context, lease time.Duration) *Handle {\n\thandle := &Handle{\n\t\tddlEventCh: make(chan *util.Event, 100),\n\t\tlistHead:   &SessionStatsCollector{mapper: make(tableDeltaMap), rateMap: make(errorRateDeltaMap)},\n\t\tglobalMap:  make(tableDeltaMap),\n\t\tLease:      lease,\n\t\tfeedback:   make([]*statistics.QueryFeedback, 0, MaxQueryFeedbackCount.Load()),\n\t}\n\t// It is safe to use it concurrently because the exec won't touch the ctx.\n\tif exec, ok := ctx.(sqlexec.RestrictedSQLExecutor); ok {\n\t\thandle.restrictedExec = exec\n\t}\n\thandle.mu.ctx = ctx\n\thandle.mu.rateMap = make(errorRateDeltaMap)\n\thandle.StatsCache.Store(StatsCache{})\n\treturn handle\n}"}, {"instruction": "// The Vector type represents a version vector. The zero value is a usable\n// version vector. The vector has slice semantics and some operations on it\n// are \"append-like\" in that they may return the same vector modified, or v\n// new allocated Vector with the modified contents.\n// Counter represents a single counter in the version vector.\n// Update returns a Vector with the index for the specific ID incremented by\n// one. If it is possible, the vector v is updated and returned. If it is not,\n// a copy will be created, updated and returned.", "input": "go language", "output": "func (v Vector) Update(id ShortID) Vector {\n\tfor i := range v.Counters {\n\t\tif v.Counters[i].ID == id {\n\t\t\t// Update an existing index\n\t\t\tv.Counters[i].Value++\n\t\t\treturn v\n\t\t} else if v.Counters[i].ID > id {\n\t\t\t// Insert a new index\n\t\t\tnv := make([]Counter, len(v.Counters)+1)\n\t\t\tcopy(nv, v.Counters[:i])\n\t\t\tnv[i].ID = id\n\t\t\tnv[i].Value = 1\n\t\t\tcopy(nv[i+1:], v.Counters[i:])\n\t\t\treturn Vector{Counters: nv}\n\t\t}\n\t}\n\t// Append a new index\n\treturn Vector{Counters: append(v.Counters, Counter{ID: id, Value: 1})}\n}"}, {"instruction": "// ConfigInspectWithRaw returns the config information with raw data", "input": "go language", "output": "func (cli *Client) ConfigInspectWithRaw(ctx context.Context, id string) (swarm.Config, []byte, error) {\n\tif id == \"\" {\n\t\treturn swarm.Config{}, nil, objectNotFoundError{object: \"config\", id: id}\n\t}\n\tif err := cli.NewVersionError(\"1.30\", \"config inspect\"); err != nil {\n\t\treturn swarm.Config{}, nil, err\n\t}\n\tresp, err := cli.get(ctx, \"/configs/\"+id, nil, nil)\n\tdefer ensureReaderClosed(resp)\n\tif err != nil {\n\t\treturn swarm.Config{}, nil, wrapResponseError(err, resp, \"config\", id)\n\t}\n\n\tbody, err := ioutil.ReadAll(resp.body)\n\tif err != nil {\n\t\treturn swarm.Config{}, nil, err\n\t}\n\n\tvar config swarm.Config\n\trdr := bytes.NewReader(body)\n\terr = json.NewDecoder(rdr).Decode(&config)\n\n\treturn config, body, err\n}"}, {"instruction": "// parseMessageBlock", "input": "go language", "output": "func parseMessageBlock(data []byte) (*hapi.Metadata, *SumCollection, error) {\n\t// This sucks.\n\tparts := bytes.Split(data, []byte(\"\\n...\\n\"))\n\tif len(parts) < 2 {\n\t\treturn nil, nil, errors.New(\"message block must have at least two parts\")\n\t}\n\n\tmd := &hapi.Metadata{}\n\tsc := &SumCollection{}\n\n\tif err := yaml.Unmarshal(parts[0], md); err != nil {\n\t\treturn md, sc, err\n\t}\n\terr := yaml.Unmarshal(parts[1], sc)\n\treturn md, sc, err\n}"}, {"instruction": "// Normalize args convert multiple resources to resource tuples, a,b,c d\n// as a transform to a/d b/d c/d", "input": "go language", "output": "func normalizeMultipleResourcesArgs(args []string) []string {\n\tif len(args) >= 2 {\n\t\tresources := []string{}\n\t\tresources = append(resources, SplitResourceArgument(args[0])...)\n\t\tif len(resources) > 1 {\n\t\t\tnames := []string{}\n\t\t\tnames = append(names, args[1:]...)\n\t\t\tnewArgs := []string{}\n\t\t\tfor _, resource := range resources {\n\t\t\t\tfor _, name := range names {\n\t\t\t\t\tnewArgs = append(newArgs, strings.Join([]string{resource, name}, \"/\"))\n\t\t\t\t}\n\t\t\t}\n\t\t\treturn newArgs\n\t\t}\n\t}\n\treturn args\n}"}, {"instruction": "// ConfigCreate applies the given config entry update.", "input": "go language", "output": "func (s *HTTPServer) ConfigApply(resp http.ResponseWriter, req *http.Request) (interface{}, error) {\n\targs := structs.ConfigEntryRequest{\n\t\tOp: structs.ConfigEntryUpsert,\n\t}\n\ts.parseDC(req, &args.Datacenter)\n\ts.parseToken(req, &args.Token)\n\n\tvar raw map[string]interface{}\n\tif err := decodeBody(req, &raw, nil); err != nil {\n\t\treturn nil, BadRequestError{Reason: fmt.Sprintf(\"Request decoding failed: %v\", err)}\n\t}\n\n\tif entry, err := structs.DecodeConfigEntry(raw); err == nil {\n\t\targs.Entry = entry\n\t} else {\n\t\treturn nil, BadRequestError{Reason: fmt.Sprintf(\"Request decoding failed: %v\", err)}\n\t}\n\n\t// Check for cas value\n\tif casStr := req.URL.Query().Get(\"cas\"); casStr != \"\" {\n\t\tcasVal, err := strconv.ParseUint(casStr, 10, 64)\n\t\tif err != nil {\n\t\t\treturn nil, err\n\t\t}\n\t\targs.Op = structs.ConfigEntryUpsertCAS\n\t\targs.Entry.GetRaftIndex().ModifyIndex = casVal\n\t}\n\n\tvar reply bool\n\tif err := s.agent.RPC(\"ConfigEntry.Apply\", &args, &reply); err != nil {\n\t\treturn nil, err\n\t}\n\n\treturn reply, nil\n}"}, {"instruction": "// Dereference removes an existing reference from a root node.", "input": "go language", "output": "func (db *Database) Dereference(root common.Hash) {\n\t// Sanity check to ensure that the meta-root is not removed\n\tif root == (common.Hash{}) {\n\t\tlog.Error(\"Attempted to dereference the trie cache meta root\")\n\t\treturn\n\t}\n\tdb.lock.Lock()\n\tdefer db.lock.Unlock()\n\n\tnodes, storage, start := len(db.dirties), db.dirtiesSize, time.Now()\n\tdb.dereference(root, common.Hash{})\n\n\tdb.gcnodes += uint64(nodes - len(db.dirties))\n\tdb.gcsize += storage - db.dirtiesSize\n\tdb.gctime += time.Since(start)\n\n\tmemcacheGCTimeTimer.Update(time.Since(start))\n\tmemcacheGCSizeMeter.Mark(int64(storage - db.dirtiesSize))\n\tmemcacheGCNodesMeter.Mark(int64(nodes - len(db.dirties)))\n\n\tlog.Debug(\"Dereferenced trie from memory database\", \"nodes\", nodes-len(db.dirties), \"size\", storage-db.dirtiesSize, \"time\", time.Since(start),\n\t\t\"gcnodes\", db.gcnodes, \"gcsize\", db.gcsize, \"gctime\", db.gctime, \"livenodes\", len(db.dirties), \"livesize\", db.dirtiesSize)\n}"}, {"instruction": "// popOldEvents finds events that should be scheduled for scanning recursively in dirs,\n// removes those events and empty eventDirs and returns a map with all the removed\n// events referenced by their filesystem path", "input": "go language", "output": "func (a *aggregator) popOldEventsTo(to map[string]*aggregatedEvent, dir *eventDir, dirPath string, currTime time.Time, delayRem bool) {\n\tfor childName, childDir := range dir.dirs {\n\t\ta.popOldEventsTo(to, childDir, filepath.Join(dirPath, childName), currTime, delayRem)\n\t\tif childDir.childCount() == 0 {\n\t\t\tdelete(dir.dirs, childName)\n\t\t}\n\t}\n\tfor name, event := range dir.events {\n\t\tif a.isOld(event, currTime, delayRem) {\n\t\t\tto[filepath.Join(dirPath, name)] = event\n\t\t\tdelete(dir.events, name)\n\t\t\ta.counts[event.evType]--\n\t\t}\n\t}\n}"}, {"instruction": "// handleMultiplexV2 is used to multiplex a single incoming connection\n// using the Yamux multiplexer", "input": "go language", "output": "func (s *Server) handleMultiplexV2(conn net.Conn) {\n\tdefer conn.Close()\n\tconf := yamux.DefaultConfig()\n\tconf.LogOutput = s.config.LogOutput\n\tserver, _ := yamux.Server(conn, conf)\n\tfor {\n\t\tsub, err := server.Accept()\n\t\tif err != nil {\n\t\t\tif err != io.EOF {\n\t\t\t\ts.logger.Printf(\"[ERR] consul.rpc: multiplex conn accept failed: %v %s\", err, logConn(conn))\n\t\t\t}\n\t\t\treturn\n\t\t}\n\t\tgo s.handleConsulConn(sub)\n\t}\n}"}, {"instruction": "// convertToRuntimeSecurityContext converts v1.SecurityContext to runtimeapi.SecurityContext.", "input": "go language", "output": "func convertToRuntimeSecurityContext(securityContext *v1.SecurityContext) *runtimeapi.LinuxContainerSecurityContext {\n\tif securityContext == nil {\n\t\treturn nil\n\t}\n\n\tsc := &runtimeapi.LinuxContainerSecurityContext{\n\t\tCapabilities:   convertToRuntimeCapabilities(securityContext.Capabilities),\n\t\tSelinuxOptions: convertToRuntimeSELinuxOption(securityContext.SELinuxOptions),\n\t}\n\tif securityContext.RunAsUser != nil {\n\t\tsc.RunAsUser = &runtimeapi.Int64Value{Value: int64(*securityContext.RunAsUser)}\n\t}\n\tif securityContext.RunAsGroup != nil {\n\t\tsc.RunAsGroup = &runtimeapi.Int64Value{Value: int64(*securityContext.RunAsGroup)}\n\t}\n\tif securityContext.Privileged != nil {\n\t\tsc.Privileged = *securityContext.Privileged\n\t}\n\tif securityContext.ReadOnlyRootFilesystem != nil {\n\t\tsc.ReadonlyRootfs = *securityContext.ReadOnlyRootFilesystem\n\t}\n\n\treturn sc\n}"}, {"instruction": "// initPvtdataStoreFromExistingBlockchain updates the initial state of the pvtdata store\n// if an existing block store has a blockchain and the pvtdata store is empty.\n// This situation is expected to happen when a peer is upgrated from version 1.0\n// and an existing blockchain is present that was generated with version 1.0.\n// Under this scenario, the pvtdata store is brought upto the point as if it has\n// processed existing blocks with no pvt data. This function returns true if the\n// above mentioned condition is found to be true and pvtdata store is successfully updated", "input": "go language", "output": "func (s *Store) initPvtdataStoreFromExistingBlockchain() (bool, error) {\n\tvar bcInfo *common.BlockchainInfo\n\tvar pvtdataStoreEmpty bool\n\tvar err error\n\n\tif bcInfo, err = s.BlockStore.GetBlockchainInfo(); err != nil {\n\t\treturn false, err\n\t}\n\tif pvtdataStoreEmpty, err = s.pvtdataStore.IsEmpty(); err != nil {\n\t\treturn false, err\n\t}\n\tif pvtdataStoreEmpty && bcInfo.Height > 0 {\n\t\tif err = s.pvtdataStore.InitLastCommittedBlock(bcInfo.Height - 1); err != nil {\n\t\t\treturn false, err\n\t\t}\n\t\treturn true, nil\n\t}\n\treturn false, nil\n}"}, {"instruction": "// LoadConfig extract the KubeConfigFile from configFile", "input": "go language", "output": "func LoadConfig(configFile io.Reader) (string, error) {\n\tvar kubeconfigFile string\n\tif configFile != nil {\n\t\t// we have a config so parse it.\n\t\tdata, err := ioutil.ReadAll(configFile)\n\t\tif err != nil {\n\t\t\treturn \"\", err\n\t\t}\n\t\tdecoder := codecs.UniversalDecoder()\n\t\tdecodedObj, err := runtime.Decode(decoder, data)\n\t\tif err != nil {\n\t\t\treturn \"\", err\n\t\t}\n\t\tconfig, ok := decodedObj.(*webhookadmission.WebhookAdmission)\n\t\tif !ok {\n\t\t\treturn \"\", fmt.Errorf(\"unexpected type: %T\", decodedObj)\n\t\t}\n\n\t\tif !path.IsAbs(config.KubeConfigFile) {\n\t\t\treturn \"\", field.Invalid(field.NewPath(\"kubeConfigFile\"), config.KubeConfigFile, \"must be an absolute file path\")\n\t\t}\n\n\t\tkubeconfigFile = config.KubeConfigFile\n\t}\n\treturn kubeconfigFile, nil\n}"}, {"instruction": "// GetSecrets returns all secrets of a managed swarm cluster.", "input": "go language", "output": "func (c *Cluster) GetSecrets(options apitypes.SecretListOptions) ([]types.Secret, error) {\n\tc.mu.RLock()\n\tdefer c.mu.RUnlock()\n\n\tstate := c.currentNodeState()\n\tif !state.IsActiveManager() {\n\t\treturn nil, c.errNoManager(state)\n\t}\n\n\tfilters, err := newListSecretsFilters(options.Filters)\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\tctx, cancel := c.getRequestContext()\n\tdefer cancel()\n\n\tr, err := state.controlClient.ListSecrets(ctx,\n\t\t&swarmapi.ListSecretsRequest{Filters: filters})\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\n\tsecrets := make([]types.Secret, 0, len(r.Secrets))\n\n\tfor _, secret := range r.Secrets {\n\t\tsecrets = append(secrets, convert.SecretFromGRPC(secret))\n\t}\n\n\treturn secrets, nil\n}"}, {"instruction": "// GetDriver initializes and returns the registered driver", "input": "go language", "output": "func GetDriver(name string, pg plugingetter.PluginGetter, config Options) (Driver, error) {\n\tif initFunc, exists := drivers[name]; exists {\n\t\treturn initFunc(filepath.Join(config.Root, name), config.DriverOptions, config.UIDMaps, config.GIDMaps)\n\t}\n\n\tpluginDriver, err := lookupPlugin(name, pg, config)\n\tif err == nil {\n\t\treturn pluginDriver, nil\n\t}\n\tlogrus.WithError(err).WithField(\"driver\", name).WithField(\"home-dir\", config.Root).Error(\"Failed to GetDriver graph\")\n\treturn nil, ErrNotSupported\n}"}, {"instruction": "// DeleteVolumes removes the ScaleIO volume", "input": "go language", "output": "func (m *sioMgr) DeleteVolume(volName string) error {\n\tclient, err := m.getClient()\n\tif err != nil {\n\t\treturn err\n\t}\n\n\tvol, err := client.FindVolume(volName)\n\tif err != nil {\n\t\treturn err\n\t}\n\n\tif err := client.DeleteVolume(sioVolumeID(vol.ID)); err != nil {\n\t\tklog.Error(log(\"failed to delete volume %s: %v\", volName, err))\n\t\treturn err\n\t}\n\n\tklog.V(4).Info(log(\"deleted volume %s successfully\", volName))\n\treturn nil\n\n}"}, {"instruction": "// Get returns the overall codebase version. It's for detecting\n// what code a binary was built from.", "input": "go language", "output": "func Get() apimachineryversion.Info {\n\t// These variables typically come from -ldflags settings and in\n\t// their absence fallback to the settings in ./base.go\n\treturn apimachineryversion.Info{\n\t\tMajor:        gitMajor,\n\t\tMinor:        gitMinor,\n\t\tGitVersion:   gitVersion,\n\t\tGitCommit:    gitCommit,\n\t\tGitTreeState: gitTreeState,\n\t\tBuildDate:    buildDate,\n\t\tGoVersion:    runtime.Version(),\n\t\tCompiler:     runtime.Compiler,\n\t\tPlatform:     fmt.Sprintf(\"%s/%s\", runtime.GOOS, runtime.GOARCH),\n\t}\n}"}, {"instruction": "// modifiableCharsetAndCollation returns error when the charset or collation is not modifiable.", "input": "go language", "output": "func modifiableCharsetAndCollation(toCharset, toCollate, origCharset, origCollate string) error {\n\tif !charset.ValidCharsetAndCollation(toCharset, toCollate) {\n\t\treturn ErrUnknownCharacterSet.GenWithStack(\"Unknown character set: '%s', collation: '%s'\", toCharset, toCollate)\n\t}\n\tif toCharset == charset.CharsetUTF8MB4 && origCharset == charset.CharsetUTF8 {\n\t\t// TiDB only allow utf8 to be changed to utf8mb4.\n\t\treturn nil\n\t}\n\n\tif toCharset != origCharset {\n\t\tmsg := fmt.Sprintf(\"charset from %s to %s\", origCharset, toCharset)\n\t\treturn errUnsupportedModifyCharset.GenWithStackByArgs(msg)\n\t}\n\tif toCollate != origCollate {\n\t\tmsg := fmt.Sprintf(\"collate from %s to %s\", origCollate, toCollate)\n\t\treturn errUnsupportedModifyCharset.GenWithStackByArgs(msg)\n\t}\n\treturn nil\n}"}, {"instruction": "// determineSubnetURL queries for all subnetworks in a region for a given network and returns\n// the URL of the subnetwork which exists in the auto-subnet range.", "input": "go language", "output": "func determineSubnetURL(service *compute.Service, networkProjectID, networkName, region string) (string, error) {\n\tsubnets, err := listSubnetworksOfNetwork(service, networkProjectID, networkName, region)\n\tif err != nil {\n\t\treturn \"\", err\n\t}\n\n\tautoSubnets, err := subnetsInCIDR(subnets, autoSubnetIPRange)\n\tif err != nil {\n\t\treturn \"\", err\n\t}\n\n\tif len(autoSubnets) == 0 {\n\t\treturn \"\", fmt.Errorf(\"no subnet exists in auto CIDR\")\n\t}\n\n\tif len(autoSubnets) > 1 {\n\t\treturn \"\", fmt.Errorf(\"multiple subnetworks in the same region exist in auto CIDR\")\n\t}\n\n\treturn autoSubnets[0].SelfLink, nil\n}"}, {"instruction": "// TODO: Remove this when extensions/v1beta1 and apps/v1beta1 Deployment are dropped.", "input": "go language", "output": "func getRollbackTo(d *apps.Deployment) *extensions.RollbackConfig {\n\t// Extract the annotation used for round-tripping the deprecated RollbackTo field.\n\trevision := d.Annotations[apps.DeprecatedRollbackTo]\n\tif revision == \"\" {\n\t\treturn nil\n\t}\n\trevision64, err := strconv.ParseInt(revision, 10, 64)\n\tif err != nil {\n\t\t// If it's invalid, ignore it.\n\t\treturn nil\n\t}\n\treturn &extensions.RollbackConfig{\n\t\tRevision: revision64,\n\t}\n}"}, {"instruction": "// SubmitTransaction is a helper function that submits tx to txPool and logs a message.", "input": "go language", "output": "func SubmitTransaction(ctx context.Context, b Backend, tx *types.Transaction) (common.Hash, error) {\n\tif err := b.SendTx(ctx, tx); err != nil {\n\t\treturn common.Hash{}, err\n\t}\n\tif tx.To() == nil {\n\t\tsigner := types.MakeSigner(b.ChainConfig(), b.CurrentBlock().Number())\n\t\tfrom, err := types.Sender(signer, tx)\n\t\tif err != nil {\n\t\t\treturn common.Hash{}, err\n\t\t}\n\t\taddr := crypto.CreateAddress(from, tx.Nonce())\n\t\tlog.Info(\"Submitted contract creation\", \"fullhash\", tx.Hash().Hex(), \"contract\", addr.Hex())\n\t} else {\n\t\tlog.Info(\"Submitted transaction\", \"fullhash\", tx.Hash().Hex(), \"recipient\", tx.To())\n\t}\n\treturn tx.Hash(), nil\n}"}, {"instruction": "// ListPredicate returns a list of all the items matching the given\n// SelectionPredicate.", "input": "go language", "output": "func (e *Store) ListPredicate(ctx context.Context, p storage.SelectionPredicate, options *metainternalversion.ListOptions) (runtime.Object, error) {\n\tif options == nil {\n\t\t// By default we should serve the request from etcd.\n\t\toptions = &metainternalversion.ListOptions{ResourceVersion: \"\"}\n\t}\n\tp.Limit = options.Limit\n\tp.Continue = options.Continue\n\tlist := e.NewListFunc()\n\tqualifiedResource := e.qualifiedResourceFromContext(ctx)\n\tif name, ok := p.MatchesSingle(); ok {\n\t\tif key, err := e.KeyFunc(ctx, name); err == nil {\n\t\t\terr := e.Storage.GetToList(ctx, key, options.ResourceVersion, p, list)\n\t\t\treturn list, storeerr.InterpretListError(err, qualifiedResource)\n\t\t}\n\t\t// if we cannot extract a key based on the current context, the optimization is skipped\n\t}\n\n\terr := e.Storage.List(ctx, e.KeyRootFunc(ctx), options.ResourceVersion, p, list)\n\treturn list, storeerr.InterpretListError(err, qualifiedResource)\n}"}, {"instruction": "// updateClaim runs in worker thread and handles \"claim added\",\n// \"claim updated\" and \"periodic sync\" events.", "input": "go language", "output": "func (ctrl *PersistentVolumeController) updateClaim(claim *v1.PersistentVolumeClaim) {\n\t// Store the new claim version in the cache and do not process it if this is\n\t// an old version.\n\tnew, err := ctrl.storeClaimUpdate(claim)\n\tif err != nil {\n\t\tklog.Errorf(\"%v\", err)\n\t}\n\tif !new {\n\t\treturn\n\t}\n\terr = ctrl.syncClaim(claim)\n\tif err != nil {\n\t\tif errors.IsConflict(err) {\n\t\t\t// Version conflict error happens quite often and the controller\n\t\t\t// recovers from it easily.\n\t\t\tklog.V(3).Infof(\"could not sync claim %q: %+v\", claimToClaimKey(claim), err)\n\t\t} else {\n\t\t\tklog.Errorf(\"could not sync volume %q: %+v\", claimToClaimKey(claim), err)\n\t\t}\n\t}\n}"}, {"instruction": "// GetDDLJobs get all DDL jobs and sorts jobs by job.ID.", "input": "go language", "output": "func GetDDLJobs(txn kv.Transaction) ([]*model.Job, error) {\n\tt := meta.NewMeta(txn)\n\tgeneralJobs, err := getDDLJobsInQueue(t, meta.DefaultJobListKey)\n\tif err != nil {\n\t\treturn nil, errors.Trace(err)\n\t}\n\taddIdxJobs, err := getDDLJobsInQueue(t, meta.AddIndexJobListKey)\n\tif err != nil {\n\t\treturn nil, errors.Trace(err)\n\t}\n\tjobs := append(generalJobs, addIdxJobs...)\n\tsort.Sort(jobArray(jobs))\n\treturn jobs, nil\n}"}, {"instruction": "// GetModifiedAccountsByNumber returns all accounts that have changed between the\n// two blocks specified. A change is defined as a difference in nonce, balance,\n// code hash, or storage hash.\n//\n// With one parameter, returns the list of accounts modified in the specified block.", "input": "go language", "output": "func (api *PrivateDebugAPI) GetModifiedAccountsByNumber(startNum uint64, endNum *uint64) ([]common.Address, error) {\n\tvar startBlock, endBlock *types.Block\n\n\tstartBlock = api.eth.blockchain.GetBlockByNumber(startNum)\n\tif startBlock == nil {\n\t\treturn nil, fmt.Errorf(\"start block %x not found\", startNum)\n\t}\n\n\tif endNum == nil {\n\t\tendBlock = startBlock\n\t\tstartBlock = api.eth.blockchain.GetBlockByHash(startBlock.ParentHash())\n\t\tif startBlock == nil {\n\t\t\treturn nil, fmt.Errorf(\"block %x has no parent\", endBlock.Number())\n\t\t}\n\t} else {\n\t\tendBlock = api.eth.blockchain.GetBlockByNumber(*endNum)\n\t\tif endBlock == nil {\n\t\t\treturn nil, fmt.Errorf(\"end block %d not found\", *endNum)\n\t\t}\n\t}\n\treturn api.getModifiedAccounts(startBlock, endBlock)\n}"}, {"instruction": "// newEventRateLimit configures an admission controller that can enforce event rate limits", "input": "go language", "output": "func newEventRateLimit(config *eventratelimitapi.Configuration, clock flowcontrol.Clock) (*Plugin, error) {\n\tlimitEnforcers := make([]*limitEnforcer, 0, len(config.Limits))\n\tfor _, limitConfig := range config.Limits {\n\t\tenforcer, err := newLimitEnforcer(limitConfig, clock)\n\t\tif err != nil {\n\t\t\treturn nil, err\n\t\t}\n\t\tlimitEnforcers = append(limitEnforcers, enforcer)\n\t}\n\n\teventRateLimitAdmission := &Plugin{\n\t\tHandler:        admission.NewHandler(admission.Create, admission.Update),\n\t\tlimitEnforcers: limitEnforcers,\n\t}\n\n\treturn eventRateLimitAdmission, nil\n}"}, {"instruction": "// estimateGracefulTerminationForPods determines the graceful termination period for pods in the namespace", "input": "go language", "output": "func (d *namespacedResourcesDeleter) estimateGracefulTerminationForPods(ns string) (int64, error) {\n\tklog.V(5).Infof(\"namespace controller - estimateGracefulTerminationForPods - namespace %s\", ns)\n\testimate := int64(0)\n\tpodsGetter := d.podsGetter\n\tif podsGetter == nil || reflect.ValueOf(podsGetter).IsNil() {\n\t\treturn estimate, fmt.Errorf(\"unexpected: podsGetter is nil. Cannot estimate grace period seconds for pods\")\n\t}\n\titems, err := podsGetter.Pods(ns).List(metav1.ListOptions{})\n\tif err != nil {\n\t\treturn estimate, err\n\t}\n\tfor i := range items.Items {\n\t\tpod := items.Items[i]\n\t\t// filter out terminal pods\n\t\tphase := pod.Status.Phase\n\t\tif v1.PodSucceeded == phase || v1.PodFailed == phase {\n\t\t\tcontinue\n\t\t}\n\t\tif pod.Spec.TerminationGracePeriodSeconds != nil {\n\t\t\tgrace := *pod.Spec.TerminationGracePeriodSeconds\n\t\t\tif grace > estimate {\n\t\t\t\testimate = grace\n\t\t\t}\n\t\t}\n\t}\n\treturn estimate, nil\n}"}, {"instruction": "// List takes label and field selectors, and returns the list of Jobs that match those selectors.", "input": "go language", "output": "func (c *FakeJobs) List(opts v1.ListOptions) (result *batchv1.JobList, err error) {\n\tobj, err := c.Fake.\n\t\tInvokes(testing.NewListAction(jobsResource, jobsKind, c.ns, opts), &batchv1.JobList{})\n\n\tif obj == nil {\n\t\treturn nil, err\n\t}\n\n\tlabel, _, _ := testing.ExtractFromListOptions(opts)\n\tif label == nil {\n\t\tlabel = labels.Everything()\n\t}\n\tlist := &batchv1.JobList{ListMeta: obj.(*batchv1.JobList).ListMeta}\n\tfor _, item := range obj.(*batchv1.JobList).Items {\n\t\tif label.Matches(labels.Set(item.Labels)) {\n\t\t\tlist.Items = append(list.Items, item)\n\t\t}\n\t}\n\treturn list, err\n}"}, {"instruction": "// MustGetGlobalVersion implements SchemaSyncer.MustGetGlobalVersion interface.", "input": "go language", "output": "func (s *schemaVersionSyncer) MustGetGlobalVersion(ctx context.Context) (int64, error) {\n\tstartTime := time.Now()\n\tvar (\n\t\terr  error\n\t\tver  int\n\t\tresp *clientv3.GetResponse\n\t)\n\tfailedCnt := 0\n\tintervalCnt := int(time.Second / keyOpRetryInterval)\n\n\tdefer func() {\n\t\tmetrics.OwnerHandleSyncerHistogram.WithLabelValues(metrics.OwnerGetGlobalVersion, metrics.RetLabel(err)).Observe(time.Since(startTime).Seconds())\n\t}()\n\tfor {\n\t\tif err != nil {\n\t\t\tif failedCnt%intervalCnt == 0 {\n\t\t\t\tlogutil.Logger(ddlLogCtx).Info(\"[ddl] syncer get global version failed\", zap.Error(err))\n\t\t\t}\n\t\t\ttime.Sleep(keyOpRetryInterval)\n\t\t\tfailedCnt++\n\t\t}\n\n\t\tif isContextDone(ctx) {\n\t\t\terr = errors.Trace(ctx.Err())\n\t\t\treturn 0, err\n\t\t}\n\n\t\tresp, err = s.etcdCli.Get(ctx, DDLGlobalSchemaVersion)\n\t\tif err != nil {\n\t\t\tcontinue\n\t\t}\n\t\tif len(resp.Kvs) > 0 {\n\t\t\tver, err = strconv.Atoi(string(resp.Kvs[0].Value))\n\t\t\tif err == nil {\n\t\t\t\treturn int64(ver), nil\n\t\t\t}\n\t\t}\n\t}\n}"}, {"instruction": "// DeepCopyInto is an autogenerated deepcopy function, copying the receiver, writing into out. in must be non-nil.", "input": "go language", "output": "func (in *ResourceQuotaStatus) DeepCopyInto(out *ResourceQuotaStatus) {\n\t*out = *in\n\tif in.Hard != nil {\n\t\tin, out := &in.Hard, &out.Hard\n\t\t*out = make(ResourceList, len(*in))\n\t\tfor key, val := range *in {\n\t\t\t(*out)[key] = val.DeepCopy()\n\t\t}\n\t}\n\tif in.Used != nil {\n\t\tin, out := &in.Used, &out.Used\n\t\t*out = make(ResourceList, len(*in))\n\t\tfor key, val := range *in {\n\t\t\t(*out)[key] = val.DeepCopy()\n\t\t}\n\t}\n\treturn\n}"}, {"instruction": "// DeepCopyInto is an autogenerated deepcopy function, copying the receiver, writing into out. in must be non-nil.", "input": "go language", "output": "func (in *NetworkPolicyEgressRule) DeepCopyInto(out *NetworkPolicyEgressRule) {\n\t*out = *in\n\tif in.Ports != nil {\n\t\tin, out := &in.Ports, &out.Ports\n\t\t*out = make([]NetworkPolicyPort, len(*in))\n\t\tfor i := range *in {\n\t\t\t(*in)[i].DeepCopyInto(&(*out)[i])\n\t\t}\n\t}\n\tif in.To != nil {\n\t\tin, out := &in.To, &out.To\n\t\t*out = make([]NetworkPolicyPeer, len(*in))\n\t\tfor i := range *in {\n\t\t\t(*in)[i].DeepCopyInto(&(*out)[i])\n\t\t}\n\t}\n\treturn\n}"}, {"instruction": "// VersionFromCILabel resolves a version label like \"latest\" or \"stable\" to an actual version using the public Kubernetes CI uploads", "input": "go language", "output": "func (g *KubeVersionGetter) VersionFromCILabel(ciVersionLabel, description string) (string, *versionutil.Version, error) {\n\tversionStr, err := kubeadmutil.KubernetesReleaseVersion(ciVersionLabel)\n\tif err != nil {\n\t\treturn \"\", nil, errors.Wrapf(err, \"Couldn't fetch latest %s from the internet\", description)\n\t}\n\n\tif description != \"\" {\n\t\tfmt.Fprintf(g.w, \"[upgrade/versions] Latest %s: %s\\n\", description, versionStr)\n\t}\n\n\tver, err := versionutil.ParseSemantic(versionStr)\n\tif err != nil {\n\t\treturn \"\", nil, errors.Wrapf(err, \"Couldn't parse latest %s\", description)\n\t}\n\treturn versionStr, ver, nil\n}"}, {"instruction": "// Recv receives a message from a remote cluster member.", "input": "go language", "output": "func (stream *Stream) Recv() (*orderer.StepResponse, error) {\n\tstart := time.Now()\n\tdefer func() {\n\t\tif !stream.Logger.IsEnabledFor(zap.DebugLevel) {\n\t\t\treturn\n\t\t}\n\t\tstream.Logger.Debugf(\"Receive from %s(%s) took %v\", stream.NodeName, stream.Endpoint, time.Since(start))\n\t}()\n\n\tf := func() (*orderer.StepResponse, error) {\n\t\treturn stream.Cluster_StepClient.Recv()\n\t}\n\n\treturn stream.operateWithTimeout(f)\n}"}, {"instruction": "// resolveWindowSpec resolve window specifications for sql like `select ... from t window w1 as (w2), w2 as (partition by a)`.\n// We need to resolve the referenced window to get the definition of current window spec.", "input": "go language", "output": "func resolveWindowSpec(spec *ast.WindowSpec, specs map[string]ast.WindowSpec, inStack map[string]bool) error {\n\tif inStack[spec.Name.L] {\n\t\treturn errors.Trace(ErrWindowCircularityInWindowGraph)\n\t}\n\tif spec.Ref.L == \"\" {\n\t\treturn nil\n\t}\n\tref, ok := specs[spec.Ref.L]\n\tif !ok {\n\t\treturn ErrWindowNoSuchWindow.GenWithStackByArgs(spec.Ref.O)\n\t}\n\tinStack[spec.Name.L] = true\n\terr := resolveWindowSpec(&ref, specs, inStack)\n\tif err != nil {\n\t\treturn err\n\t}\n\tinStack[spec.Name.L] = false\n\treturn mergeWindowSpec(spec, &ref)\n}"}, {"instruction": "// parseConfig returns a parsed configuration for an Azure cloudprovider config file", "input": "go language", "output": "func parseConfig(configReader io.Reader) (*Config, error) {\n\tvar config Config\n\n\tif configReader == nil {\n\t\treturn &config, nil\n\t}\n\n\tconfigContents, err := ioutil.ReadAll(configReader)\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\terr = yaml.Unmarshal(configContents, &config)\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\n\t// The resource group name may be in different cases from different Azure APIs, hence it is converted to lower here.\n\t// See more context at https://github.com/kubernetes/kubernetes/issues/71994.\n\tconfig.ResourceGroup = strings.ToLower(config.ResourceGroup)\n\treturn &config, nil\n}"}, {"instruction": "// NewFilteredCustomResourceDefinitionInformer constructs a new informer for CustomResourceDefinition type.\n// Always prefer using an informer factory to get a shared informer instead of getting an independent\n// one. This reduces memory footprint and number of connections to the server.", "input": "go language", "output": "func NewFilteredCustomResourceDefinitionInformer(client internalclientset.Interface, resyncPeriod time.Duration, indexers cache.Indexers, tweakListOptions internalinterfaces.TweakListOptionsFunc) cache.SharedIndexInformer {\n\treturn cache.NewSharedIndexInformer(\n\t\t&cache.ListWatch{\n\t\t\tListFunc: func(options v1.ListOptions) (runtime.Object, error) {\n\t\t\t\tif tweakListOptions != nil {\n\t\t\t\t\ttweakListOptions(&options)\n\t\t\t\t}\n\t\t\t\treturn client.Apiextensions().CustomResourceDefinitions().List(options)\n\t\t\t},\n\t\t\tWatchFunc: func(options v1.ListOptions) (watch.Interface, error) {\n\t\t\t\tif tweakListOptions != nil {\n\t\t\t\t\ttweakListOptions(&options)\n\t\t\t\t}\n\t\t\t\treturn client.Apiextensions().CustomResourceDefinitions().Watch(options)\n\t\t\t},\n\t\t},\n\t\t&apiextensions.CustomResourceDefinition{},\n\t\tresyncPeriod,\n\t\tindexers,\n\t)\n}"}, {"instruction": "// Connect returns a handler for the pod exec proxy", "input": "go language", "output": "func (r *AttachREST) Connect(ctx context.Context, name string, opts runtime.Object, responder rest.Responder) (http.Handler, error) {\n\tattachOpts, ok := opts.(*api.PodAttachOptions)\n\tif !ok {\n\t\treturn nil, fmt.Errorf(\"Invalid options object: %#v\", opts)\n\t}\n\tlocation, transport, err := pod.AttachLocation(r.Store, r.KubeletConn, ctx, name, attachOpts)\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\treturn newThrottledUpgradeAwareProxyHandler(location, transport, false, true, true, responder), nil\n}"}, {"instruction": "// statementContextToFlags converts StatementContext to tipb.SelectRequest.Flags.", "input": "go language", "output": "func statementContextToFlags(sc *stmtctx.StatementContext) uint64 {\n\tvar flags uint64\n\tif sc.InInsertStmt {\n\t\tflags |= model.FlagInInsertStmt\n\t} else if sc.InUpdateStmt || sc.InDeleteStmt {\n\t\tflags |= model.FlagInUpdateOrDeleteStmt\n\t} else if sc.InSelectStmt {\n\t\tflags |= model.FlagInSelectStmt\n\t}\n\tif sc.IgnoreTruncate {\n\t\tflags |= model.FlagIgnoreTruncate\n\t} else if sc.TruncateAsWarning {\n\t\tflags |= model.FlagTruncateAsWarning\n\t}\n\tif sc.OverflowAsWarning {\n\t\tflags |= model.FlagOverflowAsWarning\n\t}\n\tif sc.IgnoreZeroInDate {\n\t\tflags |= model.FlagIgnoreZeroInDate\n\t}\n\tif sc.DividedByZeroAsWarning {\n\t\tflags |= model.FlagDividedByZeroAsWarning\n\t}\n\tif sc.PadCharToFullLength {\n\t\tflags |= model.FlagPadCharToFullLength\n\t}\n\treturn flags\n}"}, {"instruction": "// check opts for OpenStack", "input": "go language", "output": "func checkOpenStackOpts(openstackOpts *OpenStack) error {\n\tlbOpts := openstackOpts.lbOpts\n\n\t// if need to create health monitor for Neutron LB,\n\t// monitor-delay, monitor-timeout and monitor-max-retries should be set.\n\temptyDuration := MyDuration{}\n\tif lbOpts.CreateMonitor {\n\t\tif lbOpts.MonitorDelay == emptyDuration {\n\t\t\treturn fmt.Errorf(\"monitor-delay not set in cloud provider config\")\n\t\t}\n\t\tif lbOpts.MonitorTimeout == emptyDuration {\n\t\t\treturn fmt.Errorf(\"monitor-timeout not set in cloud provider config\")\n\t\t}\n\t\tif lbOpts.MonitorMaxRetries == uint(0) {\n\t\t\treturn fmt.Errorf(\"monitor-max-retries not set in cloud provider config\")\n\t\t}\n\t}\n\treturn checkMetadataSearchOrder(openstackOpts.metadataOpts.SearchOrder)\n}"}, {"instruction": "// getIPv4DefaultRoutes obtains the IPv4 routes, and filters out non-default routes.", "input": "go language", "output": "func getIPv4DefaultRoutes(input io.Reader) ([]Route, error) {\n\troutes := []Route{}\n\tscanner := bufio.NewReader(input)\n\tfor {\n\t\tline, err := scanner.ReadString('\\n')\n\t\tif err == io.EOF {\n\t\t\tbreak\n\t\t}\n\t\t//ignore the headers in the route info\n\t\tif strings.HasPrefix(line, \"Iface\") {\n\t\t\tcontinue\n\t\t}\n\t\tfields := strings.Fields(line)\n\t\t// Interested in fields:\n\t\t//  0 - interface name\n\t\t//  1 - destination address\n\t\t//  2 - gateway\n\t\tdest, err := parseIP(fields[1], familyIPv4)\n\t\tif err != nil {\n\t\t\treturn nil, err\n\t\t}\n\t\tgw, err := parseIP(fields[2], familyIPv4)\n\t\tif err != nil {\n\t\t\treturn nil, err\n\t\t}\n\t\tif !dest.Equal(net.IPv4zero) {\n\t\t\tcontinue\n\t\t}\n\t\troutes = append(routes, Route{\n\t\t\tInterface:   fields[0],\n\t\t\tDestination: dest,\n\t\t\tGateway:     gw,\n\t\t\tFamily:      familyIPv4,\n\t\t})\n\t}\n\treturn routes, nil\n}"}, {"instruction": "// loadChartRepositories reads the repositories.yaml, and then builds a map of\n// ChartRepositories.\n//\n// The key is the local name (which is only present in the repositories.yaml).", "input": "go language", "output": "func (m *Manager) loadChartRepositories() (map[string]*repo.ChartRepository, error) {\n\tindices := map[string]*repo.ChartRepository{}\n\trepoyaml := m.HelmHome.RepositoryFile()\n\n\t// Load repositories.yaml file\n\trf, err := repo.LoadRepositoriesFile(repoyaml)\n\tif err != nil {\n\t\treturn indices, fmt.Errorf(\"failed to load %s: %s\", repoyaml, err)\n\t}\n\n\tfor _, re := range rf.Repositories {\n\t\tlname := re.Name\n\t\tcacheindex := m.HelmHome.CacheIndex(lname)\n\t\tindex, err := repo.LoadIndexFile(cacheindex)\n\t\tif err != nil {\n\t\t\treturn indices, err\n\t\t}\n\n\t\t// TODO: use constructor\n\t\tcr := &repo.ChartRepository{\n\t\t\tConfig:    re,\n\t\t\tIndexFile: index,\n\t\t}\n\t\tindices[lname] = cr\n\t}\n\treturn indices, nil\n}"}, {"instruction": "// ResolveIndices implements Plan interface.", "input": "go language", "output": "func (p *basePhysicalAgg) ResolveIndices() (err error) {\n\terr = p.physicalSchemaProducer.ResolveIndices()\n\tif err != nil {\n\t\treturn err\n\t}\n\tfor _, aggFun := range p.AggFuncs {\n\t\tfor i, arg := range aggFun.Args {\n\t\t\taggFun.Args[i], err = arg.ResolveIndices(p.children[0].Schema())\n\t\t\tif err != nil {\n\t\t\t\treturn err\n\t\t\t}\n\t\t}\n\t}\n\tfor i, item := range p.GroupByItems {\n\t\tp.GroupByItems[i], err = item.ResolveIndices(p.children[0].Schema())\n\t\tif err != nil {\n\t\t\treturn err\n\t\t}\n\t}\n\treturn\n}"}, {"instruction": "// Run starts the AvailableConditionController loop which manages the availability condition of API services.", "input": "go language", "output": "func (c *AvailableConditionController) Run(threadiness int, stopCh <-chan struct{}) {\n\tdefer utilruntime.HandleCrash()\n\tdefer c.queue.ShutDown()\n\n\tklog.Infof(\"Starting AvailableConditionController\")\n\tdefer klog.Infof(\"Shutting down AvailableConditionController\")\n\n\tif !controllers.WaitForCacheSync(\"AvailableConditionController\", stopCh, c.apiServiceSynced, c.servicesSynced, c.endpointsSynced) {\n\t\treturn\n\t}\n\n\tfor i := 0; i < threadiness; i++ {\n\t\tgo wait.Until(c.runWorker, time.Second, stopCh)\n\t}\n\n\t<-stopCh\n}"}, {"instruction": "// getH2Settings returns the []http2.Setting that are encoded in the\n// HTTP2-Settings header.", "input": "go language", "output": "func getH2Settings(h http.Header) ([]http2.Setting, error) {\n\tvals, ok := h[textproto.CanonicalMIMEHeaderKey(\"HTTP2-Settings\")]\n\tif !ok {\n\t\treturn nil, errors.New(\"missing HTTP2-Settings header\")\n\t}\n\tif len(vals) != 1 {\n\t\treturn nil, fmt.Errorf(\"expected 1 HTTP2-Settings. Got: %v\", vals)\n\t}\n\tsettings, err := decodeSettings(vals[0])\n\tif err != nil {\n\t\treturn nil, fmt.Errorf(\"invalid HTTP2-Settings: %q\", vals[0])\n\t}\n\treturn settings, nil\n}"}, {"instruction": "// Open implements the Executor Open interface.", "input": "go language", "output": "func (e *CheckIndexRangeExec) Open(ctx context.Context) error {\n\ttCols := e.table.Cols()\n\tfor _, ic := range e.index.Columns {\n\t\tcol := tCols[ic.Offset]\n\t\te.cols = append(e.cols, col)\n\t}\n\n\tcolTypeForHandle := e.schema.Columns[len(e.cols)].RetType\n\te.cols = append(e.cols, &model.ColumnInfo{\n\t\tID:        model.ExtraHandleID,\n\t\tName:      model.ExtraHandleName,\n\t\tFieldType: *colTypeForHandle,\n\t})\n\n\te.srcChunk = e.newFirstChunk()\n\tdagPB, err := e.buildDAGPB()\n\tif err != nil {\n\t\treturn err\n\t}\n\tsc := e.ctx.GetSessionVars().StmtCtx\n\tvar builder distsql.RequestBuilder\n\tkvReq, err := builder.SetIndexRanges(sc, e.table.ID, e.index.ID, ranger.FullRange()).\n\t\tSetDAGRequest(dagPB).\n\t\tSetKeepOrder(true).\n\t\tSetFromSessionVars(e.ctx.GetSessionVars()).\n\t\tBuild()\n\tif err != nil {\n\t\treturn err\n\t}\n\n\te.result, err = distsql.Select(ctx, e.ctx, kvReq, e.retFieldTypes, statistics.NewQueryFeedback(0, nil, 0, false))\n\tif err != nil {\n\t\treturn err\n\t}\n\te.result.Fetch(ctx)\n\treturn nil\n}"}, {"instruction": "// ResolveCharsetCollation will resolve the charset by the order: table charset > database charset > server default charset.", "input": "go language", "output": "func ResolveCharsetCollation(tblCharset, dbCharset string) (string, string, error) {\n\tif len(tblCharset) != 0 {\n\t\tdefCollate, err := charset.GetDefaultCollation(tblCharset)\n\t\tif err != nil {\n\t\t\t// return terror is better.\n\t\t\treturn \"\", \"\", ErrUnknownCharacterSet.GenWithStackByArgs(tblCharset)\n\t\t}\n\t\treturn tblCharset, defCollate, nil\n\t}\n\n\tif len(dbCharset) != 0 {\n\t\tdefCollate, err := charset.GetDefaultCollation(dbCharset)\n\t\tif err != nil {\n\t\t\treturn \"\", \"\", ErrUnknownCharacterSet.GenWithStackByArgs(dbCharset)\n\t\t}\n\t\treturn dbCharset, defCollate, errors.Trace(err)\n\t}\n\n\tcharset, collate := charset.GetDefaultCharsetAndCollate()\n\treturn charset, collate, nil\n}"}, {"instruction": "// setBootstrapNodesV5 creates a list of bootstrap nodes from the command line\n// flags, reverting to pre-configured ones if none have been specified.", "input": "go language", "output": "func setBootstrapNodesV5(ctx *cli.Context, cfg *p2p.Config) {\n\turls := params.DiscoveryV5Bootnodes\n\tswitch {\n\tcase ctx.GlobalIsSet(BootnodesFlag.Name) || ctx.GlobalIsSet(BootnodesV5Flag.Name):\n\t\tif ctx.GlobalIsSet(BootnodesV5Flag.Name) {\n\t\t\turls = strings.Split(ctx.GlobalString(BootnodesV5Flag.Name), \",\")\n\t\t} else {\n\t\t\turls = strings.Split(ctx.GlobalString(BootnodesFlag.Name), \",\")\n\t\t}\n\tcase ctx.GlobalBool(RinkebyFlag.Name):\n\t\turls = params.RinkebyBootnodes\n\tcase ctx.GlobalBool(GoerliFlag.Name):\n\t\turls = params.GoerliBootnodes\n\tcase cfg.BootstrapNodesV5 != nil:\n\t\treturn // already set, don't apply defaults.\n\t}\n\n\tcfg.BootstrapNodesV5 = make([]*discv5.Node, 0, len(urls))\n\tfor _, url := range urls {\n\t\tif url != \"\" {\n\t\t\tnode, err := discv5.ParseNode(url)\n\t\t\tif err != nil {\n\t\t\t\tlog.Error(\"Bootstrap URL invalid\", \"enode\", url, \"err\", err)\n\t\t\t\tcontinue\n\t\t\t}\n\t\t\tcfg.BootstrapNodesV5 = append(cfg.BootstrapNodesV5, node)\n\t\t}\n\t}\n}"}, {"instruction": "// UnmarshalBinary decodes msgpack encoded ServiceConfigResponse. It used\n// default msgpack encoding but fixes up the uint8 strings and other problems we\n// have with encoding map[string]interface{}.", "input": "go language", "output": "func (r *ServiceConfigResponse) UnmarshalBinary(data []byte) error {\n\tdec := codec.NewDecoderBytes(data, msgpackHandle)\n\n\ttype Alias ServiceConfigResponse\n\tvar a Alias\n\n\tif err := dec.Decode(&a); err != nil {\n\t\treturn err\n\t}\n\n\t*r = ServiceConfigResponse(a)\n\n\tvar err error\n\n\t// Fix strings and maps in the returned maps\n\tr.ProxyConfig, err = lib.MapWalk(r.ProxyConfig)\n\tif err != nil {\n\t\treturn err\n\t}\n\tfor k := range r.UpstreamConfigs {\n\t\tr.UpstreamConfigs[k], err = lib.MapWalk(r.UpstreamConfigs[k])\n\t\tif err != nil {\n\t\t\treturn err\n\t\t}\n\t}\n\treturn nil\n}"}, {"instruction": "// List takes label and field selectors, and returns the list of Pods that match those selectors.", "input": "go language", "output": "func (c *FakePods) List(opts v1.ListOptions) (result *corev1.PodList, err error) {\n\tobj, err := c.Fake.\n\t\tInvokes(testing.NewListAction(podsResource, podsKind, c.ns, opts), &corev1.PodList{})\n\n\tif obj == nil {\n\t\treturn nil, err\n\t}\n\n\tlabel, _, _ := testing.ExtractFromListOptions(opts)\n\tif label == nil {\n\t\tlabel = labels.Everything()\n\t}\n\tlist := &corev1.PodList{ListMeta: obj.(*corev1.PodList).ListMeta}\n\tfor _, item := range obj.(*corev1.PodList).Items {\n\t\tif label.Matches(labels.Set(item.Labels)) {\n\t\t\tlist.Items = append(list.Items, item)\n\t\t}\n\t}\n\treturn list, err\n}"}, {"instruction": "// LoadOrGenerateKeyFile looks for a key in the file at the given path. If it\n// can't find one, it will generate a new key and store it there.", "input": "go language", "output": "func LoadOrGenerateKeyFile(keyPath string) (data []byte, wasGenerated bool, err error) {\n\tloadedData, err := ioutil.ReadFile(keyPath)\n\t// Call verifyKeyData to ensure the file wasn't empty/corrupt.\n\tif err == nil && verifyKeyData(loadedData) {\n\t\treturn loadedData, false, err\n\t}\n\tif !os.IsNotExist(err) {\n\t\treturn nil, false, fmt.Errorf(\"error loading key from %s: %v\", keyPath, err)\n\t}\n\n\tgeneratedData, err := MakeEllipticPrivateKeyPEM()\n\tif err != nil {\n\t\treturn nil, false, fmt.Errorf(\"error generating key: %v\", err)\n\t}\n\tif err := WriteKey(keyPath, generatedData); err != nil {\n\t\treturn nil, false, fmt.Errorf(\"error writing key to %s: %v\", keyPath, err)\n\t}\n\treturn generatedData, true, nil\n}"}, {"instruction": "// applyPreparedQueryOperation applies the given prepared query operation to the\n// state store.", "input": "go language", "output": "func (c *FSM) applyPreparedQueryOperation(buf []byte, index uint64) interface{} {\n\tvar req structs.PreparedQueryRequest\n\tif err := structs.Decode(buf, &req); err != nil {\n\t\tpanic(fmt.Errorf(\"failed to decode request: %v\", err))\n\t}\n\n\tdefer metrics.MeasureSinceWithLabels([]string{\"fsm\", \"prepared-query\"}, time.Now(),\n\t\t[]metrics.Label{{Name: \"op\", Value: string(req.Op)}})\n\tswitch req.Op {\n\tcase structs.PreparedQueryCreate, structs.PreparedQueryUpdate:\n\t\treturn c.state.PreparedQuerySet(index, req.Query)\n\tcase structs.PreparedQueryDelete:\n\t\treturn c.state.PreparedQueryDelete(index, req.Query.ID)\n\tdefault:\n\t\tc.logger.Printf(\"[WARN] consul.fsm: Invalid PreparedQuery operation '%s'\", req.Op)\n\t\treturn fmt.Errorf(\"Invalid PreparedQuery operation '%s'\", req.Op)\n\t}\n}"}, {"instruction": "// Check if a proposed update can be committed.", "input": "go language", "output": "func (p *MemoryPool) checkUpdate(transactionData []tms.TransactionData) error {\n\tfor _, td := range transactionData {\n\t\taction := td.Tx.GetPlainAction()\n\t\tif action == nil {\n\t\t\treturn errors.Errorf(\"check update failed for transaction '%s': missing token action\", td.TxID)\n\t\t}\n\n\t\terr := p.checkAction(action, td.TxID)\n\t\tif err != nil {\n\t\t\treturn errors.WithMessage(err, \"check update failed\")\n\t\t}\n\n\t\tif p.history[td.TxID] != nil {\n\t\t\treturn errors.Errorf(\"transaction already exists: %s\", td.TxID)\n\t\t}\n\t}\n\n\treturn nil\n}"}, {"instruction": "// GetDDLInfo returns DDL information.", "input": "go language", "output": "func GetDDLInfo(txn kv.Transaction) (*DDLInfo, error) {\n\tvar err error\n\tinfo := &DDLInfo{}\n\tt := meta.NewMeta(txn)\n\n\tinfo.Jobs = make([]*model.Job, 0, 2)\n\tjob, err := t.GetDDLJobByIdx(0)\n\tif err != nil {\n\t\treturn nil, errors.Trace(err)\n\t}\n\tif job != nil {\n\t\tinfo.Jobs = append(info.Jobs, job)\n\t}\n\taddIdxJob, err := t.GetDDLJobByIdx(0, meta.AddIndexJobListKey)\n\tif err != nil {\n\t\treturn nil, errors.Trace(err)\n\t}\n\tif addIdxJob != nil {\n\t\tinfo.Jobs = append(info.Jobs, addIdxJob)\n\t}\n\n\tinfo.SchemaVer, err = t.GetSchemaVersion()\n\tif err != nil {\n\t\treturn nil, errors.Trace(err)\n\t}\n\tif addIdxJob == nil {\n\t\treturn info, nil\n\t}\n\n\tinfo.ReorgHandle, _, _, err = t.GetDDLReorgHandle(addIdxJob)\n\tif err != nil {\n\t\treturn nil, errors.Trace(err)\n\t}\n\n\treturn info, nil\n}"}, {"instruction": "// UntilWithoutRetry reads items from the watch until each provided condition succeeds, and then returns the last watch\n// encountered. The first condition that returns an error terminates the watch (and the event is also returned).\n// If no event has been received, the returned event will be nil.\n// Conditions are satisfied sequentially so as to provide a useful primitive for higher level composition.\n// Waits until context deadline or until context is canceled.\n//\n// Warning: Unless you have a very specific use case (probably a special Watcher) don't use this function!!!\n// Warning: This will fail e.g. on API timeouts and/or 'too old resource version' error.\n// Warning: You are most probably looking for a function *Until* or *UntilWithSync* below,\n// Warning: solving such issues.\n// TODO: Consider making this function private to prevent misuse when the other occurrences in our codebase are gone.", "input": "go language", "output": "func UntilWithoutRetry(ctx context.Context, watcher watch.Interface, conditions ...ConditionFunc) (*watch.Event, error) {\n\tch := watcher.ResultChan()\n\tdefer watcher.Stop()\n\tvar lastEvent *watch.Event\n\tfor _, condition := range conditions {\n\t\t// check the next condition against the previous event and short circuit waiting for the next watch\n\t\tif lastEvent != nil {\n\t\t\tdone, err := condition(*lastEvent)\n\t\t\tif err != nil {\n\t\t\t\treturn lastEvent, err\n\t\t\t}\n\t\t\tif done {\n\t\t\t\tcontinue\n\t\t\t}\n\t\t}\n\tConditionSucceeded:\n\t\tfor {\n\t\t\tselect {\n\t\t\tcase event, ok := <-ch:\n\t\t\t\tif !ok {\n\t\t\t\t\treturn lastEvent, ErrWatchClosed\n\t\t\t\t}\n\t\t\t\tlastEvent = &event\n\n\t\t\t\tdone, err := condition(event)\n\t\t\t\tif err != nil {\n\t\t\t\t\treturn lastEvent, err\n\t\t\t\t}\n\t\t\t\tif done {\n\t\t\t\t\tbreak ConditionSucceeded\n\t\t\t\t}\n\n\t\t\tcase <-ctx.Done():\n\t\t\t\treturn lastEvent, wait.ErrWaitTimeout\n\t\t\t}\n\t\t}\n\t}\n\treturn lastEvent, nil\n}"}, {"instruction": "// RetrieveValidatedConfigInfo connects to the API Server and makes sure it can talk\n// securely to the API Server using the provided CA cert and\n// optionally refreshes the cluster-info information from the cluster-info ConfigMap", "input": "go language", "output": "func RetrieveValidatedConfigInfo(httpsURL, clustername string) (*clientcmdapi.Config, error) {\n\tclient := &http.Client{Transport: netutil.SetOldTransportDefaults(&http.Transport{})}\n\tresponse, err := client.Get(httpsURL)\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\tdefer response.Body.Close()\n\n\tkubeconfig, err := ioutil.ReadAll(response.Body)\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\n\tconfig, err := clientcmd.Load(kubeconfig)\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\treturn file.ValidateConfigInfo(config, clustername)\n}"}, {"instruction": "// Visit will call the visitor function on the path if it's a file, or for each\n// file in the path if it's a directory. Directories will not be recursed into,\n// and files in the directory will be visited in alphabetical order.", "input": "go language", "output": "func Visit(path string, visitor VisitFn) error {\n\tf, err := os.Open(path)\n\tif err != nil {\n\t\treturn fmt.Errorf(\"error reading %q: %v\", path, err)\n\t}\n\tdefer f.Close()\n\n\tfi, err := f.Stat()\n\tif err != nil {\n\t\treturn fmt.Errorf(\"error checking %q: %v\", path, err)\n\t}\n\n\tif !fi.IsDir() {\n\t\tif err := visitor(path); err != nil {\n\t\t\treturn fmt.Errorf(\"error in %q: %v\", path, err)\n\t\t}\n\t\treturn nil\n\t}\n\n\tcontents, err := f.Readdir(-1)\n\tif err != nil {\n\t\treturn fmt.Errorf(\"error listing %q: %v\", path, err)\n\t}\n\n\tsort.Sort(dirEnts(contents))\n\tfor _, fi := range contents {\n\t\tif fi.IsDir() {\n\t\t\tcontinue\n\t\t}\n\n\t\tfullPath := filepath.Join(path, fi.Name())\n\t\tif err := visitor(fullPath); err != nil {\n\t\t\treturn fmt.Errorf(\"error in %q: %v\", fullPath, err)\n\t\t}\n\t}\n\n\treturn nil\n}"}, {"instruction": "// getPullSecretsForPod inspects the Pod and retrieves the referenced pull\n// secrets.", "input": "go language", "output": "func (kl *Kubelet) getPullSecretsForPod(pod *v1.Pod) []v1.Secret {\n\tpullSecrets := []v1.Secret{}\n\n\tfor _, secretRef := range pod.Spec.ImagePullSecrets {\n\t\tsecret, err := kl.secretManager.GetSecret(pod.Namespace, secretRef.Name)\n\t\tif err != nil {\n\t\t\tklog.Warningf(\"Unable to retrieve pull secret %s/%s for %s/%s due to %v.  The image pull may not succeed.\", pod.Namespace, secretRef.Name, pod.Namespace, pod.Name, err)\n\t\t\tcontinue\n\t\t}\n\n\t\tpullSecrets = append(pullSecrets, *secret)\n\t}\n\n\treturn pullSecrets\n}"}, {"instruction": "// EvalString returns string representation of Column.", "input": "go language", "output": "func (col *Column) EvalString(ctx sessionctx.Context, row chunk.Row) (string, bool, error) {\n\tif row.IsNull(col.Index) {\n\t\treturn \"\", true, nil\n\t}\n\n\t// Specially handle the ENUM/SET/BIT input value.\n\tif col.GetType().Hybrid() {\n\t\tval := row.GetDatum(col.Index, col.RetType)\n\t\tres, err := val.ToString()\n\t\treturn res, err != nil, err\n\t}\n\n\tval := row.GetString(col.Index)\n\tif ctx.GetSessionVars().StmtCtx.PadCharToFullLength && col.GetType().Tp == mysql.TypeString {\n\t\tvalLen := len([]rune(val))\n\t\tif valLen < col.RetType.Flen {\n\t\t\tval = val + strings.Repeat(\" \", col.RetType.Flen-valLen)\n\t\t}\n\t}\n\treturn val, false, nil\n}"}, {"instruction": "// GraphNodeReferenceOutside implementation", "input": "go language", "output": "func (n *NodeApplyableModuleVariable) ReferenceOutside() (selfPath, referencePath addrs.ModuleInstance) {\n\n\t// Module input variables have their value expressions defined in the\n\t// context of their calling (parent) module, and so references from\n\t// a node of this type should be resolved in the parent module instance.\n\treferencePath = n.Addr.Module.Parent()\n\n\t// Input variables are _referenced_ from their own module, though.\n\tselfPath = n.Addr.Module\n\n\treturn // uses named return values\n}"}, {"instruction": "// IpcMounts returns the list of IPC mounts", "input": "go language", "output": "func (container *Container) IpcMounts() []Mount {\n\tvar mounts []Mount\n\tparser := volumemounts.NewParser(container.OS)\n\n\tif container.HasMountFor(\"/dev/shm\") {\n\t\treturn mounts\n\t}\n\tif container.ShmPath == \"\" {\n\t\treturn mounts\n\t}\n\n\tlabel.SetFileLabel(container.ShmPath, container.MountLabel)\n\tmounts = append(mounts, Mount{\n\t\tSource:      container.ShmPath,\n\t\tDestination: \"/dev/shm\",\n\t\tWritable:    true,\n\t\tPropagation: string(parser.DefaultPropagationMode()),\n\t})\n\n\treturn mounts\n}"}, {"instruction": "// evalInt evals a builtinInetAtonSig.\n// See https://dev.mysql.com/doc/refman/5.7/en/miscellaneous-functions.html#function_inet-aton", "input": "go language", "output": "func (b *builtinInetAtonSig) evalInt(row chunk.Row) (int64, bool, error) {\n\tval, isNull, err := b.args[0].EvalString(b.ctx, row)\n\tif err != nil || isNull {\n\t\treturn 0, true, err\n\t}\n\t// ip address should not end with '.'.\n\tif len(val) == 0 || val[len(val)-1] == '.' {\n\t\treturn 0, true, nil\n\t}\n\n\tvar (\n\t\tbyteResult, result uint64\n\t\tdotCount           int\n\t)\n\tfor _, c := range val {\n\t\tif c >= '0' && c <= '9' {\n\t\t\tdigit := uint64(c - '0')\n\t\t\tbyteResult = byteResult*10 + digit\n\t\t\tif byteResult > 255 {\n\t\t\t\treturn 0, true, nil\n\t\t\t}\n\t\t} else if c == '.' {\n\t\t\tdotCount++\n\t\t\tif dotCount > 3 {\n\t\t\t\treturn 0, true, nil\n\t\t\t}\n\t\t\tresult = (result << 8) + byteResult\n\t\t\tbyteResult = 0\n\t\t} else {\n\t\t\treturn 0, true, nil\n\t\t}\n\t}\n\t// 127 \t\t-> 0.0.0.127\n\t// 127.255 \t-> 127.0.0.255\n\t// 127.256\t-> NULL\n\t// 127.2.1\t-> 127.2.0.1\n\tswitch dotCount {\n\tcase 1:\n\t\tresult <<= 8\n\t\tfallthrough\n\tcase 2:\n\t\tresult <<= 8\n\t}\n\treturn int64((result << 8) + byteResult), false, nil\n}"}, {"instruction": "// capabilities builds a Capabilities from discovery information.", "input": "go language", "output": "func capabilities(disc discovery.DiscoveryInterface) (*chartutil.Capabilities, error) {\n\tsv, err := disc.ServerVersion()\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\tvs, err := GetVersionSet(disc)\n\tif err != nil {\n\t\treturn nil, fmt.Errorf(\"Could not get apiVersions from Kubernetes: %s\", err)\n\t}\n\treturn &chartutil.Capabilities{\n\t\tAPIVersions:   vs,\n\t\tKubeVersion:   sv,\n\t\tTillerVersion: version.GetVersionProto(),\n\t}, nil\n}"}, {"instruction": "// filterNodeDump is used to filter through all parts of a node dump and\n// remove elements the provided ACL token cannot access.", "input": "go language", "output": "func (f *aclFilter) filterNodeDump(dump *structs.NodeDump) {\n\tnd := *dump\n\tfor i := 0; i < len(nd); i++ {\n\t\tinfo := nd[i]\n\n\t\t// Filter nodes\n\t\tif node := info.Node; !f.allowNode(node) {\n\t\t\tf.logger.Printf(\"[DEBUG] consul: dropping node %q from result due to ACLs\", node)\n\t\t\tnd = append(nd[:i], nd[i+1:]...)\n\t\t\ti--\n\t\t\tcontinue\n\t\t}\n\n\t\t// Filter services\n\t\tfor j := 0; j < len(info.Services); j++ {\n\t\t\tsvc := info.Services[j].Service\n\t\t\tif f.allowService(svc) {\n\t\t\t\tcontinue\n\t\t\t}\n\t\t\tf.logger.Printf(\"[DEBUG] consul: dropping service %q from result due to ACLs\", svc)\n\t\t\tinfo.Services = append(info.Services[:j], info.Services[j+1:]...)\n\t\t\tj--\n\t\t}\n\n\t\t// Filter checks\n\t\tfor j := 0; j < len(info.Checks); j++ {\n\t\t\tchk := info.Checks[j]\n\t\t\tif f.allowService(chk.ServiceName) {\n\t\t\t\tcontinue\n\t\t\t}\n\t\t\tf.logger.Printf(\"[DEBUG] consul: dropping check %q from result due to ACLs\", chk.CheckID)\n\t\t\tinfo.Checks = append(info.Checks[:j], info.Checks[j+1:]...)\n\t\t\tj--\n\t\t}\n\t}\n\t*dump = nd\n}"}, {"instruction": "// LSet updates an element in the list by its index.", "input": "go language", "output": "func (t *TxStructure) LSet(key []byte, index int64, value []byte) error {\n\tif t.readWriter == nil {\n\t\treturn errWriteOnSnapshot\n\t}\n\tmetaKey := t.encodeListMetaKey(key)\n\tmeta, err := t.loadListMeta(metaKey)\n\tif err != nil || meta.IsEmpty() {\n\t\treturn errors.Trace(err)\n\t}\n\n\tindex = adjustIndex(index, meta.LIndex, meta.RIndex)\n\n\tif index >= meta.LIndex && index < meta.RIndex {\n\t\treturn t.readWriter.Set(t.encodeListDataKey(key, index), value)\n\t}\n\treturn errInvalidListIndex.GenWithStack(\"invalid list index %d\", index)\n}"}, {"instruction": "// New creates a new chunk.\n//  cap: the limit for the max number of rows.\n//  maxChunkSize: the max limit for the number of rows.", "input": "go language", "output": "func New(fields []*types.FieldType, cap, maxChunkSize int) *Chunk {\n\tchk := new(Chunk)\n\tchk.columns = make([]*column, 0, len(fields))\n\tchk.capacity = mathutil.Min(cap, maxChunkSize)\n\tfor _, f := range fields {\n\t\telemLen := getFixedLen(f)\n\t\tif elemLen == varElemLen {\n\t\t\tchk.columns = append(chk.columns, newVarLenColumn(chk.capacity, nil))\n\t\t} else {\n\t\t\tchk.columns = append(chk.columns, newFixedLenColumn(elemLen, chk.capacity))\n\t\t}\n\t}\n\tchk.numVirtualRows = 0\n\n\t// set the default value of requiredRows to maxChunkSize to let chk.IsFull() behave\n\t// like how we judge whether a chunk is full now, then the statement\n\t// \"chk.NumRows() < maxChunkSize\"\n\t// is equal to\n\t// \"!chk.IsFull()\".\n\tchk.requiredRows = maxChunkSize\n\treturn chk\n}"}, {"instruction": "// getAffinityTermProperties receives a Pod and affinity terms and returns the namespaces and\n// selectors of the terms.", "input": "go language", "output": "func getAffinityTermProperties(pod *v1.Pod, terms []v1.PodAffinityTerm) (properties []*affinityTermProperties, err error) {\n\tif terms == nil {\n\t\treturn properties, nil\n\t}\n\n\tfor _, term := range terms {\n\t\tnamespaces := priorityutil.GetNamespacesFromPodAffinityTerm(pod, &term)\n\t\tselector, err := metav1.LabelSelectorAsSelector(term.LabelSelector)\n\t\tif err != nil {\n\t\t\treturn nil, err\n\t\t}\n\t\tproperties = append(properties, &affinityTermProperties{namespaces: namespaces, selector: selector})\n\t}\n\treturn properties, nil\n}"}, {"instruction": "// getCgroupProcs takes a cgroup directory name as an argument\n// reads through the cgroup's procs file and returns a list of tgid's.\n// It returns an empty list if a procs file doesn't exists", "input": "go language", "output": "func getCgroupProcs(dir string) ([]int, error) {\n\tprocsFile := filepath.Join(dir, \"cgroup.procs\")\n\tf, err := os.Open(procsFile)\n\tif err != nil {\n\t\tif os.IsNotExist(err) {\n\t\t\t// The procsFile does not exist, So no pids attached to this directory\n\t\t\treturn []int{}, nil\n\t\t}\n\t\treturn nil, err\n\t}\n\tdefer f.Close()\n\n\ts := bufio.NewScanner(f)\n\tout := []int{}\n\tfor s.Scan() {\n\t\tif t := s.Text(); t != \"\" {\n\t\t\tpid, err := strconv.Atoi(t)\n\t\t\tif err != nil {\n\t\t\t\treturn nil, fmt.Errorf(\"unexpected line in %v; could not convert to pid: %v\", procsFile, err)\n\t\t\t}\n\t\t\tout = append(out, pid)\n\t\t}\n\t}\n\treturn out, nil\n}"}, {"instruction": "// GetLackHandles gets the handles in expectedHandles but not in obtainedHandlesMap.", "input": "go language", "output": "func GetLackHandles(expectedHandles []int64, obtainedHandlesMap map[int64]struct{}) []int64 {\n\tdiffCnt := len(expectedHandles) - len(obtainedHandlesMap)\n\tdiffHandles := make([]int64, 0, diffCnt)\n\tvar cnt int\n\tfor _, handle := range expectedHandles {\n\t\tisExist := false\n\t\tif _, ok := obtainedHandlesMap[handle]; ok {\n\t\t\tdelete(obtainedHandlesMap, handle)\n\t\t\tisExist = true\n\t\t}\n\t\tif !isExist {\n\t\t\tdiffHandles = append(diffHandles, handle)\n\t\t\tcnt++\n\t\t\tif cnt == diffCnt {\n\t\t\t\tbreak\n\t\t\t}\n\t\t}\n\t}\n\n\treturn diffHandles\n}"}, {"instruction": "// HistoryViewerFor returns an implementation of HistoryViewer interface for the given schema kind", "input": "go language", "output": "func HistoryViewerFor(kind schema.GroupKind, c kubernetes.Interface) (HistoryViewer, error) {\n\telem := kapps.GroupKindElement(kind)\n\tvisitor := &HistoryVisitor{\n\t\tclientset: c,\n\t}\n\n\t// Determine which HistoryViewer we need here\n\terr := elem.Accept(visitor)\n\n\tif err != nil {\n\t\treturn nil, fmt.Errorf(\"error retrieving history for %q, %v\", kind.String(), err)\n\t}\n\n\tif visitor.result == nil {\n\t\treturn nil, fmt.Errorf(\"no history viewer has been implemented for %q\", kind.String())\n\t}\n\n\treturn visitor.result, nil\n}"}, {"instruction": "// Visit in a FileVisitor is just taking care of opening/closing files", "input": "go language", "output": "func (v *FileVisitor) Visit(fn VisitorFunc) error {\n\tvar f *os.File\n\tif v.Path == constSTDINstr {\n\t\tf = os.Stdin\n\t} else {\n\t\tvar err error\n\t\tf, err = os.Open(v.Path)\n\t\tif err != nil {\n\t\t\treturn err\n\t\t}\n\t\tdefer f.Close()\n\t}\n\n\t// TODO: Consider adding a flag to force to UTF16, apparently some\n\t// Windows tools don't write the BOM\n\tutf16bom := unicode.BOMOverride(unicode.UTF8.NewDecoder())\n\tv.StreamVisitor.Reader = transform.NewReader(f, utf16bom)\n\n\treturn v.StreamVisitor.Visit(fn)\n}"}, {"instruction": "// evalString evals a builtinGreatestStringSig.\n// See http://dev.mysql.com/doc/refman/5.7/en/comparison-operators.html#function_greatest", "input": "go language", "output": "func (b *builtinGreatestStringSig) evalString(row chunk.Row) (max string, isNull bool, err error) {\n\tmax, isNull, err = b.args[0].EvalString(b.ctx, row)\n\tif isNull || err != nil {\n\t\treturn max, isNull, err\n\t}\n\tfor i := 1; i < len(b.args); i++ {\n\t\tvar v string\n\t\tv, isNull, err = b.args[i].EvalString(b.ctx, row)\n\t\tif isNull || err != nil {\n\t\t\treturn max, isNull, err\n\t\t}\n\t\tif types.CompareString(v, max) > 0 {\n\t\t\tmax = v\n\t\t}\n\t}\n\treturn\n}"}, {"instruction": "// evalString evals a builtinInet6AtonSig.\n// See https://dev.mysql.com/doc/refman/5.7/en/miscellaneous-functions.html#function_inet6-aton", "input": "go language", "output": "func (b *builtinInet6AtonSig) evalString(row chunk.Row) (string, bool, error) {\n\tval, isNull, err := b.args[0].EvalString(b.ctx, row)\n\tif err != nil || isNull {\n\t\treturn \"\", true, err\n\t}\n\n\tif len(val) == 0 {\n\t\treturn \"\", true, nil\n\t}\n\n\tip := net.ParseIP(val)\n\tif ip == nil {\n\t\treturn \"\", true, nil\n\t}\n\n\tvar isMappedIpv6 bool\n\tif ip.To4() != nil && strings.Contains(val, \":\") {\n\t\t//mapped ipv6 address.\n\t\tisMappedIpv6 = true\n\t}\n\n\tvar result []byte\n\tif isMappedIpv6 || ip.To4() == nil {\n\t\tresult = make([]byte, net.IPv6len)\n\t} else {\n\t\tresult = make([]byte, net.IPv4len)\n\t}\n\n\tif isMappedIpv6 {\n\t\tcopy(result[12:], ip.To4())\n\t\tresult[11] = 0xff\n\t\tresult[10] = 0xff\n\t} else if ip.To4() == nil {\n\t\tcopy(result, ip.To16())\n\t} else {\n\t\tcopy(result, ip.To4())\n\t}\n\n\treturn string(result[:]), false, nil\n}"}, {"instruction": "// adjustColumnInfoInAddColumn is used to set the correct position of column info when adding column.\n// 1. The added column was append at the end of tblInfo.Columns, due to ddl state was not public then.\n//    It should be moved to the correct position when the ddl state to be changed to public.\n// 2. The offset of column should also to be set to the right value.", "input": "go language", "output": "func adjustColumnInfoInAddColumn(tblInfo *model.TableInfo, offset int) {\n\toldCols := tblInfo.Columns\n\tnewCols := make([]*model.ColumnInfo, 0, len(oldCols))\n\tnewCols = append(newCols, oldCols[:offset]...)\n\tnewCols = append(newCols, oldCols[len(oldCols)-1])\n\tnewCols = append(newCols, oldCols[offset:len(oldCols)-1]...)\n\t// Adjust column offset.\n\toffsetChanged := make(map[int]int)\n\tfor i := offset + 1; i < len(newCols); i++ {\n\t\toffsetChanged[newCols[i].Offset] = i\n\t\tnewCols[i].Offset = i\n\t}\n\tnewCols[offset].Offset = offset\n\t// Update index column offset info.\n\t// TODO: There may be some corner cases for index column offsets, we may check this later.\n\tfor _, idx := range tblInfo.Indices {\n\t\tfor _, col := range idx.Columns {\n\t\t\tnewOffset, ok := offsetChanged[col.Offset]\n\t\t\tif ok {\n\t\t\t\tcol.Offset = newOffset\n\t\t\t}\n\t\t}\n\t}\n\ttblInfo.Columns = newCols\n}"}, {"instruction": "// ValidateConditionalService validates conditionally valid fields.", "input": "go language", "output": "func ValidateConditionalService(service, oldService *api.Service) field.ErrorList {\n\tvar errs field.ErrorList\n\t// If the SCTPSupport feature is disabled, and the old object isn't using the SCTP feature, prevent the new object from using it\n\tif !utilfeature.DefaultFeatureGate.Enabled(features.SCTPSupport) && len(serviceSCTPFields(oldService)) == 0 {\n\t\tfor _, f := range serviceSCTPFields(service) {\n\t\t\terrs = append(errs, field.NotSupported(f, api.ProtocolSCTP, []string{string(api.ProtocolTCP), string(api.ProtocolUDP)}))\n\t\t}\n\t}\n\treturn errs\n}"}, {"instruction": "// NewXForwarded creates a new XForwarded.", "input": "go language", "output": "func NewXForwarded(insecure bool, trustedIps []string, next http.Handler) (*XForwarded, error) {\n\tvar ipChecker *ip.Checker\n\tif len(trustedIps) > 0 {\n\t\tvar err error\n\t\tipChecker, err = ip.NewChecker(trustedIps)\n\t\tif err != nil {\n\t\t\treturn nil, err\n\t\t}\n\t}\n\n\thostname, err := os.Hostname()\n\tif err != nil {\n\t\thostname = \"localhost\"\n\t}\n\n\treturn &XForwarded{\n\t\tinsecure:   insecure,\n\t\ttrustedIps: trustedIps,\n\t\tipChecker:  ipChecker,\n\t\tnext:       next,\n\t\thostname:   hostname,\n\t}, nil\n}"}, {"instruction": "// setLimits updates the allowed peer count and total capacity of the priority\n// client pool. Since the free client pool is a child of the priority pool the\n// remaining peer count and capacity is assigned to the free pool by calling its\n// own setLimits function.\n//\n// Note: a decreasing change of the total capacity is applied with a delay.", "input": "go language", "output": "func (v *priorityClientPool) setLimits(count int, totalCap uint64) {\n\tv.lock.Lock()\n\tdefer v.lock.Unlock()\n\n\tv.totalCapAnnounced = totalCap\n\tif totalCap > v.totalCap {\n\t\tv.setLimitsNow(count, totalCap)\n\t\tv.subs.send(totalCap, false)\n\t\treturn\n\t}\n\tv.setLimitsNow(count, v.totalCap)\n\tif totalCap < v.totalCap {\n\t\tv.subs.send(totalCap, totalCap < v.totalConnectedCap)\n\t\tfor i, s := range v.updateSchedule {\n\t\t\tif totalCap >= s.totalCap {\n\t\t\t\ts.totalCap = totalCap\n\t\t\t\tv.updateSchedule = v.updateSchedule[:i+1]\n\t\t\t\treturn\n\t\t\t}\n\t\t}\n\t\tv.updateSchedule = append(v.updateSchedule, scheduledUpdate{time: mclock.Now() + mclock.AbsTime(dropCapacityDelay), totalCap: totalCap})\n\t\tif len(v.updateSchedule) == 1 {\n\t\t\tv.scheduleCounter++\n\t\t\tid := v.scheduleCounter\n\t\t\tv.updateSchedule[0].id = id\n\t\t\ttime.AfterFunc(dropCapacityDelay, func() { v.checkUpdate(id) })\n\t\t}\n\t} else {\n\t\tv.updateSchedule = nil\n\t}\n}"}, {"instruction": "// NewEnvelopeTransformer returns a transformer which implements a KEK-DEK based envelope encryption scheme.\n// It uses envelopeService to encrypt and decrypt DEKs. Respective DEKs (in encrypted form) are prepended to\n// the data items they encrypt. A cache (of size cacheSize) is maintained to store the most recently\n// used decrypted DEKs in memory.", "input": "go language", "output": "func NewEnvelopeTransformer(envelopeService Service, cacheSize int, baseTransformerFunc func(cipher.Block) value.Transformer) (value.Transformer, error) {\n\tif cacheSize == 0 {\n\t\tcacheSize = defaultCacheSize\n\t}\n\tcache, err := lru.New(cacheSize)\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\treturn &envelopeTransformer{\n\t\tenvelopeService:     envelopeService,\n\t\ttransformers:        cache,\n\t\tbaseTransformerFunc: baseTransformerFunc,\n\t}, nil\n}"}, {"instruction": "// RawReverseScan implements the RawKV interface.\n// Scan the range of [endKey, startKey)\n// It doesn't support Scanning from \"\", because locating the last Region is not yet implemented.", "input": "go language", "output": "func (mvcc *MVCCLevelDB) RawReverseScan(startKey, endKey []byte, limit int) []Pair {\n\tmvcc.mu.Lock()\n\tdefer mvcc.mu.Unlock()\n\n\titer := mvcc.db.NewIterator(&util.Range{\n\t\tLimit: startKey,\n\t}, nil)\n\n\tsuccess := iter.Last()\n\n\tvar pairs []Pair\n\tfor success && len(pairs) < limit {\n\t\tkey := iter.Key()\n\t\tvalue := iter.Value()\n\t\terr := iter.Error()\n\t\tif bytes.Compare(key, endKey) < 0 {\n\t\t\tbreak\n\t\t}\n\t\tpairs = append(pairs, Pair{\n\t\t\tKey:   append([]byte{}, key...),\n\t\t\tValue: append([]byte{}, value...),\n\t\t\tErr:   err,\n\t\t})\n\t\tsuccess = iter.Prev()\n\t}\n\treturn pairs\n}"}, {"instruction": "// scaleForResourceMappings attempts to fetch the scale for the\n// resource with the given name and namespace, trying each RESTMapping\n// in turn until a working one is found.  If none work, the first error\n// is returned.  It returns both the scale, as well as the group-resource from\n// the working mapping.", "input": "go language", "output": "func (a *HorizontalController) scaleForResourceMappings(namespace, name string, mappings []*apimeta.RESTMapping) (*autoscalingv1.Scale, schema.GroupResource, error) {\n\tvar firstErr error\n\tfor i, mapping := range mappings {\n\t\ttargetGR := mapping.Resource.GroupResource()\n\t\tscale, err := a.scaleNamespacer.Scales(namespace).Get(targetGR, name)\n\t\tif err == nil {\n\t\t\treturn scale, targetGR, nil\n\t\t}\n\n\t\t// if this is the first error, remember it,\n\t\t// then go on and try other mappings until we find a good one\n\t\tif i == 0 {\n\t\t\tfirstErr = err\n\t\t}\n\t}\n\n\t// make sure we handle an empty set of mappings\n\tif firstErr == nil {\n\t\tfirstErr = fmt.Errorf(\"unrecognized resource\")\n\t}\n\n\treturn nil, schema.GroupResource{}, firstErr\n}"}, {"instruction": "// NewProviderAggregator returns an aggregate of all the providers configured in the static configuration.", "input": "go language", "output": "func NewProviderAggregator(conf static.Providers) ProviderAggregator {\n\tp := ProviderAggregator{}\n\n\tif conf.File != nil {\n\t\tp.quietAddProvider(conf.File)\n\t}\n\n\tif conf.Docker != nil {\n\t\tp.quietAddProvider(conf.Docker)\n\t}\n\n\tif conf.Marathon != nil {\n\t\tp.quietAddProvider(conf.Marathon)\n\t}\n\n\tif conf.Rest != nil {\n\t\tp.quietAddProvider(conf.Rest)\n\t}\n\n\tif conf.Kubernetes != nil {\n\t\tp.quietAddProvider(conf.Kubernetes)\n\t}\n\n\tif conf.KubernetesCRD != nil {\n\t\tp.quietAddProvider(conf.KubernetesCRD)\n\t}\n\tif conf.Rancher != nil {\n\t\tp.quietAddProvider(conf.Rancher)\n\t}\n\n\treturn p\n}"}, {"instruction": "// DataForAnalyzeStatus gets all the analyze jobs.", "input": "go language", "output": "func DataForAnalyzeStatus() (rows [][]types.Datum) {\n\tfor _, job := range statistics.GetAllAnalyzeJobs() {\n\t\tjob.Lock()\n\t\tvar startTime interface{}\n\t\tif job.StartTime.IsZero() {\n\t\t\tstartTime = nil\n\t\t} else {\n\t\t\tstartTime = types.Time{Time: types.FromGoTime(job.StartTime), Type: mysql.TypeDatetime}\n\t\t}\n\t\trows = append(rows, types.MakeDatums(\n\t\t\tjob.DBName,        // TABLE_SCHEMA\n\t\t\tjob.TableName,     // TABLE_NAME\n\t\t\tjob.PartitionName, // PARTITION_NAME\n\t\t\tjob.JobInfo,       // JOB_INFO\n\t\t\tjob.RowCount,      // ROW_COUNT\n\t\t\tstartTime,         // START_TIME\n\t\t\tjob.State,         // STATE\n\t\t))\n\t\tjob.Unlock()\n\t}\n\treturn\n}"}, {"instruction": "// Stop stops the node with the given ID", "input": "go language", "output": "func (net *Network) Stop(id enode.ID) error {\n\t// IMPORTANT: node.Stop() must NOT be called under net.lock as\n\t// node.Reachable() closure has a reference to the network and\n\t// calls net.InitConn() what also locks the network. => DEADLOCK\n\t// That holds until the following ticket is not resolved:\n\n\tvar err error\n\n\tnode, err := func() (*Node, error) {\n\t\tnet.lock.Lock()\n\t\tdefer net.lock.Unlock()\n\n\t\tnode := net.getNode(id)\n\t\tif node == nil {\n\t\t\treturn nil, fmt.Errorf(\"node %v does not exist\", id)\n\t\t}\n\t\tif !node.Up() {\n\t\t\treturn nil, fmt.Errorf(\"node %v already down\", id)\n\t\t}\n\t\tnode.SetUp(false)\n\t\treturn node, nil\n\t}()\n\tif err != nil {\n\t\treturn err\n\t}\n\n\terr = node.Stop() // must be called without net.lock\n\n\tnet.lock.Lock()\n\tdefer net.lock.Unlock()\n\n\tif err != nil {\n\t\tnode.SetUp(true)\n\t\treturn err\n\t}\n\tlog.Info(\"Stopped node\", \"id\", id, \"err\", err)\n\tev := ControlEvent(node)\n\tnet.events.Send(ev)\n\treturn nil\n}"}, {"instruction": "// NewCachingSecretManager creates a manager that keeps a cache of all secrets\n// necessary for registered pods.\n// It implements the following logic:\n// - whenever a pod is created or updated, the cached versions of all secrets\n//   are invalidated\n// - every GetObject() call tries to fetch the value from local cache; if it is\n//   not there, invalidated or too old, we fetch it from apiserver and refresh the\n//   value in cache; otherwise it is just fetched from cache", "input": "go language", "output": "func NewCachingSecretManager(kubeClient clientset.Interface, getTTL manager.GetObjectTTLFunc) Manager {\n\tgetSecret := func(namespace, name string, opts metav1.GetOptions) (runtime.Object, error) {\n\t\treturn kubeClient.CoreV1().Secrets(namespace).Get(name, opts)\n\t}\n\tsecretStore := manager.NewObjectStore(getSecret, clock.RealClock{}, getTTL, defaultTTL)\n\treturn &secretManager{\n\t\tmanager: manager.NewCacheBasedManager(secretStore, getSecretNames),\n\t}\n}"}, {"instruction": "// StringFlagPutHandler wraps an http Handler to set string type flag.", "input": "go language", "output": "func StringFlagPutHandler(setter StringFlagSetterFunc) http.HandlerFunc {\n\treturn http.HandlerFunc(func(w http.ResponseWriter, req *http.Request) {\n\t\tswitch {\n\t\tcase req.Method == \"PUT\":\n\t\t\tbody, err := ioutil.ReadAll(req.Body)\n\t\t\tif err != nil {\n\t\t\t\twritePlainText(http.StatusBadRequest, \"error reading request body: \"+err.Error(), w)\n\t\t\t\treturn\n\t\t\t}\n\t\t\tdefer req.Body.Close()\n\t\t\tresponse, err := setter(string(body))\n\t\t\tif err != nil {\n\t\t\t\twritePlainText(http.StatusBadRequest, err.Error(), w)\n\t\t\t\treturn\n\t\t\t}\n\t\t\twritePlainText(http.StatusOK, response, w)\n\t\t\treturn\n\t\tdefault:\n\t\t\twritePlainText(http.StatusNotAcceptable, \"unsupported http method\", w)\n\t\t\treturn\n\t\t}\n\t})\n}"}, {"instruction": "// ToPB implements PhysicalPlan ToPB interface.", "input": "go language", "output": "func (p *PhysicalIndexScan) ToPB(ctx sessionctx.Context) (*tipb.Executor, error) {\n\tcolumns := make([]*model.ColumnInfo, 0, p.schema.Len())\n\ttableColumns := p.Table.Cols()\n\tfor _, col := range p.schema.Columns {\n\t\tif col.ID == model.ExtraHandleID {\n\t\t\tcolumns = append(columns, model.NewExtraHandleColInfo())\n\t\t} else {\n\t\t\tcolumns = append(columns, model.FindColumnInfo(tableColumns, col.ColName.L))\n\t\t}\n\t}\n\tidxExec := &tipb.IndexScan{\n\t\tTableId: p.Table.ID,\n\t\tIndexId: p.Index.ID,\n\t\tColumns: model.ColumnsToProto(columns, p.Table.PKIsHandle),\n\t\tDesc:    p.Desc,\n\t}\n\tunique := checkCoverIndex(p.Index, p.Ranges)\n\tidxExec.Unique = &unique\n\treturn &tipb.Executor{Tp: tipb.ExecType_TypeIndexScan, IdxScan: idxExec}, nil\n}"}, {"instruction": "// numCPU queries the system for the count of threads available\n// for use to this process.\n//\n// Issues two syscalls.\n// Returns 0 on errors. Use |runtime.NumCPU| in that case.", "input": "go language", "output": "func numCPU() int {\n\t// Gets the affinity mask for a process: The very one invoking this function.\n\tpid, _, _ := unix.RawSyscall(unix.SYS_GETPID, 0, 0, 0)\n\n\tvar mask [1024 / 64]uintptr\n\t_, _, err := unix.RawSyscall(unix.SYS_SCHED_GETAFFINITY, pid, uintptr(len(mask)*8), uintptr(unsafe.Pointer(&mask[0])))\n\tif err != 0 {\n\t\treturn 0\n\t}\n\n\t// For every available thread a bit is set in the mask.\n\tncpu := 0\n\tfor _, e := range mask {\n\t\tif e == 0 {\n\t\t\tcontinue\n\t\t}\n\t\tncpu += int(popcnt(uint64(e)))\n\t}\n\treturn ncpu\n}"}, {"instruction": "// PrepareForUpdate clears fields that are not allowed to be set by end users on update.", "input": "go language", "output": "func (networkPolicyStrategy) PrepareForUpdate(ctx context.Context, obj, old runtime.Object) {\n\tnewNetworkPolicy := obj.(*networking.NetworkPolicy)\n\toldNetworkPolicy := old.(*networking.NetworkPolicy)\n\n\t// Any changes to the spec increment the generation number, any changes to the\n\t// status should reflect the generation number of the corresponding object.\n\t// See metav1.ObjectMeta description for more information on Generation.\n\tif !reflect.DeepEqual(oldNetworkPolicy.Spec, newNetworkPolicy.Spec) {\n\t\tnewNetworkPolicy.Generation = oldNetworkPolicy.Generation + 1\n\t}\n}"}]