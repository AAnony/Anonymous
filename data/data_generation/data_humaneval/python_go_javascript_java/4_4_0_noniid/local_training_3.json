[{"instruction": "def get_report(self):\n    \"\"\" describe the graph\n\n    :returns: report\n    :rtype: string\n    \"\"\"\n", "input": "", "output": "    ostr = ''\n    ostr += \"Nodes: \"+str(len(self.__nodes.keys()))+\"\\n\"\n    ostr += \"Edges: \"+str(len(self.__edges.keys()))+\"\\n\"\n    return ostr"}, {"instruction": "def to_task(self):\n        \"\"\"Return a task object representing this message.\"\"\"\n", "input": "", "output": "        from google.appengine.api.taskqueue import Task\n\n        task_args = self.get_task_args().copy()\n\n        payload = None\n        if 'payload' in task_args:\n            payload = task_args.pop('payload')\n\n        kwargs = {\n            'method': METHOD_TYPE,\n            'payload': json.dumps(payload)\n        }\n\n        kwargs.update(task_args)\n\n        return Task(**kwargs)"}, {"instruction": "def _cellrepr(value, allow_formulas):\n    \"\"\"\n    Get a string representation of dataframe value.\n\n    :param :value: the value to represent\n    :param :allow_formulas: if True, allow values starting with '='\n            to be interpreted as formulas; otherwise, escape\n            them with an apostrophe to avoid formula interpretation.\n    \"\"\"\n", "input": "", "output": "    if pd.isnull(value) is True:\n        return \"\"\n    if isinstance(value, float):\n        value = repr(value)\n    else:\n        value = str(value)\n    if (not allow_formulas) and value.startswith('='):\n        value = \"'%s\" % value\n    return value"}, {"instruction": "def __flush(self, async=True):\n        \"\"\" Flushes messages through current HttpRequest and closes it.\n            It assumes a current requesthandler and requires a lock\n            on self.lock \"\"\"\n", "input": "", "output": "        rh = self.rh\n        messages = list(self.messages)\n        stream_notices = list(self.stream_notices)\n        self.stream_notices = []\n        self.messages = []\n        args = (rh, messages, stream_notices)\n        if async:\n            self.hub.threadPool.execute_named(self.__inner_flush,\n                '%s __inner__flush' % self.hub.l.name, *args)\n        else:\n            self.__inner_flush(*args)\n        self.rh = None\n        self._set_timeout(int(time.time() + self.hub.timeout))"}, {"instruction": "def end(self):\n        \"\"\"\n        Ends the response. Useful for quickly ending connection with no data\n        sent\n        \"\"\"\n", "input": "", "output": "        self.send_headers()\n        self.write()\n        self.write_eof()\n        self.has_ended = True"}, {"instruction": "def encode_request(name, max_size):\n    \"\"\" Encode request into client_message\"\"\"\n", "input": "", "output": "    client_message = ClientMessage(payload_size=calculate_size(name, max_size))\n    client_message.set_message_type(REQUEST_TYPE)\n    client_message.set_retryable(RETRYABLE)\n    client_message.append_str(name)\n    client_message.append_int(max_size)\n    client_message.update_frame_length()\n    return client_message"}, {"instruction": "def remove_query_param(self, key, value=None):\n        \"\"\"\n        Remove a query param from a URL\n\n        Set the value parameter if removing from a list.\n\n        :param string key: The key to delete\n        :param string value: The value of the param to delete (of more than one)\n        \"\"\"\n", "input": "", "output": "        parse_result = self.query_params()\n        if value is not None:\n            index = parse_result[key].index(value)\n            del parse_result[key][index]\n        else:\n            del parse_result[key]\n        return URL._mutate(self, query=unicode_urlencode(parse_result, doseq=True))"}, {"instruction": "def list_domains():\n    '''\n    Return a list of virtual machine names on the minion\n\n    CLI Example:\n\n    .. code-block:: bash\n\n        salt '*' virt.list_domains\n    '''\n", "input": "", "output": "    data = __salt__['vmadm.list'](keyed=True)\n    vms = [\"UUID                                  TYPE  RAM      STATE             ALIAS\"]\n    for vm in data:\n        vms.append(\"{vmuuid}{vmtype}{vmram}{vmstate}{vmalias}\".format(\n            vmuuid=vm.ljust(38),\n            vmtype=data[vm]['type'].ljust(6),\n            vmram=data[vm]['ram'].ljust(9),\n            vmstate=data[vm]['state'].ljust(18),\n            vmalias=data[vm]['alias'],\n        ))\n    return vms"}, {"instruction": "def modifie_options(self, field_option, value):\n        \"\"\"Set options in modifications.\n        All options will be stored since it should be grouped in the DB.\"\"\"\n", "input": "", "output": "        options = dict(self[\"options\"] or {}, **{field_option: value})\n        self.modifications[\"options\"] = options"}, {"instruction": "def cli(yamlfile, format, classes, directory):\n    \"\"\" Generate a UML representation of a biolink model \"\"\"\n", "input": "", "output": "    print(YumlGenerator(yamlfile, format).serialize(classes=classes, directory=directory), end=\"\")"}, {"instruction": "def cancel(self):\n        \"\"\"\n        Cancel itself and following NOTs as far as possible.\n        Returns the simplified expression.\n        \"\"\"\n", "input": "", "output": "        expr = self\n        while True:\n            arg = expr.args[0]\n            if not isinstance(arg, self.__class__):\n                return expr\n            expr = arg.args[0]\n            if not isinstance(expr, self.__class__):\n                return expr"}, {"instruction": "def set_compression_pool_size(pool_size):\n    \"\"\"\n    Set the size of the compression workers thread pool.\n    If the pool is already created, it waits until all jobs are finished, and then proceeds with setting the new size.\n\n    Parameters\n    ----------\n        pool_size : `int`\n            The size of the pool (must be a positive integer)\n\n    Returns\n    -------\n    `None`\n    \"\"\"\n", "input": "", "output": "    pool_size = int(pool_size)\n    if pool_size < 1:\n        raise ValueError(\"The compression thread pool size cannot be of size {}\".format(pool_size))\n\n    global _compress_thread_pool\n    if _compress_thread_pool is not None:\n        _compress_thread_pool.close()\n        _compress_thread_pool.join()\n    _compress_thread_pool = ThreadPool(pool_size)"}, {"instruction": "def path_to(self, p):\n        \"\"\"Returns the absolute path to a given relative path.\"\"\"\n", "input": "", "output": "        if os.path.isabs(p):\n            return p\n\n        return os.sep.join([self._original_dir, p])"}, {"instruction": "def unpack_grad_tuple(gv, gpt):\n    \"\"\"Unpack a previously packed collection of gradient tensors.\n\n  Args:\n    gv: A (grad, var) pair to be unpacked.\n    gpt: A GradPackTuple describing the packing operation that produced gv.\n\n  Returns:\n    A list of (grad, var) pairs corresponding to the values that were\n     originally packed into gv, maybe following subsequent operations like\n     reduction.\n  \"\"\"\n", "input": "", "output": "    elt_widths = [x.num_elements() for x in gpt.shapes]\n    with tf.device(gv[0][0].device):\n        with tf.name_scope(\"unpack\"):\n            splits = tf.split(gv[0], elt_widths)\n            unpacked_gv = []\n            for idx, s in enumerate(splits):\n                unpacked_gv.append((tf.reshape(s, gpt.shapes[idx]),\n                                    gpt.vars[idx]))\n    return unpacked_gv"}, {"instruction": "def generate_tar_files(directory_list):\n    \"\"\"Public function that reads a list of local directories and generates tar archives from them\"\"\"\n", "input": "", "output": "    \n    tar_file_list = []\n\n    for directory in directory_list:\n        if dir_exists(directory):\n            _generate_tar(directory)                  # create the tar archive\n            tar_file_list.append(directory + '.tar')  # append the tar archive filename to the returned tar_file_list list\n        else:\n            stderr(\"The directory '\" + directory + \"' does not exist and a tar archive could not be created from it.\", exit=1)            \n\n    return tar_file_list"}, {"instruction": "def route_table_get(name, resource_group, **kwargs):\n    '''\n    .. versionadded:: 2019.2.0\n\n    Get details about a specific route table.\n\n    :param name: The name of the route table to query.\n\n    :param resource_group: The resource group name assigned to the\n        route table.\n\n    CLI Example:\n\n    .. code-block:: bash\n\n        salt-call azurearm_network.route_table_get test-rt-table testgroup\n\n    '''\n", "input": "", "output": "    expand = kwargs.get('expand')\n\n    netconn = __utils__['azurearm.get_client']('network', **kwargs)\n\n    try:\n        table = netconn.route_tables.get(\n            route_table_name=name,\n            resource_group_name=resource_group,\n            expand=expand\n        )\n        result = table.as_dict()\n    except CloudError as exc:\n        __utils__['azurearm.log_cloud_error']('network', str(exc), **kwargs)\n        result = {'error': str(exc)}\n\n    return result"}, {"instruction": "def update_bounds_boxes(self):\n        \"\"\"\n        updates bounds boxes with bounds of current specimen and fit\n        \"\"\"\n", "input": "", "output": "        if self.s not in list(self.Data.keys()):\n            self.select_specimen(list(self.Data.keys())[0])\n        self.T_list = self.Data[self.s]['zijdblock_steps']\n        if self.current_fit:\n            self.tmin_box.SetItems(self.T_list)\n            self.tmax_box.SetItems(self.T_list)\n            if type(self.current_fit.tmin) == str and type(self.current_fit.tmax) == str:\n                self.tmin_box.SetStringSelection(self.current_fit.tmin)\n                self.tmax_box.SetStringSelection(self.current_fit.tmax)\n        if self.ie_open:\n            self.ie.update_bounds_boxes(self.T_list)"}, {"instruction": "def future(self, rev=None):\n        \"\"\"Return a Mapping of items after the given revision.\n\n        Default revision is the last one looked up.\n\n        \"\"\"\n", "input": "", "output": "        if rev is not None:\n            self.seek(rev)\n        return WindowDictFutureView(self._future)"}, {"instruction": "def import_sanitizer(sanitizer):\n    \"\"\"\n    Imports the sanitizer python module.\n\n    :param sanitizer: Sanitizer python module file.\n    :type sanitizer: unicode\n    :return: Module.\n    :rtype: object\n    \"\"\"\n", "input": "", "output": "\n    directory = os.path.dirname(sanitizer)\n    not directory in sys.path and sys.path.append(directory)\n\n    namespace = __import__(foundations.strings.get_splitext_basename(sanitizer))\n    if hasattr(namespace, \"bleach\"):\n        return namespace\n    else:\n        raise foundations.exceptions.ProgrammingError(\n            \"{0} | '{1}' is not a valid sanitizer module file!\".format(sanitizer))"}, {"instruction": "def walk(self, visitor):\n        \"\"\"\n        Walk the branch and call the visitor function\n        on each node.\n        @param visitor: A function.\n        @return: self\n        @rtype: L{Element}\n        \"\"\"\n", "input": "", "output": "        visitor(self)\n        for c in self.children:\n            c.walk(visitor)\n        return self"}, {"instruction": "def get_url(\n    width, height=None, background_color=\"cccccc\",\n    text_color=\"969696\", text=None, random_background_color=False\n):\n    \"\"\"\n    Craft the URL for a placeholder image.\n\n    You can customize the background color, text color and text using\n    the optional keyword arguments\n\n    If you want to use a random color pass in random_background_color as True.\n    \"\"\"\n", "input": "", "output": "    if random_background_color:\n        background_color = _get_random_color()\n\n    # If height is not provided, presume it is will be a square\n    if not height:\n        height = width\n    d = dict(\n        width=width,\n        height=height,\n        bcolor=background_color,\n        tcolor=text_color\n    )\n    url = URL % d\n    if text:\n        text = text.replace(\" \", \"+\")\n        url = url + \"?text=\" + text\n    return url"}, {"instruction": "def to_xml(self):\n        '''\n        Returns an XMLi representation of the shipping details.\n        @return: Element\n        '''\n", "input": "", "output": "        for n, v in {\"recipient\": self.recipient}.items():\n            if is_empty_or_none(v):\n                raise ValueError(\"'%s' attribute cannot be empty or None.\" % n)\n\n        doc = Document()\n        root = doc.createElement(\"shipping\")\n        root.appendChild(self.recipient.to_xml(\"recipient\"))\n        return root"}, {"instruction": "def deserialize(self, mimetypes):  # pylint: disable=arguments-differ\n        \"\"\" Invoke the deserializer\n\n        Upon successful deserialization a dict will be returned\n        containing the following key/vals:\n\n            {\n                'content': <uploaded object>,\n                'content-type': <content-type of content>,\n                'file-ext': <file extension based on content-type>,\n                'file-name': <file name of content>,\n            }\n\n        :param mimetypes:\n            allowed mimetypes of the object in the request\n            payload\n        :return:\n            normalized dict\n        \"\"\"\n", "input": "", "output": "\n        super(Deserializer, self).deserialize()\n\n        parts = self.parse(mimetypes)\n        data = self.normalize(parts)\n\n        return data"}, {"instruction": "def get_beautiful_soup(self, source=None):\n        \"\"\" BeautifulSoup is a toolkit for dissecting an HTML document\n            and extracting what you need. It's great for screen-scraping! \"\"\"\n", "input": "", "output": "        from bs4 import BeautifulSoup\n        if not source:\n            self.wait_for_ready_state_complete()\n            source = self.get_page_source()\n        soup = BeautifulSoup(source, \"html.parser\")\n        return soup"}, {"instruction": "def disconnect(self):\n        \"\"\"Disconnect from event stream.\"\"\"\n", "input": "", "output": "        _LOGGING.debug('Disconnecting from stream: %s', self.name)\n        self.kill_thrd.set()\n        self.thrd.join()\n        _LOGGING.debug('Event stream thread for %s is stopped', self.name)\n        self.kill_thrd.clear()"}, {"instruction": "def add_gateway_responses(self, gateway_responses):\n        \"\"\"\n        Add Gateway Response definitions to Swagger.\n\n        :param dict gateway_responses: Dictionary of GatewayResponse configuration which gets translated.\n        \"\"\"\n", "input": "", "output": "        self.gateway_responses = self.gateway_responses or {}\n\n        for response_type, response in gateway_responses.items():\n            self.gateway_responses[response_type] = response.generate_swagger()"}, {"instruction": "def returned(n):\n\t\"\"\"Generate a random walk and return True if the walker has returned to\n\tthe origin after taking `n` steps.\n\t\"\"\"\n", "input": "", "output": "\t## `takei` yield lazily so we can short-circuit and avoid computing the rest of the walk\n\tfor pos in randwalk() >> drop(1) >> takei(xrange(n-1)):\n\t\tif pos == Origin:\n\t\t\treturn True\n\treturn False"}, {"instruction": "def cmap(self, background_color='#000000', random_state=None):\n        \"\"\"\n        Define a matplotlib colormap consisting of (random) muted\n        colors.\n\n        This is very useful for plotting the segmentation image.\n\n        Parameters\n        ----------\n        background_color : str or `None`, optional\n            A hex string in the \"#rrggbb\" format defining the first\n            color in the colormap.  This color will be used as the\n            background color (label = 0) when plotting the segmentation\n            image.  The default is black ('#000000').\n\n        random_state : int or `~numpy.random.RandomState`, optional\n            The pseudo-random number generator state used for random\n            sampling.  Separate function calls with the same\n            ``random_state`` will generate the same colormap.\n        \"\"\"\n", "input": "", "output": "\n        return self.make_cmap(background_color=background_color,\n                              random_state=random_state)"}, {"instruction": "def create_url(urlbase, urlargd, escape_urlargd=True, urlhash=None):\n    \"\"\"Creates a W3C compliant URL. Output will look like this:\n    'urlbase?param1=value1&amp;param2=value2'\n    @param urlbase: base url (e.g. config.CFG_SITE_URL/search)\n    @param urlargd: dictionary of parameters. (e.g. p={'recid':3, 'of'='hb'}\n    @param escape_urlargd: boolean indicating if the function should escape\n                           arguments (e.g. < becomes &lt; or \" becomes &quot;)\n    @param urlhash: hash string to add at the end of the link\n    \"\"\"\n", "input": "", "output": "    separator = '&amp;'\n    output = urlbase\n    if urlargd:\n        output += '?'\n        if escape_urlargd:\n            arguments = [escape(quote(str(key)), quote=True) + '=' +\n                         escape(quote(str(urlargd[key])), quote=True)\n                         for key in urlargd.keys()]\n        else:\n            arguments = [str(key) + '=' + str(urlargd[key])\n                         for key in urlargd.keys()]\n        output += separator.join(arguments)\n    if urlhash:\n        output += \"#\" + escape(quote(str(urlhash)))\n    return output"}, {"instruction": "def _get_appointee(id):\n    \"\"\"\n    Return a restclients.models.hrp.AppointeePerson object\n    \"\"\"\n", "input": "", "output": "    url = \"%s%s.json\" % (URL_PREFIX, id)\n    response = get_resource(url)\n    return process_json(response)"}, {"instruction": "def _iand(self, other):\n\t\t\"\"\"Set multiplicity of each element to the minimum of the two collections.\n\n\t\tif isinstance(other, _basebag):\n\t\t\tThis runs in O(other.num_unique_elements())\n\t\telse:\n\t\t\tThis runs in O(len(other))\n\t\t\"\"\"\n", "input": "", "output": "\t\t# TODO do we have to create a bag from the other first?\n\t\tif not isinstance(other, _basebag):\n\t\t\tother = self._from_iterable(other)\n\t\tfor elem, old_count in set(self.counts()):\n\t\t\tother_count = other.count(elem)\n\t\t\tnew_count = min(other_count, old_count)\n\t\t\tself._set_count(elem, new_count)\n\t\treturn self"}, {"instruction": "def i18n(msg, event=None, lang='en', domain='backend'):\n    \"\"\"Gettext function wrapper to return a message in a specified language by domain\n\n    To use internationalization (i18n) on your messages, import it as '_' and use as usual.\n    Do not forget to supply the client's language setting.\"\"\"\n", "input": "", "output": "\n    if event is not None:\n        language = event.client.language\n    else:\n        language = lang\n\n    domain = Domain(domain)\n    return domain.get(language, msg)"}, {"instruction": "def validate(self):\n        \"\"\"  Confirms the current token is still valid.\n        Returns True if it is valid, False otherwise. \"\"\"\n", "input": "", "output": "\n        try:\n            resp = self.request().get(self.validate_url, verify=self.verifySSL).json()\n        except TokenExpiredError:\n            return False\n        except AttributeError:\n            return False\n\n        if 'error' in resp:\n            return False\n        return True"}, {"instruction": "def decodeEntities(self, len, what, end, end2, end3):\n        \"\"\"This function is deprecated, we now always process entities\n          content through xmlStringDecodeEntities  TODO: remove it in\n          next major release.  [67] Reference ::= EntityRef | CharRef\n            [69] PEReference ::= '%' Name ';' \"\"\"\n", "input": "", "output": "        ret = libxml2mod.xmlDecodeEntities(self._o, len, what, end, end2, end3)\n        return ret"}, {"instruction": "def get(self, sid):\n        \"\"\"\n        Constructs a TranscriptionContext\n\n        :param sid: The unique string that identifies the resource\n\n        :returns: twilio.rest.api.v2010.account.transcription.TranscriptionContext\n        :rtype: twilio.rest.api.v2010.account.transcription.TranscriptionContext\n        \"\"\"\n", "input": "", "output": "        return TranscriptionContext(self._version, account_sid=self._solution['account_sid'], sid=sid, )"}, {"instruction": "def loadXMLGenericData(filename):  # not tested\n    \"\"\"Read any type of vtk data object encoded in XML format. Return an ``Actor(vtkActor)`` object.\"\"\"\n", "input": "", "output": "    reader = vtk.vtkXMLGenericDataObjectReader()\n    reader.SetFileName(filename)\n    reader.Update()\n    return Actor(reader.GetOutput())"}, {"instruction": "def create_powerflow_problem(timerange, components):\n    \"\"\"\n    Create PyPSA network object and fill with data\n    Parameters\n    ----------\n    timerange: Pandas DatetimeIndex\n        Time range to be analyzed by PF\n    components: dict\n    Returns\n    -------\n    network: PyPSA powerflow problem object\n    \"\"\"\n", "input": "", "output": "\n    # initialize powerflow problem\n    network, snapshots = init_pypsa_network(timerange)\n\n    # add components to network\n    for component in components.keys():\n        network.import_components_from_dataframe(components[component],\n                                                 component)\n\n    return network, snapshots"}, {"instruction": "def cmpxchg(self, ptr, cmp, val, ordering, failordering=None, name=''):\n        \"\"\"\n        Atomic compared-and-set:\n            atomic {\n                old = *ptr\n                success = (old == cmp)\n                if (success)\n                    *ptr = val\n                }\n            name = { old, success }\n\n        If failordering is `None`, the value of `ordering` is used.\n        \"\"\"\n", "input": "", "output": "        failordering = ordering if failordering is None else failordering\n        inst = instructions.CmpXchg(self.block, ptr, cmp, val, ordering,\n                                    failordering, name=name)\n        self._insert(inst)\n        return inst"}, {"instruction": "def _CopyDateFromString(self, date_string):\n    \"\"\"Copies a date from a string.\n\n    Args:\n      date_string (str): date value formatted as: YYYY-MM-DD\n\n    Returns:\n      tuple[int, int, int]: year, month, day of month.\n\n    Raises:\n      ValueError: if the date string is invalid or not supported.\n    \"\"\"\n", "input": "", "output": "    date_string_length = len(date_string)\n\n    # The date string should at least contain 'YYYY-MM-DD'.\n    if date_string_length < 10:\n      raise ValueError('Date string too short.')\n\n    if date_string[4] != '-' or date_string[7] != '-':\n      raise ValueError('Invalid date string.')\n\n    try:\n      year = int(date_string[0:4], 10)\n    except ValueError:\n      raise ValueError('Unable to parse year.')\n\n    try:\n      month = int(date_string[5:7], 10)\n    except ValueError:\n      raise ValueError('Unable to parse month.')\n\n    try:\n      day_of_month = int(date_string[8:10], 10)\n    except ValueError:\n      raise ValueError('Unable to parse day of month.')\n\n    days_per_month = self._GetDaysPerMonth(year, month)\n    if day_of_month < 1 or day_of_month > days_per_month:\n      raise ValueError('Day of month value out of bounds.')\n\n    return year, month, day_of_month"}, {"instruction": "def conf_merger(user_dict, variable):\n    \"\"\"\n    Merge global configuration with user's personal configuration.\n\n    Global configuration has always higher priority.\n    \"\"\"\n", "input": "", "output": "    if variable not in globals().keys():\n        raise NameError(\"Unknown variable '%s'.\" % variable)\n\n    if variable not in user_dict:\n        return globals()[variable]\n\n    return globals()[variable] and user_dict[variable]"}, {"instruction": "def track_exists(self, localdir):\n        \"\"\" Check if track exists in local directory. \"\"\"\n", "input": "", "output": "        path = glob.glob(self.gen_localdir(localdir) +\n                         self.gen_filename() + \"*\")\n        if len(path) > 0 and os.path.getsize(path[0]) > 0:\n            return True\n        return False"}, {"instruction": "def cublasZgerc(handle, m, n, alpha, x, incx, y, incy, A, lda):\n    \"\"\"\n    Rank-1 operation on complex general matrix.\n\n    \"\"\"\n", "input": "", "output": "\n    status = _libcublas.cublasZgerc_v2(handle,\n                                       m, n, ctypes.byref(cuda.cuDoubleComplex(alpha.real,\n                                                                               alpha.imag)),\n                                       int(x), incx, int(y), incy, int(A), lda)\n    cublasCheckStatus(status)"}, {"instruction": "def instagram_config(self, id, secret, scope=None, **_):\n        \"\"\" Get config dictionary for instagram oauth \"\"\"\n", "input": "", "output": "        scope = scope if scope else 'basic'\n        token_params = dict(scope=scope)\n\n        config = dict(\n            # request_token_url=None,\n            access_token_url='/oauth/access_token/',\n            authorize_url='/oauth/authorize/',\n            base_url='https://api.instagram.com/',\n            consumer_key=id,\n            consumer_secret=secret,\n            request_token_params=token_params\n        )\n        return config"}, {"instruction": "def unit_ball_L_inf(shape, precondition=True):\n  \"\"\"A tensorflow variable tranfomed to be constrained in a L_inf unit ball.\n\n  Note that this code also preconditions the gradient to go in the L_inf\n  direction of steepest descent.\n\n  EXPERIMENTAL: Do not use for adverserial examples if you need to be confident\n  they are strong attacks. We are not yet confident in this code.\n  \"\"\"\n", "input": "", "output": "  x = tf.Variable(tf.zeros(shape))\n  if precondition:\n    return constrain_L_inf_precondition(x)\n  else:\n    return constrain_L_inf(x)"}, {"instruction": "def __copy_tree(src_dir, dest_dir):\r\n    \"\"\"\r\n    The shutil.copytree() or distutils.dir_util.copy_tree() will happen to report\r\n    error list below if we invoke it again and again ( at least in python 2.7.4 ):\r\n\r\n    IOError: [Errno 2] No such file or directory: ...\r\n\r\n    So we have to write our's copy_tree() for that purpose.\r\n    \"\"\"\n", "input": "", "output": "\r\n    if not os.path.exists(dest_dir):\r\n        os.makedirs(dest_dir)\r\n        shutil.copystat(src_dir, dest_dir)\r\n\r\n    for entry in os.listdir(src_dir):\r\n        from_path = os.path.join(src_dir, entry)\r\n        to_path = os.path.join(dest_dir, entry)\r\n        if os.path.isdir(from_path):\r\n            __copy_tree(from_path, to_path)\r\n        else:\r\n            shutil.copy2(from_path, to_path)"}, {"instruction": "def copy(self, deep=True, data=None):\n        \"\"\"Returns a copy of this object.\n\n        `deep` is ignored since data is stored in the form of\n        pandas.Index, which is already immutable. Dimensions, attributes\n        and encodings are always copied.\n\n        Use `data` to create a new object with the same structure as\n        original but entirely new data.\n\n        Parameters\n        ----------\n        deep : bool, optional\n            Deep is always ignored.\n        data : array_like, optional\n            Data to use in the new object. Must have same shape as original.\n\n        Returns\n        -------\n        object : Variable\n            New object with dimensions, attributes, encodings, and optionally\n            data copied from original.\n        \"\"\"\n", "input": "", "output": "        if data is None:\n            data = self._data\n        else:\n            data = as_compatible_data(data)\n            if self.shape != data.shape:\n                raise ValueError(\"Data shape {} must match shape of object {}\"\n                                 .format(data.shape, self.shape))\n        return type(self)(self.dims, data, self._attrs,\n                          self._encoding, fastpath=True)"}, {"instruction": "def handle(self, *args, **options):\n        \"\"\"\n        Iterates a command over all registered schemata.\n        \"\"\"\n", "input": "", "output": "        if options['schema_name']:\n            # only run on a particular schema\n            connection.set_schema_to_public()\n            self.execute_command(get_tenant_model().objects.get(schema_name=options['schema_name']), self.COMMAND_NAME,\n                                 *args, **options)\n        else:\n            for tenant in get_tenant_model().objects.all():\n                if not (options['skip_public'] and tenant.schema_name == get_public_schema_name()):\n                    self.execute_command(tenant, self.COMMAND_NAME, *args, **options)"}, {"instruction": "def sub(self, b):\n        \"\"\"\n        Binary operation: sub\n\n        :param b: The other operand\n        :return: self - b\n        \"\"\"\n", "input": "", "output": "        new_bits = max(self.bits, b.bits)\n\n        overflow = self._wrapped_overflow_sub(self, b)\n        if overflow:\n            return StridedInterval.top(self.bits)\n\n        lb = self._modular_sub(self.lower_bound, b.upper_bound, new_bits)\n        ub = self._modular_sub(self.upper_bound, b.lower_bound, new_bits)\n\n        # Is it initialized?\n        uninitialized = self.uninitialized or b.uninitialized\n\n        # Take the GCD of two operands' strides\n        stride = fractions.gcd(self.stride, b.stride)\n\n        return StridedInterval(bits=new_bits, stride=stride, lower_bound=lb, upper_bound=ub,\n                               uninitialized=uninitialized).normalize()"}, {"instruction": "def _set(self, **kwargs):\n        \"\"\"\n        Sets user-supplied params.\n        \"\"\"\n", "input": "", "output": "        for param, value in kwargs.items():\n            p = getattr(self, param)\n            if value is not None:\n                try:\n                    value = p.typeConverter(value)\n                except TypeError as e:\n                    raise TypeError('Invalid param value given for param \"%s\". %s' % (p.name, e))\n            self._paramMap[p] = value\n        return self"}, {"instruction": "def _generate_overview_note(pass_count, only_warning_count, error_count, total_count):\n    \"\"\" Generates and returns the HTML note that provides a summary of validation status. \"\"\"\n", "input": "", "output": "\n    note_html = ['<div class=\"progress\">']\n    pbars = [\n        [ float(error_count), 'danger', 'had errors' ],\n        [ float(only_warning_count), 'warning', 'had warnings' ],\n        [ float(pass_count), 'success', 'passed' ]\n    ]\n    for b in pbars:\n        if b[0]:\n            note_html.append(\n                '<div class=\"progress-bar progress-bar-{pbcol}\" style=\"width: {pct}%\" data-toggle=\"tooltip\" title=\"{count} {sample} {txt}\">{count}</div>'. \\\n                format(\n                    pbcol = b[1],\n                    count = int(b[0]),\n                    pct = (b[0]/float(total_count))*100.0,\n                    txt = b[2],\n                    sample = 'samples' if b[0] > 1 else 'sample'\n                )\n            )\n    note_html.append('</div>')\n\n    return \"\\n\".join(note_html)"}, {"instruction": "def torrent_from_url(self, url, cache=True, prefetch=False):\n        \"\"\"Create a Torrent object from a given URL.\n\n        If the cache option is set, check to see if we already have a Torrent\n        object representing it. If prefetch is set, automatically query the\n        torrent's info page to fill in the torrent object. (If prefetch is\n        false, then the torrent page will be queried lazily on-demand.)\n\n        \"\"\"\n", "input": "", "output": "        if self._use_cache(cache) and url in self._torrent_cache:\n            return self._torrent_cache[url]\n        torrent = Torrent(url, cache, prefetch)\n        if cache:\n            self._torrent_cache[url] = torrent\n        return torrent"}, {"instruction": "def random_id(length):\n    \"\"\"Generates a random ID of given length\"\"\"\n", "input": "", "output": "\n    def char():\n        "}, {"instruction": "def flag_forgotten_entries(session, today=None):\n    \"\"\"Flag any entries from previous days where users forgot to sign\n    out.\n\n    :param session: SQLAlchemy session through which to access the database.\n    :param today: (optional) The current date as a `datetime.date` object. Used for testing.\n    \"\"\"\n", "input": "", "output": "    today = date.today() if today is None else today\n\n    forgotten = (\n        session\n        .query(Entry)\n        .filter(Entry.time_out.is_(None))\n        .filter(Entry.forgot_sign_out.is_(False))\n        .filter(Entry.date < today)\n    )\n\n    for entry in forgotten:\n        e = sign_out(entry, forgot=True)\n        logger.debug('Signing out forgotten entry: {}'.format(e))\n        session.add(e)\n\n    session.commit()"}, {"instruction": "def find_content(self, text):\n        \"\"\"Find content.\"\"\"\n", "input": "", "output": "\n        for m in self.pattern.finditer(self.norm_nl(text)):\n            self.evaluate(m)"}, {"instruction": "def remove(self):\n        \"\"\"Remove this file.\"\"\"\n", "input": "", "output": "        if self.exists() or self.islink():\n            self.fs.unlink(self.get_internal_path())\n            return 1\n        return None"}, {"instruction": "def compute_empirical(cls, X):\n        \"\"\"Compute empirical distribution.\"\"\"\n", "input": "", "output": "        z_left = []\n        z_right = []\n        L = []\n        R = []\n\n        U, V = cls.split_matrix(X)\n        N = len(U)\n        base = np.linspace(EPSILON, 1.0 - EPSILON, COMPUTE_EMPIRICAL_STEPS)\n        # See https://github.com/DAI-Lab/Copulas/issues/45\n\n        for k in range(COMPUTE_EMPIRICAL_STEPS):\n            left = sum(np.logical_and(U <= base[k], V <= base[k])) / N\n            right = sum(np.logical_and(U >= base[k], V >= base[k])) / N\n\n            if left > 0:\n\n                z_left.append(base[k])\n                L.append(left / base[k] ** 2)\n\n            if right > 0:\n                z_right.append(base[k])\n                R.append(right / (1 - z_right[k]) ** 2)\n\n        return z_left, L, z_right, R"}, {"instruction": "def get_link(self, path, method, callback, view):\n        \"\"\"\n        Return a `coreapi.Link` instance for the given endpoint.\n        \"\"\"\n", "input": "", "output": "        fields = self.get_path_fields(path, method, callback, view)\n        fields += self.get_serializer_fields(path, method, callback, view)\n        fields += self.get_pagination_fields(path, method, callback, view)\n        fields += self.get_filter_fields(path, method, callback, view)\n\n        if fields and any([field.location in ('form', 'body')\n                           for field in fields]):\n            encoding = self.get_encoding(path, method, callback, view)\n        else:\n            encoding = None\n\n        description = self.get_description(path, method, callback, view)\n\n        link = coreapi.Link(\n            url=urlparse.urljoin(self.url, path),\n            action=method.lower(),\n            encoding=encoding,\n            description=description,\n            fields=fields,\n            transform=None,  # Not handled, but here for future reference\n        )\n        link._responses = self.get_responses(path, method, callback, view)\n        link._produces = self.get_produces(path, method, callback, view)\n\n        return link"}, {"instruction": "def _humanSortKey(s):\n  \"\"\"Sort strings with numbers in a way that makes sense to humans (e.g., 5 < 20)\"\"\"\n", "input": "", "output": "  if isinstance(s, str):\n    return [w.isdigit() and int(w) or w for w in re.split(r'(\\d+)', s)]\n  else:\n    return s"}, {"instruction": "def match_one(template, image, options=None):\n    \"\"\"\n    Match template and find exactly one match in the Image using specified features.\n\n    :param template: Template Image\n    :param image: Search Image\n    :param options: Options include\n        - features: List of options for each feature\n    :return: (Box, Score) Bounding box of the matched object, Heatmap value\n    \"\"\"\n", "input": "", "output": "    heatmap, scale = multi_feat_match(template, image, options)\n\n    min_val, _, min_loc, _ = cv.minMaxLoc(heatmap)\n    top_left = tuple(scale * x for x in min_loc)\n    score = min_val\n\n    h, w = template.shape[:2]\n    return Box(top_left[0], top_left[1], w, h), score"}, {"instruction": "def points(self, size=1.0, highlight=None, colorlist=None, opacity=1.0):\n        \"\"\"Display the system as points.\n\n        :param float size: the size of the points.\n\n\n        \"\"\"\n", "input": "", "output": "        if colorlist is None:\n            colorlist = [get_atom_color(t) for t in self.topology['atom_types']]\n        if highlight is not None:\n            if isinstance(highlight, int):\n                colorlist[highlight] = 0xff0000\n            if isinstance(highlight, (list, np.ndarray)):\n                for i in highlight:\n                    colorlist[i] = 0xff0000\n\n        sizes = [size] * len(self.topology['atom_types'])\n\n        points = self.add_representation('points', {'coordinates': self.coordinates.astype('float32'),\n                                                    'colors': colorlist,\n                                                    'sizes': sizes,\n                                                    'opacity': opacity})\n        # Update closure\n        def update(self=self, points=points):\n            self.update_representation(points, {'coordinates': self.coordinates.astype('float32')})\n\n        self.update_callbacks.append(update)\n        self.autozoom(self.coordinates)"}, {"instruction": "def create_or_edit(self, id, seq, resource): # pylint: disable=invalid-name,redefined-builtin\n        \"\"\"Create or edit a highlight.\n\n        :param id: Result ID as an int.\n        :param seq: TestResult sequence ID as an int.\n        :param resource: :class:`highlights.Highlight <highlights.Highlight>` object\n        :return: :class:`highlights.Highlight <highlights.Highlight>` object\n        :rtype: highlights.Highlight\n        \"\"\"\n", "input": "", "output": "        schema = HighlightSchema(exclude=('id', 'seq'))\n        json = self.service.encode(schema, resource)\n\n        schema = HighlightSchema()\n        resp = self.service.edit(self._base(id, seq), resource.line, json)\n        return self.service.decode(schema, resp)"}, {"instruction": "def _create_paths(self, basedir, name=None):\n        \"\"\"Create datadir and subdir paths.\"\"\"\n", "input": "", "output": "        if name:\n            datapath = os.path.join(basedir, name)\n        else:\n            datapath = basedir\n\n        dbpath = os.path.join(datapath, 'db')\n        if not os.path.exists(dbpath):\n            os.makedirs(dbpath)\n        if self.args['verbose']:\n            print('creating directory: %s' % dbpath)\n\n        return datapath"}, {"instruction": "def tangent(obj, params, **kwargs):\n    \"\"\" Evaluates the tangent vector of the curves or surfaces at the input parameter values.\n\n    This function is designed to evaluate tangent vectors of the B-Spline and NURBS shapes at single or\n    multiple parameter positions.\n\n    :param obj: input shape\n    :type obj: abstract.Curve or abstract.Surface\n    :param params: parameters\n    :type params: float, list or tuple\n    :return: a list containing \"point\" and \"vector\" pairs\n    :rtype: tuple\n    \"\"\"\n", "input": "", "output": "    normalize = kwargs.get('normalize', True)\n    if isinstance(obj, abstract.Curve):\n        if isinstance(params, (list, tuple)):\n            return ops.tangent_curve_single_list(obj, params, normalize)\n        else:\n            return ops.tangent_curve_single(obj, params, normalize)\n    if isinstance(obj, abstract.Surface):\n        if isinstance(params[0], float):\n            return ops.tangent_surface_single(obj, params, normalize)\n        else:\n            return ops.tangent_surface_single_list(obj, params, normalize)"}, {"instruction": "def get(self, item, default=None):\n        \"\"\"\n        Returns the value ``item`` from the host or hosts group variables.\n\n        Arguments:\n            item(``str``): The variable to get\n            default(``any``): Return value if item not found\n        \"\"\"\n", "input": "", "output": "        if hasattr(self, item):\n            return getattr(self, item)\n        try:\n            return self.__getitem__(item)\n\n        except KeyError:\n            return default"}, {"instruction": "def _floatize_x(x, new_x):\n    \"\"\" Make x and new_x float.\n    This is particulary useful for datetime dtype.\n    x, new_x: tuple of np.ndarray\n    \"\"\"\n", "input": "", "output": "    x = list(x)\n    new_x = list(new_x)\n    for i in range(len(x)):\n        if _contains_datetime_like_objects(x[i]):\n            # Scipy casts coordinates to np.float64, which is not accurate\n            # enough for datetime64 (uses 64bit integer).\n            # We assume that the most of the bits are used to represent the\n            # offset (min(x)) and the variation (x - min(x)) can be\n            # represented by float.\n            xmin = x[i].values.min()\n            x[i] = x[i]._to_numeric(offset=xmin, dtype=np.float64)\n            new_x[i] = new_x[i]._to_numeric(offset=xmin, dtype=np.float64)\n    return x, new_x"}, {"instruction": "def _GetStat(self):\n    \"\"\"Retrieves information about the file entry.\n\n    Returns:\n      VFSStat: a stat object.\n    \"\"\"\n", "input": "", "output": "    stat_object = super(BDEFileEntry, self)._GetStat()\n\n    stat_object.size = self._bde_volume.get_size()\n\n    return stat_object"}, {"instruction": "def peers(cls):\n        \"\"\"Return others of the same concrete type.\"\"\"\n", "input": "", "output": "        contentType = ContentType.objects.get_for_model(cls)\n        return cls.objects.filter(content_type=contentType)"}, {"instruction": "def unflatten(obj):\n  '''\n  TODO: add docs\n  '''\n", "input": "", "output": "  if not isdict(obj):\n    raise ValueError(\n      'only dict-like objects can be unflattened, not %r' % (obj,))\n  ret = dict()\n  sub = dict()\n  for key, value in obj.items():\n    if '.' not in key and '[' not in key:\n      ret[key] = value\n      continue\n    if '.' in key and '[' in key:\n      idx = min(key.find('.'), key.find('['))\n    elif '.' in key:\n      idx = key.find('.')\n    else:\n      idx = key.find('[')\n    prefix = key[:idx]\n    if prefix not in sub:\n      sub[prefix] = dict()\n    sub[prefix][key[idx:]] = value\n  for pfx, values in sub.items():\n    if pfx in ret:\n      raise ValueError(\n        'conflicting scalar vs. structure for prefix: %s' % (pfx,))\n    ret[pfx] = _relunflatten(pfx, values)\n  return ret"}, {"instruction": "def reentrancies(self):\n        \"\"\"\n        Return a mapping of variables to their re-entrancy count.\n\n        A re-entrancy is when more than one edge selects a node as its\n        target. These graphs are rooted, so the top node always has an\n        implicit entrancy. Only nodes with re-entrancies are reported,\n        and the count is only for the entrant edges beyond the first.\n        Also note that these counts are for the interpreted graph, not\n        for the linearized form, so inverted edges are always\n        re-entrant.\n        \"\"\"\n", "input": "", "output": "        entrancies = defaultdict(int)\n        entrancies[self.top] += 1  # implicit entrancy to top\n        for t in self.edges():\n            entrancies[t.target] += 1\n        return dict((v, cnt - 1) for v, cnt in entrancies.items() if cnt >= 2)"}, {"instruction": "def retire(self, process_schemas):\n        \"\"\"Retire obsolete processes.\n\n        Remove old process versions without data. Find processes that have been\n        registered but do not exist in the code anymore, then:\n\n        - If they do not have data: remove them\n        - If they have data: flag them not active (``is_active=False``)\n\n        \"\"\"\n", "input": "", "output": "        process_slugs = set(ps['slug'] for ps in process_schemas)\n\n        # Processes that are in DB but not in the code\n        retired_processes = Process.objects.filter(~Q(slug__in=process_slugs))\n\n        # Remove retired processes which do not have data\n        retired_processes.filter(data__exact=None).delete()\n\n        # Remove non-latest processes which do not have data\n        latest_version_processes = Process.objects.order_by('slug', '-version').distinct('slug')\n        Process.objects.filter(data__exact=None).difference(latest_version_processes).delete()\n\n        # Deactivate retired processes which have data\n        retired_processes.update(is_active=False)"}, {"instruction": "def _R2deriv(self,R,phi=0.,t=0.):\n        \"\"\"\n        NAME:\n           _R2deriv\n        PURPOSE:\n           evaluate the second radial derivative\n        INPUT:\n           R\n           phi\n           t\n        OUTPUT:\n           d2phi/dR2\n        HISTORY:\n           2016-06-02 - Written - Bovy (UofT)\n        \"\"\"\n", "input": "", "output": "        return self._Pot.R2deriv(R,0.,phi=phi,t=t,use_physical=False)"}, {"instruction": "def get(self, filepath):\n        \"\"\"\n        Get the contents of the specified file.\n        \"\"\"\n", "input": "", "output": "        exists = self.fs.exists(filepath)\n        if exists:\n            mime = magic.Magic(mime=True)\n            mime_type = mime.from_file(filepath)\n            if mime_type in self.unsupported_types:\n                self.set_status(204)\n                return\n            else:\n                contents = self.fs.read_file(filepath)\n            self.write({'filepath':filepath,'contents': contents})\n        else:\n            raise tornado.web.HTTPError(404)"}, {"instruction": "def iter(self, bucket):\n        \"\"\"https://github.com/frictionlessdata/tableschema-bigquery-py#storage\n        \"\"\"\n", "input": "", "output": "\n        # Get schema/data\n        schema = tableschema.Schema(self.describe(bucket))\n        table_name = self.__mapper.convert_bucket(bucket)\n        response = self.__service.tabledata().list(\n            projectId=self.__project,\n            datasetId=self.__dataset,\n            tableId=table_name).execute()\n\n        # Collect rows\n        rows = []\n        for fields in response['rows']:\n            row = [field['v'] for field in fields['f']]\n            rows.append(row)\n\n        # Sort rows\n        # TODO: provide proper sorting solution\n        rows = sorted(rows, key=lambda row: row[0] if row[0] is not None else 'null')\n\n        # Emit rows\n        for row in rows:\n            row = self.__mapper.restore_row(row, schema=schema)\n            yield row"}, {"instruction": "def remove_end_optionals(ir_blocks):\n    \"\"\"Return a list of IR blocks as a copy of the original, with EndOptional blocks removed.\"\"\"\n", "input": "", "output": "    new_ir_blocks = []\n    for block in ir_blocks:\n        if not isinstance(block, EndOptional):\n            new_ir_blocks.append(block)\n    return new_ir_blocks"}, {"instruction": "def format_page(self, page):\n        \"\"\"\n        Banana banana\n        \"\"\"\n", "input": "", "output": "        self.formatting_page_signal(self, page)\n        return self._format_page(page)"}, {"instruction": "def ls(self, src, extra_args=[]):\n        '''List files in a directory'''\n", "input": "", "output": "        src = [self._full_hdfs_path(x) for x in src]\n        output = self._getStdOutCmd([self._hadoop_cmd, 'fs', '-ls'] + extra_args + src, True)\n        return self._transform_ls_output(output, self.hdfs_url)"}, {"instruction": "def apply_flat(self, config, namespace_separator='_', prefix=''):\n        # type: (Dict[str, Any], str, str) -> None\n        \"\"\"Apply additional configuration from a flattened dictionary\n\n        This will look for dictionary items that match flattened keys from base_config and apply their values on the\n        current configuration object.\n\n        This can be useful for applying configuration from environment variables and flat configuration file formats\n        such as INI files.\n        \"\"\"\n", "input": "", "output": "        self._init_flat_pointers()\n        for key_stack, (container, orig_key) in self._flat_pointers.items():\n            flat_key = '{prefix}{joined_key}'.format(prefix=prefix, joined_key=namespace_separator.join(key_stack))\n            if flat_key in config:\n                container[orig_key] = config[flat_key]"}, {"instruction": "def get(cls, device_server_id, custom_headers=None):\n        \"\"\"\n        Get one of your DeviceServers.\n\n        :type api_context: context.ApiContext\n        :type device_server_id: int\n        :type custom_headers: dict[str, str]|None\n\n        :rtype: BunqResponseDeviceServer\n        \"\"\"\n", "input": "", "output": "\n        if custom_headers is None:\n            custom_headers = {}\n\n        api_client = client.ApiClient(cls._get_api_context())\n        endpoint_url = cls._ENDPOINT_URL_READ.format(device_server_id)\n        response_raw = api_client.get(endpoint_url, {}, custom_headers)\n\n        return BunqResponseDeviceServer.cast_from_bunq_response(\n            cls._from_json(response_raw, cls._OBJECT_TYPE_GET)\n        )"}, {"instruction": "def get_client(self):\n        \"\"\"\n        Retrieves or creates a client instance from this configuration object. If instantiated from this configuration,\n        the resulting object is also cached in the property ``client`` and a reference to this configuration is stored\n        on the client object.\n\n        :return: Client object instance.\n        :rtype: docker.client.Client\n        \"\"\"\n", "input": "", "output": "        client = self._client\n        if not client:\n            self._client = client = self.client_constructor(**self.get_init_kwargs())\n            client.client_configuration = self\n            # Client might update the version number after construction.\n            updated_version = getattr(client, 'api_version', None)\n            if updated_version:\n                self.version = updated_version\n        return client"}, {"instruction": "def defaultFile(self):\n        \"\"\"Produce a reasonable default.\"\"\"\n", "input": "", "output": "        filename = self.curframe.f_code.co_filename\n        if filename == '<string>' and self.mainpyfile:\n            filename = self.mainpyfile\n        return filename"}, {"instruction": "def reflected_light_intensity(self):\n        \"\"\"\n        A measurement of the reflected light intensity, as a percentage.\n        \"\"\"\n", "input": "", "output": "        self._ensure_mode(self.MODE_REFLECT)\n        return self.value(0) * self._scale('REFLECT')"}, {"instruction": "def is_not_null_predicate(\n    raw_crash, dumps, processed_crash, processor, key=''\n):\n    \"\"\"a predicate that converts the key'd source to boolean.\n\n    parameters:\n        raw_crash - dict\n        dumps - placeholder in a fat interface - unused\n        processed_crash - placeholder in a fat interface - unused\n        processor - placeholder in a fat interface - unused\n    \"\"\"\n", "input": "", "output": "    try:\n        return bool(raw_crash[key])\n    except KeyError:\n        return False"}, {"instruction": "def wrap_sequence(sequence, books=None, tensor_shape=None):\n  \"\"\"Creates an input layer representing the given sequence of tensors.\n\n  Args:\n    sequence: A sequence of tensors.\n    books: The bookkeeper.\n    tensor_shape: An optional shape that will be set on the Tensor or verified\n      to match the tensor.\n  Returns:\n    A layer.\n  \"\"\"\n", "input": "", "output": "  if books is None:\n    books = bookkeeper.for_default_graph()\n  my_sequence = [\n      wrap(t, books=books, tensor_shape=tensor_shape) for t in sequence]\n  return Layer(books, sequence=my_sequence, name=my_sequence[0].name)"}, {"instruction": "def from_ros_pose_msg(pose_msg,\n                          from_frame='unassigned',\n                          to_frame='world'):\n        \"\"\"Creates a RigidTransform from a ROS pose msg. \n        \n        Parameters\n        ----------\n        pose_msg : :obj:`geometry_msgs.msg.Pose`\n            ROS pose message\n        \"\"\"\n", "input": "", "output": "        quaternion = np.array([pose_msg.orientation.w,\n                               pose_msg.orientation.x,\n                               pose_msg.orientation.y,\n                               pose_msg.orientation.z])\n        position = np.array([pose_msg.position.x,\n                             pose_msg.position.y,\n                             pose_msg.position.z])\n        pose = RigidTransform(rotation=quaternion,\n                              translation=position,\n                              from_frame=from_frame,\n                              to_frame=to_frame)\n        return pose"}, {"instruction": "def rho2sigma(self, rho0, Ra, Rs):\n        \"\"\"\n        converts 3d density into 2d projected density parameter\n        :param rho0:\n        :param Ra:\n        :param Rs:\n        :return:\n        \"\"\"\n", "input": "", "output": "        return np.pi * rho0 * Ra * Rs / (Rs + Ra)"}, {"instruction": "def help(ctx, topic, **kw):\n    \"\"\"Show help for any command.\n    \"\"\"\n", "input": "", "output": "    # The help command implementation is taken from\n    # https://www.burgundywall.com/post/having-click-help-subcommand\n    if topic is None:\n        click.echo(ctx.parent.get_help())\n    else:\n        click.echo(main.commands[topic].get_help(ctx))"}, {"instruction": "def _file_md5(file_):\n    \"\"\"\n    Compute the md5 digest of a file in base64 encoding.\n    \"\"\"\n", "input": "", "output": "    md5 = hashlib.md5()\n    chunk_size = 128 * md5.block_size\n    for chunk in iter(lambda: file_.read(chunk_size), b''):\n        md5.update(chunk)\n    file_.seek(0)\n    byte_digest = md5.digest()\n    return base64.b64encode(byte_digest).decode()"}, {"instruction": "def query(self, queryEngine, query=None, vendorSpecific=None, **kwargs):\n        \"\"\"See Also: queryResponse()\n\n        Args:\n          queryEngine:\n          query:\n          vendorSpecific:\n          **kwargs:\n\n        Returns:\n\n        \"\"\"\n", "input": "", "output": "        response = self.queryResponse(queryEngine, query, vendorSpecific, **kwargs)\n        return self._read_stream_response(response)"}, {"instruction": "def discussions_notifications(user):\n    '''Notify user about open discussions'''\n", "input": "", "output": "    notifications = []\n\n    # Only fetch required fields for notification serialization\n    # Greatly improve performances and memory usage\n    qs = discussions_for(user).only('id', 'created', 'title', 'subject')\n\n    # Do not dereference subject (so it's a DBRef)\n    # Also improve performances and memory usage\n    for discussion in qs.no_dereference():\n        notifications.append((discussion.created, {\n            'id': discussion.id,\n            'title': discussion.title,\n            'subject': {\n                'id': discussion.subject['_ref'].id,\n                'type': discussion.subject['_cls'].lower(),\n            }\n        }))\n\n    return notifications"}, {"instruction": "def median(array):\n    \"\"\"\n    Return the median value of a list of numbers.\n    \"\"\"\n", "input": "", "output": "    n = len(array)\n\n    if n < 1:\n        return 0\n    elif n == 1:\n        return array[0]\n\n    sorted_vals = sorted(array)\n    midpoint = int(n / 2)\n    if n % 2 == 1:\n        return sorted_vals[midpoint]\n    else:\n        return (sorted_vals[midpoint - 1] + sorted_vals[midpoint]) / 2.0"}, {"instruction": "def extract_points(self, pid, points):\n        \"\"\"Extract values at certain points in the grid from a given parameter\n        set. Cells are selected by interpolating the centroids of the cells\n        towards the line using a \"nearest\" scheme.\n\n        Note that data is only returned for the points provided. If you want to\n        extract multiple data points along a line, defined by start and end\n        point, use the **extract_along_line** function.\n\n        Parameters\n        ----------\n        pid: int\n            The parameter id to extract values from\n        points: Nx2 numpy.ndarray\n            (x, y) pairs\n\n        Returns\n        -------\n        values: numpy.ndarray (n x 1)\n            data values for extracted data points\n        \"\"\"\n", "input": "", "output": "        xy = self.grid.get_element_centroids()\n        data = self.parsets[pid]\n\n        iobj = spi.NearestNDInterpolator(xy, data)\n        values = iobj(points)\n        return values"}, {"instruction": "def vertex_fingerprints(self):\n        \"\"\"A fingerprint for each vertex\n\n           The result is invariant under permutation of the vertex indexes.\n           Vertices that are symmetrically equivalent will get the same\n           fingerprint, e.g. the hydrogens in methane would get the same\n           fingerprint.\n        \"\"\"\n", "input": "", "output": "        return self.get_vertex_fingerprints(\n            [self.get_vertex_string(i) for i in range(self.num_vertices)],\n            [self.get_edge_string(i) for i in range(self.num_edges)],\n        )"}, {"instruction": "def require_representation(self, req):\n        \"\"\"Require raw representation dictionary from falcon request object.\n\n        This does not perform any field parsing or validation but only uses\n        allowed content-encoding handler to decode content body.\n\n        Note:\n            Currently only JSON is allowed as content type.\n\n        Args:\n            req (falcon.Request): request object\n\n        Returns:\n            dict: raw dictionary of representation supplied in request body\n\n        \"\"\"\n", "input": "", "output": "        try:\n            type_, subtype, _ = parse_mime_type(req.content_type)\n            content_type = '/'.join((type_, subtype))\n        except:\n            raise falcon.HTTPUnsupportedMediaType(\n                description=\"Invalid Content-Type header: {}\".format(\n                    req.content_type\n                )\n            )\n\n        if content_type == 'application/json':\n            body = req.stream.read()\n            return json.loads(body.decode('utf-8'))\n        else:\n            raise falcon.HTTPUnsupportedMediaType(\n                description=\"only JSON supported, got: {}\".format(content_type)\n            )"}, {"instruction": "def apply(self, something: 'Reader') -> 'Reader':\n        r\"\"\"(<*>) :: f (a -> b) -> f a -> f b.\n\n        Haskell: f <*> g = \\x -> f x (g x)\n\n        Apply (<*>) is a beefed up map. It takes a Reader that\n        has a function in it and another Reader, and extracts that\n        function from the first Reader and then maps it over the second\n        one (composes the two functions).\n        \"\"\"\n", "input": "", "output": "\n        def _compose(x: Any):\n            f = self.run(x)\n            try:\n                ret = f(something.run(x))\n            except TypeError:\n                ret = partial(f, something.run(x))\n            return ret\n\n        return Reader(_compose)"}, {"instruction": "def CreateFeedItemAddOperation(name, price, date, ad_customizer_feed):\n  \"\"\"Creates a FeedItemOperation.\n\n  The generated FeedItemOperation will create a FeedItem with the specified\n  values when sent to FeedItemService.mutate.\n\n  Args:\n    name: the value for the name attribute of the FeedItem.\n    price: the value for the price attribute of the FeedItem.\n    date: the value for the date attribute of the FeedItem.\n    ad_customizer_feed: the AdCustomizerFeed we're associating the FeedItems\n        with.\n\n  Returns:\n    A new FeedItemOperation for adding a FeedItem.\n  \"\"\"\n", "input": "", "output": "  feed_item = {\n      'feedId': ad_customizer_feed['feedId'],\n      'attributeValues': [\n          {\n              'feedAttributeId': ad_customizer_feed['feedAttributes'][0]['id'],\n              'stringValue': name\n          },\n          {\n              'feedAttributeId': ad_customizer_feed['feedAttributes'][1]['id'],\n              'stringValue': price\n          },\n          {\n              'feedAttributeId': ad_customizer_feed['feedAttributes'][2]['id'],\n              'stringValue': date\n          }\n      ]\n  }\n\n  operation = {\n      'operator': 'ADD',\n      'operand': feed_item\n  }\n\n  return operation"}, {"instruction": "def initialize_segment_register_x64(self, state, concrete_target):\n        \"\"\"\n        Set the gs register in the angr to the value of the fs register in the concrete process\n\n        :param state:               state which will be modified\n        :param concrete_target:     concrete target that will be used to read the fs register\n        :return: None\n       \"\"\"\n", "input": "", "output": "        _l.debug(\"Synchronizing gs segment register\")\n        state.regs.gs = self._read_gs_register_x64(concrete_target)"}, {"instruction": "def set_virtualization_realm_type(self):\n        \"\"\"Sets the virtualization realm type from deployment properties\n\n        :return: None\n        \"\"\"\n", "input": "", "output": "        log = logging.getLogger(self.cls_logger + '.set_virtualization_realm_type')\n        self.virtualization_realm_type = self.get_value('cons3rt.deploymentRun.virtRealm.type')\n        log.info('Found virtualization realm type : {t}'.format(t=self.virtualization_realm_type))"}, {"instruction": "def load(self):\r\n        \"\"\"\r\n        Loads the children for this record item.\r\n        \r\n        :return     <bool> | changed\r\n        \"\"\"\n", "input": "", "output": "        if self.__loaded:\r\n            return False\r\n        \r\n        self.__loaded = True\r\n        self.setChildIndicatorPolicy(self.DontShowIndicatorWhenChildless)\r\n        \r\n        # loads the children for this widget\r\n        tree = self.treeWidget()\r\n        if tree.groupBy():\r\n            grps = self.childRecords().grouped(tree.groupBy())\r\n            for grp, records in grps.items():\r\n                tree.createGroupItem(grp, records, self)\r\n        else:\r\n            for record in self.childRecords():\r\n                tree.createRecordItem(record, self)\r\n        \r\n        return True"}, {"instruction": "def toString(self):\n        \"\"\"\n        Returns the network layers as a string.\n        \"\"\"\n", "input": "", "output": "        output = \"\"\n        for layer in reverse(self.layers):\n            output += layer.toString()\n        return output"}, {"instruction": "def _request(self, typ, id=0, method='GET', params=None, data=None, url=None):\n        \"\"\"\n        send the request, return response obj\n        \"\"\"\n", "input": "", "output": "\n        headers = { \"Accept\": \"application/json\" }\n        auth = None\n\n        if self.user:\n            auth = (self.user, self.password)\n\n        if not url:\n            if id:\n                url = \"%s/%s/%s\" % (self.url, typ, id)\n            else:\n                url = \"%s/%s\" % (self.url, typ)\n\n        return requests.request(method, url, params=params, data=data, auth=auth, headers=headers)"}, {"instruction": "def get_lookup_value(self, value):\n        \"\"\"\n        Override this method to convert displayed values to lookup values\n        \"\"\"\n", "input": "", "output": "        choices = self._field_choices()\n        if choices:\n            if isinstance(value, list):\n                return [c[0] for c in choices if c[1] in value]\n            else:\n                for c in choices:\n                    if c[1] == value:\n                        return c[0]\n        return value"}, {"instruction": "def _format_executable(lines, element, spacer=\"\"):\n    \"\"\"Performs formatting specific to a Subroutine or Function code\n    element for relevant docstrings.\n    \"\"\"\n", "input": "", "output": "    rlines = []\n    rlines.append(element.signature)\n    _format_summary(rlines, element)\n\n    rlines.append(\"\")\n    rlines.append(\"PARAMETERS\")\n    for p in element.ordered_parameters:\n        _format_value_element(rlines, p)\n\n    rlines.append(\"\")\n    _format_generic(rlines, element, [\"summary\"])\n\n    #Subroutines can have embedded types and functions which need to be handled.\n    if len(element.types) > 0:\n        rlines.append(\"\\nEMBEDDED TYPES\")\n        for key, value in list(element.types.items()):\n            _format_type(rlines, value, \"  \")\n\n    if len(element.executables) > 0:\n        rlines.append(\"\\nEMBEDDED EXECUTABLES\")\n        for key, value in list(element.executables.items()):\n            _format_executable(rlines, value, \"  \")\n\n    lines.extend([spacer + l for l in rlines])"}, {"instruction": "async def get_sleep_timer_settings(self) -> List[Setting]:\n        \"\"\"Get sleep timer settings.\"\"\"\n", "input": "", "output": "        return [\n            Setting.make(**x)\n            for x in await self.services[\"system\"][\"getSleepTimerSettings\"]({})\n        ]"}, {"instruction": "def from_hertz(self, hertz, standard_pitch=440):\n        \"\"\"Set the Note name and pitch, calculated from the hertz value.\n\n        The standard_pitch argument can be used to set the pitch of A-4,\n        from which the rest is calculated.\n        \"\"\"\n", "input": "", "output": "        value = ((log((float(hertz) * 1024) / standard_pitch, 2) +\n            1.0 / 24) * 12 + 9)  # notes.note_to_int(\"A\")\n        self.name = notes.int_to_note(int(value) % 12)\n        self.octave = int(value / 12) - 6\n        return self"}, {"instruction": "def erase(self, **kwargs):\n        \"\"\"Erase the job (remove job artifacts and trace).\n\n        Args:\n            **kwargs: Extra options to send to the server (e.g. sudo)\n\n        Raises:\n            GitlabAuthenticationError: If authentication is not correct\n            GitlabJobEraseError: If the job could not be erased\n        \"\"\"\n", "input": "", "output": "        path = '%s/%s/erase' % (self.manager.path, self.get_id())\n        self.manager.gitlab.http_post(path)"}, {"instruction": "def reset_network(roles, extra_vars=None):\n    \"\"\"Reset the network constraints (latency, bandwidth ...)\n\n    Remove any filter that have been applied to shape the traffic.\n\n    Args:\n        roles (dict): role->hosts mapping as returned by\n            :py:meth:`enoslib.infra.provider.Provider.init`\n        inventory (str): path to the inventory\n    \"\"\"\n", "input": "", "output": "    logger.debug('Reset the constraints')\n\n    if not extra_vars:\n        extra_vars = {}\n\n    tmpdir = os.path.join(os.getcwd(), TMP_DIRNAME)\n\n    _check_tmpdir(tmpdir)\n    utils_playbook = os.path.join(ANSIBLE_DIR, 'utils.yml')\n    options = {'enos_action': 'tc_reset',\n               'tc_output_dir': tmpdir}\n    options.update(extra_vars)\n    run_ansible([utils_playbook], roles=roles, extra_vars=options)"}, {"instruction": "def prune_empty_node(node, seen):\n    \"\"\"\n    Recursively remove empty branches and return whether this makes the node\n    itself empty.\n\n    The ``seen`` parameter is used to avoid infinite recursion due to cycles\n    (you never know).\n    \"\"\"\n", "input": "", "output": "    if node.methods:\n        return False\n    if id(node) in seen:\n        return True\n    seen = seen | {id(node)}\n    for branch in list(node.branches):\n        if prune_empty_node(branch, seen):\n            node.branches.remove(branch)\n        else:\n            return False\n    return True"}, {"instruction": "def _FormatDataToken(self, token_data):\n    \"\"\"Formats a data token as a dictionary of values.\n\n    Args:\n      token_data (bsm_token_data_data): AUT_DATA token data.\n\n    Returns:\n      dict[str, str]: token values.\n    \"\"\"\n", "input": "", "output": "    format_string = bsmtoken.BSM_TOKEN_DATA_PRINT.get(\n        token_data.data_format, 'UNKNOWN')\n\n    if token_data.data_format == 4:\n      data = bytes(bytearray(token_data.data)).split(b'\\x00')[0]\n      data = data.decode('utf-8')\n    else:\n      data = ''.join(['{0:02x}'.format(byte) for byte in token_data.data])\n    return {\n        'format': format_string,\n        'data': data}"}, {"instruction": "def _iterparse(xmlfile):\n    \"\"\"\n    Avoid bug in python 3.{2,3}. See http://bugs.python.org/issue9257.\n\n    :param xmlfile: XML file or file-like object\n    \"\"\"\n", "input": "", "output": "    try:\n        return ET.iterparse(xmlfile, events=(\"start-ns\", ))\n    except TypeError:\n        return ET.iterparse(xmlfile, events=(b\"start-ns\", ))"}, {"instruction": "def query_ssos(self):\n        \"\"\"\n        Use the MPC file that has been built up in processing this work\n        unit to generate another workunit.\n        \"\"\"\n", "input": "", "output": "        self._ssos_queried = True\n        mpc_filename = self.save()\n        return self.builder.build_workunit(mpc_filename)"}, {"instruction": "def codepoints2bitstream(codepoints):\n    \"\"\"\n    >>> list(codepoints2bitstream([0x48,0x45]))\n    [0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0]\n    >>> list(codepoints2bitstream(0x48))\n    [0, 0, 0, 1, 0, 0, 1, 0]\n    \"\"\"\n", "input": "", "output": "    if isinstance(codepoints, int):\n        codepoints = [codepoints]\n    for codepoint in codepoints:\n        bit_string = byte2bit_string(codepoint)\n        for bit in bit_string:\n            yield int(bit)"}, {"instruction": "def _to_dict(self):\n        \"\"\"Return a json dictionary representing this model.\"\"\"\n", "input": "", "output": "        _dict = {}\n        if hasattr(self, 'aggregations') and self.aggregations is not None:\n            _dict['aggregations'] = [x._to_dict() for x in self.aggregations]\n        return _dict"}, {"instruction": "def mkdir(self, pathobj, _):\n        \"\"\"\n        Creates remote directory\n        Note that this operation is not recursive\n        \"\"\"\n", "input": "", "output": "        if not pathobj.drive or not pathobj.root:\n            raise RuntimeError(\"Full path required: '%s'\" % str(pathobj))\n\n        if pathobj.exists():\n            raise OSError(17, \"File exists: '%s'\" % str(pathobj))\n\n        url = str(pathobj) + '/'\n        text, code = self.rest_put(url, session=pathobj.session, verify=pathobj.verify, cert=pathobj.cert)\n\n        if not code == 201:\n            raise RuntimeError(\"%s %d\" % (text, code))"}, {"instruction": "def stop_program(self, turn_off_load=True):\n        \"\"\"\n        Stops running programmed test sequence\n        :return: None\n        \"\"\"\n", "input": "", "output": "        self.__set_buffer_start(self.CMD_STOP_PROG)\n        self.__set_checksum()\n        self.__send_buffer()\n        if turn_off_load and self.load_on:\n            self.load_on = False"}, {"instruction": "def add_selected(self, ):\n        \"\"\"Create a new reftrack with the selected element and type and add it to the root.\n\n        :returns: None\n        :rtype: None\n        :raises: NotImplementedError\n        \"\"\"\n", "input": "", "output": "        browser = self.shot_browser if self.browser_tabw.currentIndex() == 1 else self.asset_browser\n        selelements = browser.selected_indexes(2)\n        if not selelements:\n            return\n        seltypes = browser.selected_indexes(3)\n        if not seltypes:\n            return\n        elementi = selelements[0]\n        typi = seltypes[0]\n        if not elementi.isValid() or not typi.isValid():\n            return\n        element = elementi.internalPointer().internal_data()\n        typ = typi.internalPointer().internal_data()[0]\n\n        reftrack.Reftrack(self.root, self.refobjinter, typ=typ, element=element)"}, {"instruction": "def new_worker(self, name: str):\n        \"\"\"Creates a new Worker and start a new Thread with it. Returns the Worker.\"\"\"\n", "input": "", "output": "        if not self.running:\n            return self.immediate_worker\n        worker = self._new_worker(name)\n        self._start_worker(worker)\n        return worker"}, {"instruction": "def reset_password(self, user, password):\n        \"\"\"\n        Service method to reset a user's password. The same as :meth:`change_password`\n        except we this method sends a different notification email.\n\n        Sends signal `password_reset`.\n\n        :param user:\n        :param password:\n        :return:\n        \"\"\"\n", "input": "", "output": "        user.password = password\n        self.user_manager.save(user)\n        if app.config.SECURITY_SEND_PASSWORD_RESET_NOTICE_EMAIL:\n            self.send_mail(\n                _('flask_unchained.bundles.security:email_subject.password_reset_notice'),\n                to=user.email,\n                template='security/email/password_reset_notice.html',\n                user=user)\n        password_reset.send(app._get_current_object(), user=user)"}, {"instruction": "def _delete(self, pk):\n        \"\"\"\n            Delete function logic, override to implement different logic\n            deletes the record with primary_key = pk\n\n            :param pk:\n                record primary key to delete\n        \"\"\"\n", "input": "", "output": "        item = self.datamodel.get(pk, self._base_filters)\n        if not item:\n            abort(404)\n        try:\n            self.pre_delete(item)\n        except Exception as e:\n            flash(str(e), \"danger\")\n        else:\n            if self.datamodel.delete(item):\n                self.post_delete(item)\n            flash(*self.datamodel.message)\n            self.update_redirect()"}, {"instruction": "def basemz(df):\n    \"\"\"\n    The mz of the most abundant ion.\n    \"\"\"\n", "input": "", "output": "    # returns the\n    d = np.array(df.columns)[df.values.argmax(axis=1)]\n    return Trace(d, df.index, name='basemz')"}, {"instruction": "def filesystem_absent(name, force=False, recursive=False):\n    '''\n    ensure filesystem is absent on the system\n\n    name : string\n        name of filesystem\n    force : boolean\n        try harder to destroy the dataset (zfs destroy -f)\n    recursive : boolean\n        also destroy all the child datasets (zfs destroy -r)\n\n    .. warning::\n\n        If a volume with ``name`` exists, this state will succeed without\n        destroying the volume specified by ``name``. This module is dataset type sensitive.\n\n    '''\n", "input": "", "output": "    if not __utils__['zfs.is_dataset'](name):\n        ret = {'name': name,\n               'changes': {},\n               'result': False,\n               'comment': 'invalid dataset name: {0}'.format(name)}\n    else:\n        ret = _absent(name, 'filesystem', force, recursive)\n    return ret"}, {"instruction": "def get_extents(self):\n        \"\"\"Return the extents of the recording-surface.\n\n        :returns:\n            A ``(x, y, width, height)`` tuple of floats,\n            or :obj:`None` if the surface is unbounded.\n\n        *New in cairo 1.12*\n\n        \"\"\"\n", "input": "", "output": "        extents = ffi.new('cairo_rectangle_t *')\n        if cairo.cairo_recording_surface_get_extents(self._pointer, extents):\n            return (extents.x, extents.y, extents.width, extents.height)"}, {"instruction": "def calc_prob_mom(returns, other_returns):\n    \"\"\"\n    `Probabilistic momentum <http://cssanalytics.wordpress.com/2014/01/28/are-simple-momentum-strategies-too-dumb-introducing-probabilistic-momentum/>`_ (see `momentum investing <https://www.investopedia.com/terms/m/momentum_investing.asp>`_)\n\n    Basically the \"probability or confidence that one asset\n    is going to outperform the other\".\n\n    Source:\n        http://cssanalytics.wordpress.com/2014/01/28/are-simple-momentum-strategies-too-dumb-introducing-probabilistic-momentum/ # NOQA\n    \"\"\"\n", "input": "", "output": "    return t.cdf(returns.calc_information_ratio(other_returns),\n                 len(returns) - 1)"}, {"instruction": "def all_stats(klass, account, ids, metric_groups, **kwargs):\n        \"\"\"\n        Pulls a list of metrics for a specified set of object IDs.\n        \"\"\"\n", "input": "", "output": "        params = klass._standard_params(ids, metric_groups, **kwargs)\n\n        resource = klass.RESOURCE_SYNC.format(account_id=account.id)\n        response = Request(account.client, 'get', resource, params=params).perform()\n        return response.body['data']"}, {"instruction": "def contract_to_dict(contract):\n    \"\"\"Convert an IBPy Contract object to a dict containing any non-default values.\"\"\"\n", "input": "", "output": "    default = Contract()\n    return {field: val for field, val in vars(contract).items() if val != getattr(default, field, None)}"}, {"instruction": "def get_packages_of_type(self, package_types, mask=None):\n        \"\"\"Get packages that match a certain type.\n\n        Each ordering package has a type, so return all packages that match\n        the types we are looking for\n\n        :param list package_types: List of strings representing the package\n                                   type keynames we are interested in.\n        :param string mask: Mask to specify the properties we want to retrieve\n        \"\"\"\n", "input": "", "output": "\n        _filter = {\n            'type': {\n                'keyName': {\n                    'operation': 'in',\n                    'options': [\n                        {'name': 'data',\n                         'value': package_types}\n                    ],\n                },\n            },\n        }\n\n        packages = self.package_svc.getAllObjects(mask=mask, filter=_filter)\n        packages = self.filter_outlet_packages(packages)\n        return packages"}, {"instruction": "def parse_response_start_line(line: str) -> ResponseStartLine:\n    \"\"\"Returns a (version, code, reason) tuple for an HTTP 1.x response line.\n\n    The response is a `collections.namedtuple`.\n\n    >>> parse_response_start_line(\"HTTP/1.1 200 OK\")\n    ResponseStartLine(version='HTTP/1.1', code=200, reason='OK')\n    \"\"\"\n", "input": "", "output": "    line = native_str(line)\n    match = re.match(\"(HTTP/1.[0-9]) ([0-9]+) ([^\\r]*)\", line)\n    if not match:\n        raise HTTPInputError(\"Error parsing response start line\")\n    return ResponseStartLine(match.group(1), int(match.group(2)), match.group(3))"}, {"instruction": "def txt_line_iterator(txt_path):\n  \"\"\"Iterate through lines of file.\"\"\"\n", "input": "", "output": "  with tf.gfile.Open(txt_path) as f:\n    for line in f:\n      yield line.strip()"}, {"instruction": "def run(self, app):\n        \"\"\"Function starts the web server given configuration.\"\"\"\n", "input": "", "output": "        GlimLog.info('Glim server started on %s environment' % self.args.env)\n        try:\n            kwargs = Config.get('app.server.options')\n            run(app.wsgi,\n                host=Config.get('app.server.host'),\n                port=Config.get('app.server.port'),\n                debug=Config.get('app.server.debugger'),\n                reloader=Config.get('app.server.reloader'),\n                server=Config.get('app.server.wsgi'),\n                **kwargs)\n        except Exception as e:\n            print(traceback.format_exc())\n            exit()"}, {"instruction": "def get_datas(callback, macs=[], run_flag=RunFlag(), bt_device=''):\n        \"\"\"\n        Get data for all ruuvitag sensors or sensors in the MAC's list.\n\n        Args:\n            callback (func): callback funcion to be called when new data is received\n            macs (list): MAC addresses\n            run_flag (object): RunFlag object. Function executes while run_flag.running\n            bt_device (string): Bluetooth device id\n        \"\"\"\n", "input": "", "output": "\n        log.info('Get latest data for sensors. Stop with Ctrl+C.')\n        log.info('MACs: %s', macs)\n\n        for new_data in RuuviTagSensor._get_ruuvitag_datas(macs, None, run_flag, bt_device):\n            callback(new_data)"}, {"instruction": "def diff(self, n=1):\n    \"\"\"\n    Differentiate (n-th derivative, where the default n is 1).\n    \"\"\"\n", "input": "", "output": "    d = self._data\n    for unused in xrange(n):\n      d = OrderedDict((k - 1, k * v) for k, v in iteritems(d) if k != 0)\n    return Poly(d, zero=self.zero)"}, {"instruction": "def flatten_iterable(iterable):\r\n    \"\"\"flatten iterable, but leaves out strings\r\n\r\n    [[[1, 2, 3], [4, 5]], 6] -> [1, 2, 3, 4, 5, 6]\r\n\r\n    \"\"\"\n", "input": "", "output": "    for item in iterable:\r\n        if isinstance(item, collections.Iterable) and not isinstance(item, basestring):\r\n            for sub in flatten_iterable(item):\r\n                yield sub\r\n        else:\r\n            yield item"}, {"instruction": "def invalid_multipoly_handler(gdf, relation, way_ids):\n    \"\"\"\n    Handles invalid multipolygon geometries when there exists e.g. a feature without \n    geometry (geometry == NaN)\n\n    Parameters\n    ----------\n\n    gdf : gpd.GeoDataFrame\n        GeoDataFrame with Polygon geometries that should be converted into a MultiPolygon object.\n    relation : dict\n        OSM 'relation' dictionary\n    way_ids : list\n        A list of 'way' ids that should be converted into a MultiPolygon object. \n    \"\"\"\n", "input": "", "output": "\n    try:\n        gdf_clean = gdf.dropna(subset=['geometry'])\n        multipoly = MultiPolygon(list(gdf_clean['geometry']))\n        return multipoly\n\n    except Exception:\n        log(\"Invalid geometry at relation id %s.\\nWay-ids of the invalid MultiPolygon:\" % (\n        relation['id'], str(way_ids)))\n        return None"}, {"instruction": "def new(cls, freeform_builder, x, y):\n        \"\"\"Return a new _LineSegment object ending at point *(x, y)*.\n\n        Both *x* and *y* are rounded to the nearest integer before use.\n        \"\"\"\n", "input": "", "output": "        return cls(freeform_builder, int(round(x)), int(round(y)))"}, {"instruction": "def connectionMade(self):\n        \"\"\"Keep a reference to the protocol on the factory, and uses the\n        factory's store to find multiplexed connection factories.\n\n        Unfortunately, we can't add the protocol by TLS certificate\n        fingerprint, because the TLS handshake won't have completed\n        yet, so ``self.transport.getPeerCertificate()`` is still\n        ``None``.\n\n        \"\"\"\n", "input": "", "output": "        self.factory.protocols.add(self)\n        self._factories = multiplexing.FactoryDict(self.store)\n        super(AMP, self).connectionMade()"}, {"instruction": "def remove_description(self, id, **kwargs):  # noqa: E501\n        \"\"\"Remove description from a specific source  # noqa: E501\n\n          # noqa: E501\n        This method makes a synchronous HTTP request by default. To make an\n        asynchronous HTTP request, please pass async_req=True\n        >>> thread = api.remove_description(id, async_req=True)\n        >>> result = thread.get()\n\n        :param async_req bool\n        :param str id: (required)\n        :return: ResponseContainer\n                 If the method is called asynchronously,\n                 returns the request thread.\n        \"\"\"\n", "input": "", "output": "        kwargs['_return_http_data_only'] = True\n        if kwargs.get('async_req'):\n            return self.remove_description_with_http_info(id, **kwargs)  # noqa: E501\n        else:\n            (data) = self.remove_description_with_http_info(id, **kwargs)  # noqa: E501\n            return data"}, {"instruction": "def update_metadata_from_rmd_options(name, value, metadata):\n    \"\"\"\n    Update metadata using the _BOOLEAN_OPTIONS_DICTIONARY mapping\n    :param name: option name\n    :param value: option value\n    :param metadata:\n    :return:\n    \"\"\"\n", "input": "", "output": "    for jupyter_option, rmd_option, rev in _BOOLEAN_OPTIONS_DICTIONARY:\n        if name == rmd_option:\n            try:\n                metadata[jupyter_option] = _py_logical_values(value) != rev\n                return True\n            except RLogicalValueError:\n                pass\n    return False"}, {"instruction": "def _startReapingProcesses(self):\n        \"\"\"\n        Start a LoopingCall that calls reapAllProcesses.\n        \"\"\"\n", "input": "", "output": "        lc = LoopingCall(self._reapAllProcesses)\n        lc.clock = self._reactor\n        lc.start(0.1, False)"}, {"instruction": "def _refresh_editor_and_scrollbars(self):\n        \"\"\"\n        Refrehes editor content and scollbars.\n\n        We generate a fake resize event to refresh scroll bar.\n\n        We have the same problem as described here:\n        http://www.qtcentre.org/threads/44803 and we apply the same solution\n        (don't worry, there is no visual effect, the editor does not grow up\n        at all, even with a value = 500)\n        \"\"\"\n", "input": "", "output": "        TextHelper(self.editor).mark_whole_doc_dirty()\n        self.editor.repaint()\n        s = self.editor.size()\n        s.setWidth(s.width() + 1)\n        self.editor.resizeEvent(QResizeEvent(self.editor.size(), s))"}, {"instruction": "async def insert(self, **kwargs):\n\t\t\"\"\"\n\t\tAccepts request object, retrieves data from the one`s body\n\t\tand creates new account. \n\t\t\"\"\"\n", "input": "", "output": "\t\t\n\t\tif kwargs:\n\t\t\t# Create autoincrement for account\n\t\t\tpk = await self.autoincrement()\n\t\t\tkwargs.update({\"id\": pk})\n\n\t\t\t# Create account with received data and autoincrement\n\t\t\tawait self.collection.insert_one(kwargs)\n\n\t\t\trow = await self.collection.find_one({\"id\": pk})\n\n\t\telse:\n\t\t\trow = None\n\n\t\tif row:\n\t\t\treturn {i:row[i] for i in row if i != \"_id\"}\n\t\telse:\n\t\t\treturn {\"error\":500, \n\t\t\t\t\t\"reason\":\"Not created\"}"}, {"instruction": "def min_periodic_distance(self, xyz0, xyz1):\n        \"\"\"Vectorized distance calculation considering minimum image.\n\n        Parameters\n        ----------\n        xyz0 : np.ndarray, shape=(3,), dtype=float\n            Coordinates of first point\n        xyz1 : np.ndarray, shape=(3,), dtype=float\n            Coordinates of second point\n\n        Returns\n        -------\n        float\n            Vectorized distance between the two points following minimum\n            image convention\n\n        \"\"\"\n", "input": "", "output": "        d = np.abs(xyz0 - xyz1)\n        d = np.where(d > 0.5 * self.periodicity, self.periodicity - d, d)\n        return np.sqrt((d ** 2).sum(axis=-1))"}, {"instruction": "def get_street(street, areacode, api=None):\n    \"\"\"\n    Retrieve streets in a given bounding area\n\n    :param overpy.Overpass api: First street of intersection\n    :param String street: Name of street\n    :param String areacode: The OSM id of the bounding area\n    :return: Parsed result\n    :raises overpy.exception.OverPyException: If something bad happens.\n    \"\"\"\n", "input": "", "output": "    if api is None:\n        api = overpy.Overpass()\n\n    query = "}, {"instruction": "def reward_bonus(self, assignment_id, amount, reason):\n        \"\"\"Print out bonus info for the assignment\"\"\"\n", "input": "", "output": "        logger.info(\n            'Award ${} for assignment {}, with reason \"{}\"'.format(\n                amount, assignment_id, reason\n            )\n        )"}, {"instruction": "def schedule_and_propagate_host_downtime(self, host, start_time, end_time,\n                                             fixed, trigger_id, duration, author, comment):\n        \"\"\"DOES NOTHING (Should create host downtime and start it?)\n        Format of the line that triggers function call::\n\n        SCHEDULE_AND_PROPAGATE_HOST_DOWNTIME;<host_name>;<start_time>;<end_time>;\n        <fixed>;<trigger_id>;<duration>;<author>;<comment>\n\n        :return: None\n        \"\"\"\n", "input": "", "output": "        logger.warning(\"The external command 'SCHEDULE_AND_PROPAGATE_HOST_DOWNTIME' \"\n                       \"is not currently implemented in Alignak. If you really need it, \"\n                       \"request for its implementation in the project repository: \"\n                       \"https://github.com/Alignak-monitoring/alignak\")\n        self.send_an_element(make_monitoring_log(\n            'warning', 'SCHEDULE_AND_PROPAGATE_HOST_DOWNTIME: this command is not implemented!'))"}, {"instruction": "def _get_error_response(self, exception):\n        \"\"\"\n        Trasform pyston exceptions to Is-core exceptions and raise it\n        \"\"\"\n", "input": "", "output": "        response_exceptions = {\n            MimerDataException: HTTPBadRequestResponseException,\n            NotAllowedException: HTTPForbiddenResponseException,\n            UnsupportedMediaTypeException: HTTPUnsupportedMediaTypeResponseException,\n            Http404: Http404,\n            ResourceNotFoundException: Http404,\n            NotAllowedMethodException: HTTPMethodNotAllowedResponseException,\n            DuplicateEntryException: HTTPDuplicateResponseException,\n            ConflictException: HTTPDuplicateResponseException,\n        }\n        response_exception = response_exceptions.get(type(exception))\n        if response_exception:\n            raise response_exception\n        return super(RESTResourceMixin, self)._get_error_response(exception)"}, {"instruction": "def get_kernel_spec(self, kernel_name):\n        \"\"\"Returns a :class:`KernelSpec` instance for the given kernel_name.\n\n        Raises :exc:`NoSuchKernel` if the given kernel name is not found.\n        \"\"\"\n", "input": "", "output": "        try:\n            return super(EnvironmentKernelSpecManager,\n                         self).get_kernel_spec(kernel_name)\n        except (NoSuchKernel, FileNotFoundError):\n            venv_kernel_name = kernel_name.lower()\n            specs = self.get_all_kernel_specs_for_envs()\n            if venv_kernel_name in specs:\n                return specs[venv_kernel_name]\n            else:\n                raise NoSuchKernel(kernel_name)"}, {"instruction": "def distribute(self, func, partitioned_chunks, kwargs):\n        \"\"\"\n        Calculates the features in a sequential fashion by pythons map command\n\n        :param func: the function to send to each worker.\n        :type func: callable\n        :param partitioned_chunks: The list of data chunks - each element is again\n            a list of chunks - and should be processed by one worker.\n        :type partitioned_chunks: iterable\n        :param kwargs: parameters for the map function\n        :type kwargs: dict of string to parameter\n\n        :return: The result of the calculation as a list - each item should be the result of the application of func\n            to a single element.\n        \"\"\"\n", "input": "", "output": "        return map(partial(func, **kwargs), partitioned_chunks)"}, {"instruction": "def check_positive_integer(name, value):\n    \"\"\"Check a value is a positive integer.\n\n    Returns the value if so, raises ValueError otherwise.\n\n    \"\"\"\n", "input": "", "output": "    try:\n        value = int(value)\n        is_positive = (value > 0)\n    except ValueError:\n        raise ValueError('%s should be an integer; got %r' % (name, value))\n\n    if is_positive:\n        return value\n    else:\n        raise ValueError('%s should be positive; got %r' % (name, value))"}, {"instruction": "def stat(filename):\n    \"\"\"Returns os.stat for a given file, adjusting the timestamps as appropriate.\"\"\"\n", "input": "", "output": "    import os\n    try:\n        # on the host, lstat won't try to follow symlinks\n        rstat = os.lstat(filename)\n    except:\n        rstat = os.stat(filename)\n    return rstat[:7] + tuple(tim + TIME_OFFSET for tim in rstat[7:])"}, {"instruction": "def raw(self, klass, _name=\"default\", **attributes):\n        \"\"\"\n        Get the raw attribute dict for a given named model.\n\n        :param klass: The class\n        :type klass: class\n\n        :param _name: The type\n        :type _name: str\n\n        :param attributes: The instance attributes\n        :type attributes: dict\n\n        :return: dict\n        \"\"\"\n", "input": "", "output": "        raw = self._definitions[klass][_name](self._faker)\n\n        raw.update(attributes)\n\n        return raw"}, {"instruction": "def normalize(dt, tz=UTC):\n    \"\"\"\n    Convert date or datetime to datetime with timezone.\n\n    :param dt: date to normalize\n    :param tz: the normalized date's timezone\n    :return: date as datetime with timezone\n    \"\"\"\n", "input": "", "output": "    if type(dt) is date:\n        dt = dt + relativedelta(hour=0)\n    elif type(dt) is datetime:\n        pass\n    else:\n        raise ValueError(\"unknown type %s\" % type(dt))\n\n    if dt.tzinfo:\n        dt = dt.astimezone(tz)\n    else:\n        dt = dt.replace(tzinfo=tz)\n\n    return dt"}, {"instruction": "def get_comments_content_object(parser, token):\n    \"\"\"\n    Get a limited set of comments for a given object.\n    Defaults to a limit of 5. Setting the limit to -1 disables limiting.\n\n    usage:\n\n        {% get_comments_content_object for form_object as variable_name %}\n\n    \"\"\"\n", "input": "", "output": "    keywords = token.contents.split()\n    if len(keywords) != 5:\n        raise template.TemplateSyntaxError(\n            \"'%s' tag takes exactly 2 arguments\" % (keywords[0],))\n    if keywords[1] != 'for':\n        raise template.TemplateSyntaxError(\n            \"first argument to '%s' tag must be 'for'\" % (keywords[0],))\n    if keywords[3] != 'as':\n        raise template.TemplateSyntaxError(\n            \"first argument to '%s' tag must be 'as'\" % (keywords[0],))\n    return GetCommentsContentObject(keywords[2], keywords[4])"}, {"instruction": "def group_concat(arg, sep=',', where=None):\n    \"\"\"\n    Concatenate values using the indicated separator (comma by default) to\n    produce a string\n\n    Parameters\n    ----------\n    arg : array expression\n    sep : string, default ','\n    where : bool, default None\n\n    Returns\n    -------\n    concatenated : string scalar\n    \"\"\"\n", "input": "", "output": "    return ops.GroupConcat(arg, sep, where).to_expr()"}, {"instruction": "def cat_net_img(url='', indent=4, img_height=0, img_cache_dir='/tmp', use_cache=False):\n    \"\"\"\n        - \u4f18\u5148 \u4ece\u5fae\u535a\u7f13\u5b58\u76ee\u5f55\u8bfb\u53d6\u56fe\u7247\n        - \u5982\u679c\u5931\u8d25, \u518d\u4ece\u76f8\u5e94\u7684url\u8bfb\u53d6\u7167\u7247\n\n    :param use_cache: ``\u4f7f\u7528\u7f13\u5b58``\n    :type use_cache:\n    :param img_cache_dir:\n    :type img_cache_dir:\n    :param url:\n    :type url:\n    :param indent:\n    :type indent:\n    :param img_height:\n    :type img_height:\n    :return:\n    :rtype:\n    \"\"\"\n", "input": "", "output": "    name = url.split('/')[-1]\n    pth = os.path.join(img_cache_dir, name)\n\n    # \u5982\u679c\u4e0d\u4f7f\u7528\u7f13\u5b58 \u6216\u8005 \u6587\u4ef6\u4e0d\u5b58\u5728, \u5219\u5148\u4e0b\u8f7d\u5230\u672c\u5730, \u7136\u540e\u518d\u8bfb\u53d6\n    if not use_cache or not is_file_ok(pth):\n        raw = requests.get(url)\n        write_file(raw.content, pth)\n\n    with textui.indent(indent, quote=' {}'.format(' ')):\n        textui.puts(cat_img_by_path(pth, img_height))"}, {"instruction": "def split_locale(loc):\n    '''\n    Split a locale specifier.  The general format is\n\n    language[_territory][.codeset][@modifier] [charmap]\n\n    For example:\n\n    ca_ES.UTF-8@valencia UTF-8\n    '''\n", "input": "", "output": "    def split(st, char):\n        "}, {"instruction": "def context_changed(self, context):\n        \"\"\" :type context: dict \"\"\"\n", "input": "", "output": "        self._image.set_cmap(context['colormap'])\n        self._image.set_clim(context['min'], context['max'])\n        self._image.set_interpolation(context['interpolation'])\n        self._update_indicators(context)\n\n        self._set_view_limits()\n\n        if self._model.index_direction is not SliceDirection.depth:\n            self._image.axes.set_ylabel(context['samples_unit'])"}, {"instruction": "def save_dir(key, dir_path, *refs):\n    \"\"\"Convert the given parameters to a special JSON object.\n\n    JSON object is of the form:\n    { key: {\"dir\": dir_path}}, or\n    { key: {\"dir\": dir_path, \"refs\": [refs[0], refs[1], ... ]}}\n\n    \"\"\"\n", "input": "", "output": "    if not os.path.isdir(dir_path):\n        return error(\n            \"Output '{}' set to a missing directory: '{}'.\".format(key, dir_path)\n        )\n\n    result = {key: {\"dir\": dir_path}}\n\n    if refs:\n        missing_refs = [\n            ref for ref in refs if not (os.path.isfile(ref) or os.path.isdir(ref))\n        ]\n        if len(missing_refs) > 0:\n            return error(\n                \"Output '{}' set to missing references: '{}'.\".format(\n                    key, ', '.join(missing_refs)\n                )\n            )\n        result[key][\"refs\"] = refs\n\n    return json.dumps(result)"}, {"instruction": "def trim(self, video_name, out, start, duration):\n        \"\"\"\n        Trims a clip to be duration starting at start\n        @param video_name : name of the input video\n        @param out : name of the output video\n        @param start : starting position after the trim\n        @param duration : duration of video after start\n        \"\"\"\n", "input": "", "output": "        command = ['ffmpeg', '-ss', start, '-i', video_name, '-c:v', 'huffyuv',\n                   '-y', '-preset', 'veryslow', '-t', duration, out]\n        if self.verbose:\n            print 'Trimming {0} into {1}'.format(video_name, out)\n            print ' '.join(command)\n        call(command)"}, {"instruction": "def _get_file_content(source):\n    \"\"\"Return a tuple, each value being a line of the source file.\n\n    Remove empty lines and comments (lines starting with a '#').\n\n    \"\"\"\n", "input": "", "output": "    filepath = os.path.join('siglists', source + '.txt')\n\n    lines = []\n    with resource_stream(__name__, filepath) as f:\n        for i, line in enumerate(f):\n            line = line.decode('utf-8', 'strict').strip()\n            if not line or line.startswith('#'):\n                continue\n\n            try:\n                re.compile(line)\n            except Exception as ex:\n                raise BadRegularExpressionLineError(\n                    'Regex error: {} in file {} at line {}'.format(\n                        str(ex),\n                        filepath,\n                        i\n                    )\n                )\n\n            lines.append(line)\n\n    if source in _SPECIAL_EXTENDED_VALUES:\n        lines = lines + _SPECIAL_EXTENDED_VALUES[source]\n\n    return tuple(lines)"}, {"instruction": "def _after_handler(self, iid, callback, args):\n        \"\"\"Proxy to called by after() in mainloop\"\"\"\n", "input": "", "output": "        self._after_id = None\n        self.update_state(iid, \"normal\")\n        self.call_callbacks(iid, callback, args)"}, {"instruction": "def end_range(self):\n      \"\"\"Similar to the junction range but don't need to check for leftmost or rightmost\"\"\"\n", "input": "", "output": "      if len(self._exons) == 0: return None\n      return GenomicRange(self._exons[0].chr,\n             min([x.end for x in self._exons]),\n             max([x.end for x in self._exons]))"}, {"instruction": "def compile(self, expr, params=None, limit=None):\n        \"\"\"\n        Translate expression to one or more queries according to backend target\n\n        Returns\n        -------\n        output : single query or list of queries\n        \"\"\"\n", "input": "", "output": "        query_ast = self._build_ast_ensure_limit(expr, limit, params=params)\n        return query_ast.compile()"}, {"instruction": "def find_class_files(self):\n        \"\"\"Find compiled class files recursively in the root path\n\n        :return: list of absolute file paths\n        \"\"\"\n", "input": "", "output": "        files = self._find_files()\n        self.announce(\n            \"found '{}' compiled class files in '{}'\".format(\n                len(files), self.root\n            )\n        )\n        return files"}, {"instruction": "def _error(self, x):\n        \"\"\"Error function.\n        Once self.y_desired has been defined, compute the error\n        of input x using the forward model.\n        \"\"\"\n", "input": "", "output": "        y_pred = self.fmodel.predict_y(x)\n        err_v  = y_pred - self.goal\n        error = sum(e*e for e in err_v)\n        return error"}, {"instruction": "def translate_sites(self, indices, vector, frac_coords=True,\n                        to_unit_cell=True):\n        \"\"\"\n        Translate specific sites by some vector, keeping the sites within the\n        unit cell.\n\n        Args:\n            indices: Integer or List of site indices on which to perform the\n                translation.\n            vector: Translation vector for sites.\n            frac_coords (bool): Whether the vector corresponds to fractional or\n                cartesian coordinates.\n            to_unit_cell (bool): Whether new sites are transformed to unit\n                cell\n        \"\"\"\n", "input": "", "output": "        if not isinstance(indices, collections.abc.Iterable):\n            indices = [indices]\n\n        for i in indices:\n            site = self._sites[i]\n            if frac_coords:\n                fcoords = site.frac_coords + vector\n            else:\n                fcoords = self._lattice.get_fractional_coords(\n                    site.coords + vector)\n            if to_unit_cell:\n                fcoords = np.mod(fcoords, 1)\n            self._sites[i].frac_coords = fcoords"}, {"instruction": "def string_width(word: str) -> int:\n    \"\"\"\n    :param word:\n    :return: Widths of word\n\n    Usage:\n\n        >>> string_width('abc')\n        3\n        >>> string_width('\uff21b\u3057\u30fc')\n        7\n        >>> string_width('')\n        0\n    \"\"\"\n", "input": "", "output": "    return sum(map(lambda x: 2 if east_asian_width(x) in 'FWA' else 1, word))"}, {"instruction": "def ComputeRoot(hashes):\n        \"\"\"\n        Compute the root hash.\n\n        Args:\n            hashes (list): the list of hashes to build the root from.\n\n        Returns:\n            bytes: the root hash.\n        \"\"\"\n", "input": "", "output": "        if not len(hashes):\n            raise Exception('Hashes must have length')\n        if len(hashes) == 1:\n            return hashes[0]\n\n        tree = MerkleTree(hashes)\n        return tree.Root.Hash"}, {"instruction": "def _get_from_riak(self, key):\n        \"\"\"\n        Args:\n            key (str): riak key\n        Returns:\n            (tuple): riak obj json data and riak key\n        \"\"\"\n", "input": "", "output": "\n        obj = self.bucket.get(key)\n\n        if obj.exists:\n            return obj.data, obj.key\n\n        raise ObjectDoesNotExist(\"%s %s\" % (key, self.compiled_query))"}, {"instruction": "def select(table, index_track, field_name, op, value, includeMissing):\n    '''Modifies the table and index_track lists based on the comparison.\n    '''\n", "input": "", "output": "    result = []\n    result_index = []\n    counter = 0\n    for row in table:\n        if detect_fields(field_name, convert_to_dict(row)):\n            final_value = get_value(row, field_name)\n            if do_op(final_value, op, value):\n                result.append(row)\n                result_index.append(index_track[counter])\n        else:\n            if includeMissing:\n                result.append(row)\n                result_index.append(index_track[counter])\n        counter += 1\n    #table = result\n    #index_track = result_index\n    return (result, result_index)"}, {"instruction": "def WaitUntilDone(self, timeout=None):\n    \"\"\"Wait until the operation is done.\n\n    Args:\n      timeout: timeout in seconds. None means default timeout (1 hour).\n               0 means no timeout (wait forever).\n    Returns:\n      Operation object with refreshed target_file.\n    Raises:\n      PollTimeoutError: if timeout is reached.\n    \"\"\"\n", "input": "", "output": "\n    utils.Poll(\n        generator=self.GetState,\n        condition=lambda s: s != self.__class__.STATE_RUNNING,\n        timeout=timeout)\n    self.target_file = self.target_file.Get()\n    return self"}, {"instruction": "def equivalent_to(std_function):\n    \"\"\"\n    Decorates a cloud object compatible function\n    to provides fall back to standard function if\n    used on local files.\n\n    Args:\n        std_function (function): standard function to\n            used with local files.\n\n    Returns:\n        function: new function\n    \"\"\"\n", "input": "", "output": "\n    def decorate(cos_function):\n        "}, {"instruction": "def _bubbleP(cls, T):\n        \"\"\"Using ancillary equation return the pressure of bubble point\"\"\"\n", "input": "", "output": "        c = cls._blend[\"bubble\"]\n        Tj = cls._blend[\"Tj\"]\n        Pj = cls._blend[\"Pj\"]\n        Tita = 1-T/Tj\n\n        suma = 0\n        for i, n in zip(c[\"i\"], c[\"n\"]):\n            suma += n*Tita**(i/2.)\n        P = Pj*exp(Tj/T*suma)\n        return P"}, {"instruction": "def convert(self, vroot, entry_variables):\n        \"\"\"\n        All functions are replaced with the same `new` function.\n\n        Args:\n            vroot (:obj:`Variable`): NNabla Variable\n            entry_variables (:obj:`Variable`): Entry variable from which the conversion starts.\n        \"\"\"\n", "input": "", "output": "        self.graph_info = GraphInfo(vroot)\n        self.entry_variables = entry_variables\n\n        with nn.parameter_scope(self.name):\n            # Function loop in the forward order\n            for t, func in enumerate(self.graph_info.funcs):\n                if func.name in self.inner_prod_functions:\n                    inner_prod_func = func\n                    o = self._fixed_point_weight_conversion(inner_prod_func)\n                    continue\n                # Identity conversion\n                o = self._identity_conversion(func)\n\n        self.end_variable = o\n\n        if self.call_forward:\n            o.forward(clear_buffer=True)\n        return self.end_variable"}, {"instruction": "def to_satoshis(input_quantity, input_type):\n    ''' convert to satoshis, no rounding '''\n", "input": "", "output": "    assert input_type in UNIT_CHOICES, input_type\n\n    # convert to satoshis\n    if input_type in ('btc', 'mbtc', 'bit'):\n        satoshis = float(input_quantity) * float(UNIT_MAPPINGS[input_type]['satoshis_per'])\n    elif input_type == 'satoshi':\n        satoshis = input_quantity\n    else:\n        raise Exception('Invalid Unit Choice: %s' % input_type)\n\n    return int(satoshis)"}, {"instruction": "def flip_labels(obj):\n    \"\"\"\n    Rename fields x to y and y to x\n\n    Parameters\n    ----------\n    obj : dict_like\n        Object with labels to rename\n    \"\"\"\n", "input": "", "output": "    def sub(a, b):\n        "}, {"instruction": "def add_database_args(parser):\n    '''\n    Add a standard set of database arguments for argparse\n    '''\n", "input": "", "output": "    parser.add_argument(\n        'url',\n        nargs='?',\n        default='sqlite:///ncbi_taxonomy.db',\n        type=sqlite_default(),\n        help=('Database string URI or filename.  If no database scheme '\n              'specified \\\"sqlite:///\\\" will be prepended. [%(default)s]'))\n    db_parser = parser.add_argument_group(title='database options')\n\n    # TODO: better description of what --schema does\n    db_parser.add_argument(\n        '--schema',\n        help=('Name of SQL schema in database to query '\n              '(if database flavor supports this).'))\n\n    return parser"}, {"instruction": "def readFAM(basefilename,usecols=None):\n    \"\"\"\n    helper method for speeding up read FAM\n    \"\"\"\n", "input": "", "output": "    fam = basefilename+'.fam'\n    fam = SP.loadtxt(fam,dtype=bytes,usecols=usecols)\n    return fam"}, {"instruction": "def extract_features(self, phrase):\n        \"\"\"\n        This function will extract features from the phrase being used. \n        Currently, the feature we are extracting are unigrams of the text corpus.\n        \"\"\"\n", "input": "", "output": "        \n        words = nltk.word_tokenize(phrase)\n        features = {}\n        for word in words:\n            features['contains(%s)' % word] = (word in words)\n        return features"}, {"instruction": "def ensure_regex_namespace(self, keyword: str, pattern: str) -> Namespace:\n        \"\"\"Get or create a regular expression namespace.\n\n        :param keyword: The keyword of a regular expression namespace\n        :param pattern: The pattern for a regular expression namespace\n        \"\"\"\n", "input": "", "output": "        if pattern is None:\n            raise ValueError('cannot have null pattern')\n\n        namespace = self.get_namespace_by_keyword_pattern(keyword, pattern)\n\n        if namespace is None:\n            log.info('creating regex namespace: %s:%s', keyword, pattern)\n            namespace = Namespace(\n                keyword=keyword,\n                pattern=pattern\n            )\n            self.session.add(namespace)\n            self.session.commit()\n\n        return namespace"}, {"instruction": "def rm_keys_from_dict(d, keys):\n    \"\"\"\n    Given a dictionary and a key list, remove any data in the dictionary with the given keys.\n\n    :param dict d: Metadata\n    :param list keys: Keys to be removed\n    :return dict d: Metadata\n    \"\"\"\n", "input": "", "output": "    # Loop for each key given\n    for key in keys:\n        # Is the key in the dictionary?\n        if key in d:\n            try:\n                d.pop(key, None)\n            except KeyError:\n                # Not concerned with an error. Keep going.\n                pass\n    return d"}, {"instruction": "def copy(self):\n        \"\"\"\n        Returns a deep copy of the particle. The particle is not added to any simulation by default.\n        \"\"\"\n", "input": "", "output": "        np = Particle()\n        memmove(byref(np), byref(self), sizeof(self))\n        return np"}, {"instruction": "def _check_vpcs_version(self):\n        \"\"\"\n        Checks if the VPCS executable version is >= 0.8b or == 0.6.1.\n        \"\"\"\n", "input": "", "output": "        try:\n            output = yield from subprocess_check_output(self._vpcs_path(), \"-v\", cwd=self.working_dir)\n            match = re.search(\"Welcome to Virtual PC Simulator, version ([0-9a-z\\.]+)\", output)\n            if match:\n                version = match.group(1)\n                self._vpcs_version = parse_version(version)\n                if self._vpcs_version < parse_version(\"0.6.1\"):\n                    raise VPCSError(\"VPCS executable version must be >= 0.6.1 but not a 0.8\")\n            else:\n                raise VPCSError(\"Could not determine the VPCS version for {}\".format(self._vpcs_path()))\n        except (OSError, subprocess.SubprocessError) as e:\n            raise VPCSError(\"Error while looking for the VPCS version: {}\".format(e))"}, {"instruction": "def get_stp_brief_info_output_spanning_tree_info_spanning_tree_mode_pvstp_pvstp_port_interface_type(self, **kwargs):\n        \"\"\"Auto Generated Code\n        \"\"\"\n", "input": "", "output": "        config = ET.Element(\"config\")\n        get_stp_brief_info = ET.Element(\"get_stp_brief_info\")\n        config = get_stp_brief_info\n        output = ET.SubElement(get_stp_brief_info, \"output\")\n        spanning_tree_info = ET.SubElement(output, \"spanning-tree-info\")\n        spanning_tree_mode = ET.SubElement(spanning_tree_info, \"spanning-tree-mode\")\n        pvstp = ET.SubElement(spanning_tree_mode, \"pvstp\")\n        pvstp = ET.SubElement(pvstp, \"pvstp\")\n        vlan_id_key = ET.SubElement(pvstp, \"vlan-id\")\n        vlan_id_key.text = kwargs.pop('vlan_id')\n        port = ET.SubElement(pvstp, \"port\")\n        interface_type = ET.SubElement(port, \"interface-type\")\n        interface_type.text = kwargs.pop('interface_type')\n\n        callback = kwargs.pop('callback', self._callback)\n        return callback(config)"}, {"instruction": "def __upload(self, resource, bytes):\n        \"\"\"Performs a single chunk upload.\"\"\"\n", "input": "", "output": "\n        # note: string conversion required here due to open encoding bug in requests-oauthlib.\n        headers = {\n            'x-ton-expires': http_time(self.options.get('x-ton-expires', self._DEFAULT_EXPIRE)),\n            'content-length': str(self._file_size),\n            'content-type': self.content_type\n        }\n\n        return Request(self._client, 'post', resource,\n                       domain=self._DEFAULT_DOMAIN, headers=headers, body=bytes).perform()"}, {"instruction": "def _get_device_info(self, device_id):\n        \"\"\"Queries the Spark Cloud for detailed information about a device.\"\"\"\n", "input": "", "output": "        params = {'access_token': self.access_token}\n        r = self.spark_api(device_id).GET(params=params, timeout=30)\n        self._check_error(r)\n        return r.json()"}, {"instruction": "def make_temp(string, suffix='', decode=True, delete=True):\n    \"\"\" xmlsec needs files in some cases where only strings exist, hence the\n    need for this function. It creates a temporary file with the\n    string as only content.\n\n    :param string: The information to be placed in the file\n    :param suffix: The temporary file might have to have a specific\n        suffix in certain circumstances.\n    :param decode: The input string might be base64 coded. If so it\n        must, in some cases, be decoded before being placed in the file.\n    :return: 2-tuple with file pointer ( so the calling function can\n        close the file) and filename (which is for instance needed by the\n        xmlsec function).\n    \"\"\"\n", "input": "", "output": "    ntf = NamedTemporaryFile(suffix=suffix, delete=delete)\n    # Python3 tempfile requires byte-like object\n    if not isinstance(string, six.binary_type):\n        string = string.encode('utf-8')\n\n    if decode:\n        ntf.write(base64.b64decode(string))\n    else:\n        ntf.write(string)\n    ntf.seek(0)\n    return ntf, ntf.name"}, {"instruction": "def _api_delete(path, data, server=None):\n    '''\n    Do a DELETE request to the API\n    '''\n", "input": "", "output": "    server = _get_server(server)\n    response = requests.delete(\n            url=_get_url(server['ssl'], server['url'], server['port'], path),\n            auth=_get_auth(server['user'], server['password']),\n            headers=_get_headers(),\n            params=data,\n            verify=False\n    )\n    return _api_response(response)"}, {"instruction": "def pitching_stats(start_season, end_season=None, league='all', qual=1, ind=1):\n    \"\"\"\n    Get season-level pitching data from FanGraphs. \n\n    ARGUMENTS:\n    start_season : int : first season you want data for (or the only season if you do not specify an end_season)\n    end_season : int : final season you want data for \n    league : \"all\", \"nl\", or \"al\"\n    qual: minimum number of pitches thrown to be included in the data (integer). Use the string 'y' for fangraphs default.\n    ind : int : =1 if you want individual season-level data, =0 if you want a player's aggreagate data over all seasons in the query\n    \"\"\"\n", "input": "", "output": "    if start_season is None:\n        raise ValueError(\"You need to provide at least one season to collect data for. Try pitching_leaders(season) or pitching_leaders(start_season, end_season).\")\n    if end_season is None:\n        end_season = start_season\n    soup = get_soup(start_season=start_season, end_season=end_season, league=league, qual=qual, ind=ind)\n    table = get_table(soup, ind)\n    return table"}, {"instruction": "def iresolve(self, *keys):\n        '''\n        Iterates over resolved instances for given provider keys.\n\n        :param keys: Provider keys\n        :type keys: tuple\n        :return: Iterator of resolved instances\n        :rtype: generator\n        '''\n", "input": "", "output": "        for key in keys:\n            missing = self.get_missing_deps(key)\n            if missing:\n                raise UnresolvableError(\"Missing dependencies for %s: %s\" % (key, missing))\n\n            provider = self._providers.get(key)\n            if not provider:\n                raise UnresolvableError(\"Provider does not exist for %s\" % key)\n\n            yield provider()"}, {"instruction": "def timestamp_from_datetime(date_time):\n    \"\"\"Returns POSIX timestamp as float\"\"\"\n", "input": "", "output": "    if date_time.tzinfo is None:\n      return time.mktime((date_time.year, date_time.month, date_time.day, date_time.hour,\n                          date_time.minute, date_time.second,\n                          -1, -1, -1)) + date_time.microsecond / 1e6\n    return (date_time - _EPOCH).total_seconds()"}, {"instruction": "def blocked(self):\n        r\"\"\"Returns a :class:`list` of :class:`User`\\s that the user has blocked.\n\n        .. note::\n\n            This only applies to non-bot accounts.\n        \"\"\"\n", "input": "", "output": "        return [r.user for r in self._relationships.values() if r.type is RelationshipType.blocked]"}, {"instruction": "def scalar_summary(tag, scalar):\n    \"\"\"Outputs a `Summary` protocol buffer containing a single scalar value.\n    The generated Summary has a Tensor.proto containing the input Tensor.\n    Adapted from the TensorFlow function `scalar()` at\n    https://github.com/tensorflow/tensorflow/blob/r1.6/tensorflow/python/summary/summary.py\n\n    Parameters\n    ----------\n      tag : str\n          A name for the generated summary. Will also serve as the series name in TensorBoard.\n      scalar : int, MXNet `NDArray`, or `numpy.ndarray`\n          A scalar value or an ndarray of shape (1,).\n\n    Returns\n    -------\n      A `Summary` protobuf of the `scalar` value.\n\n    Raises\n    ------\n      ValueError: If the scalar has the wrong shape or type.\n    \"\"\"\n", "input": "", "output": "    tag = _clean_tag(tag)\n    scalar = _make_numpy_array(scalar)\n    assert(scalar.squeeze().ndim == 0), 'scalar should be 0D'\n    scalar = float(scalar)\n    return Summary(value=[Summary.Value(tag=tag, simple_value=scalar)])"}, {"instruction": "def _read_depth_image(self):\n        \"\"\" Reads a depth image from the device \"\"\"\n", "input": "", "output": "        # read raw uint16 buffer\n        im_arr = self._depth_stream.read_frame()\n        raw_buf = im_arr.get_buffer_as_uint16()\n        buf_array = np.array([raw_buf[i] for i in range(PrimesenseSensor.DEPTH_IM_WIDTH * PrimesenseSensor.DEPTH_IM_HEIGHT)])\n\n        # convert to image in meters\n        depth_image = buf_array.reshape(PrimesenseSensor.DEPTH_IM_HEIGHT,\n                                        PrimesenseSensor.DEPTH_IM_WIDTH)\n        depth_image = depth_image * MM_TO_METERS # convert to meters\n        if self._flip_images:\n            depth_image = np.flipud(depth_image)\n        else:\n            depth_image = np.fliplr(depth_image)\n        return DepthImage(depth_image, frame=self._frame)"}, {"instruction": "def rel_path(name, available_tools):\n        \"\"\"\n        Extracts relative path to a tool (from the main cloned directory) out\n        of available_tools based on the name it is given\n        \"\"\"\n", "input": "", "output": "        if name == '@' or name == '.' or name == '/':\n            name = ''\n        multi_tool = '@' in name\n        for tool in available_tools:\n            t_name = tool[0].lower()\n            if multi_tool:\n                if name.split('@')[-1] == t_name.split('@')[-1]:\n                    return t_name, t_name\n            else:\n                if name == t_name.split('/')[-1]:\n                    return t_name, tool[0]\n                elif name == '' and t_name.split('@')[-1] == 'unspecified':\n                    return '', ''\n        return None, None"}, {"instruction": "def CanSetRoleTo(self, Role):\n        \"\"\"Checks if the new role can be applied to the member.\n\n        :Parameters:\n          Role : `enums`.chatMemberRole*\n            New chat member role.\n\n        :return: True if the new role can be applied, False otherwise.\n        :rtype: bool\n        \"\"\"\n", "input": "", "output": "        t = self._Owner._Alter('CHATMEMBER', self.Id, 'CANSETROLETO', Role,\n                               'ALTER CHATMEMBER CANSETROLETO')\n        return (chop(t, 1)[-1] == 'TRUE')"}, {"instruction": "def join_session(self, sid):\n        \"\"\"Attach to an existing session.\"\"\"\n", "input": "", "output": "        self._rest.add_header('X-STC-API-Session', sid)\n        self._sid = sid\n        try:\n            status, data = self._rest.get_request('objects', 'system1',\n                                                  ['version', 'name'])\n        except resthttp.RestHttpError as e:\n            self._rest.del_header('X-STC-API-Session')\n            self._sid = None\n            raise RuntimeError('failed to join session \"%s\": %s' % (sid, e))\n\n        return data['version']"}, {"instruction": "def process(self):\n        \"\"\"Process current event.\"\"\"\n", "input": "", "output": "        try:\n            self.receiver(self)\n        # TODO RESTException\n        except Exception as e:\n            current_app.logger.exception('Could not process event.')\n            self.response_code = 500\n            self.response = dict(status=500, message=str(e))\n        return self"}, {"instruction": "def Escape(self, string=\"\", **_):\n    \"\"\"Support standard string escaping.\"\"\"\n", "input": "", "output": "    # Translate special escapes:\n    self.stack[-1] += self.STRING_ESCAPES.get(string, string)"}, {"instruction": "def apply_to_segmentlist(self, seglist):\n\t\t\"\"\"\n\t\tApply our low and high windows to the segments in a\n\t\tsegmentlist.\n\t\t\"\"\"\n", "input": "", "output": "\t\tfor i, seg in enumerate(seglist):\n\t\t\tseglist[i] = seg.__class__(seg[0] - self.low_window, seg[1] + self.high_window)"}, {"instruction": "def get_rows( self, key ):\n        \"\"\"Get the set of rows for the type-key\"\"\"\n", "input": "", "output": "        if key not in self.roots:\n            self.get_root( key )\n        if key == 'location':\n            return self.location_rows \n        else:\n            return self.rows"}, {"instruction": "def _get_image_size(self, maxcharno, maxlineno):\n        \"\"\"\n        Get the required image size.\n        \"\"\"\n", "input": "", "output": "        return (self._get_char_x(maxcharno) + self.image_pad,\n                self._get_line_y(maxlineno + 0) + self.image_pad)"}, {"instruction": "def _subperiod_tick(self, current_interval, intervals):\n        \"\"\"Tick each sub-period, copying group_decisions to subperiod_group_decisions.\"\"\"\n", "input": "", "output": "        self.refresh_from_db()\n        for key, value in self.group_decisions.items():\n            self.subperiod_group_decisions[key] = value\n        self.send('group_decisions', self.subperiod_group_decisions)\n        self.save(update_fields=['subperiod_group_decisions'])"}, {"instruction": "def get_cloud_masks(self, threshold=None, non_valid_value=False):\n        \"\"\" The binary cloud mask is computed on the fly. Be cautious. The pixels without valid data are assigned\n        non_valid_value.\n\n        :param threshold: A float from [0,1] specifying threshold\n        :type threshold: float\n        :param non_valid_value: Value which will be assigned to pixels without valid data\n        :type non_valid_value: int in range `[-254, 255]`\n        :return: Binary cloud masks of shape `(times, height, width)` and `dtype=numpy.int8`\n        :rtype: numpy.ndarray\n        \"\"\"\n", "input": "", "output": "        self.get_probability_masks()\n\n        cloud_masks = self.cloud_detector.get_mask_from_prob(self.probability_masks, threshold)\n        cloud_masks[~self.valid_data] = non_valid_value\n\n        return cloud_masks"}, {"instruction": "def isfile(self, version=None, *args, **kwargs):\n        '''\n        Check whether the path exists and is a file\n        '''\n", "input": "", "output": "        version = _process_version(self, version)\n\n        path = self.get_version_path(version)\n        self.authority.fs.isfile(path, *args, **kwargs)"}, {"instruction": "def get_group(self):\n        \"\"\"Get the group of the Dataset.\n\n        Returns\n        -------\n        group : numpy array or None\n            Group size of each group.\n        \"\"\"\n", "input": "", "output": "        if self.group is None:\n            self.group = self.get_field('group')\n            if self.group is not None:\n                # group data from LightGBM is boundaries data, need to convert to group size\n                self.group = np.diff(self.group)\n        return self.group"}, {"instruction": "def save(self, list_file):\n    \"\"\"Saves the current list of annotations to the given file.\n\n    **Parameters:**\n\n    ``list_file`` : str\n      The name of a list file to write the currently stored list into\n    \"\"\"\n", "input": "", "output": "    bob.io.base.create_directories_safe(os.path.dirname(list_file))\n    with open(list_file, 'w') as f:\n      for i in range(len(self.image_paths)):\n        f.write(self.image_paths[i])\n        for bbx in self.bounding_boxes[i]:\n          f.write(\"\\t[%f %f %f %f]\" % (bbx.top_f, bbx.left_f, bbx.size_f[0], bbx.size_f[1]))\n        f.write(\"\\n\")"}, {"instruction": "def search_registered_query_for_facets(self, **kwargs):  # noqa: E501\n        \"\"\"Lists the values of one or more facets over the customer's non-deleted derived metric definition  # noqa: E501\n\n          # noqa: E501\n        This method makes a synchronous HTTP request by default. To make an\n        asynchronous HTTP request, please pass async_req=True\n        >>> thread = api.search_registered_query_for_facets(async_req=True)\n        >>> result = thread.get()\n\n        :param async_req bool\n        :param FacetsSearchRequestContainer body:\n        :return: ResponseContainerFacetsResponseContainer\n                 If the method is called asynchronously,\n                 returns the request thread.\n        \"\"\"\n", "input": "", "output": "        kwargs['_return_http_data_only'] = True\n        if kwargs.get('async_req'):\n            return self.search_registered_query_for_facets_with_http_info(**kwargs)  # noqa: E501\n        else:\n            (data) = self.search_registered_query_for_facets_with_http_info(**kwargs)  # noqa: E501\n            return data"}, {"instruction": "def _colored_time(self, time_taken, color=None):\n        \"\"\"Get formatted and colored string for a given time taken.\"\"\"\n", "input": "", "output": "        if self.timer_no_color:\n            return \"{0:0.4f}s\".format(time_taken)\n\n        return _colorize(\"{0:0.4f}s\".format(time_taken), color)"}, {"instruction": "def build(self):\n        \"\"\"\n        Builds the query string, which can be used for a search query\n\n        :return: the query string\n        \"\"\"\n", "input": "", "output": "        if self.es_version == '1':\n            if len(self.filters) > 0:\n                return {\n                    'filtered': {\n                        'query': self.query,\n                        'filter': {\n                            'and': self.filters\n                        }\n                    }\n                }\n            else:\n                return self.query\n        else:\n            query = {\n                'bool': {\n                    'must': self.query\n                }\n            }\n            if len(self.filters) > 0:\n                query[\"bool\"][\"filter\"] = self.filters\n            return query"}, {"instruction": "def reduced_chi_squareds(self, p=None):\n        \"\"\"\n        Returns the reduced chi squared for each massaged data set. \n\n        p=None means use the fit results.\n        \"\"\"\n", "input": "", "output": "        if len(self._set_xdata)==0 or len(self._set_ydata)==0: return None\n\n        if p is None: p = self.results[0]\n        r = self.studentized_residuals(p)\n        \n        # In case it's not possible to calculate\n        if r is None: return\n\n        # calculate the number of points\n        N = 0\n        for i in range(len(r)): N += len(r[i])\n\n        # degrees of freedom\n        dof_per_point = self.degrees_of_freedom()/N\n\n        for n in range(len(r)):\n            r[n] = sum(r[n]**2)/(len(r[n])*dof_per_point)\n\n        return r"}, {"instruction": "async def is_user_authorized(self):\n        \"\"\"\n        Returns ``True`` if the user is authorized.\n        \"\"\"\n", "input": "", "output": "        if self._authorized is None:\n            try:\n                # Any request that requires authorization will work\n                await self(functions.updates.GetStateRequest())\n                self._authorized = True\n            except errors.RPCError:\n                self._authorized = False\n\n        return self._authorized"}, {"instruction": "def edit_config_input_edit_content_url_url(self, **kwargs):\n        \"\"\"Auto Generated Code\n        \"\"\"\n", "input": "", "output": "        config = ET.Element(\"config\")\n        edit_config = ET.Element(\"edit_config\")\n        config = edit_config\n        input = ET.SubElement(edit_config, \"input\")\n        edit_content = ET.SubElement(input, \"edit-content\")\n        url = ET.SubElement(edit_content, \"url\")\n        url = ET.SubElement(url, \"url\")\n        url.text = kwargs.pop('url')\n\n        callback = kwargs.pop('callback', self._callback)\n        return callback(config)"}, {"instruction": "def learn_ids(self, item_list):\n        \"\"\" read in already set ids on objects \"\"\"\n", "input": "", "output": "        self._reset_sequence()\n        for item in item_list:\n            key = self.nondup_key_for_item(item)\n            self.ids[key] = item[self.id_key]"}, {"instruction": "def decode(self, offset):\n        \"\"\"Decode a section of the data section starting at offset\n\n        Arguments:\n        offset -- the location of the data structure to decode\n        \"\"\"\n", "input": "", "output": "        new_offset = offset + 1\n        (ctrl_byte,) = struct.unpack(b'!B', self._buffer[offset:new_offset])\n        type_num = ctrl_byte >> 5\n        # Extended type\n        if not type_num:\n            (type_num, new_offset) = self._read_extended(new_offset)\n\n        (size, new_offset) = self._size_from_ctrl_byte(\n            ctrl_byte, new_offset, type_num)\n        return self._type_decoder[type_num](self, size, new_offset)"}, {"instruction": "def post(self, url, body=None):\n        \"\"\"Sends this `Resource` instance to the service with a\n        ``POST`` request to the given URL. Takes an optional body\"\"\"\n", "input": "", "output": "        response = self.http_request(url, 'POST', body or self, {'Content-Type': 'application/xml; charset=utf-8'})\n        if response.status not in (200, 201, 204):\n            self.raise_http_error(response)\n\n        self._url = response.getheader('Location')\n\n        if response.status in (200, 201):\n            response_xml = response.read()\n            logging.getLogger('recurly.http.response').debug(response_xml)\n            self.update_from_element(ElementTree.fromstring(response_xml))"}, {"instruction": "def highpass(cutoff):\n  \"\"\"\n  This strategy uses an exponential approximation for cut-off frequency\n  calculation, found by matching the single pole and single zero Laplace\n  highpass filter.\n  \"\"\"\n", "input": "", "output": "  R = thub(exp(-cutoff), 2)\n  G = (R + 1) / 2\n  return G * (1 - z ** -1) / (1 - R * z ** -1)"}, {"instruction": "def get_command(self, name):\n        \"\"\"Get a :class:`.Command` or subclasses from the internal list\n        of commands.\n\n        This could also be used as a way to get aliases.\n\n        The name could be fully qualified (e.g. ``'foo bar'``) will get\n        the subcommand ``bar`` of the group command ``foo``. If a\n        subcommand is not found then ``None`` is returned just as usual.\n\n        Parameters\n        -----------\n        name: :class:`str`\n            The name of the command to get.\n\n        Returns\n        --------\n        :class:`Command` or subclass\n            The command that was requested. If not found, returns ``None``.\n        \"\"\"\n", "input": "", "output": "\n        # fast path, no space in name.\n        if ' ' not in name:\n            return self.all_commands.get(name)\n\n        names = name.split()\n        obj = self.all_commands.get(names[0])\n        if not isinstance(obj, GroupMixin):\n            return obj\n\n        for name in names[1:]:\n            try:\n                obj = obj.all_commands[name]\n            except (AttributeError, KeyError):\n                return None\n\n        return obj"}, {"instruction": "def script_level(self, container):\n        \"\"\"Nesting level of super/subscript.\"\"\"\n", "input": "", "output": "        try:\n            level = self.parent.script_level(container)\n        except AttributeError:\n            level = -1\n        return level + 1 if self.is_script(container) else level"}, {"instruction": "def generatePlugins(widgetPath = None, buildPath = None):\r\n    \"\"\"\r\n    Generates all the plugin files for the system and imports them.\r\n    \r\n    :param      widgetPath | <str> || None\r\n                buildPath  | <str> || None\r\n    \"\"\"\n", "input": "", "output": "    if widgetPath is None:\r\n        widgetPath = WIDGET_PATH\r\n        \r\n    if buildPath is None:\r\n        buildPath = BUILD_PATH\r\n    \r\n    for basepath in widgetPath.split(os.path.pathsep):\r\n        if not basepath:\r\n            continue\r\n            \r\n        # load packaged widgets\r\n        for filepath in glob.glob(os.path.join(basepath, '*/__init__.py')):\r\n            generatePlugins(os.path.dirname(filepath), buildPath)\r\n        \r\n        # load module widgets\r\n        for filepath in glob.glob(os.path.join(basepath, '*.py')):\r\n            generatePlugin(filepath, buildPath)"}, {"instruction": "def extract_mfd(dstore, what):\n    \"\"\"\n    Display num_ruptures by magnitude for event based calculations.\n    Example: http://127.0.0.1:8800/v1/calc/30/extract/event_based_mfd\n    \"\"\"\n", "input": "", "output": "    dd = collections.defaultdict(int)\n    for rup in dstore['ruptures'].value:\n        dd[rup['mag']] += 1\n    dt = numpy.dtype([('mag', float), ('freq', int)])\n    magfreq = numpy.array(sorted(dd.items(), key=operator.itemgetter(0)), dt)\n    return magfreq"}, {"instruction": "def get_tile_gid(self, x, y, layer):\n        \"\"\" Return the tile image GID for this location\n\n        :param x: x coordinate\n        :param y: y coordinate\n        :param layer: layer number\n        :rtype: surface if found, otherwise ValueError\n        \"\"\"\n", "input": "", "output": "        try:\n            assert (x >= 0 and y >= 0 and layer >= 0)\n        except AssertionError:\n            raise ValueError\n\n        try:\n            return self.layers[int(layer)].data[int(y)][int(x)]\n        except (IndexError, ValueError):\n            msg = \"Coords: ({0},{1}) in layer {2} is invalid\"\n            logger.debug(msg, (x, y, layer))\n            raise ValueError"}, {"instruction": "def addMonitor(self, monitorFriendlyName, monitorURL):\n        \"\"\"\n        Returns True if Monitor was added, otherwise False.\n        \"\"\"\n", "input": "", "output": "        url = self.baseUrl\n        url += \"newMonitor?apiKey=%s\" % self.apiKey\n        url += \"&monitorFriendlyName=%s\" % monitorFriendlyName\n        url += \"&monitorURL=%s&monitorType=1\" % monitorURL\n        url += \"&monitorAlertContacts=%s\" % monitorAlertContacts\n        url += \"&noJsonCallback=1&format=json\"\n        success, response = self.requestApi(url)\n        if success:\n            return True\n        else:\n            return False"}, {"instruction": "def parse_options_header(value):\n    \"\"\"Parse a ``Content-Type`` like header into a tuple with the content\n    type and the options:\n\n    >>> parse_options_header('text/html; charset=utf8')\n    ('text/html', {'charset': 'utf8'})\n\n    This should not be used to parse ``Cache-Control`` like headers that use\n    a slightly different format.  For these headers use the\n    :func:`parse_dict_header` function.\n\n    .. versionadded:: 0.5\n\n    :param value: the header to parse.\n    :return: (str, options)\n    \"\"\"\n", "input": "", "output": "    def _tokenize(string):\n        for match in _option_header_piece_re.finditer(string):\n            key, value = match.groups()\n            key = unquote_header_value(key)\n            if value is not None:\n                value = unquote_header_value(value, key == 'filename')\n            yield key, value\n\n    if not value:\n        return '', {}\n\n    parts = _tokenize(';' + value)\n    name = next(parts)[0]\n    extra = dict(parts)\n    return name, extra"}, {"instruction": "def num_nanoclusters(ConcAluminum, coag):\n    \"\"\"Return the number of Aluminum nanoclusters.\"\"\"\n", "input": "", "output": "    return (ConcAluminum / (dens_alum_nanocluster(coag).magnitude\n                            * np.pi * coag.Diameter**3))"}, {"instruction": "def get_validator_list(self):\n        \"\"\"Return a list of validators specified in the override file\"\"\"\n", "input": "", "output": "        ignore = [\n            'dict',\n        ]\n        vlist = []\n        if not self.override:\n            return vlist\n\n        for k, v in list(self.override['classes'].items()):\n            if 'validator' in v:\n                validator = v['validator']\n                if validator not in ignore and validator not in vlist:\n                    vlist.append(validator)\n\n        for k, v in list(self.override['classes'].items()):\n            for kp, vp in list(v.items()):\n                if 'validator' in vp:\n                    validator = vp['validator']\n                    if validator not in ignore and validator not in vlist:\n                        vlist.append(validator)\n        return sorted(vlist)"}, {"instruction": "def voidage_experimental(m, rho, D, H):\n    r'''Calculates voidage of a bed or mesh given an experimental weight and\n    fixed density, diameter, and height, as shown in [1]_. The formula is also\n    self-evident.\n\n    .. math::\n        \\epsilon = 1 - \\frac{\\frac{m_{mesh}}{\\frac{\\pi}{4}d_{column}^2\n        L_{mesh}}}{\\rho_{material}}\n\n    Parameters\n    ----------\n    m : float\n        Mass of mesh or bed particles weighted, [kg]\n    rho : float\n        Density of solid particles or mesh [kg/m^3]\n    D : float\n        Diameter of the cylindrical bed [m]\n    H : float\n        Height of the demister or bed [m]\n\n    Returns\n    -------\n    voidage : float\n        Voidage of bed of the material []\n\n    Notes\n    -----\n    Should be trusted over manufacturer data.\n\n    Examples\n    --------\n    >>> voidage_experimental(m=126, rho=8000, D=1, H=1)\n    0.9799464771704212\n\n    References\n    ----------\n    .. [1] Hels\u00f8r, T., and H. Svendsen. \"Experimental Characterization of\n       Pressure Drop in Dry Demisters at Low and Elevated Pressures.\" Chemical\n       Engineering Research and Design 85, no. 3 (2007): 377-85.\n       doi:10.1205/cherd06048.\n    '''\n", "input": "", "output": "    return 1 - m/(pi/4*D**2*H)/rho"}, {"instruction": "def shp2geom(shp_fn):\n    \"\"\"Extract geometries from input shapefile\n    \n    Need to handle multi-part geom: http://osgeo-org.1560.x6.nabble.com/Multipart-to-singlepart-td3746767.html\n    \"\"\"\n", "input": "", "output": "    ds = ogr.Open(shp_fn)\n    lyr = ds.GetLayer()\n    srs = lyr.GetSpatialRef()\n    lyr.ResetReading()\n    geom_list = []\n    for feat in lyr:\n        geom = feat.GetGeometryRef()\n        geom.AssignSpatialReference(srs)\n        #Duplicate the geometry, or segfault\n        #See: http://trac.osgeo.org/gdal/wiki/PythonGotchas\n        #g = ogr.CreateGeometryFromWkt(geom.ExportToWkt())\n        #g.AssignSpatialReference(srs)\n        g = geom_dup(geom)\n        geom_list.append(g)\n    #geom = ogr.ForceToPolygon(' '.join(geom_list))    \n    #Dissolve should convert multipolygon to single polygon \n    #return geom_list[0]\n    ds = None\n    return geom_list"}, {"instruction": "def getResourceMapPid(self):\n        \"\"\"Returns:\n\n        str : PID of the Resource Map itself.\n\n        \"\"\"\n", "input": "", "output": "        ore = [\n            o for o in self.subjects(predicate=rdflib.RDF.type, object=ORE.ResourceMap)\n        ][0]\n        pid = [str(o) for o in self.objects(predicate=DCTERMS.identifier, subject=ore)][\n            0\n        ]\n        return pid"}, {"instruction": "def delete_cors(Bucket,\n           region=None, key=None, keyid=None, profile=None):\n    '''\n    Delete the CORS configuration for the given bucket\n\n    Returns {deleted: true} if CORS was deleted and returns\n    {deleted: False} if CORS was not deleted.\n\n    CLI Example:\n\n    .. code-block:: bash\n\n        salt myminion boto_s3_bucket.delete_cors my_bucket\n\n    '''\n", "input": "", "output": "\n    try:\n        conn = _get_conn(region=region, key=key, keyid=keyid, profile=profile)\n        conn.delete_bucket_cors(Bucket=Bucket)\n        return {'deleted': True, 'name': Bucket}\n    except ClientError as e:\n        return {'deleted': False, 'error': __utils__['boto3.get_error'](e)}"}, {"instruction": "def _serialize_parameters(parameters):\n        \"\"\"Serialize some parameters to match python native types with formats\n        specified in google api docs like:\n        * True/False -> \"true\"/\"false\",\n        * {\"a\": 1, \"b\":2} -> \"a:1|b:2\"\n\n        :type parameters: dict oif query parameters\n        \"\"\"\n", "input": "", "output": "\n        for key, value in parameters.items():\n            if isinstance(value, bool):\n                parameters[key] = \"true\" if value else \"false\"\n            elif isinstance(value, dict):\n                parameters[key] = \"|\".join(\n                    (\"%s:%s\" % (k, v) for k, v in value.items()))\n            elif isinstance(value, (list, tuple)):\n                parameters[key] = \"|\".join(value)\n        return parameters"}, {"instruction": "def get_host_address(host=None, default_address=DEFAULT_HOST_IP):\n    \"\"\"\n    Returns the given host address.\n\n    :param host: Host to retrieve the address.\n    :type host: unicode\n    :param default_address: Default address if the host is unreachable.\n    :type default_address: unicode\n    :return: Host address.\n    :rtype: unicode\n    \"\"\"\n", "input": "", "output": "\n    try:\n        return unicode(socket.gethostbyname(host or socket.gethostname()),\n                       Constants.default_codec,\n                       Constants.codec_error)\n    except Exception as error:\n        return default_address"}, {"instruction": "def _sendAction(self, action, attrs=None, chan_vars=None):\n        \"\"\"Send action to Asterisk Manager Interface.\n        \n        @param action:    Action name\n        @param attrs:     Tuple of key-value pairs for action attributes.\n        @param chan_vars: Tuple of key-value pairs for channel variables.\n\n        \"\"\"\n", "input": "", "output": "        self._conn.write(\"Action: %s\\r\\n\" % action)\n        if attrs:\n            for (key,val) in attrs:\n                self._conn.write(\"%s: %s\\r\\n\" % (key, val))\n        if chan_vars:\n            for (key,val) in chan_vars:\n                self._conn.write(\"Variable: %s=%s\\r\\n\" % (key, val))\n        self._conn.write(\"\\r\\n\")"}, {"instruction": "def _is_not_considered_falsey(value, ignore_types=()):\n    '''\n    Helper function for filter_falsey to determine if something is not to be\n    considered falsey.\n\n    :param any value: The value to consider\n    :param list ignore_types: The types to ignore when considering the value.\n\n    :return bool\n    '''\n", "input": "", "output": "    return isinstance(value, bool) or type(value) in ignore_types or value"}, {"instruction": "def _loop_thread_main(self):\n        \"\"\"Main background thread running the event loop.\"\"\"\n", "input": "", "output": "\n        asyncio.set_event_loop(self.loop)\n        self._loop_check.inside_loop = True\n\n        try:\n            self._logger.debug(\"Starting loop in background thread\")\n            self.loop.run_forever()\n            self._logger.debug(\"Finished loop in background thread\")\n        except:  # pylint:disable=bare-except;This is a background worker thread.\n            self._logger.exception(\"Exception raised from event loop thread\")\n        finally:\n            self.loop.close()"}, {"instruction": "def getControls(self):\n        '''\n        Calculates consumption for each consumer of this type using the consumption functions.\n\n        Parameters\n        ----------\n        None\n\n        Returns\n        -------\n        None\n        '''\n", "input": "", "output": "        cLvlNow = np.zeros(self.AgentCount) + np.nan\n        MPCnow = np.zeros(self.AgentCount) + np.nan\n        for t in range(self.T_cycle):\n            these = t == self.t_cycle\n            cLvlNow[these] = self.solution[t].cFunc(self.mLvlNow[these],self.pLvlNow[these])\n            MPCnow[these]  = self.solution[t].cFunc.derivativeX(self.mLvlNow[these],self.pLvlNow[these])\n        self.cLvlNow = cLvlNow\n        self.MPCnow  = MPCnow"}, {"instruction": "def load_fn_matches_ext(file_path, file_type):\n    \"\"\"\n    Check that the file extension matches the target extension given.\n\n    :param str file_path: Path to be checked\n    :param str file_type: Target extension\n    :return bool correct_ext: Extension match or does not match\n    \"\"\"\n", "input": "", "output": "    correct_ext = False\n    curr_ext = os.path.splitext(file_path)[1]\n    exts = [curr_ext, file_type]\n    try:\n        # special case: if file type is excel, both extensions are valid.\n        if \".xlsx\" in exts and \".xls\" in exts:\n            correct_ext = True\n        elif curr_ext == file_type:\n            correct_ext = True\n        else:\n            print(\"Use '{}' to load this file: {}\".format(FILE_TYPE_MAP[curr_ext][\"load_fn\"],\n                                                          os.path.basename(file_path)))\n    except Exception as e:\n        logger_misc.debug(\"load_fn_matches_ext: {}\".format(e))\n\n    return correct_ext"}, {"instruction": "def abrt(self, s=None, post=None, noraise=False):\n        \"\"\" Prints the abrt banner and raises ``ProgressAbrt`` exception\n\n        When ``noraise`` flag is set to ``True``, then the exception is not\n        raised, and progress is allowed to continue.\n\n        If ``post`` function is supplied it is invoked with no arguments after\n        the close banner is printed, but before exceptions are raised. The\n        ``post`` function takes no arguments.\n        \"\"\"\n", "input": "", "output": "        s = s or self.abrt_msg\n        self.printer(self.color.red(s))\n        if post:\n            post()\n        if noraise:\n            return\n        raise ProgressAbrt()"}, {"instruction": "def open(hdfs_path, mode=\"r\", buff_size=0, replication=0, blocksize=0,\n         user=None, encoding=None, errors=None):\n    \"\"\"\n    Open a file, returning an :class:`~.file.hdfs_file` object.\n\n    ``hdfs_path`` and ``user`` are passed to :func:`~path.split`,\n    while the other args are passed to the :class:`~.file.hdfs_file`\n    constructor.\n    \"\"\"\n", "input": "", "output": "    host, port, path_ = path.split(hdfs_path, user)\n    fs = hdfs(host, port, user)\n    return fs.open_file(path_, mode, buff_size, replication, blocksize,\n                        encoding, errors)"}, {"instruction": "def in_casapy (helper, caltable=None, selectcals={}, plotoptions={},\n               xaxis=None, yaxis=None, figfile=None):\n    \"\"\"This function is run inside the weirdo casapy IPython environment! A\n    strange set of modules is available, and the\n    `pwkit.environments.casa.scripting` system sets up a very particular\n    environment to allow encapsulated scripting.\n\n    \"\"\"\n", "input": "", "output": "    if caltable is None:\n        raise ValueError ('caltable')\n\n    show_gui = (figfile is None)\n    cp = helper.casans.cp\n\n    helper.casans.tp.setgui (show_gui)\n    cp.open (caltable)\n    cp.selectcal (**selectcals)\n    cp.plotoptions (**plotoptions)\n    cp.plot (xaxis, yaxis)\n\n    if show_gui:\n        import pylab as pl\n        pl.show ()\n    else:\n        cp.savefig (figfile)"}, {"instruction": "def public_decrypt(pub, message):\n    '''\n    Verify an M2Crypto-compatible signature\n\n    :param Crypto.PublicKey.RSA._RSAobj key: The RSA public key object\n    :param str message: The signed message to verify\n    :rtype: str\n    :return: The message (or digest) recovered from the signature, or an\n        empty string if the verification failed\n    '''\n", "input": "", "output": "    if HAS_M2:\n        return pub.public_decrypt(message, salt.utils.rsax931.RSA_X931_PADDING)\n    else:\n        verifier = salt.utils.rsax931.RSAX931Verifier(pub.exportKey('PEM'))\n        return verifier.verify(message)"}, {"instruction": "def _to_dict(self):\n        \"\"\"\n        Converts object into a dictionary.\n        \"\"\"\n", "input": "", "output": "        for i, tag in enumerate(self.tags):\n            if tag in (\"\", None):\n                self.tags.pop(i)\n\n        data = {\n            'name': self.name,\n            'referenceId': self.reference_id,\n            'shortDescription': self.short_description,\n            'longDescription': self.long_description,\n            'itemState': self.item_state,\n            'linkURL': self.link_url,\n            'linkText': self.link_text,\n            'tags': self.tags,\n            'economics': self.economics,\n            'id': self.id,\n            'end_date': _make_tstamp(self.end_date),\n            'start_date': _make_tstamp(self.start_date)}\n        if len(self.renditions) > 0:\n            data['renditions'] = []\n            for r in self.renditions:\n                data['renditions'].append(r.to_dict())\n        if len(self.metadata) > 0:\n            data['customFields'] = {}\n            for meta in self.metadata:\n                data['customFields'][meta['key']] = meta['value']\n        [data.pop(key) for key in data.keys() if data[key] == None]\n        return data"}, {"instruction": "def add_acquisition_source(\n        self,\n        method,\n        submission_number=None,\n        internal_uid=None,\n        email=None,\n        orcid=None,\n        source=None,\n        datetime=None,\n    ):\n        \"\"\"Add acquisition source.\n\n        :type submission_number: integer\n\n        :type email: integer\n\n        :type source: string\n\n        :param method: method of acquisition for the suggested document\n        :type method: string\n\n        :param orcid: orcid of the user that is creating the record\n        :type orcid: string\n\n        :param internal_uid: id of the user that is creating the record\n        :type internal_uid: string\n\n        :param datetime: UTC datetime in ISO 8601 format\n        :type datetime: string\n        \"\"\"\n", "input": "", "output": "        acquisition_source = self._sourced_dict(source)\n\n        acquisition_source['submission_number'] = str(submission_number)\n        for key in ('datetime', 'email', 'method', 'orcid', 'internal_uid'):\n            if locals()[key] is not None:\n                acquisition_source[key] = locals()[key]\n\n        self.obj['acquisition_source'] = acquisition_source"}, {"instruction": "def service(cls):\n    '''\n    Marks the decorated class as a singleton ``service``.\n\n    Injects following classmethods:\n\n        .. py:method:: .get(context)\n\n            Returns a singleton instance of the class for given ``context``\n\n            :param context: context to look in\n            :type context: :class:`Context`\n            :returns: ``cls``\n    '''\n", "input": "", "output": "\n    if not cls:\n        return None\n\n    # Inject methods\n    def get(cls, context):\n        return context.get_service(cls)\n    cls.get = get.__get__(cls)\n\n    log.debug('Registering [%s] (service)', get_fqdn(cls))\n\n    return cls"}, {"instruction": "def setcreated(self, dt=None):\n        \"\"\"\n        Set I{created}.\n        @param dt: The created date & time.\n            Set as datetime.utc() when I{None}.\n        @type dt: L{datetime}\n        \"\"\"\n", "input": "", "output": "        if dt is None:\n            self.created = Token.utc()\n        else:\n            self.created = dt"}, {"instruction": "def iscsi_iqn():\n    '''\n    Return iSCSI IQN\n    '''\n", "input": "", "output": "    grains = {}\n    grains['iscsi_iqn'] = False\n    if salt.utils.platform.is_linux():\n        grains['iscsi_iqn'] = _linux_iqn()\n    elif salt.utils.platform.is_windows():\n        grains['iscsi_iqn'] = _windows_iqn()\n    elif salt.utils.platform.is_aix():\n        grains['iscsi_iqn'] = _aix_iqn()\n    return grains"}, {"instruction": "def with_router(func):\n    \"\"\"\n    Decorator version of :func:`run_with_router`. Example:\n\n    .. code-block:: python\n\n        @with_router\n        def do_stuff(router, arg):\n            pass\n\n        do_stuff(blah, 123)\n    \"\"\"\n", "input": "", "output": "    def wrapper(*args, **kwargs):\n        return run_with_router(func, *args, **kwargs)\n    if mitogen.core.PY3:\n        wrapper.func_name = func.__name__\n    else:\n        wrapper.func_name = func.func_name\n    return wrapper"}, {"instruction": "def on_event(self, evt, is_final):\n        \"\"\" this is invoked from in response to COM PumpWaitingMessages - different thread \"\"\"\n", "input": "", "output": "        for msg in XmlHelper.message_iter(evt):\n            for node, error in XmlHelper.security_iter(msg.GetElement('securityData')):\n                if error:\n                    self.security_errors.append(error)\n                else:\n                    self.on_security_node(node)\n\n        if is_final and self.response_type == 'frame':\n            index = self.response.pop('security')\n            frame = DataFrame(self.response, columns=self.fields, index=index)\n            frame.index.name = 'security'\n            self.response = frame"}, {"instruction": "def bipartition(seq):\n    \"\"\"Return a list of bipartitions for a sequence.\n\n    Args:\n        a (Iterable): The sequence to partition.\n\n    Returns:\n        list[tuple[tuple]]: A list of tuples containing each of the two\n        partitions.\n\n    Example:\n        >>> bipartition((1,2,3))\n        [((), (1, 2, 3)), ((1,), (2, 3)), ((2,), (1, 3)), ((1, 2), (3,))]\n    \"\"\"\n", "input": "", "output": "    return [(tuple(seq[i] for i in part0_idx),\n             tuple(seq[j] for j in part1_idx))\n            for part0_idx, part1_idx in bipartition_indices(len(seq))]"}, {"instruction": "def populate_all_metadata():\n    \"\"\" Create metadata instances for all models in seo_models if empty.\n        Once you have created a single metadata instance, this will not run.\n        This is because it is a potentially slow operation that need only be\n        done once. If you want to ensure that everything is populated, run the\n        populate_metadata management command.\n    \"\"\"\n", "input": "", "output": "    for Metadata in registry.values():\n        InstanceMetadata = Metadata._meta.get_model('modelinstance')\n        if InstanceMetadata is not None:\n            for model in Metadata._meta.seo_models:\n                populate_metadata(model, InstanceMetadata)"}, {"instruction": "def checkout(self, revision_id):\n        \"\"\"\n        :param revision_id: :class:`revision.data.Revision` ID.\n        :type revision_id: str\n        \"\"\"\n", "input": "", "output": "        index = 0\n        found = False\n        for revision in self.revisions:\n            if revision.revision_id == revision_id:\n                self.current_index = index\n                found = True\n\n            index += 1\n\n        if not found:\n            raise RuntimeError(\"\")"}, {"instruction": "def chi_squared(source_frequency, target_frequency):\n    \"\"\"Calculate the Chi Squared statistic by comparing ``source_frequency`` with ``target_frequency``.\n\n    Example:\n        >>> chi_squared({'a': 2, 'b': 3}, {'a': 1, 'b': 2})\n        0.1\n\n    Args:\n        source_frequency (dict): Frequency map of the text you are analyzing\n        target_frequency (dict): Frequency map of the target language to compare with\n\n    Returns:\n        Decimal value of the chi-squared statistic\n    \"\"\"\n", "input": "", "output": "    # Ignore any symbols from source that are not in target.\n    # TODO: raise Error if source_len is 0?\n    target_prob = frequency_to_probability(target_frequency)\n    source_len = sum(v for k, v in source_frequency.items() if k in target_frequency)\n\n    result = 0\n    for symbol, prob in target_prob.items():\n        symbol_frequency = source_frequency.get(symbol, 0)  # Frequecy is 0 if it doesnt appear in source\n        result += _calculate_chi_squared(symbol_frequency, prob, source_len)\n\n    return result"}, {"instruction": "def get_credentials():\n    \"\"\"Gets valid user credentials from storage.\n\n    If nothing has been stored, or if the stored credentials are invalid,\n    the OAuth2 flow is completed to obtain the new credentials.\n\n    Returns:\n        Credentials, the obtained credential.\n    \"\"\"\n", "input": "", "output": "    home_dir = os.path.expanduser('~')\n    credential_dir = os.path.join(home_dir, '.credentials')\n    if not os.path.exists(credential_dir):\n        os.makedirs(credential_dir)\n    credential_path = os.path.join(credential_dir,\n                                   'calendar-python-quickstart.json')\n\n    store = Storage(credential_path)\n    credentials = store.get()\n    if not credentials or credentials.invalid:\n        flow = client.flow_from_clientsecrets(CLIENT_SECRET_FILE, SCOPES)\n        flow.user_agent = APPLICATION_NAME\n        if flags:\n            credentials = tools.run_flow(flow, store, flags)\n        else: # Needed only for compatibility with Python 2.6\n            credentials = tools.run(flow, store)\n        print('Storing credentials to ' + credential_path)\n    return credentials"}, {"instruction": "def _notebook(trigger, note_store):\n        \"\"\"\n        :param trigger:\u00a0trigger object\n        :param note_store: note_store object\n        :return: note object\n        \"\"\"\n", "input": "", "output": "        note = Types.Note()\n        if trigger.notebook:\n            # get the notebookGUID ...\n            notebook_id = EvernoteMgr.get_notebook(note_store, trigger.notebook)\n            # create notebookGUID if it does not exist then return its id\n            note.notebookGuid = EvernoteMgr.set_notebook(note_store, trigger.notebook, notebook_id)\n\n            if trigger.tag:\n                # ... and get the tagGUID if a tag has been provided\n                tag_id = EvernoteMgr.get_tag(note_store, trigger.tag)\n                if tag_id is False:\n                    tag_id = EvernoteMgr.set_tag(note_store, trigger.tag, tag_id)\n                    # set the tag to the note if a tag has been provided\n                    if tag_id:\n                        note.tagGuids = tag_id\n\n            logger.debug(\"notebook that will be used %s\", trigger.notebook)\n        return note"}, {"instruction": "def get_features(self, jid):\n        \"\"\"\n        Return the features supported by a service.\n\n        :param jid: Address of the PubSub service to query.\n        :type jid: :class:`aioxmpp.JID`\n        :return: Set of supported features\n        :rtype: set containing :class:`~.pubsub.xso.Feature` enumeration\n                members.\n\n        This simply uses service discovery to obtain the set of features and\n        converts the features to :class:`~.pubsub.xso.Feature` enumeration\n        members. To get the full feature information, resort to using\n        :meth:`.DiscoClient.query_info` directly on `jid`.\n\n        Features returned by the peer which are not valid pubsub features are\n        not returned.\n        \"\"\"\n", "input": "", "output": "\n        response = yield from self._disco.query_info(jid)\n        result = set()\n        for feature in response.features:\n            try:\n                result.add(pubsub_xso.Feature(feature))\n            except ValueError:\n                continue\n        return result"}, {"instruction": "def plotJacobi(self,*args,**kwargs):\n        \"\"\"\n        NAME:\n\n           plotJacobi\n\n        PURPOSE:\n\n           plot the Jacobi integral along the orbit\n\n        INPUT:\n\n           OmegaP= pattern speed\n\n           pot= - Potential instance or list of instances in which the orbit \n                 was integrated\n\n           d1= - plot Ez vs d1: e.g., 't', 'z', 'R', 'vR', 'vT', 'vz'      \n\n           normed= if set, plot E(t)/E(0) rather than E(t)\n\n           ro= (Object-wide default) physical scale for distances to use to convert (can be Quantity)\n\n           vo= (Object-wide default) physical scale for velocities to use to convert (can be Quantity)\n\n           use_physical= use to override Object-wide default for using a physical scale for output\n\n           +bovy_plot.bovy_plot inputs\n\n        OUTPUT:\n\n           figure to output device\n\n        HISTORY:\n\n           2011-10-10 - Written - Bovy (IAS)\n\n        \"\"\"\n", "input": "", "output": "        if not kwargs.get('pot',None) is None: kwargs['pot']= flatten_potential(kwargs.get('pot'))\n        return self._orb.plotJacobi(*args,**kwargs)"}, {"instruction": "def NoExclusions(self):\n        \"\"\"Determine that there are no exclusion criterion in play\n\n        :return: True if there is no real boundary specification of any kind.\n\n        Simple method allowing parsers to short circuit the determination of\n        missingness, which can be moderately compute intensive.\n        \"\"\"\n", "input": "", "output": "        if len(self.start_bounds) + len(self.target_rs) + len(self.ignored_rs) == 0:\n            return BoundaryCheck.chrom == -1\n        return False"}, {"instruction": "def _download(self, force_overwrite=False, verbose=False):\n        \"\"\"\n        Download the file if it's not already there.\n        We shouldn't *need* to overwrite; the xml is not supposed to update.\n        \"\"\"\n", "input": "", "output": "        if not force_overwrite:\n            # If the file is already there, we're done\n            if os.path.isfile(self.filepath):\n                if verbose:\n                    print(\n                        \"File already available at %s -- skipping\"\n                        % (self.filepath)\n                    )\n                return False\n        stream_download(self.URL, self.filepath, verbose=verbose)\n        return True"}, {"instruction": "def local_minima(img, min_distance = 4):\n    r\"\"\"\n    Returns all local minima from an image.\n    \n    Parameters\n    ----------\n    img : array_like\n        The image.\n    min_distance : integer\n        The minimal distance between the minimas in voxels. If it is less, only the lower minima is returned.\n    \n    Returns\n    -------\n    indices : sequence\n        List of all minima indices.\n    values : sequence\n        List of all minima values.\n    \"\"\"\n", "input": "", "output": "    # @TODO: Write a unittest for this.\n    fits = numpy.asarray(img)\n    minfits = minimum_filter(fits, size=min_distance) # default mode is reflect\n    minima_mask = fits == minfits\n    good_indices = numpy.transpose(minima_mask.nonzero())\n    good_fits = fits[minima_mask]\n    order = good_fits.argsort()\n    return good_indices[order], good_fits[order]"}, {"instruction": "def getResponse(self, http_request, request):\n        \"\"\"\n        Processes the AMF request, returning an AMF response.\n\n        @param http_request: The underlying HTTP Request.\n        @type http_request: U{HTTPRequest<http://docs.djangoproject.com\n            /en/dev/ref/request-response/#httprequest-objects>}\n        @param request: The AMF Request.\n        @type request: L{Envelope<pyamf.remoting.Envelope>}\n        @rtype: L{Envelope<pyamf.remoting.Envelope>}\n        \"\"\"\n", "input": "", "output": "        response = remoting.Envelope(request.amfVersion)\n\n        for name, message in request:\n            http_request.amf_request = message\n\n            processor = self.getProcessor(message)\n            response[name] = processor(message, http_request=http_request)\n\n        return response"}, {"instruction": "async def quit(self, message=None):\n        \"\"\" Quit network. \"\"\"\n", "input": "", "output": "        if message is None:\n            message = self.DEFAULT_QUIT_MESSAGE\n\n        await self.rawmsg('QUIT', message)\n        await self.disconnect(expected=True)"}, {"instruction": "def unassign_comment_from_book(self, comment_id, book_id):\n        \"\"\"Removes a ``Comment`` from a ``Book``.\n\n        arg:    comment_id (osid.id.Id): the ``Id`` of the ``Comment``\n        arg:    book_id (osid.id.Id): the ``Id`` of the ``Book``\n        raise:  NotFound - ``comment_id`` or ``book_id`` not found or\n                ``comment_id`` not assigned to ``book_id``\n        raise:  NullArgument - ``comment_id`` or ``book_id`` is ``null``\n        raise:  OperationFailed - unable to complete request\n        raise:  PermissionDenied - authorization failure\n        *compliance: mandatory -- This method must be implemented.*\n\n        \"\"\"\n", "input": "", "output": "        # Implemented from template for\n        # osid.resource.ResourceBinAssignmentSession.unassign_resource_from_bin\n        mgr = self._get_provider_manager('COMMENTING', local=True)\n        lookup_session = mgr.get_book_lookup_session(proxy=self._proxy)\n        lookup_session.get_book(book_id)  # to raise NotFound\n        self._unassign_object_from_catalog(comment_id, book_id)"}, {"instruction": "def get_large_image(self, page=1):\n        \"\"\"\n        Downloads and returns the large sized image of a single page.\n\n        The page kwarg specifies which page to return. One is the default.\n        \"\"\"\n", "input": "", "output": "        url = self.get_large_image_url(page=page)\n        return self._get_url(url)"}, {"instruction": "def update_rec(self, rec, name, value):\n        \"\"\"Update current GOTerm with optional record.\"\"\"\n", "input": "", "output": "        # 'def' is a reserved word in python, do not use it as a Class attr.\n        if name == \"def\":\n            name = \"defn\"\n\n        # If we have a relationship, then we will split this into a further\n        # dictionary.\n\n        if hasattr(rec, name):\n            if name not in self.attrs_scalar:\n                if name not in self.attrs_nested:\n                    getattr(rec, name).add(value)\n                else:\n                    self._add_nested(rec, name, value)\n            else:\n                raise Exception(\"ATTR({NAME}) ALREADY SET({VAL})\".format(\n                    NAME=name, VAL=getattr(rec, name)))\n        else: # Initialize new GOTerm attr\n            if name in self.attrs_scalar:\n                setattr(rec, name, value)\n            elif name not in self.attrs_nested:\n                setattr(rec, name, set([value]))\n            else:\n                name = '_{:s}'.format(name)\n                setattr(rec, name, defaultdict(list))\n                self._add_nested(rec, name, value)"}, {"instruction": "def get_country_info_from_iso3(cls, iso3, use_live=True, exception=None):\n        # type: (str, bool, Optional[ExceptionUpperBound]) -> Optional[Dict[str]]\n        \"\"\"Get country information from ISO3 code\n\n        Args:\n            iso3 (str): ISO3 code for which to get country information\n            use_live (bool): Try to get use latest data from web rather than file in package. Defaults to True.\n            exception (Optional[ExceptionUpperBound]): An exception to raise if country not found. Defaults to None.\n\n        Returns:\n            Optional[Dict[str]]: country information\n        \"\"\"\n", "input": "", "output": "        countriesdata = cls.countriesdata(use_live=use_live)\n        country = countriesdata['countries'].get(iso3.upper())\n        if country is not None:\n            return country\n\n        if exception is not None:\n            raise exception\n        return None"}, {"instruction": "def get_aoi(self, solar_zenith, solar_azimuth):\n        \"\"\"Get the angle of incidence on the system.\n\n        Parameters\n        ----------\n        solar_zenith : float or Series.\n            Solar zenith angle.\n        solar_azimuth : float or Series.\n            Solar azimuth angle.\n\n        Returns\n        -------\n        aoi : Series\n            The angle of incidence\n        \"\"\"\n", "input": "", "output": "\n        aoi = irradiance.aoi(self.surface_tilt, self.surface_azimuth,\n                             solar_zenith, solar_azimuth)\n        return aoi"}, {"instruction": "def get_argv_for_command(self):\n        \"\"\"\n        Returns stripped arguments that would be passed into the command.\n        \"\"\"\n", "input": "", "output": "        argv = [a for a in self.argv]\n        argv.insert(0, self.prog_name)\n        return argv"}, {"instruction": "def logpost(self, theta, t=None):\n        \"\"\"Posterior log-density at given parameter values. \n\n        Parameters\n        ----------\n        theta: dict-like\n            theta['par'] is a ndarray containing the N values for parameter par\n        t: int \n            time (if set to None, the full posterior is returned)\n\n        Returns\n        -------\n        l: float numpy.ndarray\n            the N log-likelihood values \n        \"\"\"\n", "input": "", "output": "        return self.prior.logpdf(theta) + self.loglik(theta, t)"}, {"instruction": "def post(f, *args, **kwargs):\n    \"\"\"Automatically log progress on function exit. Default logging value:\n    info.\n\n    *Logging with values contained in the parameters of the decorated function*\n    Message (args[0]) may be a string to be formatted with parameters passed to\n    the decorated function. Each '{varname}' will be replaced by the value of\n    the parameter of the same name.\n\n    *Keyword parameters*\n    - log :: integer\n      - Specifies a custom level of logging to pass to the active logger.\n      - Default: INFO\n\n    *Exceptions:*\n    - IndexError and ValueError\n      - will be returned if *args contains a string that does not correspond to\n        a parameter name of the decorated function, or if there are more '{}'s\n        than there are *args.\n\n    \"\"\"\n", "input": "", "output": "    kwargs.update({'postfix_only': True})\n    return _stump(f, *args, **kwargs)"}, {"instruction": "def post(method, hmc, uri, uri_parms, body, logon_required,\n             wait_for_completion):\n        \"\"\"Operation: Reassign Storage Adapter Port (requires DPM mode).\"\"\"\n", "input": "", "output": "        assert wait_for_completion is True  # async not supported yet\n        partition_oid = uri_parms[0]\n        partition_uri = '/api/partitions/' + partition_oid\n        hba_oid = uri_parms[1]\n        hba_uri = '/api/partitions/' + partition_oid + '/hbas/' + hba_oid\n        try:\n            hba = hmc.lookup_by_uri(hba_uri)\n        except KeyError:\n            raise InvalidResourceError(method, uri)\n        partition = hmc.lookup_by_uri(partition_uri)  # assert it exists\n        cpc = partition.manager.parent\n        assert cpc.dpm_enabled\n        check_valid_cpc_status(method, uri, cpc)\n        check_partition_status(method, uri, partition,\n                               invalid_statuses=['starting', 'stopping'])\n        check_required_fields(method, uri, body, ['adapter-port-uri'])\n\n        # Reflect the effect of the operation on the HBA\n        new_port_uri = body['adapter-port-uri']\n        hba.properties['adapter-port-uri'] = new_port_uri"}, {"instruction": "def get_field_list(fields, schema):\n  \"\"\" Convert a field list spec into a real list of field names.\n\n      For tables, we return only the top-level non-RECORD fields as Google charts\n      can't handle nested data.\n  \"\"\"\n", "input": "", "output": "  # If the fields weren't supplied get them from the schema.\n  if schema:\n    all_fields = [f['name'] for f in schema._bq_schema if f['type'] != 'RECORD']\n\n  if isinstance(fields, list):\n    if schema:\n      # validate fields exist\n      for f in fields:\n        if f not in all_fields:\n          raise Exception('Cannot find field %s in given schema' % f)\n    return fields\n  if isinstance(fields, basestring) and fields != '*':\n    if schema:\n      # validate fields exist\n      for f in fields.split(','):\n        if f not in all_fields:\n          raise Exception('Cannot find field %s in given schema' % f)\n      return fields.split(',')\n  if not schema:\n    return []\n  return all_fields"}, {"instruction": "def RIBNextHopLimitExceeded_RIBNextHopLimit(self, **kwargs):\n        \"\"\"Auto Generated Code\n        \"\"\"\n", "input": "", "output": "        config = ET.Element(\"config\")\n        RIBNextHopLimitExceeded = ET.SubElement(config, \"RIBNextHopLimitExceeded\", xmlns=\"http://brocade.com/ns/brocade-notification-stream\")\n        RIBNextHopLimit = ET.SubElement(RIBNextHopLimitExceeded, \"RIBNextHopLimit\")\n        RIBNextHopLimit.text = kwargs.pop('RIBNextHopLimit')\n\n        callback = kwargs.pop('callback', self._callback)\n        return callback(config)"}, {"instruction": "def valid_api_plugin(self, plugin):\n        \"\"\"\n        Validate an API plugin, ensuring it is an API plugin and has the\n        necessary fields present.\n\n        `plugin` is a subclass of scruffy's Plugin class.\n        \"\"\"\n", "input": "", "output": "        if (issubclass(plugin, APIPlugin)       and\n            hasattr(plugin, 'plugin_type')      and plugin.plugin_type == 'api' and\n            hasattr(plugin, 'request')          and plugin.request != None and\n            hasattr(plugin, 'request_class')    and plugin.request_class != None and\n            hasattr(plugin, 'response_class')   and plugin.response_class != None):\n            return True\n        return False"}, {"instruction": "def _sumterm_prime(lexer):\n    \"\"\"Return a sum term' expression, eliminates left recursion.\"\"\"\n", "input": "", "output": "    tok = next(lexer)\n    # '|' XORTERM SUMTERM'\n    if isinstance(tok, OP_or):\n        xorterm = _xorterm(lexer)\n        sumterm_prime = _sumterm_prime(lexer)\n        if sumterm_prime is None:\n            return xorterm\n        else:\n            return ('or', xorterm, sumterm_prime)\n    # null\n    else:\n        lexer.unpop_token(tok)\n        return None"}, {"instruction": "def cli(ctx, id_number, new_value):\n    \"\"\"Update a status name\n\nOutput:\n\n    an empty dictionary\n    \"\"\"\n", "input": "", "output": "    return ctx.gi.status.update_status(id_number, new_value)"}, {"instruction": "def setButtonText(self, which, text):\n        \"\"\"\n        Sets the display text for the inputed button to the given text.\n\n        :param      which | <XOverlayWizard.WizardButton>\n                    text  | <str>\n        \"\"\"\n", "input": "", "output": "        try:\n            self._buttons[which].setText(text)\n        except KeyError:\n            pass"}, {"instruction": "def build_indentation_list(parser: str = 'github'):\n    r\"\"\"Create a data structure that holds the state of indentations.\n\n    :parameter parser: decides the length of the list.\n         Defaults to ``github``.\n    :type parser: str\n    :returns: indentation_list, a list that contains the state of\n         indentations given a header type.\n    :rtype: list\n    :raises: a built-in exception.\n    \"\"\"\n", "input": "", "output": "    indentation_list = list()\n\n    if (parser == 'github' or parser == 'cmark' or parser == 'gitlab'\n            or parser == 'commonmarker' or parser == 'redcarpet'):\n        for i in range(0, md_parser[parser]['header']['max_levels']):\n            indentation_list.append(False)\n\n    return indentation_list"}, {"instruction": "def save_config(self, cmd=\"save\", confirm=False, confirm_response=\"\"):\n        \"\"\" Save Config for HuaweiSSH\"\"\"\n", "input": "", "output": "        return super(HuaweiBase, self).save_config(\n            cmd=cmd, confirm=confirm, confirm_response=confirm_response\n        )"}, {"instruction": "def indent(self):\n        \"\"\"\n        Indents text at cursor position.\n        \"\"\"\n", "input": "", "output": "        cursor = self.editor.textCursor()\n        assert isinstance(cursor, QtGui.QTextCursor)\n        if cursor.hasSelection():\n            self.indent_selection(cursor)\n        else:\n            # simply insert indentation at the cursor position\n            tab_len = self.editor.tab_length\n            if cursor.positionInBlock() < self.min_column and not cursor.atBlockEnd():\n                cursor.movePosition(cursor.Right, cursor.MoveAnchor, self.min_column)\n            cursor.beginEditBlock()\n            if self.editor.use_spaces_instead_of_tabs:\n                nb_space_to_add = tab_len - (cursor.positionInBlock() - self.min_column) % tab_len\n                cursor.insertText(nb_space_to_add * \" \")\n            else:\n                cursor.insertText('\\t')\n            cursor.endEditBlock()\n            self.editor.setTextCursor(cursor)"}, {"instruction": "def handle_attribute_value(self, value):\n        \"\"\"Check attribute. Especially designed for avoiding URLs in the form:\n        javascript:myXSSFunction();\"\"\"\n", "input": "", "output": "        if self.re_js.match(value) or self.re_vb.match(value):\n            return ''\n        return value"}, {"instruction": "def sha_hash(self) -> str:\n        \"\"\"\n        Return uppercase hex sha256 hash from signed raw document\n\n        :return:\n        \"\"\"\n", "input": "", "output": "        return hashlib.sha256(self.signed_raw().encode(\"ascii\")).hexdigest().upper()"}, {"instruction": "def current_state(self, *,\n                      chat: typing.Union[str, int, None] = None,\n                      user: typing.Union[str, int, None] = None) -> FSMContext:\n        \"\"\"\n        Get current state for user in chat as context\n\n        .. code-block:: python3\n\n            with dp.current_state(chat=message.chat.id, user=message.user.id) as state:\n                pass\n\n            state = dp.current_state()\n            state.set_state('my_state')\n\n        :param chat:\n        :param user:\n        :return:\n        \"\"\"\n", "input": "", "output": "        if chat is None:\n            chat_obj = types.Chat.get_current()\n            chat = chat_obj.id if chat_obj else None\n        if user is None:\n            user_obj = types.User.get_current()\n            user = user_obj.id if user_obj else None\n\n        return FSMContext(storage=self.storage, chat=chat, user=user)"}, {"instruction": "def point_eval(M, C, x):\n    \"\"\"\n    Evaluates M(x) and C(x).\n\n    Minimizes computation; evaluating M(x) and C(x) separately would\n    evaluate the off-diagonal covariance term twice, but callling\n    point_eval(M,C,x) would only evaluate it once.\n\n    Also chunks the evaluations if the off-diagonal term.\n    \"\"\"\n", "input": "", "output": "\n    x_ = regularize_array(x)\n\n    M_out = empty(x_.shape[0])\n    V_out = empty(x_.shape[0])\n\n    if isinstance(C, pymc.gp.BasisCovariance):\n        y_size = len(C.basis)\n    elif C.obs_mesh is not None:\n        y_size = C.obs_mesh.shape[0]\n    else:\n        y_size = 1\n\n    n_chunks = ceil(y_size * x_.shape[0] / float(chunksize))\n    bounds = array(linspace(0, x_.shape[0], n_chunks + 1), dtype='int')\n    cmin = bounds[:-1]\n    cmax = bounds[1:]\n    for (cmin, cmax) in zip(bounds[:-1], bounds[1:]):\n        x__ = x_[cmin:cmax]\n        V_out[cmin:cmax], Uo_Cxo = C(x__, regularize=False, return_Uo_Cxo=True)\n        M_out[cmin:cmax] = M(x__, regularize=False, Uo_Cxo=Uo_Cxo)\n\n    if len(x.shape) > 1:\n        targ_shape = x.shape[:-1]\n    else:\n        targ_shape = x.shape\n    return M_out.reshape(targ_shape), V_out.reshape(targ_shape)"}, {"instruction": "def from_bytes(self, string):\n        \"\"\"Deserialize the binder's annotations from a byte string.\"\"\"\n", "input": "", "output": "        msg = srsly.msgpack_loads(gzip.decompress(string))\n        self.attrs = msg[\"attrs\"]\n        self.strings = set(msg[\"strings\"])\n        lengths = numpy.fromstring(msg[\"lengths\"], dtype=\"int32\")\n        flat_spaces = numpy.fromstring(msg[\"spaces\"], dtype=bool)\n        flat_tokens = numpy.fromstring(msg[\"tokens\"], dtype=\"uint64\")\n        shape = (flat_tokens.size // len(self.attrs), len(self.attrs))\n        flat_tokens = flat_tokens.reshape(shape)\n        flat_spaces = flat_spaces.reshape((flat_spaces.size, 1))\n        self.tokens = NumpyOps().unflatten(flat_tokens, lengths)\n        self.spaces = NumpyOps().unflatten(flat_spaces, lengths)\n        for tokens in self.tokens:\n            assert len(tokens.shape) == 2, tokens.shape\n        return self"}, {"instruction": "def _find_aux_coord_vars(self, ds, refresh=False):\n        '''\n        Returns a list of auxiliary coordinate variables\n\n        An auxiliary coordinate variable is any netCDF variable that contains\n        coordinate data, but is not a coordinate variable (in the sense of the term\n        defined by CF).\n\n        :param netCDF4.Dataset ds: An open netCDF dataset\n        :param bool refresh: if refresh is set to True, the cache is\n                             invalidated.\n        :rtype: list\n        :return: List of variable names (str) that are defined to be auxiliary\n                 coordinate variables.\n        '''\n", "input": "", "output": "        if self._aux_coords.get(ds, None) and refresh is False:\n            return self._aux_coords[ds]\n\n        self._aux_coords[ds] = cfutil.get_auxiliary_coordinate_variables(ds)\n        return self._aux_coords[ds]"}, {"instruction": "def pattern(head, *args, mode=1, wc_name=None, conditions=None, **kwargs) \\\n        -> Pattern:\n    \"\"\"'Flat' constructor for the Pattern class\n\n    Positional and keyword arguments are mapped into `args` and `kwargs`,\n    respectively. Useful for defining rules that match an instantiated\n    Expression with specific arguments\n    \"\"\"\n", "input": "", "output": "    if len(args) == 0:\n        args = None\n    if len(kwargs) == 0:\n        kwargs = None\n    return Pattern(head, args, kwargs, mode=mode, wc_name=wc_name,\n                   conditions=conditions)"}, {"instruction": "def load(self, client, webpy_app, course_factory, task_factory, database, user_manager, submission_manager, config):\n        \"\"\" Loads the plugin manager. Must be done after the initialisation of the client \"\"\"\n", "input": "", "output": "        self._app = webpy_app\n        self._task_factory = task_factory\n        self._database = database\n        self._user_manager = user_manager\n        self._submission_manager = submission_manager\n        self._loaded = True\n        for entry in config:\n            module = importlib.import_module(entry[\"plugin_module\"])\n            module.init(self, course_factory, client, entry)"}, {"instruction": "def execute_ping(host_list, remote_user, remote_pass,\n                 sudo=False, sudo_user=None, sudo_pass=None):\n    '''\n    Execute ls on some hosts\n    '''\n", "input": "", "output": "    runner = spam.ansirunner.AnsibleRunner()\n    result, failed_hosts = runner.ansible_perform_operation(\n        host_list=host_list,\n        remote_user=remote_user,\n        remote_pass=remote_pass,\n        sudo=sudo,\n        sudo_pass=sudo_pass,\n        sudo_user=sudo_user,\n        module=\"ping\")\n\n    print result, failed_hosts\n    dark_hosts = runner.ansible_get_dark_hosts(result)\n    print \"dark hosts: \", dark_hosts"}, {"instruction": "def _save_stdin(self, stdin):\n\t\t\"\"\"\n\t\tCreates a temporary dir (self.temp_dir) and saves the given input\n\t\tstream to a file within that dir. Returns the path to the file. The dir\n\t\tis removed in the __del__ method.\n\t\t\"\"\"\n", "input": "", "output": "\t\tself.temp_dir = TemporaryDirectory()\n\t\tfile_path = os.path.join(self.temp_dir.name, 'dataset')\n\n\t\ttry:\n\t\t\twith open(file_path, 'w') as f:\n\t\t\t\tfor line in stdin:\n\t\t\t\t\tf.write(line)\n\t\texcept TypeError:\n\t\t\tself.temp_dir.cleanup()\n\t\t\traise ValueError('Could not read stdin')\n\n\t\treturn file_path"}, {"instruction": "def is_node(objecttype):\n    \"\"\"\n    Check if the given objecttype has Node as an interface\n    \"\"\"\n", "input": "", "output": "    if not isclass(objecttype):\n        return False\n\n    if not issubclass(objecttype, ObjectType):\n        return False\n\n    for i in objecttype._meta.interfaces:\n        if issubclass(i, Node):\n            return True\n\n    return False"}, {"instruction": "def set_custom_image(user_context, app_id, image_path):\n  \"\"\"Sets the custom image for `app_id` to be the image located at\n  `image_path`. If there already exists a custom image for `app_id` it will\n  be deleted. Returns True is setting the image was successful.\"\"\"\n", "input": "", "output": "  if image_path is None:\n    return False\n\n  if not os.path.exists(image_path):\n    return False\n\n  (root, ext) = os.path.splitext(image_path)\n  if not is_valid_extension(ext):\n    # TODO: Maybe log that this happened?\n    return False\n  # If we don't remove the old image then theres no guarantee that Steam will\n  # show our new image when it launches.\n  if has_custom_image(user_context, app_id):\n    img = get_custom_image(user_context, app_id)\n    assert(img is not None)\n    os.remove(img)\n  \n  # Set the new image\n  parent_dir = paths.custom_images_directory(user_context)\n  new_path = os.path.join(parent_dir, app_id + ext)\n  shutil.copyfile(image_path, new_path)\n  return True"}, {"instruction": "def update(self, *args, **kwargs):\n        '''Preserves order if given an assoc list.\n        '''\n", "input": "", "output": "        arg = dict_arg(*args, **kwargs)\n        if isinstance(arg, list):\n          for key, val in arg:\n              self[key] = val\n        else:\n            super(AssocDict, self).update(arg)"}, {"instruction": "def conv(col, fromBase, toBase):\n    \"\"\"\n    Convert a number in a string column from one base to another.\n\n    >>> df = spark.createDataFrame([(\"010101\",)], ['n'])\n    >>> df.select(conv(df.n, 2, 16).alias('hex')).collect()\n    [Row(hex=u'15')]\n    \"\"\"\n", "input": "", "output": "    sc = SparkContext._active_spark_context\n    return Column(sc._jvm.functions.conv(_to_java_column(col), fromBase, toBase))"}, {"instruction": "def get_notifier(provider_name: str, strict: bool = False) -> Provider:\n    \"\"\"\n    Convenience method to return an instantiated :class:`~notifiers.core.Provider` object according to it ``name``\n\n    :param provider_name: The ``name`` of the requested :class:`~notifiers.core.Provider`\n    :param strict: Raises a :class:`ValueError` if the given provider string was not found\n    :return: :class:`Provider` or None\n    :raises ValueError: In case ``strict`` is True and provider not found\n    \"\"\"\n", "input": "", "output": "    if provider_name in _all_providers:\n        log.debug(\"found a match for '%s', returning\", provider_name)\n        return _all_providers[provider_name]()\n    elif strict:\n        raise NoSuchNotifierError(name=provider_name)"}, {"instruction": "def analysis(self):\n        \"\"\"The list of analysis of ``words`` layer elements.\"\"\"\n", "input": "", "output": "        if not self.is_tagged(ANALYSIS):\n            self.tag_analysis()\n        return [word[ANALYSIS] for word in self.words]"}, {"instruction": "def xstep(self):\n        r\"\"\"Minimise Augmented Lagrangian with respect to\n        :math:`\\mathbf{x}`.\n        \"\"\"\n", "input": "", "output": "\n        b = self.AHSf + self.rho*np.sum(\n            np.conj(self.Gf)*sl.rfftn(self.Y-self.U, axes=self.axes),\n            axis=self.Y.ndim-1)\n        self.Xf = b / (self.AHAf + self.rho*self.GHGf)\n        self.X = sl.irfftn(self.Xf, self.axsz, axes=self.axes)\n\n        if self.opt['LinSolveCheck']:\n            ax = (self.AHAf + self.rho*self.GHGf)*self.Xf\n            self.xrrs = sl.rrs(ax, b)\n        else:\n            self.xrrs = None"}, {"instruction": "def _send(self, value, mode):\n        \"\"\"Send the specified value to the display with automatic 4bit / 8bit\n        selection. The rs_mode is either ``RS_DATA`` or ``RS_INSTRUCTION``.\"\"\"\n", "input": "", "output": "\n        # Assemble the parameters sent to the pigpio script\n        params = [mode]\n        params.extend([(value >> i) & 0x01 for i in range(8)])\n        # Switch off pigpio's exceptions, so that we get the return codes\n        pigpio.exceptions = False\n        while True:\n            ret = self.pi.run_script(self._writescript, params)\n            if ret >= 0:\n                break\n            elif ret != pigpio.PI_SCRIPT_NOT_READY:\n                raise pigpio.error(pigpio.error_text(ret))\n            # If pigpio script is not ready, sleep and try again\n            c.usleep(1)\n        # Switch on pigpio's exceptions\n        pigpio.exceptions = True"}, {"instruction": "def write_sheet(writer, name, df, index=False):\n    \"\"\"Write a pandas DataFrame to an ExcelWriter,\n    auto-formatting column width depending on maxwidth of data and colum header\n\n    Parameters\n    ----------\n    writer: pandas.ExcelWriter\n        an instance of a pandas ExcelWriter\n    name: string\n        name of the sheet to be written\n    df: pandas.DataFrame\n        a pandas DataFrame to be written to the sheet\n    index: boolean, default False\n        flag whether index should be written to the sheet\n    \"\"\"\n", "input": "", "output": "    if index:\n        df = df.reset_index()\n    df.to_excel(writer, name, index=False)\n    worksheet = writer.sheets[name]\n    for i, col in enumerate(df.columns):\n        if df.dtypes[col].name.startswith(('float', 'int')):\n            width = len(str(col)) + 2\n        else:\n            width = max([df[col].map(lambda x: len(str(x or 'None'))).max(),\n                         len(col)]) + 2\n        xls_col = '{c}:{c}'.format(c=NUMERIC_TO_STR[i])\n        worksheet.set_column(xls_col, width)"}, {"instruction": "def delete_webhook(self, id, **kwargs):  # noqa: E501\n        \"\"\"Delete a specific webhook  # noqa: E501\n\n          # noqa: E501\n        This method makes a synchronous HTTP request by default. To make an\n        asynchronous HTTP request, please pass async_req=True\n        >>> thread = api.delete_webhook(id, async_req=True)\n        >>> result = thread.get()\n\n        :param async_req bool\n        :param str id: (required)\n        :return: ResponseContainerNotificant\n                 If the method is called asynchronously,\n                 returns the request thread.\n        \"\"\"\n", "input": "", "output": "        kwargs['_return_http_data_only'] = True\n        if kwargs.get('async_req'):\n            return self.delete_webhook_with_http_info(id, **kwargs)  # noqa: E501\n        else:\n            (data) = self.delete_webhook_with_http_info(id, **kwargs)  # noqa: E501\n            return data"}, {"instruction": "def _cb_inform_interface_change(self, msg):\n        \"\"\"Update the sensors and requests available.\"\"\"\n", "input": "", "output": "        self._logger.debug('cb_inform_interface_change(%s)', msg)\n        self._interface_changed.set()"}, {"instruction": "def delete_build_configuration(id=None, name=None):\n    \"\"\"\n    Delete an existing BuildConfiguration\n    :param id:\n    :param name:\n    :return:\n    \"\"\"\n", "input": "", "output": "    data = delete_build_configuration_raw(id, name)\n    if data:\n        return utils.format_json(data)"}, {"instruction": "def getBool(t):\n    \"\"\"If t is of type bool, return it, otherwise raise InvalidTypeError.\n    \"\"\"\n", "input": "", "output": "    b = c_int()\n    if PL_get_long(t, byref(b)):\n        return bool(b.value)\n    else:\n        raise InvalidTypeError(\"bool\")"}, {"instruction": "def create_and_register_access_db(filename: str,\n                                  dsn: str,\n                                  description: str) -> bool:\n    \"\"\"\n    (Windows only.)\n    Creates a Microsoft Access database and registers it with ODBC.\n\n    Args:\n        filename: filename of the database to create\n        dsn: ODBC data source name to create\n        description: description of the database\n\n    Returns:\n        bool: was the DSN created?\n    \"\"\"\n", "input": "", "output": "    fullfilename = os.path.abspath(filename)\n    create_string = fullfilename + \" General\"\n    # ... filename, space, sort order (\"General\" for English)\n    return (create_user_dsn(access_driver, CREATE_DB=create_string) and\n            register_access_db(filename, dsn, description))"}, {"instruction": "def minibatch(items, size=8):\n    \"\"\"Iterate over batches of items. `size` may be an iterator,\n    so that batch-size can vary on each step.\n    \"\"\"\n", "input": "", "output": "    if isinstance(size, int):\n        size_ = itertools.repeat(size)\n    else:\n        size_ = size\n    items = iter(items)\n    while True:\n        batch_size = next(size_)\n        batch = list(itertools.islice(items, int(batch_size)))\n        if len(batch) == 0:\n            break\n        yield list(batch)"}, {"instruction": "def expand_paths(path):\n    \"\"\"When given a path with brackets, expands it to return all permutations\n       of the path with expanded brackets, similar to ant.\n\n       >>> expand_paths('../{a,b}/{c,d}')\n       ['../a/c', '../a/d', '../b/c', '../b/d']\n       >>> expand_paths('../{a,b}/{a,b}.py')\n       ['../a/a.py', '../a/b.py', '../b/a.py', '../b/b.py']\n       >>> expand_paths('../{a,b,c}/{a,b,c}')\n       ['../a/a', '../a/b', '../a/c', '../b/a', '../b/b', '../b/c', '../c/a', '../c/b', '../c/c']\n       >>> expand_paths('test')\n       ['test']\n       >>> expand_paths('')\n    \"\"\"\n", "input": "", "output": "    pr = itertools.product\n    parts = MAGIC_BRACKETS.findall(path)\n\n    if not path:\n        return\n\n    if not parts:\n        return [path]\n\n    permutations = [[(p[0], i, 1) for i in p[1].split(',')] for p in parts]\n    return [_replace_all(path, i) for i in pr(*permutations)]"}, {"instruction": "def _cz_gate(self, lines):\n        \"\"\"\n        Return the TikZ code for an n-controlled Z-gate.\n\n        :param lines: List of all qubits involved.\n        :type: list[int]\n        \"\"\"\n", "input": "", "output": "        line = lines[0]\n        delta_pos = self._gate_offset(Z)\n        gate_width = self._gate_width(Z)\n        gate_str = self._phase(line, self.pos[line])\n\n        for ctrl in lines[1:]:\n            gate_str += self._phase(ctrl, self.pos[line])\n            gate_str += self._line(ctrl, line)\n\n        new_pos = self.pos[line] + delta_pos + gate_width\n        for i in lines:\n            self.op_count[i] += 1\n        for i in range(min(lines), max(lines) + 1):\n            self.pos[i] = new_pos\n        return gate_str"}, {"instruction": "def send(\n        self, record, message, resource=None, labels=None, trace=None, span_id=None\n    ):\n        \"\"\"Overrides transport.send().\n\n        :type record: :class:`logging.LogRecord`\n        :param record: Python log record that the handler was called with.\n\n        :type message: str\n        :param message: The message from the ``LogRecord`` after being\n                        formatted by the associated log formatters.\n\n        :type resource: :class:`~google.cloud.logging.resource.Resource`\n        :param resource: (Optional) Monitored resource of the entry.\n\n        :type labels: dict\n        :param labels: (Optional) Mapping of labels for the entry.\n        \"\"\"\n", "input": "", "output": "        info = {\"message\": message, \"python_logger\": record.name}\n        self.logger.log_struct(\n            info,\n            severity=record.levelname,\n            resource=resource,\n            labels=labels,\n            trace=trace,\n            span_id=span_id,\n        )"}, {"instruction": "def calculate_size(name, sequence):\n    \"\"\" Calculates the request payload size\"\"\"\n", "input": "", "output": "    data_size = 0\n    data_size += calculate_size_str(name)\n    data_size += LONG_SIZE_IN_BYTES\n    return data_size"}, {"instruction": "def info_progress(prefix: str, value: float, max_value: float) -> None:\n    \"\"\" Display info progress in percent.\n\n    :param value: the current value\n    :param max_value: the max value\n    :param prefix: the prefix message to print\n\n\n    \"\"\"\n", "input": "", "output": "    if sys.stdout.isatty():\n        percent = float(value) / max_value * 100\n        sys.stdout.write(prefix + \": %.0f%%\\r\" % percent)\n        sys.stdout.flush()"}, {"instruction": "def cloudant_iam(account_name, api_key, **kwargs):\n    \"\"\"\n    Provides a context manager to create a Cloudant session using IAM\n    authentication and provide access to databases, docs etc.\n\n    :param account_name: Cloudant account name.\n    :param api_key: IAM authentication API key.\n\n    For example:\n\n    .. code-block:: python\n\n        # cloudant context manager\n        from cloudant import cloudant_iam\n\n        with cloudant_iam(ACCOUNT_NAME, API_KEY) as client:\n            # Context handles connect() and disconnect() for you.\n            # Perform library operations within this context.  Such as:\n            print client.all_dbs()\n            # ...\n\n    \"\"\"\n", "input": "", "output": "    cloudant_session = Cloudant.iam(account_name, api_key, **kwargs)\n\n    cloudant_session.connect()\n    yield cloudant_session\n    cloudant_session.disconnect()"}, {"instruction": "def reftrack_version_data(rt, role):\n    \"\"\"Return the data for the version that is loaded by the reftrack\n\n    :param rt: the :class:`jukeboxcore.reftrack.Reftrack` holds the data\n    :type rt: :class:`jukeboxcore.reftrack.Reftrack`\n    :param role: item data role\n    :type role: QtCore.Qt.ItemDataRole\n    :returns: data for the version\n    :rtype: depending on role\n    :raises: None\n    \"\"\"\n", "input": "", "output": "    tfi = rt.get_taskfileinfo()\n    if not tfi:\n        return\n    return filesysitemdata.taskfileinfo_version_data(tfi, role)"}, {"instruction": "def parse_structure(self, node):\n        \"\"\"\n        Parses <Structure>\n\n        @param node: Node containing the <Structure> element\n        @type node: xml.etree.Element\n        \"\"\"\n", "input": "", "output": "\n        self.current_structure = self.current_component_type.structure\n        self.process_nested_tags(node)\n        self.current_structure = None"}, {"instruction": "def cached(func):\n    \"\"\"Memoize a function result.\"\"\"\n", "input": "", "output": "    ret = None\n\n    def call_or_cache(*args, **kwargs):\n        nonlocal ret\n        if ret is None:\n            ret = func(*args, **kwargs)\n        return ret\n\n    return call_or_cache"}, {"instruction": "def stop(self):  # pylint: disable=no-self-use\n        \"\"\"Wrapper to stop the CherryPy server\n\n        :return: None\n        \"\"\"\n", "input": "", "output": "        cherrypy.log(\"Stopping CherryPy engine (current state: %s)...\" % cherrypy.engine.state)\n        try:\n            cherrypy.engine.exit()\n        except RuntimeWarning:\n            pass\n        except SystemExit:\n            cherrypy.log('SystemExit raised: shutting down bus')\n        cherrypy.log(\"Stopped\")"}, {"instruction": "def license(self, license_id: str, token: dict = None, prot: str = \"https\") -> dict:\n        \"\"\"Get details about a specific license.\n\n        :param str token: API auth token\n        :param str license_id: license UUID\n        :param str prot: https [DEFAULT] or http\n         (use it only for dev and tracking needs).\n        \"\"\"\n", "input": "", "output": "        # handling request parameters\n        payload = {\"lid\": license_id}\n\n        # search request\n        license_url = \"{}://v1.{}.isogeo.com/licenses/{}\".format(\n            prot, self.api_url, license_id\n        )\n        license_req = self.get(\n            license_url,\n            headers=self.header,\n            params=payload,\n            proxies=self.proxies,\n            verify=self.ssl,\n        )\n\n        # checking response\n        checker.check_api_response(license_req)\n\n        # end of method\n        return license_req.json()"}, {"instruction": "def key(self):\n        \"\"\"\n        Example::\n\n            /browse/homes/ca/ -> ca\n            /browse/homes/ca/los-angeles-county/ -> los-angeles-county\n            /browse/homes/ca/los-angeles-county/91001/ -> 91001\n            /browse/homes/ca/los-angeles-county/91001/tola-ave_5038895/ -> tola-ave_5038895\n\n        :return:\n        \"\"\"\n", "input": "", "output": "\n        return [part.strip() for part in self.href.split(\"/\") if part.strip()][\n            -1]"}, {"instruction": "def put(self):\n        \"\"\"Update a credential by file path\"\"\"\n", "input": "", "output": "        cred_payload = utils.uni_to_str(json.loads(request.get_data()))\n        return self.manager.update_credential(cred_payload)"}, {"instruction": "def deserialize_by_field(value, field):\n    \"\"\"\n    Some types get serialized to JSON, as strings.\n    If we know what they are supposed to be, we can deserialize them\n    \"\"\"\n", "input": "", "output": "    if isinstance(field, forms.DateTimeField):\n        value = parse_datetime(value)\n    elif isinstance(field, forms.DateField):\n        value = parse_date(value)\n    elif isinstance(field, forms.TimeField):\n        value = parse_time(value)\n    return value"}, {"instruction": "def _route(self):\n        ''' Handles server route instantiation. '''\n", "input": "", "output": "        self._app.route('/',\n                        method='GET',\n                        callback=self._get_logger_list)\n        self._app.route('/stats',\n                        method='GET',\n                        callback=self._fetch_handler_stats)\n        self._app.route('/<name>/start',\n                        method='POST',\n                        callback=self._add_logger_by_name)\n        self._app.route('/<name>/stop',\n                        method='DELETE',\n                        callback=self._stop_logger_by_name)\n        self._app.route('/<name>/config',\n                        method='GET',\n                        callback=self._get_logger_conf)\n        self._app.route('/<name>/rotate',\n                        method='POST',\n                        callback=self._rotate_capturer_log)"}, {"instruction": "def update_datastore(self, schema=None, primary_key=None,\n                         path=None):\n        # type: (Optional[List[Dict]], Optional[str], Optional[str]) -> None\n        \"\"\"For tabular data, update a resource in the HDX datastore which enables data preview in HDX. If no schema is provided\n        all fields are assumed to be text. If path is not supplied, the file is first downloaded from HDX.\n\n        Args:\n            schema (List[Dict]): List of fields and types of form {'id': 'FIELD', 'type': 'TYPE'}. Defaults to None.\n            primary_key (Optional[str]): Primary key of schema. Defaults to None.\n            path (Optional[str]): Local path to file that was uploaded. Defaults to None.\n\n        Returns:\n            None\n        \"\"\"\n", "input": "", "output": "        self.create_datastore(schema, primary_key, 2, path=path)"}, {"instruction": "def raw_corpus_rouge2(hypotheses: Iterable[str], references: Iterable[str]) -> float:\n    \"\"\"\n    Simple wrapper around ROUGE-2 implementation.\n\n    :param hypotheses: Hypotheses stream.\n    :param references: Reference stream.\n    :return: ROUGE-2 score as float between 0 and 1.\n    \"\"\"\n", "input": "", "output": "    return rouge.rouge_2(hypotheses, references)"}, {"instruction": "def ind_nodes(self, graph=None):\n        \"\"\" Returns a list of all nodes in the graph with no dependencies. \"\"\"\n", "input": "", "output": "        if graph is None:\n            graph = self.graph\n\n        dependent_nodes = set(\n            node for dependents in six.itervalues(graph) for node in dependents\n        )\n        return [node for node in graph.keys() if node not in dependent_nodes]"}, {"instruction": "def from_proto(cls, proto_mesh, scale):\n        \"\"\"\n        TODO: add documentation\n        \"\"\"\n", "input": "", "output": "\n        mesh = cls(**proto_mesh.items())\n        mesh._copy_roche_values()\n        mesh._scale_mesh(scale=scale)\n\n        return mesh"}, {"instruction": "def to_json(self, value):\n        \"\"\"\n        Serialize the data, ensuring that it is valid XML (or None).\n\n        Raises an lxml.etree.XMLSyntaxError if it is a basestring but not valid\n        XML.\n        \"\"\"\n", "input": "", "output": "        if self._enable_enforce_type:\n            value = self.enforce_type(value)\n        return super(XMLString, self).to_json(value)"}, {"instruction": "def _dedent(text, tabsize=8, skip_first_line=False):\n    \"\"\"_dedent(text, tabsize=8, skip_first_line=False) -> dedented text\n\n        \"text\" is the text to dedent.\n        \"tabsize\" is the tab width to use for indent width calculations.\n        \"skip_first_line\" is a boolean indicating if the first line should\n            be skipped for calculating the indent width and for dedenting.\n            This is sometimes useful for docstrings and similar.\n\n    textwrap.dedent(s), but don't expand tabs to spaces\n    \"\"\"\n", "input": "", "output": "    lines = text.splitlines(1)\n    _dedentlines(lines, tabsize=tabsize, skip_first_line=skip_first_line)\n    return ''.join(lines)"}, {"instruction": "def save_json(histogram: Union[HistogramBase, HistogramCollection], path: Optional[str] = None, **kwargs) -> str:\n    \"\"\"Save histogram to JSON format.\n\n    Parameters\n    ----------\n    histogram : Any histogram\n    path : If set, also writes to the path.\n\n    Returns\n    -------\n    json : The JSON representation of the histogram\n    \"\"\"\n", "input": "", "output": "    # TODO: Implement multiple histograms in one file?\n    data = histogram.to_dict()\n\n    data[\"physt_version\"] = CURRENT_VERSION\n    if isinstance(histogram, HistogramBase):\n        data[\"physt_compatible\"] = COMPATIBLE_VERSION\n    elif isinstance(histogram, HistogramCollection):\n        data[\"physt_compatible\"] = COLLECTION_COMPATIBLE_VERSION\n    else:\n        raise TypeError(\"Cannot save unknown type: {0}\".format(type(histogram)))\n\n    text = json.dumps(data, **kwargs)\n    if path:\n        with open(path, \"w\", encoding=\"utf-8\") as f:\n            f.write(text)\n    return text"}, {"instruction": "def _error_on_missing_application(self, params):\n        \"\"\"Raise an ApplicationNotFoundError if the app is not accessible\n\n        In this case, checks for the java runtime and the RDP jar file.\n        \"\"\"\n", "input": "", "output": "        if not (os.path.exists('java') or which('java')):\n            raise ApplicationNotFoundError(\n                \"Cannot find java runtime. Is it installed? Is it in your \"\n                \"path?\")\n        jar_fp = self._get_jar_fp()\n        if jar_fp is None:\n            raise ApplicationNotFoundError(\n                \"JAR file not found in current directory and the RDP_JAR_PATH \"\n                \"environment variable is not set.  Please set RDP_JAR_PATH to \"\n                \"the full pathname of the JAR file.\")\n        if not os.path.exists(jar_fp):\n            raise ApplicationNotFoundError(\n                \"JAR file %s does not exist.\" % jar_fp)"}, {"instruction": "def _parse_value(self, stats, field):\n        \"\"\"\n        Pull the specified value from the HTML contents.\n\n        Given a field, find the corresponding HTML tag for that field and parse\n        its value before returning the value as a string. A couple fields, such\n        as 'conference' and 'team_abbreviation' don't follow a standard parsing\n        scheme and need to be handled differently to get the correct value.\n\n        Parameters\n        ----------\n        stats : PyQuery object\n            A PyQuery object containing all stats in HTML format for a\n            particular player.\n        field : string\n            A string of the field to parse from the HTML.\n\n        Returns\n        -------\n        string\n            Returns the desired value as a string.\n        \"\"\"\n", "input": "", "output": "        if field == 'conference':\n            value = self._parse_conference(stats)\n        elif field == 'team_abbreviation':\n            value = self._parse_team_abbreviation(stats)\n        else:\n            value = utils._parse_field(PLAYER_SCHEME, stats, field)\n        return value"}, {"instruction": "def is_active(ext, metadata):\n    \"\"\"Is the cell active for the given file extension?\"\"\"\n", "input": "", "output": "    if metadata.get('run_control', {}).get('frozen') is True:\n        return False\n    if 'active' not in metadata:\n        return True\n    return ext.replace('.', '') in re.split('\\\\.|,', metadata['active'])"}, {"instruction": "def input_validate_nonce(nonce, name='nonce', pad = False):\n    \"\"\" Input validation for nonces. \"\"\"\n", "input": "", "output": "    if type(nonce) is not str:\n        raise pyhsm.exception.YHSM_WrongInputType( \\\n            name, str, type(nonce))\n    if len(nonce) > pyhsm.defines.YSM_AEAD_NONCE_SIZE:\n        raise pyhsm.exception.YHSM_InputTooLong(\n            name, pyhsm.defines.YSM_AEAD_NONCE_SIZE, len(nonce))\n    if pad:\n        return nonce.ljust(pyhsm.defines.YSM_AEAD_NONCE_SIZE, chr(0x0))\n    else:\n        return nonce"}, {"instruction": "def active():\n    '''\n    List existing device-mapper device details.\n    '''\n", "input": "", "output": "    ret = {}\n    # TODO: This command should be extended to collect more information, such as UUID.\n    devices = __salt__['cmd.run_stdout']('dmsetup ls --target crypt')\n    out_regex = re.compile(r'(?P<devname>\\w+)\\W+\\((?P<major>\\d+), (?P<minor>\\d+)\\)')\n\n    log.debug(devices)\n    for line in devices.split('\\n'):\n        match = out_regex.match(line)\n        if match:\n            dev_info = match.groupdict()\n            ret[dev_info['devname']] = dev_info\n        else:\n            log.warning('dmsetup output does not match expected format')\n\n    return ret"}, {"instruction": "def print_data(data):\n    \"\"\"Prints object key-value pairs in a custom format\n\n    :param data: The dict to print\n    :type data: dict\n    :rtype: None\n    \"\"\"\n", "input": "", "output": "    print(\", \".join([\"{}=>{}\".format(key, value) for key, value in data]))"}, {"instruction": "def rbac_policy_update(request, policy_id, **kwargs):\n    \"\"\"Update a RBAC Policy.\n\n    :param request: request context\n    :param policy_id: target policy id\n    :param target_tenant: target tenant of the policy\n    :return: RBACPolicy object\n    \"\"\"\n", "input": "", "output": "    body = {'rbac_policy': kwargs}\n    rbac_policy = neutronclient(request).update_rbac_policy(\n        policy_id, body=body).get('rbac_policy')\n    return RBACPolicy(rbac_policy)"}, {"instruction": "def install_handler(self, app):\n        \"\"\"Install logging handler.\"\"\"\n", "input": "", "output": "        # Configure python logging\n        if app.config['LOGGING_CONSOLE_PYWARNINGS']:\n            self.capture_pywarnings(logging.StreamHandler())\n\n        if app.config['LOGGING_CONSOLE_LEVEL'] is not None:\n            for h in app.logger.handlers:\n                h.setLevel(app.config['LOGGING_CONSOLE_LEVEL'])\n\n        # Add request_id to log record\n        app.logger.addFilter(add_request_id_filter)"}, {"instruction": "def xclaim(self, stream, group_name, consumer_name, min_idle_time,\n               id, *ids):\n        \"\"\"Claim a message for a given consumer\"\"\"\n", "input": "", "output": "        fut = self.execute(\n            b'XCLAIM', stream, group_name, consumer_name, min_idle_time,\n            id, *ids\n        )\n        return wait_convert(fut, parse_messages)"}, {"instruction": "def update(self, friendly_name=values.unset,\n               default_deployment_sid=values.unset):\n        \"\"\"\n        Update the FleetInstance\n\n        :param unicode friendly_name: A human readable description for this Fleet.\n        :param unicode default_deployment_sid: A default Deployment SID.\n\n        :returns: Updated FleetInstance\n        :rtype: twilio.rest.preview.deployed_devices.fleet.FleetInstance\n        \"\"\"\n", "input": "", "output": "        return self._proxy.update(\n            friendly_name=friendly_name,\n            default_deployment_sid=default_deployment_sid,\n        )"}, {"instruction": "def simple_layer_stack(include_encdec_attention,\n                       num_layers=6,\n                       d_ff=2048,\n                       num_heads=8,\n                       d_kv=128,\n                       dropout_rate=0.1):\n  \"\"\"Create a layer stack.\n\n  Args:\n    include_encdec_attention: a boolean\n    num_layers: an integer\n    d_ff: an integer\n    num_heads: an integer\n    d_kv: an integer\n    dropout_rate: a float\n\n  Returns:\n    a LayerStack\n  \"\"\"\n", "input": "", "output": "  ret = []\n  for _ in xrange(num_layers):\n    ret.append(\n        transformer_layers.SelfAttention(\n            num_heads=num_heads,\n            key_value_size=d_kv,\n            attention_kwargs={\"dropout_rate\": dropout_rate}))\n    if include_encdec_attention:\n      ret.append(\n          transformer_layers.EncDecAttention(\n              num_heads=num_heads,\n              key_value_size=d_kv,\n              attention_kwargs={\"dropout_rate\": dropout_rate}))\n    ret.append(\n        transformer_layers.DenseReluDense(\n            hidden_size=d_ff,\n            dropout_rate=dropout_rate))\n  return transformer.LayerStack(ret)"}, {"instruction": "def update_node_count(self, node, add_to_count):\n        \"\"\"\\\n        stores how many decent nodes are under a parent node\n        \"\"\"\n", "input": "", "output": "        current_score = 0\n        count_string = self.parser.getAttribute(node, 'gravityNodes')\n        if count_string:\n            current_score = int(count_string)\n\n        new_score = current_score + add_to_count\n        self.parser.setAttribute(node, \"gravityNodes\", str(new_score))"}, {"instruction": "def get_filebase(path, pattern):\n    \"\"\"Get the end of *path* of same length as *pattern*.\"\"\"\n", "input": "", "output": "    # A pattern can include directories\n    tail_len = len(pattern.split(os.path.sep))\n    return os.path.join(*str(path).split(os.path.sep)[-tail_len:])"}, {"instruction": "def connection_required(func):\n        \"\"\"Decorator to specify that a target connection is required in order\n        for the given method to be used.\n\n        Args:\n          func (function): function being decorated\n\n        Returns:\n          The wrapper function.\n        \"\"\"\n", "input": "", "output": "        @functools.wraps(func)\n        def wrapper(self, *args, **kwargs):\n            "}, {"instruction": "def handle(self):\n        \"\"\"\n        Executes the actual Stratum program.\n        \"\"\"\n", "input": "", "output": "        self.output = PyStratumStyle(self.input, self.output)\n\n        command = self.get_application().find('constants')\n        ret = command.execute(self.input, self.output)\n        if ret:\n            return ret\n\n        command = self.get_application().find('loader')\n        ret = command.execute(self.input, self.output)\n        if ret:\n            return ret\n\n        command = self.get_application().find('wrapper')\n        ret = command.execute(self.input, self.output)\n\n        self.output.writeln('')\n\n        return ret"}, {"instruction": "def getoptS(X, Y, M_E, E):\r\n    ''' Find Sopt given X, Y\r\n    '''\n", "input": "", "output": "    n, r = X.shape\r\n    C = np.dot(np.dot(X.T, M_E), Y)\r\n    C = C.flatten()\r\n\r\n    A = np.zeros((r * r, r * r))\r\n    for i in range(r):\r\n        for j in range(r):\r\n            ind = j * r + i\r\n            temp = np.dot(\r\n                np.dot(X.T, np.dot(X[:, i, None], Y[:, j, None].T) * E), Y)\r\n            A[:, ind] = temp.flatten()\r\n\r\n    S = np.linalg.solve(A, C)\r\n\r\n    return np.reshape(S, (r, r)).T"}, {"instruction": "def split_input(args):\n    \"\"\"Split query input into local files and URLs.\"\"\"\n", "input": "", "output": "    args['files'] = []\n    args['urls'] = []\n    for arg in args['query']:\n        if os.path.isfile(arg):\n            args['files'].append(arg)\n        else:\n            args['urls'].append(arg.strip('/'))"}, {"instruction": "def add_global(self, globalvalue):\n        \"\"\"\n        Add a new global value.\n        \"\"\"\n", "input": "", "output": "        assert globalvalue.name not in self.globals\n        self.globals[globalvalue.name] = globalvalue"}, {"instruction": "def get_changeform_initial_data(self, request):\n        \"\"\"\n        Provide initial datas when creating an entry.\n        \"\"\"\n", "input": "", "output": "        get_data = super(EntryAdmin, self).get_changeform_initial_data(request)\n        return get_data or {\n            'sites': [Site.objects.get_current().pk],\n            'authors': [request.user.pk]\n        }"}, {"instruction": "def _entry_must_exist(df, k1, k2):\n    \"\"\"Evaluate key-subkey existence.\n\n    Checks that the key-subkey combo exists in the\n    configuration options.\n    \"\"\"\n", "input": "", "output": "    count = df[(df['k1'] == k1) &\n               (df['k2'] == k2)].shape[0]\n    if count == 0:\n        raise NotRegisteredError(\n            \"Option {0}.{1} not registered\".format(k1, k2))"}, {"instruction": "def check_images(data):\n    \"\"\"\n    Check and reformat input images if needed\n    \"\"\"\n", "input": "", "output": "    if isinstance(data, ndarray):\n        data = fromarray(data)\n    \n    if not isinstance(data, Images):\n        data = fromarray(asarray(data))\n\n    if len(data.shape) not in set([3, 4]):\n        raise Exception('Number of image dimensions %s must be 2 or 3' % (len(data.shape)))\n\n    return data"}, {"instruction": "def pbkdf2(seed: str or bytes, dk_len: int) -> bytes:\n    \"\"\"\n    Derive one key from a seed.\n\n    :param seed: the secret pass phrase to generate the keys from.\n    :param dk_len: the length in bytes of every derived key.\n    :return:\n    \"\"\"\n", "input": "", "output": "    key = b''\n    index = 1\n    bytes_seed = str_to_bytes(seed)\n    while len(key) < dk_len:\n        key += Digest.sha256(b''.join([bytes_seed, index.to_bytes(4, 'big', signed=True)]))\n        index += 1\n    return key[:dk_len]"}, {"instruction": "def en_last(self):\n        \"\"\" Report the energies from the last SCF present in the output.\n\n        Returns a |dict| providing the various energy values from the\n        last SCF cycle performed in the output. Keys are those of\n        :attr:`~opan.output.OrcaOutput.p_en`.\n        Any energy value not relevant to the parsed\n        output is assigned as |None|.\n\n        Returns\n        -------\n        last_ens\n            |dict| of |npfloat_|--\n            Energies from the last SCF present in the output.\n\n        \"\"\"\n", "input": "", "output": "\n        # Initialize the return dict\n        last_ens = dict()\n\n        # Iterate and store\n        for (k,l) in self.en.items():\n            last_ens.update({ k : l[-1] if l != [] else None })\n        ##next (k,l)\n\n        # Should be ready to return?\n        return last_ens"}, {"instruction": "def from_stored(self, key):\n        \"\"\"\n        Set the current collection as based on a stored one. The key argument\n        is the key off the stored collection.\n        \"\"\"\n", "input": "", "output": "        # only one stored key allowed\n        if self.stored_key:\n            raise ValueError('This collection is already based on a stored one')\n\n        # prepare the collection\n        self.stored_key = key\n        self.intersect(_StoredCollection(self.cls.get_connection(), key))\n        self.sort(by='nosort')  # keep stored order\n\n        # count the number of results to manage empty result (to not behave like\n        # expired key)\n        self._stored_len = self.cls.get_connection().llen(key)\n\n        return self"}, {"instruction": "def new_registry(attribute=None):\n    \"\"\"\n    Returns an empty dict and a @register decorator.\n    \"\"\"\n", "input": "", "output": "    registry = {}\n\n    def register(key):\n        def decorator(func):\n            registry[key] = func\n            if attribute:\n                setattr(func, attribute, key)\n            return func\n        return decorator\n\n    return registry, register"}, {"instruction": "def flush_stream_threads(process, out_formatter=None,\n                                  err_formatter=terminal.fg.red, size=1):\n    \"\"\"\n    Context manager that creates 2 threads, one for each standard\n    stream (stdout/stderr), updating in realtime the piped data.\n    The formatters are callables that receives manipulates the data,\n    e.g. coloring it before writing to a ``sys`` stream. See\n    ``FlushStreamThread`` for more information.\n    \"\"\"\n", "input": "", "output": "    out = FlushStreamThread(process=process, stream_name=\"stdout\",\n                            formatter=out_formatter, size=size)\n    err = FlushStreamThread(process=process, stream_name=\"stderr\",\n                            formatter=err_formatter, size=size)\n    out.start()\n    err.start()\n    yield out, err\n    out.join()\n    err.join()"}, {"instruction": "def get_html_output(self):\n        \"\"\" Return line generator. \"\"\"\n", "input": "", "output": "        def html_splitlines(lines):\n            # this cool function was taken from trac.\n            # http://projects.edgewall.com/trac/\n            open_tag_re = re.compile(r'<(\\w+)(\\s.*)?[^/]?>')\n            close_tag_re = re.compile(r'</(\\w+)>')\n            open_tags = []\n            for line in lines:\n                for tag in open_tags:\n                    line = tag.group(0) + line\n                open_tags = []\n                for tag in open_tag_re.finditer(line):\n                    open_tags.append(tag)\n                open_tags.reverse()\n                for ctag in close_tag_re.finditer(line):\n                    for otag in open_tags:\n                        if otag.group(1) == ctag.group(1):\n                            open_tags.remove(otag)\n                            break\n                for tag in open_tags:\n                    line += '</%s>' % tag.group(1)\n                yield line\n\n        if self.error:\n            return escape(self.raw).splitlines()\n        return list(html_splitlines(self.out.getvalue().splitlines()))"}, {"instruction": "def disable_selinux():\n    \"\"\" disables selinux \"\"\"\n", "input": "", "output": "\n    if contains(filename='/etc/selinux/config',\n                text='SELINUX=enforcing'):\n        sed('/etc/selinux/config',\n            'SELINUX=enforcing', 'SELINUX=disabled', use_sudo=True)\n\n    if contains(filename='/etc/selinux/config',\n                text='SELINUX=permissive'):\n        sed('/etc/selinux/config',\n            'SELINUX=permissive', 'SELINUX=disabled', use_sudo=True)\n\n    if sudo('getenforce').lower() != 'disabled':\n        with settings(warn_only=True, capture=True):\n            sudo('/sbin/reboot')\n        sleep_for_one_minute()"}, {"instruction": "def hashes(self):\n        \"\"\"Hashes of all possible permutations of the URL in canonical form\"\"\"\n", "input": "", "output": "        for url_variant in self.url_permutations(self.canonical):\n            url_hash = self.digest(url_variant)\n            yield url_hash"}, {"instruction": "def validate(self, instance, value):\n        \"\"\"Checks if value is an open PNG file, valid filename, or png.Image\n\n        Returns an open bytestream of the image\n        \"\"\"\n", "input": "", "output": "        # Pass if already validated\n        if getattr(value, '__valid__', False):\n            return value\n        # Validate that value is PNG\n        if isinstance(value, png.Image):\n            pass\n        else:\n            value = super(ImagePNG, self).validate(instance, value)\n            try:\n                png.Reader(value).validate_signature()\n            except png.FormatError:\n                self.error(instance, value, extra='Open file is not PNG.')\n            value.seek(0)\n        # Write input to new bytestream\n        output = BytesIO()\n        output.name = self.filename\n        output.__valid__ = True\n        if isinstance(value, png.Image):\n            value.save(output)\n        else:\n            fid = value\n            fid.seek(0)\n            output.write(fid.read())\n            fid.close()\n        output.seek(0)\n        return output"}, {"instruction": "def uni_char_code(a: str, b: str, c: str, d: str):\n    \"\"\"Convert unicode characters to integers.\n\n    Converts four hexadecimal chars to the integer that the string represents.\n    For example, uni_char_code('0','0','0','f') will return 15,\n    and uni_char_code('0','0','f','f') returns 255.\n\n    Returns a negative number on error, if a char was invalid.\n\n    This is implemented by noting that char2hex() returns -1 on error,\n    which means the result of ORing the char2hex() will also be negative.\n    \"\"\"\n", "input": "", "output": "    return char2hex(a) << 12 | char2hex(b) << 8 | char2hex(c) << 4 | char2hex(d)"}, {"instruction": "def perform_command(self):\n        \"\"\"\n        Perform command and return the appropriate exit code.\n\n        :rtype: int\n        \"\"\"\n", "input": "", "output": "        self.log(u\"This function should be overloaded in derived classes\")\n        self.log([u\"Invoked with %s\", self.actual_arguments])\n        return self.NO_ERROR_EXIT_CODE"}, {"instruction": "def set_bpp(self, bpp):\n        \"\"\"\n        Set the bit depth (per band) for the output.\n        A typical \"32-bit RGBA\" image would be 8; a 48-bit image would\n        be 16, etc.\n        \"\"\"\n", "input": "", "output": "        self.bpp = bpp\n        self.maxc = 2 ** self.bpp - 1\n        self._set_dtype()\n        self.calc_cmap()\n        self.recalc(callback=False)"}, {"instruction": "def can_be_(self, state):\n    \"\"\"Check if machine can transit to given state.\"\"\"\n", "input": "", "output": "    translator = self._meta['translator']\n    state = translator.translate(state)\n\n    if self._meta['complete']:\n        return True\n\n    if self.actual_state is None:\n        return True\n\n    transitions = self._meta['transitions'][self.actual_state]\n    return state in transitions"}, {"instruction": "def on_unexpected_error(e):  # pragma: no cover\n    \"\"\"Catch-all error handler\n\n    Unexpected errors will be handled by this function.\n    \"\"\"\n", "input": "", "output": "    sys.stderr.write('Unexpected error: {} ({})\\n'.format(\n        str(e), e.__class__.__name__))\n    sys.stderr.write('See file slam_error.log for additional details.\\n')\n    sys.exit(1)"}, {"instruction": "def status(self, job_ids):\n        \"\"\"Get the status of a list of jobs identified by their ids.\n\n        Parameters\n        ----------\n        job_ids : list of str\n            Identifiers for the jobs.\n\n        Returns\n        -------\n        list of int\n            Status codes for each requested job.\n        \"\"\"\n", "input": "", "output": "        states = []\n        statuses = self.deployer.get_vm_status([self.resources.get(job_id) for job_id in job_ids])\n        for status in statuses:\n            states.append(translate_table.get(status.state['Name'], \"PENDING\"))\n        return states"}, {"instruction": "def _check_scalar_vertical_extents(self, ds, z_variable):\n        '''\n        Check the scalar value of Z compared to the vertical extents which\n        should also be equivalent\n\n        :param netCDF4.Dataset ds: An open netCDF dataset\n        :param str z_variable: Name of the variable representing the Z-Axis\n        '''\n", "input": "", "output": "        vert_min = ds.geospatial_vertical_min\n        vert_max = ds.geospatial_vertical_max\n        msgs = []\n        total = 2\n\n        zvalue = ds.variables[z_variable][:].item()\n        if not np.isclose(vert_min, vert_max):\n            msgs.append(\"geospatial_vertical_min != geospatial_vertical_max for scalar depth values, %s != %s\" % (\n                vert_min,\n                vert_max\n            ))\n\n        if not np.isclose(vert_max, zvalue):\n            msgs.append(\"geospatial_vertical_max != %s values, %s != %s\" % (\n                z_variable,\n                vert_max,\n                zvalue\n            ))\n\n        return Result(BaseCheck.MEDIUM,\n                      (total - len(msgs), total),\n                      'geospatial_vertical_extents_match',\n                      msgs)"}, {"instruction": "def init_random(X, n_clusters, random_state):\n    \"\"\"K-means initialization using randomly chosen points\"\"\"\n", "input": "", "output": "    logger.info(\"Initializing randomly\")\n    idx = sorted(draw_seed(random_state, 0, len(X), size=n_clusters))\n    centers = X[idx].compute()\n    return centers"}, {"instruction": "def libvlc_video_get_track_description(p_mi):\n    '''Get the description of available video tracks.\n    @param p_mi: media player.\n    @return: list with description of available video tracks, or NULL on error.\n    '''\n", "input": "", "output": "    f = _Cfunctions.get('libvlc_video_get_track_description', None) or \\\n        _Cfunction('libvlc_video_get_track_description', ((1,),), None,\n                    ctypes.POINTER(TrackDescription), MediaPlayer)\n    return f(p_mi)"}, {"instruction": "def leaveoneout(self):\n        \"\"\"Train & Test using leave one out\"\"\"\n", "input": "", "output": "        traintestfile = self.fileprefix + '.train'\n        options = \"-F \" + self.format + \" \" +  self.timbloptions + \" -t leave_one_out\"\n        if sys.version < '3':\n            self.api = timblapi.TimblAPI(b(options), b\"\")\n        else:\n            self.api = timblapi.TimblAPI(options, \"\")\n        if self.debug:\n            print(\"Enabling debug for timblapi\",file=stderr)\n            self.api.enableDebug()\n        print(\"Calling Timbl API : \" + options,file=stderr)\n        if sys.version < '3':\n            self.api.learn(b(traintestfile))\n            self.api.test(b(traintestfile), b(self.fileprefix + '.out'),b'')\n        else:\n            self.api.learn(u(traintestfile))\n            self.api.test(u(traintestfile), u(self.fileprefix + '.out'),'')\n        return self.api.getAccuracy()"}, {"instruction": "def _prompt_changer(attr, val):\n    \"\"\"Change the current prompt theme\"\"\"\n", "input": "", "output": "    try:\n        sys.ps1 = conf.color_theme.prompt(conf.prompt)\n    except Exception:\n        pass\n    try:\n        apply_ipython_style(get_ipython())\n    except NameError:\n        pass"}, {"instruction": "def qualified_name(self):\n        '''return the fully qualified name (`<module>.<interface>#<operation>`)'''\n", "input": "", "output": "        return '{0}.{1}#{2}'.format(self.module.name, self.interface.name, self.name)"}, {"instruction": "def ping(hostname: str, timeout_s: int = 5) -> bool:\n    \"\"\"\n    Pings a host, using OS tools.\n\n    Args:\n        hostname: host name or IP address\n        timeout_s: timeout in seconds\n\n    Returns:\n        was the ping successful?\n\n    \"\"\"\n", "input": "", "output": "    if sys.platform == \"win32\":\n        timeout_ms = timeout_s * 1000\n        args = [\n            \"ping\",\n            hostname,\n            \"-n\", \"1\",  # ping count\n            \"-w\", str(timeout_ms),  # timeout\n        ]\n    elif sys.platform.startswith('linux'):\n        args = [\n            \"ping\",\n            hostname,\n            \"-c\", \"1\",  # ping count\n            \"-w\", str(timeout_s),  # timeout\n        ]\n    else:\n        raise AssertionError(\"Don't know how to ping on this operating system\")\n    proc = subprocess.Popen(args,\n                            stdout=subprocess.PIPE, stderr=subprocess.PIPE)\n    proc.communicate()\n    retcode = proc.returncode\n    return retcode == 0"}, {"instruction": "def _parse_input_node(cls, node):\n        \"\"\"\n        :param node: xml node\n        :return: dict\n        \"\"\"\n", "input": "", "output": "        data = {}\n        child = node.getchildren()\n        if not child and node.get('name'):\n            val = node.text\n        elif child:  # if tag = \"{http://activiti.org/bpmn}script\" then data_typ = 'script'\n            data_typ = child[0].tag.split('}')[1]\n            val = getattr(cls, '_parse_%s' % data_typ)(child[0])\n        data[node.get('name')] = val\n        return data"}, {"instruction": "def tag(self, repository_tag, tags=[]):\n        \"\"\"\n        Tags image with one or more tags.\n\n        Raises exception on failure.\n        \"\"\"\n", "input": "", "output": "        if not isinstance(repository_tag, six.string_types):\n            raise TypeError('repository_tag must be a string')\n\n        if not isinstance(tags, list):\n            raise TypeError('tags must be a list.')\n\n        if ':' in repository_tag:\n            repository, tag = repository_tag.split(':')\n            tags.append(tag)\n        else:\n            repository = repository_tag\n\n            if not tags:\n                tags.append('latest')\n\n        for tag in tags:\n            repo_tag = \"{0}:{1}\".format(repository, tag)\n\n            if repo_tag not in self.repo_tags:\n                logger.info(\"Tagging Image: {0} Repo Tag: {1}\".format(self.identifier, repo_tag))\n                self.repo_tags = self.repo_tags + (repo_tag, )\n\n                # always going to force tags until a feature is added to allow users to specify.\n                try:\n                    self.client.tag(self.id, repository, tag)\n                except:\n                    self.client.tag(self.id, repository, tag, force=True)"}, {"instruction": "def pprint(self, file_=sys.stdout):\n        \"\"\"Print the code block to stdout.\n        Does syntax highlighting if possible.\n        \"\"\"\n", "input": "", "output": "\n        code = []\n        if self._deps:\n            code.append(\"# dependencies:\")\n        for k, v in _compat.iteritems(self._deps):\n            code.append(\"#   %s: %r\" % (k, v))\n        code.append(str(self))\n        code = \"\\n\".join(code)\n\n        if file_.isatty():\n            try:\n                from pygments import highlight\n                from pygments.lexers import PythonLexer\n                from pygments.formatters import TerminalFormatter\n            except ImportError:\n                pass\n            else:\n                formatter = TerminalFormatter(bg=\"dark\")\n                lexer = PythonLexer()\n                file_.write(highlight(code, lexer, formatter))\n                return\n        file_.write(code + \"\\n\")"}, {"instruction": "def release(self):\n        \"\"\"Release the lock\"\"\"\n", "input": "", "output": "        base64_key = _encode(self.key)\n        base64_value = _encode(self._uuid)\n\n        txn = {\n            'compare': [{\n                'key': base64_key,\n                'result': 'EQUAL',\n                'target': 'VALUE',\n                'value': base64_value\n            }],\n            'success': [{\n                'request_delete_range': {\n                    'key': base64_key\n                }\n            }]\n        }\n\n        result = self.client.transaction(txn)\n        if 'succeeded' in result:\n            return result['succeeded']\n        return False"}, {"instruction": "def add_image_description(self, dict):\n        \"\"\"Add a dict to image description.\"\"\"\n", "input": "", "output": "        if self._ef is not None:\n            self._ef['0th'][piexif.ImageIFD.ImageDescription] = json.dumps(\n                dict)"}, {"instruction": "def _symbol_bottom_simple(x, model_hparams, vocab_size, name, reuse):\n  \"\"\"Bottom transformation for symbols.\"\"\"\n", "input": "", "output": "  with tf.variable_scope(name, reuse=reuse):\n    # Ensure the inputs are 3-D\n    if len(x.get_shape()) == 4:\n      x = tf.squeeze(x, axis=3)\n    while len(x.get_shape()) < 3:\n      x = tf.expand_dims(x, axis=-1)\n\n    var = get_weights(model_hparams, vocab_size)\n    x = common_layers.dropout_no_scaling(\n        x, 1.0 - model_hparams.symbol_dropout)\n    ret = common_layers.gather(var, x)\n    if model_hparams.multiply_embedding_mode == \"sqrt_depth\":\n      ret *= model_hparams.hidden_size**0.5\n    ret *= tf.expand_dims(\n        common_layers.cast_like(tf.not_equal(x, 0), ret), -1)\n    return ret"}, {"instruction": "def list(self, identity_id, per_page=20, page=1):\n        \"\"\" Get a list of tokens\n\n            :param identity_id: The ID of the identity to retrieve tokens for\n            :param per_page: The number of results per page returned\n            :param page: The page number of the results\n            :return: dict of REST API output with headers attached\n            :rtype: :class:`~datasift.request.DictResponse`\n            :raises: :class:`~datasift.exceptions.DataSiftApiException`,\n                :class:`requests.exceptions.HTTPError`\n        \"\"\"\n", "input": "", "output": "\n        params = {'per_page': per_page, 'page': page}\n\n        return self.request.get(str(identity_id) + '/token', params)"}, {"instruction": "def _f16_to_32bit(ins):\n    \"\"\" If any of the operands within the ins(truction) are numeric,\n    convert them to its 32bit representation, otherwise leave them\n    as they are.\n    \"\"\"\n", "input": "", "output": "    ins.quad = [x for x in ins.quad]\n    for i in range(2, len(ins.quad)):\n        if is_float(ins.quad[i]):\n            de, hl = f16(ins.quad[i])\n            ins.quad[i] = str((de << 16) | hl)\n\n    ins.quad = tuple(ins.quad)\n    return ins"}, {"instruction": "def loc_adjacent_to_opponent_king(self, location, position):\n        \"\"\"\n        Finds if 2 kings are touching given the position of one of the kings.\n\n        :type: location: Location\n        :type: position: Board\n        :rtype: bool\n        \"\"\"\n", "input": "", "output": "        for fn in self.cardinal_directions:\n            try:\n                if isinstance(position.piece_at_square(fn(location)), King) and \\\n                        position.piece_at_square(fn(location)).color != self.color:\n                    return True\n\n            except IndexError:\n                pass\n\n        return False"}, {"instruction": "def p_array_literal_2(self, p):\n        \"\"\"array_literal : LBRACKET element_list RBRACKET\n                         | LBRACKET element_list COMMA elision_opt RBRACKET\n        \"\"\"\n", "input": "", "output": "        items = p[2]\n        if len(p) == 6:\n            items.extend(p[4])\n        p[0] = ast.Array(items=items)"}, {"instruction": "def sso(user, desired_username, name, email, profile_fields=None):\n    \"\"\"\n    Create a user, if the provided `user` is None, from the parameters.\n    Then log the user in, and return it.\n    \"\"\"\n", "input": "", "output": "    if not user:\n        if not settings.REGISTRATION_OPEN:\n            raise SSOError('Account registration is closed')\n        user = _create_desired_user(desired_username)\n        _configure_user(user, name, email, profile_fields)\n\n    if not user.is_active:\n        raise SSOError('Account disabled')\n\n    # login() expects the logging in backend to be set on the user.\n    # We are bypassing login, so fake it.\n    user.backend = settings.AUTHENTICATION_BACKENDS[0]\n    return user"}, {"instruction": "def _is_json(string):\n        \"\"\"Test if input string is in JSON format.\n\n        :param string: Input string.\n        :type string: :py:class:`str` or :py:class:`bytes`\n        :return: Input string if in JSON format or False otherwise.\n        :rtype: :py:class:`str` or :py:obj:`False`\n        \"\"\"\n", "input": "", "output": "        try:\n            if isinstance(string, bytes):\n                string = string.decode(\"utf-8\")\n                json.loads(string)\n            elif isinstance(string, str):\n                json.loads(string)\n            else:\n                raise TypeError(\"Expecting <class 'str'> or <class 'bytes'>, but {} was passed\".format(type(string)))\n            return string\n\n        except ValueError:\n            return False"}, {"instruction": "def require_prebuilt_dist(func):\n    \"\"\"Decorator for ToolchainCL methods. If present, the method will\n    automatically make sure a dist has been built before continuing\n    or, if no dists are present or can be obtained, will raise an\n    error.\n    \"\"\"\n", "input": "", "output": "\n    @wraps(func)\n    def wrapper_func(self, args):\n        ctx = self.ctx\n        ctx.set_archs(self._archs)\n        ctx.prepare_build_environment(user_sdk_dir=self.sdk_dir,\n                                      user_ndk_dir=self.ndk_dir,\n                                      user_android_api=self.android_api,\n                                      user_ndk_api=self.ndk_api)\n        dist = self._dist\n        if dist.needs_build:\n            if dist.folder_exists():  # possible if the dist is being replaced\n                dist.delete()\n            info_notify('No dist exists that meets your requirements, '\n                        'so one will be built.')\n            build_dist_from_args(ctx, dist, args)\n        func(self, args)\n    return wrapper_func"}, {"instruction": "def set_install_dir(self, install_dir=None, version=None, verbose=False):\n        \"\"\"\n        Sets the path to the cassandra source directory for use by this node.\n        \"\"\"\n", "input": "", "output": "        if version is None:\n            self.__install_dir = install_dir\n            if install_dir is not None:\n                common.validate_install_dir(install_dir)\n        else:\n            self.__install_dir = self.node_setup(version, verbose=verbose)\n\n        self._cassandra_version = common.get_version_from_build(self.__install_dir, cassandra=True)\n\n        if self.get_base_cassandra_version() >= 4.0:\n            self.network_interfaces['thrift'] = None\n\n        self.import_config_files()\n        self.import_bin_files()\n        self.__conf_updated = False\n        return self"}, {"instruction": "def get_print_rect(self, grid_rect):\n        \"\"\"Returns wx.Rect that is correctly positioned on the print canvas\"\"\"\n", "input": "", "output": "\n        grid = self.grid\n\n        rect_x = grid_rect.x - \\\n            grid.GetScrollPos(wx.HORIZONTAL) * grid.GetScrollLineX()\n        rect_y = grid_rect.y - \\\n            grid.GetScrollPos(wx.VERTICAL) * grid.GetScrollLineY()\n\n        return wx.Rect(rect_x, rect_y, grid_rect.width, grid_rect.height)"}, {"instruction": "def get_intersectionsbysubsets(df,cols_fracby2vals,cols_subset,col_ids):\n    \"\"\"\n    cols_fracby:\n    cols_subset:\n    \"\"\"\n", "input": "", "output": "    for col_fracby in cols_fracby2vals:\n        val=cols_fracby2vals[col_fracby]\n        ids=df.loc[(df[col_fracby]==val),col_ids].dropna().unique()\n        for col_subset in cols_subset:\n            for subset in dropna(df[col_subset].unique()):\n                ids_subset=df.loc[(df[col_subset]==subset),col_ids].dropna().unique()\n                df.loc[(df[col_subset]==subset),f'P {col_fracby} {col_subset}']=len(set(ids_subset).intersection(ids))/len(ids_subset)\n    return df"}, {"instruction": "def get_mac_address_table_input_request_type_get_interface_based_request_forwarding_interface_interface_name(self, **kwargs):\n        \"\"\"Auto Generated Code\n        \"\"\"\n", "input": "", "output": "        config = ET.Element(\"config\")\n        get_mac_address_table = ET.Element(\"get_mac_address_table\")\n        config = get_mac_address_table\n        input = ET.SubElement(get_mac_address_table, \"input\")\n        request_type = ET.SubElement(input, \"request-type\")\n        get_interface_based_request = ET.SubElement(request_type, \"get-interface-based-request\")\n        forwarding_interface = ET.SubElement(get_interface_based_request, \"forwarding-interface\")\n        interface_name = ET.SubElement(forwarding_interface, \"interface-name\")\n        interface_name.text = kwargs.pop('interface_name')\n\n        callback = kwargs.pop('callback', self._callback)\n        return callback(config)"}, {"instruction": "def get_storage_usage(access_token, subscription_id, location):\n    '''Returns storage usage and quota information for the specified subscription.\n\n    Args:\n        access_token (str): A valid Azure authentication token.\n        subscription_id (str): Azure subscription id.\n\n    Returns:\n        HTTP response. JSON body of storage account usage.\n    '''\n", "input": "", "output": "    endpoint = ''.join([get_rm_endpoint(),\n                        '/subscriptions/', subscription_id,\n                        '/providers/Microsoft.Storage/locations/', location,\n                        '/usages',\n                        '?api-version=', STORAGE_API])\n    return do_get(endpoint, access_token)"}, {"instruction": "def halt(self):\n        \"\"\"Halts the CPU Core.\n\n        Args:\n          self (JLink): the ``JLink`` instance\n\n        Returns:\n          ``True`` if halted, ``False`` otherwise.\n        \"\"\"\n", "input": "", "output": "        res = int(self._dll.JLINKARM_Halt())\n        if res == 0:\n            time.sleep(1)\n            return True\n        return False"}, {"instruction": "def get_all_licenses(self):\n        \"\"\"Retrieve license type, key, installation date, etc.\"\"\"\n", "input": "", "output": "        data = self._execute_command('GET_ALL_LICENSES', 'RIB_INFO', 'read')\n        d = {}\n        for key, val in data['GET_ALL_LICENSES']['LICENSE'].items():\n            if isinstance(val, dict):\n                d[key] = data['GET_ALL_LICENSES']['LICENSE'][key]['VALUE']\n        return d"}, {"instruction": "def stringize(\n        self,\n        rnf_profile=RnfProfile(),\n    ):\n        \"\"\"Create RNF representation of this read.\n\n\t\tArgs:\n\t\t\tread_tuple_id_width (int): Maximal expected string length of read tuple ID.\n\t\t\tgenome_id_width (int): Maximal expected string length of genome ID.\n\t\t\tchr_id_width (int): Maximal expected string length of chromosome ID.\n\t\t\tcoor_width (int): Maximal expected string length of a coordinate.\n\t\t\"\"\"\n", "input": "", "output": "\n        sorted_segments = sorted(self.segments,\n         key=lambda x: (\n          x.genome_id * (10 ** 23) +\n          x.chr_id * (10 ** 21) +\n          (x.left + (int(x.left == 0) * x.right - 1)) * (10 ** 11) +\n          x.right * (10 ** 1) +\n          int(x.direction == \"F\")\n         )\n        )\n\n        segments_strings = [x.stringize(rnf_profile) for x in sorted_segments]\n\n        read_tuple_name = \"__\".join(\n            [\n                self.prefix,\n                format(self.read_tuple_id, 'x').zfill(rnf_profile.read_tuple_id_width),\n                \",\".join(segments_strings),\n                self.suffix,\n            ]\n        )\n\n        return read_tuple_name"}, {"instruction": "def _try_instantiate(self, ipopo, factory, component):\n        # type: (Any, str, str) -> None\n        \"\"\"\n        Tries to instantiate a component from the queue. Hides all exceptions.\n\n        :param ipopo: The iPOPO service\n        :param factory: Component factory\n        :param component: Component name\n        \"\"\"\n", "input": "", "output": "        try:\n            # Get component properties\n            with self.__lock:\n                properties = self.__queue[factory][component]\n        except KeyError:\n            # Component not in queue\n            return\n        else:\n            try:\n                # Try instantiation\n                ipopo.instantiate(factory, component, properties)\n            except TypeError:\n                # Unknown factory: try later\n                pass\n            except ValueError as ex:\n                # Already known component\n                _logger.error(\"Component already running: %s\", ex)\n            except Exception as ex:\n                # Other error\n                _logger.exception(\"Error instantiating component: %s\", ex)"}, {"instruction": "def register_algorithm(self, alg_id, alg_obj):\n        \"\"\"\n        Registers a new Algorithm for use when creating and verifying tokens.\n        \"\"\"\n", "input": "", "output": "        if alg_id in self._algorithms:\n            raise ValueError('Algorithm already has a handler.')\n\n        if not isinstance(alg_obj, Algorithm):\n            raise TypeError('Object is not of type `Algorithm`')\n\n        self._algorithms[alg_id] = alg_obj\n        self._valid_algs.add(alg_id)"}, {"instruction": "def parse_scalar_type_extension(lexer: Lexer) -> ScalarTypeExtensionNode:\n    \"\"\"ScalarTypeExtension\"\"\"\n", "input": "", "output": "    start = lexer.token\n    expect_keyword(lexer, \"extend\")\n    expect_keyword(lexer, \"scalar\")\n    name = parse_name(lexer)\n    directives = parse_directives(lexer, True)\n    if not directives:\n        raise unexpected(lexer)\n    return ScalarTypeExtensionNode(\n        name=name, directives=directives, loc=loc(lexer, start)\n    )"}, {"instruction": "def has_next_assessment_section(self, assessment_section_id):\n        \"\"\"Tests if there is a next assessment section in the assessment following the given assessment section ``Id``.\n\n        arg:    assessment_section_id (osid.id.Id): ``Id`` of the\n                ``AssessmentSection``\n        return: (boolean) - ``true`` if there is a next section,\n                ``false`` otherwise\n        raise:  IllegalState - ``has_assessment_begun()`` is ``false``\n        raise:  NotFound - ``assessment_taken_id`` is not found\n        raise:  NullArgument - ``assessment_taken_id`` is ``null``\n        raise:  OperationFailed - unable to complete request\n        raise:  PermissionDenied - authorization failure occurred\n        *compliance: mandatory -- This method must be implemented.*\n\n        \"\"\"\n", "input": "", "output": "        try:\n            self.get_next_assessment_section(assessment_section_id)\n        except errors.IllegalState:\n            return False\n        else:\n            return True"}, {"instruction": "def is_logged(self, user):\n        \"\"\"Check if a logged user is trying to access the register page.\n           If so, redirect him/her to his/her profile\"\"\"\n", "input": "", "output": "\n        response = None\n        if user.is_authenticated():\n            if not user.needs_update:\n                response = redirect('user_profile', username=user.username)\n\n        return response"}, {"instruction": "def mmf(x, alpha, beta, kappa, delta):\n    \"\"\"Morgan-Mercer-Flodin\n    http://www.pisces-conservation.com/growthhelp/index.html?morgan_mercer_floden.htm\n\n    Parameters\n    ----------\n    x: int\n    alpha: float\n    beta: float\n    kappa: float\n    delta: float\n\n    Returns\n    -------\n    float\n        alpha - (alpha - beta) / (1. + (kappa * x)**delta)\n    \"\"\"\n", "input": "", "output": "    return alpha - (alpha - beta) / (1. + (kappa * x)**delta)"}, {"instruction": "def digest(dirname, glob=None):\n    \"\"\"Returns the md5 digest of all interesting files (or glob) in `dirname`.\n    \"\"\"\n", "input": "", "output": "    md5 = hashlib.md5()\n    if glob is None:\n        fnames = [fname for _, fname in list_files(Path(dirname))]\n        for fname in sorted(fnames):\n            fname = os.path.join(dirname, fname)\n            md5.update(open(fname, 'rb').read())\n    else:\n        fnames = Path(dirname).glob(glob)\n        for fname in sorted(fnames):\n            md5.update(fname.open('rb').read())\n    return md5.hexdigest()"}, {"instruction": "def _get_metric(self, metric, tablename, index_name=None):\n        \"\"\" Fetch a read/write capacity metric \"\"\"\n", "input": "", "output": "        end = time.time()\n        begin = end - 3 * 60  # 3 minute window\n        dimensions = [{\"Name\": \"TableName\", \"Value\": tablename}]\n        if index_name is not None:\n            dimensions.append({\"Name\": \"GlobalSecondaryIndexName\", \"Value\": index_name})\n        period = 60\n        data = self.cloudwatch_connection.get_metric_statistics(\n            Period=period,\n            StartTime=begin,\n            EndTime=end,\n            MetricName=metric,\n            Namespace=\"AWS/DynamoDB\",\n            Statistics=[\"Sum\"],\n            Dimensions=dimensions,\n        )\n        points = data[\"Datapoints\"]\n        if not points:\n            return 0\n        else:\n            points.sort(key=lambda r: r[\"Timestamp\"])\n            return float(points[-1][\"Sum\"]) / period"}, {"instruction": "def _fault_to_exception(f):\n    \"\"\" Converts XML-RPC Fault objects to Pynipap-exceptions.\n\n        TODO: Is this one neccesary? Can be done inline...\n    \"\"\"\n", "input": "", "output": "\n    e = _fault_to_exception_map.get(f.faultCode)\n    if e is None:\n        e = NipapError\n    return e(f.faultString)"}, {"instruction": "def _publish_stats(self, counter_prefix, stats):\n        \"\"\"Given a stats dictionary from _get_stats_from_socket,\n        publish the individual values.\n        \"\"\"\n", "input": "", "output": "        for stat_name, stat_value in flatten_dictionary(\n            stats,\n            prefix=counter_prefix,\n        ):\n            self.publish_gauge(stat_name, stat_value)"}, {"instruction": "def __interact_writen(self, fd, data):\n        '''This is used by the interact() method.\n        '''\n", "input": "", "output": "\n        while data != b'' and self.isalive():\n            n = os.write(fd, data)\n            data = data[n:]"}, {"instruction": "def wait_for_compute_global_operation(project_name, operation):\n    \"\"\"Poll for global compute operation until finished.\"\"\"\n", "input": "", "output": "    logger.info(\"wait_for_compute_global_operation: \"\n                \"Waiting for operation {} to finish...\".format(\n                    operation[\"name\"]))\n\n    for _ in range(MAX_POLLS):\n        result = compute.globalOperations().get(\n            project=project_name,\n            operation=operation[\"name\"],\n        ).execute()\n        if \"error\" in result:\n            raise Exception(result[\"error\"])\n\n        if result[\"status\"] == \"DONE\":\n            logger.info(\"wait_for_compute_global_operation: \"\n                        \"Operation done.\")\n            break\n\n        time.sleep(POLL_INTERVAL)\n\n    return result"}, {"instruction": "def read_long_description(readme_file):\n    \"\"\" Read package long description from README file \"\"\"\n", "input": "", "output": "    try:\n        import pypandoc\n    except (ImportError, OSError) as exception:\n        print('No pypandoc or pandoc: %s' % (exception,))\n        if sys.version_info.major == 3:\n            handle = open(readme_file, encoding='utf-8')\n        else:\n            handle = open(readme_file)\n        long_description = handle.read()\n        handle.close()\n        return long_description\n    else:\n        return pypandoc.convert(readme_file, 'rst')"}, {"instruction": "def _eight_byte_real_to_float(value):\n    \"\"\"\n    Convert a number from GDSII 8 byte real format to float.\n\n    Parameters\n    ----------\n    value : string\n        The GDSII binary string representation of the number.\n\n    Returns\n    -------\n    out : float\n        The number represented by ``value``.\n    \"\"\"\n", "input": "", "output": "    short1, short2, long3 = struct.unpack('>HHL', value)\n    exponent = (short1 & 0x7f00) // 256 - 64\n    mantissa = (((short1 & 0x00ff) * 65536 + short2) * 4294967296 +\n                long3) / 72057594037927936.0\n    if short1 & 0x8000:\n        return -mantissa * 16.**exponent\n    return mantissa * 16.**exponent"}, {"instruction": "def get_pr_review_status(pr: PullRequestDetails) -> Any:\n    \"\"\"\n    References:\n        https://developer.github.com/v3/pulls/reviews/#list-reviews-on-a-pull-request\n    \"\"\"\n", "input": "", "output": "    url = (\"https://api.github.com/repos/{}/{}/pulls/{}/reviews\"\n           \"?access_token={}\".format(pr.repo.organization,\n                                     pr.repo.name,\n                                     pr.pull_id,\n                                     pr.repo.access_token))\n    response = requests.get(url)\n\n    if response.status_code != 200:\n        raise RuntimeError(\n            'Get review failed. Code: {}. Content: {}.'.format(\n                response.status_code, response.content))\n\n    return json.JSONDecoder().decode(response.content.decode())"}, {"instruction": "def load(self):\n        \"\"\" Return the model from the store \"\"\"\n", "input": "", "output": "\n        filters = [Filter(self.field, 'eq', self.rid)]\n        store = goldman.sess.store\n\n        self._is_loaded = True\n        self.models = store.search(self.rtype, filters=filters)\n\n        return self.models"}, {"instruction": "def parse(cls, conn):\n        \"\"\"Read a request from the HTTP connection ``conn``.\n\n        May raise ``BadHttpRequestError``.\n        \"\"\"\n", "input": "", "output": "\n        req = cls(conn)\n        req_line = yield from conn.reader.readline()\n        logger('HttpRequest').debug('req_line = %r', req_line)\n        req._parse_req_line(req_line)\n\n        header_line = yield from conn.reader.readline()\n        while len(header_line) > 0 and header_line != b'\\r\\n':\n            try:\n                req._parse_header(header_line)\n            except BadHttpHeaderError as e:\n                # Tolerating 'minor' mistakes\n                logger('HttpRequest').debug(traceback.format_exc())\n            header_line = yield from conn.reader.readline()\n        return req"}, {"instruction": "def difference(self, *args):\n        \"\"\"\n        Take the difference between one array and a number of other arrays.\n        Only the elements present in just the first array will remain.\n        \"\"\"\n", "input": "", "output": "        setobj = set(self.obj)\n        for i, v in enumerate(args):\n            setobj = setobj - set(args[i])\n        return self._wrap(self._clean._toOriginal(setobj))"}, {"instruction": "def ifaces(cls, name):\n        \"\"\" Get vlan attached ifaces. \"\"\"\n", "input": "", "output": "        ifaces = Iface.list({'vlan_id': cls.usable_id(name)})\n        ret = []\n        for iface in ifaces:\n            ret.append(Iface.info(iface['id']))\n        return ret"}, {"instruction": "def qos_map_cos_mutation_cos1(self, **kwargs):\n        \"\"\"Auto Generated Code\n        \"\"\"\n", "input": "", "output": "        config = ET.Element(\"config\")\n        qos = ET.SubElement(config, \"qos\", xmlns=\"urn:brocade.com:mgmt:brocade-qos\")\n        map = ET.SubElement(qos, \"map\")\n        cos_mutation = ET.SubElement(map, \"cos-mutation\")\n        name_key = ET.SubElement(cos_mutation, \"name\")\n        name_key.text = kwargs.pop('name')\n        cos1 = ET.SubElement(cos_mutation, \"cos1\")\n        cos1.text = kwargs.pop('cos1')\n\n        callback = kwargs.pop('callback', self._callback)\n        return callback(config)"}, {"instruction": "def get_genus_type_metadata(self):\n        \"\"\"Overrides get_genus_type_metadata of extended object\"\"\"\n", "input": "", "output": "        metadata = dict(self.my_osid_object_form._genus_type_metadata)\n        metadata.update({'read_only': True})\n        return Metadata(**metadata)"}, {"instruction": "def ints2str(self, int_values):\n    \"\"\"Conversion list[int] => decoded string.\"\"\"\n", "input": "", "output": "    if not self._encoder:\n      raise ValueError(\n          \"Text.ints2str is not available because encoder hasn't been defined.\")\n    return self._encoder.decode(int_values)"}, {"instruction": "def range_mac(mac_start, mac_end, step=1):\n    \"\"\"Iterate over mac addresses (given as string).\"\"\"\n", "input": "", "output": "    start = int(EUI(mac_start))\n    end = int(EUI(mac_end))\n    for i_mac in range(start, end, step):\n        mac = EUI(int(EUI(i_mac)) + 1)\n        ip = ['10'] + [str(int(i, 2)) for i in mac.bits().split('-')[-3:]]\n        yield str(mac).replace('-', ':'), '.'.join(ip)"}, {"instruction": "def _join_host_port(host, port):\n    \"\"\"Adapted golang's net.JoinHostPort\"\"\"\n", "input": "", "output": "    template = \"%s:%s\"\n    host_requires_bracketing = ':' in host or '%' in host\n    if host_requires_bracketing:\n        template = \"[%s]:%s\"\n    return template % (host, port)"}, {"instruction": "def SeriesXmlRewriterFactory(chart_type, chart_data):\n    \"\"\"\n    Return a |_BaseSeriesXmlRewriter| subclass appropriate to *chart_type*.\n    \"\"\"\n", "input": "", "output": "    XL_CT = XL_CHART_TYPE\n\n    RewriterCls = {\n        # There are 73 distinct chart types, only specify non-category\n        # types, others default to _CategorySeriesXmlRewriter. Stock-type\n        # charts are multi-plot charts, so no guaratees on how they turn\n        # out.\n        XL_CT.BUBBLE:                       _BubbleSeriesXmlRewriter,\n        XL_CT.BUBBLE_THREE_D_EFFECT:        _BubbleSeriesXmlRewriter,\n        XL_CT.XY_SCATTER:                   _XySeriesXmlRewriter,\n        XL_CT.XY_SCATTER_LINES:             _XySeriesXmlRewriter,\n        XL_CT.XY_SCATTER_LINES_NO_MARKERS:  _XySeriesXmlRewriter,\n        XL_CT.XY_SCATTER_SMOOTH:            _XySeriesXmlRewriter,\n        XL_CT.XY_SCATTER_SMOOTH_NO_MARKERS: _XySeriesXmlRewriter,\n    }.get(chart_type, _CategorySeriesXmlRewriter)\n\n    return RewriterCls(chart_data)"}, {"instruction": "def _get_task_descriptor_info(self, courseid, taskid):\n        \"\"\"\n        :param courseid: the course id of the course\n        :param taskid: the task id of the task\n        :raise InvalidNameException, TaskNotFoundException\n        :return: a tuple, containing:\n            (descriptor filename,\n             task file manager for the descriptor)\n        \"\"\"\n", "input": "", "output": "        if not id_checker(courseid):\n            raise InvalidNameException(\"Course with invalid name: \" + courseid)\n        if not id_checker(taskid):\n            raise InvalidNameException(\"Task with invalid name: \" + taskid)\n\n        task_fs = self.get_task_fs(courseid, taskid)\n        for ext, task_file_manager in self._task_file_managers.items():\n            if task_fs.exists(\"task.\"+ext):\n                return \"task.\" + ext, task_file_manager\n\n        raise TaskNotFoundException()"}, {"instruction": "def glymurrc_fname():\n    \"\"\"Return the path to the configuration file.\n\n    Search order:\n        1) current working directory\n        2) environ var XDG_CONFIG_HOME\n        3) $HOME/.config/glymur/glymurrc\n    \"\"\"\n", "input": "", "output": "\n    # Current directory.\n    fname = os.path.join(os.getcwd(), 'glymurrc')\n    if os.path.exists(fname):\n        return fname\n\n    confdir = get_configdir()\n    if confdir is not None:\n        fname = os.path.join(confdir, 'glymurrc')\n        if os.path.exists(fname):\n            return fname\n\n    # didn't find a configuration file.\n    return None"}, {"instruction": "def parse_comments_for_file(filename):\n    \"\"\"\n    Return a list of all parsed comments in a file.  Mostly for testing &\n    interactive use.\n    \"\"\"\n", "input": "", "output": "    return [parse_comment(strip_stars(comment), next_line)\n            for comment, next_line in get_doc_comments(read_file(filename))]"}, {"instruction": "def delete(uid):\n        '''\n        Delete by uid\n        '''\n", "input": "", "output": "\n        q_u1 = TabPostHist.delete().where(TabPostHist.post_id == uid)\n        q_u1.execute()\n        q_u2 = TabRel.delete().where(TabRel.post_f_id == uid or TabRel.post_t_id == uid)\n        q_u2.execute()\n        q_u3 = TabCollect.delete().where(TabCollect.post_id == uid)\n        q_u3.execute()\n        q_u4 = TabPost2Tag.delete().where(TabPost2Tag.post_id == uid)\n        q_u4.execute()\n        q_u5 = TabUsage.delete().where(TabUsage.post_id == uid)\n        q_u5.execute()\n\n        reply_arr = []\n        for reply in TabUser2Reply.select().where(TabUser2Reply.reply_id == uid):\n            reply_arr.append(reply.reply_id.uid)\n\n        q_u6 = TabUser2Reply.delete().where(TabUser2Reply.reply_id == uid)\n        q_u6.execute()\n\n        for replyid in reply_arr:\n            TabReply.delete().where(TabReply.uid == replyid).execute()\n\n        q_u7 = TabEvaluation.delete().where(TabEvaluation.post_id == uid)\n        q_u7.execute()\n        q_u8 = TabRating.delete().where(TabRating.post_id == uid)\n        q_u8.execute()\n        return MHelper.delete(TabPost, uid)"}, {"instruction": "def locate(command, on):\n    \"\"\"Locate the command's man page.\"\"\"\n", "input": "", "output": "    location = find_page_location(command, on)\n    click.echo(location)"}, {"instruction": "def process_ndex_network(network_id, username=None, password=None,\n                         require_grounding=True):\n    \"\"\"Process an NDEx network into Statements.\n\n    Parameters\n    ----------\n    network_id : str\n        NDEx network ID.\n    username : str\n        NDEx username.\n    password : str\n        NDEx password.\n    require_grounding: bool\n        Whether network nodes lacking grounding information should be included\n        among the extracted Statements (default is True).\n\n    Returns\n    -------\n    NdexCxProcessor\n        Processor containing Statements. Returns None if there if the HTTP\n        status code indicates an unsuccessful request.\n    \"\"\"\n", "input": "", "output": "    nd = ndex2.client.Ndex2(username=username, password=password)\n    res = nd.get_network_as_cx_stream(network_id)\n    if res.status_code != 200:\n        logger.error('Problem downloading network: status code %s' %\n                     res.status_code)\n        logger.error('Response: %s' % res.text)\n        return None\n    json_list = res.json()\n    summary = nd.get_network_summary(network_id)\n    return process_cx(json_list, summary=summary,\n                      require_grounding=require_grounding)"}, {"instruction": "def clear_attendance(self):\n        \"\"\"\n        clear all attendance record\n\n        :return: bool\n        \"\"\"\n", "input": "", "output": "        command = const.CMD_CLEAR_ATTLOG\n        cmd_response = self.__send_command(command)\n        if cmd_response.get('status'):\n            return True\n        else:\n            raise ZKErrorResponse(\"Can't clear response\")"}, {"instruction": "def setBendLength(self, x):\n        \"\"\" set bend length\n\n        :param x: new bend length to be assigned, [m]\n        :return: None\n        \"\"\"\n", "input": "", "output": "        if x != self.bend_length:\n            self.bend_length = x\n            self.refresh = True"}, {"instruction": "def rescale_gradients(self, scale: float):\n        \"\"\"\n        Rescales gradient arrays of executors by scale.\n        \"\"\"\n", "input": "", "output": "        for exe in self.executors:\n            for arr in exe.grad_arrays:\n                if arr is None:\n                    continue\n                arr *= scale"}, {"instruction": "def preprocess_input(userinput):\n  \"\"\"\n  <Purpose>\n    Preprocess the raw command line input string.\n\n  <Arguments>\n    The raw command line input string.  We assume it is pre-stripped.\n\n  <Side Effects>\n    The string will be processed by each module that has a defined preprocessor.\n\n  <Exceptions>\n    None\n\n  <Returns>\n    The preprocessed string.\n  \"\"\"\n", "input": "", "output": "  for module in get_enabled_modules():\n    # Not every module has a preprocessor...\n    if 'input_preprocessor' in module_data[module]:\n      userinput = module_data[module]['input_preprocessor'](userinput)\n  return userinput"}, {"instruction": "def untrace_method(cls, method):\n    \"\"\"\n    Untraces given class method.\n\n    :param cls: Class of the method.\n    :type cls: object\n    :param method: Method to untrace.\n    :type method: object\n    :return: Definition success.\n    :rtype: bool\n    \"\"\"\n", "input": "", "output": "\n    if not is_traced(method):\n        return False\n\n    name = get_method_name(method)\n    if is_class_method(method):\n        setattr(cls, name, classmethod(untracer(method)))\n    elif is_static_method(method):\n        setattr(cls, name, staticmethod(untracer(method)))\n    else:\n        setattr(cls, name, untracer(method))\n    return True"}, {"instruction": "def _connect_control_flow_node(control_flow_node, next_node):\n    \"\"\"Connect a ControlFlowNode properly to the next_node.\"\"\"\n", "input": "", "output": "    for last in control_flow_node.last_nodes:\n        if isinstance(next_node, ControlFlowNode):\n            last.connect(next_node.test)  # connect to next if test case\n        elif isinstance(next_node, AssignmentCallNode):\n            call_node = next_node.call_node\n            inner_most_call_node = _get_inner_most_function_call(call_node)\n            last.connect(inner_most_call_node)\n        else:\n            last.connect(next_node)"}, {"instruction": "def calculate_lyapunov(self):\n        \"\"\"\n        Return the current Lyapunov Characteristic Number (LCN).\n        Note that you need to call init_megno() before the start of the simulation.\n        To get a timescale (the Lyapunov timescale), take the inverse of this quantity.\n        \"\"\"\n", "input": "", "output": "        if self._calculate_megno==0:\n            raise RuntimeError(\"Lyapunov Characteristic Number cannot be calculated. Make sure to call init_megno() after adding all particles but before integrating the simulation.\")\n\n        clibrebound.reb_tools_calculate_lyapunov.restype = c_double\n        return clibrebound.reb_tools_calculate_lyapunov(byref(self))"}, {"instruction": "def setDocuments(self, documenting_pid, documented_pid):\n        \"\"\"Add a CiTO, the Citation Typing Ontology, triple asserting that\n        ``documenting_pid`` documents ``documented_pid``.\n\n        Adds assertion: ``documenting_pid cito:documents documented_pid``\n\n        Args:\n          documenting_pid: str\n            PID of a Science Object that documents ``documented_pid``.\n\n          documented_pid: str\n            PID of a Science Object that is documented by ``documenting_pid``.\n\n        \"\"\"\n", "input": "", "output": "        self._check_initialized()\n        documenting_id = self.getObjectByPid(documenting_pid)\n        documented_id = self.getObjectByPid(documented_pid)\n        self.add((documenting_id, CITO.documents, documented_id))"}, {"instruction": "def login(container):\n    \"\"\"Log into container.\"\"\"\n", "input": "", "output": "    columns, lines = shutil.get_terminal_size()  # Temporary\n    try:\n        subprocess.check_call([\n            \"docker\", \"exec\",\n            \"--env\", f\"COLUMNS={str(columns)},LINES={str(lines)}\",  # Temporary\n            \"--env\", f\"LINES={str(lines)}\",  # Temporary\n            \"--interactive\",\n            \"--tty\",\n            container,\n            \"bash\",\n            \"--login\"\n        ])\n    except subprocess.CalledProcessError:\n        raise RuntimeError() from None"}, {"instruction": "def serialize(self, outbuffer):\n\t\t\"\"\"Serialize this OmapiStartupMessage to the given outbuffer.\n\t\t@type outbuffer: OutBuffer\n\t\t\"\"\"\n", "input": "", "output": "\t\toutbuffer.add_net32int(self.protocol_version)\n\t\toutbuffer.add_net32int(self.header_size)"}, {"instruction": "def generate(env):\n    \"\"\"Add Builders and construction variables for the OS/2 to an Environment.\"\"\"\n", "input": "", "output": "    cc.generate(env)\n\n    env['CC']         = 'icc'\n    env['CCCOM']      = '$CC $CFLAGS $CCFLAGS $CPPFLAGS $_CPPDEFFLAGS $_CPPINCFLAGS /c $SOURCES /Fo$TARGET'\n    env['CXXCOM']     = '$CXX $CXXFLAGS $CPPFLAGS $_CPPDEFFLAGS $_CPPINCFLAGS /c $SOURCES /Fo$TARGET'\n    env['CPPDEFPREFIX']  = '/D'\n    env['CPPDEFSUFFIX']  = ''\n    env['INCPREFIX']  = '/I'\n    env['INCSUFFIX']  = ''\n    env['CFILESUFFIX'] = '.c'\n    env['CXXFILESUFFIX'] = '.cc'"}, {"instruction": "def from_base(cls, base, repo):\n        \"\"\"\n        Create a :class:`DXF` object which uses the same host, settings and\n        session as an existing :class:`DXFBase` object.\n\n        :param base: Existing :class:`DXFBase` object.\n        :type base: :class:`DXFBase`\n\n        :param repo: Name of the repository to access on the registry. Typically this is of the form ``username/reponame`` but for your own registries you don't actually have to stick to that.\n        :type repo: str\n\n        :returns: :class:`DXF` object which shares configuration and session with ``base`` but which can also be used to operate on the ``repo`` repository.\n        :rtype: :class:`DXF`\n        \"\"\"\n", "input": "", "output": "        # pylint: disable=protected-access\n        r = cls(base._host, repo, base._auth, base._insecure, base._auth_host, base._tlsverify)\n        r._token = base._token\n        r._headers = base._headers\n        r._sessions = [base._sessions[0]]\n        return r"}, {"instruction": "def kml_network_link(href, name=None, region_coords=None, visible=True):\n    \"\"\"\n    Create the KML <NetworkLink> Tag for a certain Region in the RegionGrid.\n\n    Args:\n        region_coords (RegionCoordinate):\n        href (str): the href attribute of the NetworkLink\n        name (str): KML <name>\n        visible (bool): If true the network link will appear as 'visible'\n            (i.e. checked) in Google Earth.\n\n    Returns:\n        KMLElement: the KML <NetworkLink>\n\n    \"\"\"\n", "input": "", "output": "    nl = KML.NetworkLink()\n    if name is None and region_coords is not None:\n        name = kml_element_name(region_coords, \"NL\")\n    if name is not None:\n        nl.append(KML.name(name))\n    if region_coords is not None:\n        min_lod_pixels = DEFAULT_MIN_LOD_PIXELS * (2 ** region_coords.log_tiles_per_row)\n        nl.append(kml_region(region_coords, min_lod_pixels=min_lod_pixels))\n    if not visible:\n        nl.append(KML.visibility(0))\n\n    nl.append(KML.Link(\n        KML.href(href), KML.viewRefreshMode(\"onRegion\")))\n\n    return nl"}, {"instruction": "def _sanitize_to_unicode(obj):\n        \"\"\"Convert all strings records of the object to unicode\n\n        :param obj: object to sanitize to unicode.\n        :type obj: object\n\n        :return: Unicode string representation of the given object.\n        :rtype: str\n        \"\"\"\n", "input": "", "output": "        if isinstance(obj, dict):\n            return dict((BaseGELFHandler._sanitize_to_unicode(k), BaseGELFHandler._sanitize_to_unicode(v)) for k, v in obj.items())\n        if isinstance(obj, (list, tuple)):\n            return obj.__class__([BaseGELFHandler._sanitize_to_unicode(i) for i in obj])\n        if isinstance(obj, data):\n            obj = obj.decode('utf-8', errors='replace')\n        return obj"}, {"instruction": "def synchronized(wrapped):\n    \"\"\"The missing @synchronized decorator\n\n    https://git.io/vydTA\"\"\"\n", "input": "", "output": "    _lock = threading.RLock()\n\n    @functools.wraps(wrapped)\n    def _wrapper(*args, **kwargs):\n        with _lock:\n            return wrapped(*args, **kwargs)\n    return _wrapper"}, {"instruction": "def delete_dataset(self, dataset_name):\n        \"\"\" Deletes the dataset having the specified name\n\n        :param dataset_name: the name that the dataset has on the repository\n        :return: None\n        \"\"\"\n", "input": "", "output": "        url = self.address + \"/datasets/\" + dataset_name\n        header = self.__check_authentication()\n        response = requests.delete(url, headers=header)\n        if response.status_code != 200:\n            raise ValueError(\"Code {}: {}\".format(response.status_code, response.json().get(\"error\")))\n        self.logger.debug(\"Dataset {} was deleted from the repository\".format(dataset_name))"}, {"instruction": "def _reduce_boolean_pair(self, config_dict, key1, key2):\n        \"\"\"Ensure only one key with a boolean value is present in dict.\n\n        :param config_dict: dict -- dictionary of config or kwargs\n        :param key1: string -- first key name\n        :param key2: string -- second key name\n        :raises: BooleansToReduceHaveSameValue\n        \"\"\"\n", "input": "", "output": "\n        if key1 in config_dict and key2 in config_dict \\\n                and config_dict[key1] == config_dict[key2]:\n            msg = 'Boolean pair, %s and %s, have same value: %s. If both ' \\\n                'are given to this method, they cannot be the same, as this ' \\\n                'method cannot decide which one should be True.' \\\n                % (key1, key2, config_dict[key1])\n            raise BooleansToReduceHaveSameValue(msg)\n        elif key1 in config_dict and not config_dict[key1]:\n            config_dict[key2] = True\n            config_dict.pop(key1)\n        elif key2 in config_dict and not config_dict[key2]:\n            config_dict[key1] = True\n            config_dict.pop(key2)\n        return config_dict"}, {"instruction": "async def invites(self):\n        \"\"\"|coro|\n\n        Returns a list of all active instant invites from the guild.\n\n        You must have the :attr:`~Permissions.manage_guild` permission to get\n        this information.\n\n        Raises\n        -------\n        Forbidden\n            You do not have proper permissions to get the information.\n        HTTPException\n            An error occurred while fetching the information.\n\n        Returns\n        -------\n        List[:class:`Invite`]\n            The list of invites that are currently active.\n        \"\"\"\n", "input": "", "output": "\n        data = await self._state.http.invites_from(self.id)\n        result = []\n        for invite in data:\n            channel = self.get_channel(int(invite['channel']['id']))\n            invite['channel'] = channel\n            invite['guild'] = self\n            result.append(Invite(state=self._state, data=invite))\n\n        return result"}, {"instruction": "def _titan_cn_file(cnr_file, work_dir, data):\n    \"\"\"Convert CNVkit or GATK4 normalized input into TitanCNA ready format.\n    \"\"\"\n", "input": "", "output": "    out_file = os.path.join(work_dir, \"%s.cn\" % (utils.splitext_plus(os.path.basename(cnr_file))[0]))\n    support_cols = {\"cnvkit\": [\"chromosome\", \"start\", \"end\", \"log2\"],\n                    \"gatk-cnv\": [\"CONTIG\", \"START\", \"END\", \"LOG2_COPY_RATIO\"]}\n    cols = support_cols[cnvkit.bin_approach(data)]\n    if not utils.file_uptodate(out_file, cnr_file):\n        with file_transaction(data, out_file) as tx_out_file:\n            iterator = pd.read_table(cnr_file, sep=\"\\t\", iterator=True, header=0, comment=\"@\")\n            with open(tx_out_file, \"w\") as handle:\n                for chunk in iterator:\n                    chunk = chunk[cols]\n                    chunk.columns = [\"chrom\", \"start\", \"end\", \"logR\"]\n                    if cnvkit.bin_approach(data) == \"cnvkit\":\n                        chunk['start'] += 1\n                    chunk.to_csv(handle, mode=\"a\", sep=\"\\t\", index=False)\n    return out_file"}, {"instruction": "def get_point_source_fluxes(self, id, energies, tag=None):\n        \"\"\"\n        Get the fluxes from the id-th point source\n\n        :param id: id of the source\n        :param energies: energies at which you need the flux\n        :param tag: a tuple (integration variable, a, b) specifying the integration to perform. If this\n        parameter is specified then the returned value will be the average flux for the source computed as the integral\n        between a and b over the integration variable divided by (b-a). The integration variable must be an independent\n        variable contained in the model. If b is None, then instead of integrating the integration variable will be\n        set to a and the model evaluated in a.\n        :return: fluxes\n        \"\"\"\n", "input": "", "output": "\n        return self._point_sources.values()[id](energies, tag=tag)"}, {"instruction": "def intersection_update(self, *iterables):\n        \"\"\"\n        Update the set, keeping only elements found in it and all *iterables*.\n        \"\"\"\n", "input": "", "output": "        _set = self._set\n        _list = self._list\n        _set.intersection_update(*iterables)\n        _list.clear()\n        _list.update(_set)\n        return self"}, {"instruction": "def copyidfobject(self, idfobject):\n        \"\"\"Add an IDF object to the IDF.\n\n        Parameters\n        ----------\n        idfobject : EpBunch object\n            The IDF object to remove. This usually comes from another idf file,\n            or it can be used to copy within this idf file.\n\n        \"\"\"\n", "input": "", "output": "        return addthisbunch(self.idfobjects,\n                     self.model,\n                     self.idd_info,\n                     idfobject, self)"}, {"instruction": "def wsgi(self, environ, start_response):\n        \"\"\"Implements the mapper's WSGI interface.\"\"\"\n", "input": "", "output": "        request = Request(environ)\n        ctx = Context(request)\n        try:\n            try:\n                response = self(request, ctx)\n                ctx._run_callbacks('finalize', (request, response))\n                response = response.conditional_to(request)\n            except HTTPException as e:\n                response = e.response\n            except Exception:\n                self.handle_error(request, ctx)\n                response = InternalServerError().response\n\n            response.add_callback(lambda: ctx._run_callbacks('close'))\n            return response(environ, start_response)\n        finally:\n            ctx._run_callbacks('teardown', log_errors=True)"}, {"instruction": "def format_command(\n    command_args,  # type: List[str]\n    command_output,  # type: str\n):\n    # type: (...) -> str\n    \"\"\"\n    Format command information for logging.\n    \"\"\"\n", "input": "", "output": "    text = 'Command arguments: {}\\n'.format(command_args)\n\n    if not command_output:\n        text += 'Command output: None'\n    elif logger.getEffectiveLevel() > logging.DEBUG:\n        text += 'Command output: [use --verbose to show]'\n    else:\n        if not command_output.endswith('\\n'):\n            command_output += '\\n'\n        text += (\n            'Command output:\\n{}'\n            '-----------------------------------------'\n        ).format(command_output)\n\n    return text"}, {"instruction": "def get_creators(self, *args, **kwargs):\n        \"\"\"Fetches lists of creators.\n        \n        get /v1/public/creators\n        \n        :returns:  CreatorDataWrapper\n\n        >>> m = Marvel(public_key, private_key)\n        >>> cdw = m.get_creators(lastName=\"Lee\", orderBy=\"firstName,-modified\", limit=\"5\", offset=\"15\")\n        >>> print cdw.data.total\n        25\n        >>> print cdw.data.results[0].fullName\n        Alvin Lee\n        \"\"\"\n", "input": "", "output": "        \n        response = json.loads(self._call(Creator.resource_url(), self._params(kwargs)).text)\n        return CreatorDataWrapper(self, response)"}, {"instruction": "def _unparse_a_tags(cls, attrs_dict):\n        \"\"\" Iterates over the dictionary\n\n        :param: attrs_dict a dict of attributes\n        :returns:   a SimpleXMLElement list containing <a> tags\n        \"\"\"\n", "input": "", "output": "        prop_tags = []\n\n        for k, v in attrs_dict.items():\n            node = {cls.ATTRNAME_PROPERTY: k, '_content': utils.auto_type(v)}\n            prop_tags.append(node)\n\n        return prop_tags"}, {"instruction": "def json_api_call(req_function):\n    \"\"\" Wrap a view-like function that returns an object that\n        is convertable from json\n    \"\"\"\n", "input": "", "output": "    @wraps(req_function)\n    def newreq(request, *args, **kwargs):\n        outp = req_function(request, *args, **kwargs)\n        if issubclass(outp.__class__, HttpResponse):\n            return outp\n        else:\n            return '%s' % json.dumps(outp, cls=LazyEncoder)\n    return string_to_response(\"application/json\")(newreq)"}, {"instruction": "def tracked(self):\n        '''Return an array of job objects that are being tracked'''\n", "input": "", "output": "        results = json.loads(self.client('track'))\n        results['jobs'] = [Job(self, **job) for job in results['jobs']]\n        return results"}, {"instruction": "def do_gen(argdict):\n    '''Generate the whole site.'''\n", "input": "", "output": "    site = make_site_obj(argdict)\n    try:\n        st = time.time()\n        site.generate()\n        et = time.time()\n        print \"Generated Site in %f seconds.\"% (et-st)\n    except ValueError as e: # pragma: no cover\n        print \"Cannot generate. You are not within a simplystatic \\\ntree and you didn't specify a directory.\""}, {"instruction": "def next(self):\n        \"\"\"Next point in iteration\n        \"\"\"\n", "input": "", "output": "        if self.probability == 1:\n            x, y = next(self.scan)\n        else:\n            while True:\n                x, y = next(self.scan)\n                if random.random() <= self.probability: break\n        return x, y"}, {"instruction": "def make_private(self, client=None):\n        \"\"\"Update blob's ACL, revoking read access for anonymous users.\n\n        :type client: :class:`~google.cloud.storage.client.Client` or\n                      ``NoneType``\n        :param client: Optional. The client to use.  If not passed, falls back\n                       to the ``client`` stored on the blob's bucket.\n        \"\"\"\n", "input": "", "output": "        self.acl.all().revoke_read()\n        self.acl.save(client=client)"}, {"instruction": "def strip_spaces_and_quotes(value):\n    \"\"\"Remove invalid whitespace and/or single pair of dquotes and return None\n    for empty strings.\n\n    Used to prepare cookie values, path, and domain attributes in a way which\n    tolerates simple formatting mistakes and standards variations.\n    \"\"\"\n", "input": "", "output": "    value = value.strip() if value else \"\"\n    if value and len(value) > 1 and (value[0] == value[-1] == '\"'):\n        value = value[1:-1]\n    if not value:\n        value = \"\"\n    return value"}, {"instruction": "def _set_verbosity(self):\n        '''\n        Sets the appropriate verbosity.\n        Must be called after self._test_target_files so that self.target_files is properly set.\n        '''\n", "input": "", "output": "        # If more than one target file was specified, enable verbose mode; else, there is\n        # nothing in some outputs to indicate which scan corresponds to which\n        # file.\n        if len(self.target_files) > 1 and not self.verbose:\n            self.verbose = True"}, {"instruction": "def options(self, parser, env):\n        \"\"\"Register commandline options.\n        \"\"\"\n", "input": "", "output": "        parser.add_option(\n            \"--epdb\", action=\"store_true\", dest=\"epdb_debugErrors\",\n            default=env.get('NOSE_EPDB', False),\n            help=\"Drop into extended debugger on errors\")\n        parser.add_option(\n            \"--epdb-failures\", action=\"store_true\",\n            dest=\"epdb_debugFailures\",\n            default=env.get('NOSE_EPDB_FAILURES', False),\n            help=\"Drop into extended debugger on failures\")"}, {"instruction": "def computeDistortion(self, eEye, fU, fV):\n        \"\"\"\n        Gets the result of the distortion function for the specified eye and input UVs. UVs go from 0,0 in \n        the upper left of that eye's viewport and 1,1 in the lower right of that eye's viewport.\n        Returns true for success. Otherwise, returns false, and distortion coordinates are not suitable.\n        \"\"\"\n", "input": "", "output": "\n        fn = self.function_table.computeDistortion\n        pDistortionCoordinates = DistortionCoordinates_t()\n        result = fn(eEye, fU, fV, byref(pDistortionCoordinates))\n        return result, pDistortionCoordinates"}, {"instruction": "def _connectToWP(self):\n        \"\"\"Establish the actual TCP connection to Flickr\"\"\"\n", "input": "", "output": "\n        if self.connected_to_wp:\n            logger.debug(\"Already connected to wp\")\n            return True\n\n        # Load config from file\n        info=json.load(open(WP_LOGIN_FILE,'r'))\n        self.wp = Client(info['url'],\\\n                info['username'],\\\n                info['password'])\n\n        logger.debug(\"Connecting to wp\")\n\n        self.connected_to_wp=True\n\n        return True"}, {"instruction": "def _get_ds_descriptions_unsorted(\n            cls, data, ignore_keys=['attrs', 'plotter'], nums=None):\n        \"\"\"Recursive method to get all the file names or datasets out of a\n        dictionary `data` created with the :meth`array_info` method\"\"\"\n", "input": "", "output": "        ds_description = {'ds', 'fname', 'num', 'arr', 'store'}\n        if 'ds' in data:\n            # make sure that the data set has a number assigned to it\n            data['ds'].psy.num\n        keys_in_data = ds_description.intersection(data)\n        if keys_in_data:\n            return {key: data[key] for key in keys_in_data}\n        for key in ignore_keys:\n            data.pop(key, None)\n        func = partial(cls._get_ds_descriptions_unsorted,\n                       ignore_keys=ignore_keys, nums=nums)\n        return chain(*map(lambda d: [d] if isinstance(d, dict) else d,\n                          map(func, six.itervalues(data))))"}, {"instruction": "def build_blast_cmd(self, fname, dbname):\n        \"\"\"Return BLASTN command\"\"\"\n", "input": "", "output": "        return self.funcs.blastn_func(fname, dbname, self.outdir, self.exes.blast_exe)"}, {"instruction": "async def connect(self, timeout=None, ssl=None):\n        \"\"\"\n        Establishes a connection with the server.\n        \"\"\"\n", "input": "", "output": "        await self._connect(timeout=timeout, ssl=ssl)\n        self._connected = True\n\n        self._send_task = self._loop.create_task(self._send_loop())\n        self._recv_task = self._loop.create_task(self._recv_loop())"}, {"instruction": "def close(self):\n        \"\"\"\n        Close the file.\n        \"\"\"\n", "input": "", "output": "        if not self.closed:\n            self.closed = True\n            retval = self.f.close()\n            if self.base_mode != \"r\":\n                self.__size = self.fs.get_path_info(self.name)[\"size\"]\n            return retval"}, {"instruction": "def dedent(content):\n    \"\"\"\n    Remove leading indent from a block of text.\n    Used when generating descriptions from docstrings.\n\n    Note that python's `textwrap.dedent` doesn't quite cut it,\n    as it fails to dedent multiline docstrings that include\n    unindented text on the initial line.\n    \"\"\"\n", "input": "", "output": "    content = force_text(content)\n    whitespace_counts = [len(line) - len(line.lstrip(' '))\n                         for line in content.splitlines()[1:] if line.lstrip()]\n\n    # unindent the content if needed\n    if whitespace_counts:\n        whitespace_pattern = '^' + (' ' * min(whitespace_counts))\n        content = re.sub(re.compile(whitespace_pattern, re.MULTILINE), '', content)\n\n    return content.strip()"}, {"instruction": "def flatten_model(model):\n    \"\"\"Flatten a model to a list of models.\n    This is used to flatten a ``Binder``'ish model down to a list\n    of contained models.\n    \"\"\"\n", "input": "", "output": "    yield model\n    if isinstance(model, (TranslucentBinder, Binder,)):\n        for m in model:\n            # yield from flatten_model(m)\n            for x in flatten_model(m):\n                yield x"}, {"instruction": "def by_group_and_perm(cls, group_id, perm_name, db_session=None):\n        \"\"\"\n        return by by_user_and_perm and permission name\n\n        :param group_id:\n        :param perm_name:\n        :param db_session:\n        :return:\n        \"\"\"\n", "input": "", "output": "        db_session = get_db_session(db_session)\n        query = db_session.query(cls.model).filter(cls.model.group_id == group_id)\n        query = query.filter(cls.model.perm_name == perm_name)\n        return query.first()"}, {"instruction": "def isHereDoc(self, line, column):\n        \"\"\"Check if text at given position is a here document.\n\n        If language is not known, or text is not parsed yet, ``False`` is returned\n        \"\"\"\n", "input": "", "output": "        return self._highlighter is not None and \\\n               self._highlighter.isHereDoc(self.document().findBlockByNumber(line), column)"}, {"instruction": "def exclude_args(parser, args, excluded_args, target):\n    \"\"\"Delete options that are not appropriate for a following code path; exit\n    with an error if excluded options were passed in by the user.\n\n    argparse generates a namespace with all options it knows, but not every\n    attribute should be passed to all code paths (i.e. options about\n    interpolation should not reach `run_from_ufos()`). This function can be run\n    before entering a particular code path to clean up the kwargs passed to it.\n\n    Exit with an error message if the user actually passed the options in.\n    \"\"\"\n", "input": "", "output": "    msg = '\"%s\" option invalid for %s'\n    for argname in excluded_args:\n        if argname not in args:\n            continue\n        if args[argname]:\n            optname = \"--%s\" % argname.replace(\"_\", \"-\")\n            parser.error(msg % (optname, target))\n        del args[argname]"}, {"instruction": "def tr(text, kword, color):\n    \"\"\" tr(text, keyword, color)\n    \"\"\"\n", "input": "", "output": "    return re.sub(kword, colorize(BgColor.Null, Base.Null, color, kword), text)"}, {"instruction": "def gpib_command(self, session, command_byte):\n        \"\"\"Write GPIB command byte on the bus.\n\n        Corresponds to viGpibCommand function of the VISA library.\n        See: https://linux-gpib.sourceforge.io/doc_html/gpib-protocol.html#REFERENCE-COMMAND-BYTES\n\n        :param command_byte: command byte to send\n        :type command_byte: int, must be [0 255]\n        :return: return value of the library call\n        :rtype: :class:`pyvisa.constants.StatusCode`\n        \"\"\"\n", "input": "", "output": "        try:\n            return self.sessions[session].gpib_command(command_byte)\n\n        except KeyError:\n            return constants.StatusCode.error_invalid_object"}, {"instruction": "def update(self, td):\r\n        \"\"\"Update state of ball\"\"\"\n", "input": "", "output": "        self.sprite.last_position = self.sprite.position\r\n        self.sprite.last_velocity = self.sprite.velocity\r\n        if self.particle_group != None:\r\n            self.update_particle_group(td)"}, {"instruction": "def await_metadata_by_name(self, name, metadata_key, timeout, caster=None):\n    \"\"\"Block up to a timeout for process metadata to arrive on disk.\n\n    :param string name: The ProcessMetadataManager identity/name (e.g. 'pantsd').\n    :param string metadata_key: The metadata key (e.g. 'pid').\n    :param int timeout: The deadline to write metadata.\n    :param type caster: A type-casting callable to apply to the read value (e.g. int, str).\n    :returns: The value of the metadata key (read from disk post-write).\n    :raises: :class:`ProcessMetadataManager.Timeout` on timeout.\n    \"\"\"\n", "input": "", "output": "    file_path = self._metadata_file_path(name, metadata_key)\n    self._wait_for_file(file_path, timeout=timeout)\n    return self.read_metadata_by_name(name, metadata_key, caster)"}, {"instruction": "def _parse_ports(port_values: dict) -> dict:\n        \"\"\"Parse ports key.\n\n        Args:\n            port_values (dict): ports configuration values\n\n        Returns:\n            dict, Ports specification which contains exposed ports\n\n        \"\"\"\n", "input": "", "output": "        # Initialising empty dictionary\n        endpoints = {}\n\n        for port_element in port_values:\n            target_port = port_element.split(':')\n            for port in target_port:\n                endpoints[int(port)] = int(port)\n\n        # Setting the types\n        endpoint_spec = docker.types.EndpointSpec(ports=endpoints)\n        return endpoint_spec"}, {"instruction": "def p_field_optional1_3(self, p):\n        \"\"\"\n        field : alias name arguments selection_set\n        \"\"\"\n", "input": "", "output": "        p[0] = Field(name=p[2], alias=p[1], arguments=p[3], selections=p[4])"}, {"instruction": "def _to_docstring(doc):\n    \"\"\"\n    format from Markdown to docstring\n    \"\"\"\n", "input": "", "output": "\n    def format_fn(line, status):\n        "}, {"instruction": "def _surfdens(self,R,z,phi=0.,t=0.):\n        \"\"\"\n        NAME:\n           _surfdens\n        PURPOSE:\n           evaluate the surface density for this potential\n        INPUT:\n           R - Galactocentric cylindrical radius\n           z - vertical height\n           phi - azimuth\n           t - time\n        OUTPUT:\n           the surface density\n        HISTORY:\n           2018-08-19 - Written - Bovy (UofT)\n        \"\"\"\n", "input": "", "output": "        return 2.*integrate.quad(lambda x: self._dens(R,x,phi=phi,t=t),0,z)[0]"}, {"instruction": "def _import_lua_dependencies(lua, lua_globals):\n        \"\"\"\n        Imports lua dependencies that are supported by redis lua scripts.\n\n        The current implementation is fragile to the target platform and lua version\n        and may be disabled if these imports are not needed.\n\n        Included:\n            - cjson lib.\n        Pending:\n            - base lib.\n            - table lib.\n            - string lib.\n            - math lib.\n            - debug lib.\n            - cmsgpack lib.\n        \"\"\"\n", "input": "", "output": "        if sys.platform not in ('darwin', 'windows'):\n            import ctypes\n            ctypes.CDLL('liblua5.2.so', mode=ctypes.RTLD_GLOBAL)\n\n        try:\n            lua_globals.cjson = lua.eval('require \"cjson\"')\n        except RuntimeError:\n            raise RuntimeError(\"cjson not installed\")"}, {"instruction": "def contains_one_of(self, elements):\n        \"\"\"\n        Ensures :attr:`subject` contains exactly one of *elements*, which must be an iterable.\n        \"\"\"\n", "input": "", "output": "        if sum(e in self._subject for e in elements) != 1:\n            raise self._error_factory(_format(\"Expected {} to have exactly one of {}\", self._subject, elements))\n        return ChainInspector(self._subject)"}, {"instruction": "def _keygen(self, event, ts=None):\n        \"\"\"Generate redis key for event at timestamp.\n\n        :param event: event name\n        :param ts: timestamp, default to current timestamp if left as None\n        \"\"\"\n", "input": "", "output": "        return \"%s:%s\" % (self.namespace(ts or time.time()), event)"}, {"instruction": "def get_date_of_author(_id):\n    \"\"\"Pass author id and return the name of its associated date.\"\"\"\n", "input": "", "output": "    _dict = get_date_author()\n    for date, ids in _dict.items():\n        if _id in ids:\n            return date\n    return None"}, {"instruction": "def card_auth(self, auth_mode, block_address, key, uid):\n        \"\"\"\n        Authenticates to use specified block address. Tag must be selected using select_tag(uid) before auth.\n        auth_mode -- RFID.auth_a or RFID.auth_b\n        key -- list or tuple with six bytes key\n        uid -- list or tuple with four bytes tag ID\n        Returns error state.\n        \"\"\"\n", "input": "", "output": "        buf = []\n        buf.append(auth_mode)\n        buf.append(block_address)\n\n        for i in range(len(key)):\n            buf.append(key[i])\n\n        for i in range(4):\n            buf.append(uid[i])\n\n        (error, back_data, back_length) = self.card_write(self.mode_auth, buf)\n        if not (self.dev_read(0x08) & 0x08) != 0:\n            error = True\n\n        if not error:\n            self.authed = True\n\n        return error"}, {"instruction": "def kernel_restarted_message(self, msg):\r\n        \"\"\"Show kernel restarted/died messages.\"\"\"\n", "input": "", "output": "        if not self.is_error_shown:\r\n            # If there are kernel creation errors, jupyter_client will\r\n            # try to restart the kernel and qtconsole prints a\r\n            # message about it.\r\n            # So we read the kernel's stderr_file and display its\r\n            # contents in the client instead of the usual message shown\r\n            # by qtconsole.\r\n            try:\r\n                stderr = self._read_stderr()\r\n            except Exception:\r\n                stderr = None\r\n            if stderr:\r\n                self.show_kernel_error('<tt>%s</tt>' % stderr)\r\n        else:\r\n            self.shellwidget._append_html(\"<br>%s<hr><br>\" % msg,\r\n                                          before_prompt=False)"}, {"instruction": "def p_do_loop_while(p):\n    \"\"\" statement : do_start program_co label_loop WHILE expr\n                  | do_start label_loop WHILE expr\n                  | DO label_loop WHILE expr\n    \"\"\"\n", "input": "", "output": "    if len(p) == 6:\n        q = make_block(p[2], p[3])\n        r = p[5]\n    else:\n        q = p[2]\n        r = p[4]\n\n    if p[1] == 'DO':\n        gl.LOOPS.append(('DO',))\n\n    p[0] = make_sentence('DO_WHILE', r, q)\n    gl.LOOPS.pop()\n\n    if is_number(r):\n        api.errmsg.warning_condition_is_always(p.lineno(3), bool(r.value))\n    if q is None:\n        api.errmsg.warning_empty_loop(p.lineno(3))"}, {"instruction": "def as_cwd():\n    \"\"\" Use workdir.options.path as a temporary working directory \"\"\"\n", "input": "", "output": "    _set_log_level()\n    owd = os.getcwd()\n    logger.debug('entering working directory: ' + options.path)\n    os.chdir(os.path.expanduser(options.path))\n    yield\n    logger.debug('returning to original directory: ' + owd)\n    os.chdir(owd)"}, {"instruction": "def output_size(self) -> Tuple[Sequence[Shape], Sequence[Shape], Sequence[Shape], int]:\n        '''Returns the simulation output size.'''\n", "input": "", "output": "        return self._cell.output_size"}, {"instruction": "def serialize(self):\n        \"\"\" Get the serialized Dynamo format for the update \"\"\"\n", "input": "", "output": "        if self.action == 'Create':\n            payload = self.extra['index'].schema()\n        else:\n            payload = {\n                'IndexName': self.index_name,\n            }\n            if self.action == 'Update':\n                payload['ProvisionedThroughput'] = \\\n                    self.extra['throughput'].schema()\n        return {\n            self.action: payload\n        }"}, {"instruction": "def get_intercom_data(self):\n        \"\"\"Specify the data sent to Intercom API according to event type\"\"\"\n", "input": "", "output": "        data = {\n            \"event_name\": self.get_type_display(),  # event type\n            \"created_at\": calendar.timegm(self.created.utctimetuple()),  # date\n            \"metadata\": self.metadata\n        }\n        if self.user:\n            data[\"user_id\"] = self.user.intercom_id\n        return data"}, {"instruction": "def get_trunk_interfaces(devId):\n    \"\"\"Function takes devId as input to RESTFULL call to HP IMC platform\n    :param devId: output of get_dev_details\n    :return: list of dictionaries containing of interfaces configured as an 802.1q trunk\n    \"\"\"\n", "input": "", "output": "\n    # checks to see if the imc credentials are already available\n    if auth is None or url is None:\n        set_imc_creds()\n    global r\n    get_trunk_interfaces_url = \"/imcrs/vlan/trunk?devId=\" + str(devId) + \"&start=1&size=5000&total=false\"\n    f_url = url + get_trunk_interfaces_url\n    payload = None\n    # creates the URL using the payload variable as the contents\n    r = requests.get(f_url, auth=auth, headers=headers)\n    # r.status_code\n    if r.status_code == 200:\n        dev_trunk_interfaces = (json.loads(r.text))\n        if len(dev_trunk_interfaces) == 2:\n            return dev_trunk_interfaces['trunkIf']\n        else:\n            dev_trunk_interfaces['trunkIf'] = [\"No trunk inteface\"]\n            return dev_trunk_interfaces['trunkIf']"}, {"instruction": "def union(*argv):\n        \"\"\"Returns union of sets as a new set. basically it's\n        Items are ordered by set1, set2, ...\n        \n        **\u4e2d\u6587\u6587\u6863**\n        \n        \u6c42\u591a\u4e2a\u6709\u5e8f\u96c6\u5408\u7684\u5e76\u96c6, \u6309\u7167\u7b2c\u4e00\u4e2a\u96c6\u5408, \u7b2c\u4e8c\u4e2a, ..., \u8fd9\u6837\u7684\u987a\u5e8f\u3002\n        \"\"\"\n", "input": "", "output": "        res = OrderedSet()\n        for ods in argv:\n            res = res | ods\n        return res"}, {"instruction": "def dist(self, x1, x2):\n        \"\"\"Return the distance between ``x1`` and ``x2``.\n\n        Parameters\n        ----------\n        x1, x2 : `LinearSpaceElement`\n            Elements whose distance to compute.\n\n        Returns\n        -------\n        dist : float\n            Distance between ``x1`` and ``x2``.\n        \"\"\"\n", "input": "", "output": "        if x1 not in self:\n            raise LinearSpaceTypeError('`x1` {!r} is not an element of '\n                                       '{!r}'.format(x1, self))\n        if x2 not in self:\n            raise LinearSpaceTypeError('`x2` {!r} is not an element of '\n                                       '{!r}'.format(x2, self))\n        return float(self._dist(x1, x2))"}, {"instruction": "def getHeader(self):\n        \"\"\"\n        Returns the file header as dict\n\n        Parameters\n        ----------\n        None\n        \"\"\"\n", "input": "", "output": "        return {\"technician\": self.getTechnician(), \"recording_additional\": self.getRecordingAdditional(),\n                \"patientname\": self.getPatientName(), \"patient_additional\": self.getPatientAdditional(),\n                \"patientcode\": self.getPatientCode(), \"equipment\": self.getEquipment(),\n                \"admincode\": self.getAdmincode(), \"gender\": self.getGender(), \"startdate\": self.getStartdatetime(),\n                \"birthdate\": self.getBirthdate()}"}, {"instruction": "def comment (self, data):\n        \"\"\"\n        Print HTML comment.\n\n        @param data: the comment\n        @type data: string\n        @return: None\n        \"\"\"\n", "input": "", "output": "        data = data.encode(self.encoding, \"ignore\")\n        self.fd.write(\"<!--%s-->\" % data)"}, {"instruction": "def runUAT(self, args):\n\t\t\"\"\"\n\t\tRuns the Unreal Automation Tool with the supplied arguments\n\t\t\"\"\"\n", "input": "", "output": "\t\tUtility.run([self.getRunUATScript()] + args, cwd=self.getEngineRoot(), raiseOnError=True)"}, {"instruction": "def create_media_asset(access_token, name, options=\"0\"):\n    '''Create Media Service Asset.\n\n    Args:\n        access_token (str): A valid Azure authentication token.\n        name (str): Media Service Asset Name.\n        options (str): Media Service Options.\n\n    Returns:\n        HTTP response. JSON body.\n    '''\n", "input": "", "output": "    path = '/Assets'\n    endpoint = ''.join([ams_rest_endpoint, path])\n    body = '{\"Name\": \"' + name + '\", \"Options\": \"' + str(options) + '\"}'\n    return do_ams_post(endpoint, path, body, access_token)"}, {"instruction": "def untlpydict2dcformatteddict(untl_dict, **kwargs):\n    \"\"\"Convert a UNTL data dictionary to a formatted DC data dictionary.\"\"\"\n", "input": "", "output": "    ark = kwargs.get('ark', None)\n    domain_name = kwargs.get('domain_name', None)\n    scheme = kwargs.get('scheme', 'http')\n    resolve_values = kwargs.get('resolve_values', None)\n    resolve_urls = kwargs.get('resolve_urls', None)\n    verbose_vocabularies = kwargs.get('verbose_vocabularies', None)\n    # Get the UNTL object.\n    untl_py = untldict2py(untl_dict)\n    # Convert it to a DC object.\n    dc_py = untlpy2dcpy(\n        untl_py,\n        ark=ark,\n        domain_name=domain_name,\n        resolve_values=resolve_values,\n        resolve_urls=resolve_urls,\n        verbose_vocabularies=verbose_vocabularies,\n        scheme=scheme\n    )\n    # Return a formatted DC dictionary.\n    return dcpy2formatteddcdict(dc_py)"}, {"instruction": "def p_class_declaration_statement(p):\n    '''class_declaration_statement : class_entry_type STRING extends_from implements_list LBRACE class_statement_list RBRACE\n                                   | INTERFACE STRING interface_extends_list LBRACE class_statement_list RBRACE'''\n", "input": "", "output": "    if len(p) == 8:\n        p[0] = ast.Class(p[2], p[1], p[3], p[4], p[6], lineno=p.lineno(2))\n    else:\n        p[0] = ast.Interface(p[2], p[3], p[5], lineno=p.lineno(1))"}, {"instruction": "def handle(self, *args, **options):\n        \"\"\"Django command handler.\"\"\"\n", "input": "", "output": "        self.verbosity = int(options.get('verbosity'))\n        self.quiet = options.get('quiet')\n        self._set_logger_level()\n\n        self.servername = options.get('servername')\n        self.decrypt = options.get('decrypt')\n        self.uncompress = options.get('uncompress')\n\n        self.filename = options.get('input_filename')\n        self.path = options.get('input_path')\n\n        self.replace = options.get('replace')\n        self.passphrase = options.get('passphrase')\n        self.interactive = options.get('interactive')\n\n        self.storage = get_storage()\n        self.media_storage = get_storage_class()()\n        self._restore_backup()"}, {"instruction": "def dump_conndata(self):\n        \"\"\"Developer tool for debugging forensics.\"\"\"\n", "input": "", "output": "        attrs = vars(self)\n        return ', '.join(\"%s: %s\" % item for item in attrs.items())"}, {"instruction": "def _get_iterator(self):\n        \"\"\"The iterator passed in can take several forms: a class that can be\n        instantiated and then iterated over; a function that when called\n        returns an iterator; an actual iterator/generator or an iterable\n        collection.  This function sorts all that out and returns an iterator\n        that can be used\"\"\"\n", "input": "", "output": "        try:\n            return self.job_param_source_iter(self.config)\n        except TypeError:\n            try:\n                return self.job_param_source_iter()\n            except TypeError:\n                return self.job_param_source_iter"}, {"instruction": "def _build_url(self):\n        \"\"\"Build url based on searching by date or by show.\"\"\"\n", "input": "", "output": "        url_params = [\n            BASE_URL, self.category + ' ratings', self.day, self.year, self.month\n        ]\n\n        return SEARCH_URL.format(*url_params)"}, {"instruction": "def _select_row_by_column_value(tree_view, list_store, column, value):\n        \"\"\"Helper method to select a tree view row\n\n        :param Gtk.TreeView tree_view: Tree view who's row is to be selected\n        :param Gtk.ListStore list_store: List store of the tree view\n        :param int column: Column in which the value is searched\n        :param value: Value to search for\n        :returns: Row of list store that has selected\n        :rtype: int\n        \"\"\"\n", "input": "", "output": "        for row_num, iter_elem in enumerate(list_store):\n            if iter_elem[column] == value:\n                tree_view.set_cursor(row_num)\n                return row_num"}, {"instruction": "def paths_from_iddname(iddname):\n    \"\"\"Get the EnergyPlus install directory and executable path.\n\n    Parameters\n    ----------\n    iddname : str, optional\n        File path to the IDD.\n\n    Returns\n    -------\n    eplus_exe : str\n        Full path to the EnergyPlus executable.\n    eplus_home : str\n        Full path to the EnergyPlus install directory.\n\n    Raises\n    ------\n    AttributeError (TypeError on Windows)\n        If iddname does not have a directory component (e.g. if None).\n    ValueError\n        If eplus_exe is not a file.\n\n    \"\"\"\n", "input": "", "output": "    eplus_home = os.path.abspath(os.path.dirname(iddname))\n    if platform.system() == 'Windows':\n        eplus_exe = os.path.join(eplus_home, 'energyplus.exe')\n    elif platform.system() == \"Linux\":\n        eplus_exe = os.path.join(eplus_home, 'energyplus')\n    else:\n        eplus_exe = os.path.join(eplus_home, 'energyplus')\n    if not os.path.isfile(eplus_exe):\n        raise ValueError\n    return eplus_exe, eplus_home"}, {"instruction": "def _full_pipeline(self):\n        \"\"\"Return the full aggregation pipeline for this ChangeStream.\"\"\"\n", "input": "", "output": "        options = self._pipeline_options()\n        full_pipeline = [{'$changeStream': options}]\n        full_pipeline.extend(self._pipeline)\n        return full_pipeline"}, {"instruction": "def iter(self, match=\"*\", count=1000):\n        \"\"\" Iterates the set of keys in :prop:key_prefix in :prop:_client\n            @match: #str pattern to match after the :prop:key_prefix\n            @count: the user specified the amount of work that should be done\n                at every call in order to retrieve elements from the collection\n\n            -> yields redis keys within this instance\n        \"\"\"\n", "input": "", "output": "        replace_this = self.key_prefix+\":\"\n        for key in self._client.scan_iter(\n           match=\"{}:{}\".format(self.key_prefix, match), count=count):\n            yield self._decode(key).replace(replace_this, \"\", 1)"}, {"instruction": "def get(self, *args, **kwargs):\n        \"\"\"\n        Works just like the default Manager's :func:`get` method, but\n        you can pass an additional keyword argument named ``path`` specifying\n        the full path of the object you want to retrieve, e.g.\n        ``\"path/to/folder/readme.txt\"``.\n        \"\"\"\n", "input": "", "output": "        if 'path' in kwargs:\n            kwargs = self.get_filter_args_with_path(True, **kwargs)\n        return super(FileNodeManager, self).get(\n            *args, **kwargs)"}, {"instruction": "async def on_raw_422(self, message):\n        \"\"\" MOTD is missing. \"\"\"\n", "input": "", "output": "        await self._registration_completed(message)\n        self.motd = None\n        await self.on_connect()"}, {"instruction": "def cube2matrix(data_cube):\n    r\"\"\"Cube to Matrix\n\n    This method transforms a 3D cube to a 2D matrix\n\n    Parameters\n    ----------\n    data_cube : np.ndarray\n        Input data cube, 3D array\n\n    Returns\n    -------\n    np.ndarray 2D matrix\n\n    Examples\n    --------\n    >>> from modopt.base.transform import cube2matrix\n    >>> a = np.arange(16).reshape((4, 2, 2))\n    >>> cube2matrix(a)\n    array([[ 0,  4,  8, 12],\n           [ 1,  5,  9, 13],\n           [ 2,  6, 10, 14],\n           [ 3,  7, 11, 15]])\n\n    \"\"\"\n", "input": "", "output": "\n    return data_cube.reshape([data_cube.shape[0]] +\n                             [np.prod(data_cube.shape[1:])]).T"}, {"instruction": "def missing_nodes(self):\n        \"\"\"The set of targets known as dependencies but not yet defined.\"\"\"\n", "input": "", "output": "        missing = set()\n        for target_addr, target_attrs in self.graph.node.items():\n            if 'target_obj' not in target_attrs:\n                missing.add(target_addr)\n        return missing"}, {"instruction": "def asset(self, asset_id, asset_type, action='GET'):\n        \"\"\"\n        Gets the asset with the provided id\n        Args:\n            asset_id: The id of the asset to be retrieved\n            asset_type: (str) Either PHONE, HANDLER, or URL\n            action:\n\n        Returns:\n\n        \"\"\"\n", "input": "", "output": "        if not self.can_update():\n            self._tcex.handle_error(910, [self.type])\n\n        if asset_type == 'PHONE':\n            return self.tc_requests.adversary_phone_asset(\n                self.api_type, self.api_sub_type, self.unique_id, asset_id, action=action\n            )\n        if asset_type == 'HANDLER':\n            return self.tc_requests.adversary_handle_asset(\n                self.api_type, self.api_sub_type, self.unique_id, asset_id, action=action\n            )\n        if asset_type == 'URL':\n            return self.tc_requests.adversary_url_asset(\n                self.api_type, self.api_sub_type, self.unique_id, asset_id, action=action\n            )\n        self._tcex.handle_error(\n            925, ['asset_type', 'assets', 'asset_type', 'asset_type', asset_type]\n        )\n        return None"}, {"instruction": "def parse_example_line(lisp_string: str) -> Dict:\n    \"\"\"\n    Training data in WikitableQuestions comes with examples in the form of lisp strings in the format:\n        (example (id <example-id>)\n                 (utterance <question>)\n                 (context (graph tables.TableKnowledgeGraph <table-filename>))\n                 (targetValue (list (description <answer1>) (description <answer2>) ...)))\n\n    We parse such strings and return the parsed information here.\n    \"\"\"\n", "input": "", "output": "    id_piece, rest = lisp_string.split(') (utterance \"')\n    example_id = id_piece.split('(id ')[1]\n    question, rest = rest.split('\") (context (graph tables.TableKnowledgeGraph ')\n    table_filename, rest = rest.split(')) (targetValue (list')\n    target_value_strings = rest.strip().split(\"(description\")\n    target_values = []\n    for string in target_value_strings:\n        string = string.replace(\")\", \"\").replace('\"', '').strip()\n        if string != \"\":\n            target_values.append(string)\n    return {'id': example_id,\n            'question': question,\n            'table_filename': table_filename,\n            'target_values': target_values}"}, {"instruction": "def split_filename(filename):\n    \"\"\"\n    Received a standard style rpm fullname and returns\n    name, version, release, epoch, arch\n    Example: foo-1.0-1.i386.rpm returns foo, 1.0, 1, i386\n             1:bar-9-123a.ia64.rpm returns bar, 9, 123a, 1, ia64\n\n    This function replaces rpmUtils.miscutils.splitFilename, see\n    https://bugzilla.redhat.com/1452801\n    \"\"\"\n", "input": "", "output": "\n    # Remove .rpm suffix\n    if filename.endswith('.rpm'):\n        filename = filename.split('.rpm')[0]\n\n    # is there an epoch?\n    components = filename.split(':')\n    if len(components) > 1:\n        epoch = components[0]\n    else:\n        epoch = ''\n\n    # Arch is the last item after .\n    arch = filename.rsplit('.')[-1]\n    remaining = filename.rsplit('.%s' % arch)[0]\n    release = remaining.rsplit('-')[-1]\n    version = remaining.rsplit('-')[-2]\n    name = '-'.join(remaining.rsplit('-')[:-2])\n\n    return name, version, release, epoch, arch"}, {"instruction": "def ngrok_url():\n    \"\"\"\n    If ngrok is running, it exposes an API on port 4040. We can use that\n    to figure out what URL it has assigned, and suggest that to the user.\n    https://ngrok.com/docs#list-tunnels\n    \"\"\"\n", "input": "", "output": "    try:\n        ngrok_resp = requests.get(\"http://localhost:4040/api/tunnels\")\n    except requests.ConnectionError:\n        # I guess ngrok isn't running.\n        return None\n    ngrok_data = ngrok_resp.json()\n    secure_urls = [\n        tunnel[\"public_url\"]\n        for tunnel in ngrok_data[\"tunnels\"]\n        if tunnel[\"proto\"] == \"https\"\n    ]\n    return secure_urls[0]"}, {"instruction": "def delete_assessment(self, assessment):\n        \"\"\"\n        To delete a Assessment\n        :param assessment: string\n        \"\"\"\n", "input": "", "output": "        response = self.http.delete('/Assessment/' + str(assessment))\n        return response"}, {"instruction": "def check_int(self, param, error_msg):\n        \"\"\"\n        This function check if the parameter is int.\n        If yes, the function returns the parameter,\n        if not, it raises error message.\n        \n        **Args:**\n        \n        * `param` : parameter to check (int or similar)\n\n        * `error_ms` : lowest allowed value (int), or None        \n        \n        **Returns:**\n        \n        * `param` : parameter (int)\n        \"\"\"\n", "input": "", "output": "        if type(param) == int:\n            return int(param)\n        else:\n            raise ValueError(error_msg)"}, {"instruction": "def Take(self: dict, n):\n    \"\"\"\n    [\n        {\n            'self': [1, 2, 3],\n            'n': 2,\n            'assert': lambda ret: list(ret)  == [1, 2]\n         }\n    ]\n    \"\"\"\n", "input": "", "output": "\n    for i, e in enumerate(self.items()):\n        if i == n:\n            break\n        yield e"}, {"instruction": "def run_blast_commands(ncbicommandline_method, **keywords):\n    \"\"\"Runs blastplus/tblastn search, collects result and pass as a xml temporary file.  \"\"\"\n", "input": "", "output": "\n    # temporary files for output\n    blast_out_tmp = tempfile.NamedTemporaryFile(mode=\"w+\",delete=False)\n    keywords['out'] = blast_out_tmp.name\n\n    # unpack query temp file object\n    query_file_object_tmp = keywords['query']\n    keywords['query'] = query_file_object_tmp.name\n\n    stderr = ''\n    error_string = ''\n    try:\n        # formating blastplus command\n        blastplusx_cline = ncbicommandline_method(**keywords)\n        stdout, stderr = blastplusx_cline()\n\n    except ApplicationError as e:\n        error_string = \"Runtime error: \" + stderr + \"\\n\" + e.cmd\n\n    # remove query temp file\n    os.unlink(query_file_object_tmp.name)\n    # os.remove(query_file_object_tmp.name)\n\n    return blast_out_tmp, error_string"}, {"instruction": "def floatify_latlng(input_value):\n    \"\"\"\n    Work around a JSON dict with string, not float, lat/lngs.\n\n    Given anything (list/dict/etc) it will return that thing again, *but* any\n    dict (at any level) that has only 2 elements lat & lng, will be replaced\n    with the lat & lng turned into floats.\n\n    If the API returns the lat/lng as strings, and not numbers, then this\n    function will 'clean them up' to be floats.\n    \"\"\"\n", "input": "", "output": "    if isinstance(input_value, collections.Mapping):\n        if len(input_value) == 2 and sorted(input_value.keys()) == ['lat', 'lng']:\n            # This dict has only 2 keys 'lat' & 'lon'\n            return {'lat': float_if_float(input_value[\"lat\"]), 'lng': float_if_float(input_value[\"lng\"])}\n        else:\n            return dict((key, floatify_latlng(value)) for key, value in input_value.items())\n    elif isinstance(input_value, collections.MutableSequence):\n        return [floatify_latlng(x) for x in input_value]\n    else:\n        return input_value"}, {"instruction": "def add(self, dpid, ports):\n        \"\"\"add a setting of a bonding i/f.\n        'add' method takes the corresponding args in this order.\n\n        ========= =====================================================\n        Attribute Description\n        ========= =====================================================\n        dpid      datapath id.\n\n        ports     a list of integer values that means the ports face\n                  with the slave i/fs.\n        ========= =====================================================\n\n        if you want to use multi LAG, call 'add' method more than once.\n        \"\"\"\n", "input": "", "output": "        assert isinstance(ports, list)\n        assert len(ports) >= 2\n        ifs = {}\n        for port in ports:\n            ifs[port] = {'enabled': False, 'timeout': 0}\n        bond = {dpid: ifs}\n        self._bonds.append(bond)"}, {"instruction": "def half_light_radius_source(self, kwargs_source, center_x=0, center_y=0, deltaPix=None, numPix=None):\n        \"\"\"\n        computes numerically the half-light-radius of the deflector light and the total photon flux\n\n        :param kwargs_source:\n        :return:\n        \"\"\"\n", "input": "", "output": "        if numPix is None:\n            numPix = 1000\n        if deltaPix is None:\n            deltaPix = 0.005\n        x_grid, y_grid = util.make_grid(numPix=numPix, deltapix=deltaPix)\n        x_grid += center_x\n        y_grid += center_y\n        source_light = self.SourceModel.surface_brightness(x_grid, y_grid, kwargs_source)\n        R_h = analysis_util.half_light_radius(source_light, x_grid, y_grid, center_x=center_x, center_y=center_y)\n        return R_h"}, {"instruction": "def _rapply(d, func, *args, **kwargs):\n    \"\"\"Apply a function to all values in a dictionary or list of dictionaries, recursively.\"\"\"\n", "input": "", "output": "    if isinstance(d, (tuple, list)):\n        return [_rapply(each, func, *args, **kwargs) for each in d]\n    if isinstance(d, dict):\n        return {\n            key: _rapply(value, func, *args, **kwargs) for key, value in iteritems(d)\n        }\n    else:\n        return func(d, *args, **kwargs)"}, {"instruction": "def _create_latent_variables(self):\n        \"\"\" Creates model latent variables\n\n        Returns\n        ----------\n        None (changes model attributes)\n        \"\"\"\n", "input": "", "output": "        for parm in range(self.z_no):\n            self.latent_variables.add_z('Sigma^2 ' + self.X_names[parm], fam.Flat(transform='exp'), fam.Normal(0,3))"}, {"instruction": "def _init_metadata(self):\n        \"\"\"stub\"\"\"\n", "input": "", "output": "        super(edXNumericResponseQuestionFormRecord, self)._init_metadata()\n        QuestionTextFormRecord._init_metadata(self)\n        QuestionFilesFormRecord._init_metadata(self)"}, {"instruction": "def call(self, itemMethod):\n        \"\"\"\n        Invoke the given bound item method in the batch process.\n\n        Return a Deferred which fires when the method has been invoked.\n        \"\"\"\n", "input": "", "output": "        item = itemMethod.im_self\n        method = itemMethod.im_func.func_name\n        return self.batchController.getProcess().addCallback(\n            CallItemMethod(storepath=item.store.dbdir,\n                           storeid=item.storeID,\n                           method=method).do)"}, {"instruction": "def get_hcurves(self, imtls=None):\n        \"\"\"\n        :param imtls: intensity measure types and levels\n        :returns: an array of (R, N) hazard curves\n        \"\"\"\n", "input": "", "output": "        self.init()\n        if imtls is None:\n            imtls = self.imtls\n        pmaps = [pmap.convert2(imtls, self.sids)\n                 for pmap in self.get_pmaps()]\n        return numpy.array(pmaps)"}, {"instruction": "def protocol_authenticate(self, account=None):\n        \"\"\"\n        Low-level API to perform protocol-level authentication on protocols\n        that support it.\n\n        .. HINT::\n           In most cases, you want to use the login() method instead, as\n           it automatically chooses the best login method for each protocol.\n\n        :type  account: Account\n        :param account: An account object, like login().\n        \"\"\"\n", "input": "", "output": "        with self._get_account(account) as account:\n            user = account.get_name()\n            password = account.get_password()\n            key = account.get_key()\n            if key is None:\n                self._dbg(1, \"Attempting to authenticate %s.\" % user)\n                self._protocol_authenticate(user, password)\n            else:\n                self._dbg(1, \"Authenticate %s with key.\" % user)\n                self._protocol_authenticate_by_key(user, key)\n        self.proto_authenticated = True"}, {"instruction": "def Add(self, other):\n    \"\"\"Add other to self pointwise.\n\n    Requires that both self and other are of the same length, and contain\n    identical timestamps. Typically this means that Normalize has been called\n    on both with identical time parameters.\n\n    Args:\n      other: The sequence to add to self.\n\n    Raises:\n      RuntimeError: other does not contain the same timestamps as self.\n    \"\"\"\n", "input": "", "output": "    if len(self.data) != len(other.data):\n      raise RuntimeError(\"Can only add series of identical lengths.\")\n    for i in range(len(self.data)):\n      if self.data[i][1] != other.data[i][1]:\n        raise RuntimeError(\"Timestamp mismatch.\")\n      if self.data[i][0] is None and other.data[i][0] is None:\n        continue\n      self.data[i][0] = (self.data[i][0] or 0) + (other.data[i][0] or 0)"}, {"instruction": "def lookup_family_by_name(name):\n    \"\"\"https://github.com/thom311/libnl/blob/libnl3_2_25/lib/genl/mngt.c#L106.\n\n    Positional arguments:\n    name -- string.\n\n    Returns:\n    genl_ops class instance or None.\n    \"\"\"\n", "input": "", "output": "    for ops in nl_list_for_each_entry(genl_ops(), genl_ops_list, 'o_list'):\n        if ops.o_name == name:\n            return ops\n    return None"}, {"instruction": "def getExtensions(self, extname='SCI', section=None):\n        \"\"\" Return the list of EXTVER values for extensions with name specified\n        in extname.\n\n        \"\"\"\n", "input": "", "output": "        if section is None:\n            numext = 0\n            section = []\n            for hdu in self._image:\n                if 'extname' in hdu.header and hdu.header['extname'] == extname:\n                    section.append(hdu.header['extver'])\n        else:\n            if not isinstance(section,list):\n                section = [section]\n\n        return section"}, {"instruction": "def get_remaining_width(sample_string, max_terminal_width=None):\n    \"\"\"Returns the number of characters available if sample string were to be printed in the terminal.\n\n    Positional arguments:\n    sample_string -- gets the length of this string.\n\n    Keyword arguments:\n    max_terminal_width -- limit the overall width of everything to these many characters.\n\n    Returns:\n    Integer.\n    \"\"\"\n", "input": "", "output": "    if max_terminal_width is not None:\n        available_width = min(terminal_width(), max_terminal_width)\n    else:\n        available_width = terminal_width()\n    return available_width - len(sample_string)"}, {"instruction": "def cacheable(self):\n        \"\"\"Return the cacheable attribute of the BFD file being processed.\"\"\"\n", "input": "", "output": "        if not self._ptr:\n            raise BfdException(\"BFD not initialized\")\n\n        return _bfd.get_bfd_attribute(self._ptr, BfdAttributes.CACHEABLE)"}, {"instruction": "def filter(self, datax, datay):\n        \"\"\"Filter a set of datax and datay according to `self.points`\"\"\"\n", "input": "", "output": "        f = np.ones(datax.shape, dtype=bool)\n        for i, p in enumerate(zip(datax, datay)):\n            f[i] = PolygonFilter.point_in_poly(p, self.points)\n\n        if self.inverted:\n            np.invert(f, f)\n\n        return f"}, {"instruction": "def hit(self, pt):\n        \"\"\"Find the view (self, child, or None) under the point `pt`.\"\"\"\n", "input": "", "output": "\n        if self.hidden or not self._enabled:\n            return None\n\n        if not self.frame.collidepoint(pt):\n            return None\n\n        local_pt = (pt[0] - self.frame.topleft[0],\n                    pt[1] - self.frame.topleft[1])\n\n        for child in reversed(self.children):   # front to back\n            hit_view = child.hit(local_pt)\n            if hit_view is not None:\n                return hit_view\n\n        return self"}, {"instruction": "def get_newest(blocks, layout_blocks):\n    \"\"\"Filter out old layout blocks from list\n\n    Arguments:\n    List:blocks        -- List of block objects\n    List:layout_blocks -- List of layout block indexes\n\n    Returns:\n    List -- Newest layout blocks in list\n    \"\"\"\n", "input": "", "output": "    layout_temp = list(layout_blocks)\n\n    for i in range(0, len(layout_temp)):\n        for k in range(0, len(layout_blocks)):\n            if blocks[layout_temp[i]].ec_hdr.image_seq != blocks[layout_blocks[k]].ec_hdr.image_seq:\n                continue\n\n            if blocks[layout_temp[i]].leb_num != blocks[layout_blocks[k]].leb_num:\n                continue\n\n            if blocks[layout_temp[i]].vid_hdr.sqnum > blocks[layout_blocks[k]].vid_hdr.sqnum:\n                del layout_blocks[k]\n                break\n\n    return layout_blocks"}, {"instruction": "def requeue(self):\n        \"\"\"Reject this message and put it back on the queue.\n\n        You must not use this method as a means of selecting messages\n        to process.\n\n        :raises MessageStateError: If the message has already been\n            acknowledged/requeued/rejected.\n\n        \"\"\"\n", "input": "", "output": "        if self.acknowledged:\n            raise self.MessageStateError(\n                \"Message already acknowledged with state: %s\" % self._state)\n        self.backend.requeue(self.delivery_tag)\n        self._state = \"REQUEUED\""}, {"instruction": "def handleConnectionState(self, msg):\n        \"\"\":Return: True if IBPy message `msg` indicates the connection is unavailable for any reason, else False.\"\"\"\n", "input": "", "output": "        self.connected = not (msg.typeName == \"error\" and\n                              msg.errorCode in dataTypes[\"DISCONNECT_ERROR_CODES\"])\n\n        if self.connected:\n            self.connection_tracking[\"errors\"] = []\n            self.connection_tracking[\"disconnected\"] = False\n\n            if msg.typeName == dataTypes[\"MSG_CURRENT_TIME\"] and not self.connection_tracking[\"connected\"]:\n                self.log.info(\"[CONNECTION TO IB ESTABLISHED]\")\n                self.connection_tracking[\"connected\"] = True\n                self.ibCallback(caller=\"handleConnectionOpened\", msg=\"<connectionOpened>\")\n        else:\n            self.connection_tracking[\"connected\"] = False\n\n            if not self.connection_tracking[\"disconnected\"]:\n                self.connection_tracking[\"disconnected\"] = True\n                self.log.info(\"[CONNECTION TO IB LOST]\")"}, {"instruction": "def register_job_definition(self, json_fpath):\n        \"\"\"Register a job definition with AWS Batch, using a JSON\"\"\"\n", "input": "", "output": "        with open(json_fpath) as f:\n            job_def = json.load(f)\n        response = self._client.register_job_definition(**job_def)\n        status_code = response['ResponseMetadata']['HTTPStatusCode']\n        if status_code != 200:\n            msg = 'Register job definition request received status code {0}:\\n{1}'\n            raise Exception(msg.format(status_code, response))\n        return response"}, {"instruction": "def _setDefaults(configObj={}):\n    \"\"\"set up the default parameters to run drizzle\n       build,single,units,wt_scl,pixfrac,kernel,fillval,\n       rot,scale,xsh,ysh,blotnx,blotny,outnx,outny,data\n\n       Used exclusively for unit-testing, if any are defined.\n\n    \"\"\"\n", "input": "", "output": "\n    paramDict={\"build\":True,\n              \"single\":True,\n              \"stepsize\":10,\n              \"in_units\":\"cps\",\n              \"wt_scl\":1.,\n              \"pixfrac\":1.,\n              \"kernel\":\"square\",\n              \"fillval\":999.,\n              \"maskval\": None,\n              \"rot\":0.,\n              \"scale\":1.,\n              \"xsh\":0.,\n              \"ysh\":0.,\n              \"blotnx\":2048,\n              \"blotny\":2048,\n              \"outnx\":4096,\n              \"outny\":4096,\n              \"data\":None,\n              \"driz_separate\":True,\n              \"driz_combine\":False}\n\n    if(len(configObj) !=0):\n        for key in configObj.keys():\n            paramDict[key]=configObj[key]\n\n    return paramDict"}, {"instruction": "def check_value(self, value):\n        \"\"\"Check the validity of a value for the field.\"\"\"\n", "input": "", "output": "        #if self.readonly:\n        #    raise error.Error(\n        #        \"'{field_name}' field is readonly\".format(\n        #            field_name=self.name))\n        if value and self.size:\n            if not is_string(value):\n                raise ValueError(\"Value supplied has to be a string\")\n            if len(value) > self.size:\n                raise ValueError(\n                    \"Lenght of the '{0}' is limited to {1}\".format(\n                        self.name, self.size))\n        if not value and self.required:\n            raise ValueError(\"'{0}' field is required\".format(self.name))\n        return value"}, {"instruction": "def checkOutputPath(self, output_path):\n        \"\"\"\n        Create or clean up output path\n        \"\"\"\n", "input": "", "output": "        if not output_path:\n            # output_path = self.output_path_DEFAULT\n            output_path = os.path.join(self.output_path_DEFAULT,\n                                       slugify(unicode(self.title)))\n        if os.path.exists(output_path):\n            shutil.rmtree(output_path)\n        os.makedirs(output_path)\n        return output_path"}, {"instruction": "def _extract_auth_config(self):\n        \"\"\"Obtains the authentication configurations.\"\"\"\n", "input": "", "output": "\n        service = self._service\n        if not service.authentication:\n            return {}\n\n        auth_infos = {}\n        for auth_rule in service.authentication.rules:\n            selector = auth_rule.selector\n            provider_ids_to_audiences = {}\n            for requirement in auth_rule.requirements:\n                provider_id = requirement.providerId\n                if provider_id and requirement.audiences:\n                    audiences = requirement.audiences.split(u\",\")\n                    provider_ids_to_audiences[provider_id] = audiences\n            auth_infos[selector] = AuthInfo(provider_ids_to_audiences)\n        return auth_infos"}, {"instruction": "def render(self, template, **data):\n        \"\"\"Renders the template using Jinja2 with given data arguments.\n\n        \"\"\"\n", "input": "", "output": "        if(type(template) != str):\n            raise TypeError(\"String expected\")\n        \n        env = Environment(\n            loader=FileSystemLoader(os.getcwd() + '/View'),\n            autoescape=select_autoescape()\n        )\n\n        template = env.get_template(template)\n        return self.finish(template.render(data))"}, {"instruction": "def get_checks_paths(checks_paths=None):\n    \"\"\"\n    Get path to checks.\n\n    :param checks_paths: list of str, directories where the checks are present\n    :return: list of str (absolute path of directory with checks)\n    \"\"\"\n", "input": "", "output": "    p = os.path.join(__file__, os.pardir, os.pardir, os.pardir, \"checks\")\n    p = os.path.abspath(p)\n    # let's utilize the default upstream checks always\n    if checks_paths:\n        p += [os.path.abspath(x) for x in checks_paths]\n    return [p]"}, {"instruction": "def _container_start_handler(ion_type, length, ctx):\n    \"\"\"Handles container delegation.\"\"\"\n", "input": "", "output": "    _, self = yield\n\n    container_ctx = ctx.derive_container_context(length)\n    if ctx.annotations and ctx.limit != container_ctx.limit:\n        # 'ctx' is the annotation wrapper context. `container_ctx` represents the wrapper's 'value' subfield. Their\n        # limits must match.\n        raise IonException('Incorrect annotation wrapper length.')\n    delegate = _container_handler(ion_type, container_ctx)\n\n    # We start the container, and transition to the new container processor.\n    yield ctx.event_transition(\n        IonEvent, IonEventType.CONTAINER_START, ion_type, value=None, whence=delegate\n    )"}, {"instruction": "def get_formats(\n        self, token: dict = None, format_code: str = None, prot: str = \"https\"\n    ) -> dict:\n        \"\"\"Get formats.\n\n        :param str token: API auth token\n        :param str format_code: code of a specific format\n        :param str prot: https [DEFAULT] or http\n         (use it only for dev and tracking needs).\n        \"\"\"\n", "input": "", "output": "        # if specific format\n        if isinstance(format_code, str):\n            specific_format = \"/{}\".format(format_code)\n        else:\n            specific_format = \"\"\n\n        # search request\n        req_url = \"{}://v1.{}.isogeo.com/formats{}\".format(\n            prot, self.api_url, specific_format\n        )\n\n        req = self.get(\n            req_url, headers=self.header, proxies=self.proxies, verify=self.ssl\n        )\n\n        # checking response\n        checker.check_api_response(req)\n\n        # end of method\n        return req.json()"}, {"instruction": "def current_branch(self):\n        \"\"\"The name of the branch that's currently checked out in the working tree (a string or :data:`None`).\"\"\"\n", "input": "", "output": "        output = self.context.capture('git', 'rev-parse', '--abbrev-ref', 'HEAD', check=False, silent=True)\n        return output if output != 'HEAD' else None"}, {"instruction": "async def sources(client: Client, pubkey: str) -> dict:\n    \"\"\"\n    GET transaction sources\n\n    :param client: Client to connect to the api\n    :param pubkey: Public key\n    :return:\n    \"\"\"\n", "input": "", "output": "    return await client.get(MODULE + '/sources/%s' % pubkey, schema=SOURCES_SCHEMA)"}, {"instruction": "def redef(obj, key, value, **kwargs):\n    '''A static constructor helper function'''\n", "input": "", "output": "    return Redef(obj, key, value=value, **kwargs)"}, {"instruction": "def _shape(self):\n        \"\"\" Returns the shape of the data array associated with this file.\"\"\"\n", "input": "", "output": "        hdu = self.open()\n        _shape = hdu.shape\n        if not self.inmemory:\n            self.close()\n            del hdu\n        return _shape"}, {"instruction": "def remove_setting(self, section, name, save=False):\n    '''remove a setting from the global config\n    '''\n", "input": "", "output": "    configfile = get_configfile()\n    return _remove_setting(section, name, configfile, save)"}, {"instruction": "def _run_train_step(self, train_set):\n        \"\"\"Run a training step.\n\n        A training step is made by randomly shuffling the training set,\n        divide into batches and run the variable update nodes for each batch.\n        :param train_set: training set\n        :return: self\n        \"\"\"\n", "input": "", "output": "        np.random.shuffle(train_set)\n\n        batches = [_ for _ in utilities.gen_batches(train_set,\n                                                    self.batch_size)]\n        updates = [self.w_upd8, self.bh_upd8, self.bv_upd8]\n\n        for batch in batches:\n            self.tf_session.run(updates,\n                                feed_dict=self._create_feed_dict(batch))"}, {"instruction": "def _get_battery(self):\n        \"\"\"\n        Get the battery\n        \"\"\"\n", "input": "", "output": "        try:\n            battery = {\n                \"charge\": self._dev.charge(),\n                \"isCharging\": self._dev.isCharging() == 1,\n            }\n        except Exception:\n            return None\n\n        return battery"}, {"instruction": "def find_java_home(cratedb_version: tuple) -> str:\n    \"\"\" Return a path to a JAVA_HOME suites for the given CrateDB version \"\"\"\n", "input": "", "output": "    if MIN_VERSION_FOR_JVM11 <= cratedb_version < (4, 0):\n        # Supports 8 to 11+, use whatever is set\n        return os.environ.get('JAVA_HOME', '')\n    if cratedb_version < MIN_VERSION_FOR_JVM11:\n        return _find_matching_java_home(lambda ver: ver[0] == 8)\n    else:\n        return _find_matching_java_home(lambda ver: ver[0] >= 11)"}, {"instruction": "async def delete(self, key, param=None):\n        \"\"\"\n        delete cache corresponding to identity\n        generated from key and param\n        \"\"\"\n", "input": "", "output": "        identity = self._gen_identity(key, param)\n        return await self.client.delete(identity)"}, {"instruction": "def histogram_summary(tag, values, bins):\n    \"\"\"Outputs a `Summary` protocol buffer with a histogram.\n    Adding a histogram summary makes it possible to visualize the data's distribution in\n    TensorBoard. See detailed explanation of the TensorBoard histogram dashboard at\n    https://www.tensorflow.org/get_started/tensorboard_histograms\n    This op reports an `InvalidArgument` error if any value is not finite.\n    Adapted from the TensorFlow function `histogram()` at\n    https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/summary/summary.py\n\n    Parameters\n    ----------\n        tag : str\n            A name for the summary of the histogram. Will also serve as a series name in\n            TensorBoard.\n        values : MXNet `NDArray` or `numpy.ndarray`\n            Values for building the histogram.\n\n    Returns\n    -------\n        A `Summary` protobuf of the histogram.\n    \"\"\"\n", "input": "", "output": "    tag = _clean_tag(tag)\n    values = _make_numpy_array(values)\n    hist = _make_histogram(values.astype(float), bins)\n    return Summary(value=[Summary.Value(tag=tag, histo=hist)])"}, {"instruction": "def load(self, cellpy_file, parent_level=\"CellpyData\"):\n        \"\"\"Loads a cellpy file.\n\n        Args:\n            cellpy_file (path, str): Full path to the cellpy file.\n            parent_level (str, optional): Parent level\n\n        \"\"\"\n", "input": "", "output": "\n        try:\n            self.logger.debug(\"loading cellpy-file (hdf5):\")\n            self.logger.debug(cellpy_file)\n            new_datasets = self._load_hdf5(cellpy_file, parent_level)\n            self.logger.debug(\"cellpy-file loaded\")\n        except AttributeError:\n            new_datasets = []\n            self.logger.warning(\"This cellpy-file version is not supported by\"\n                                \"current reader (try to update cellpy).\")\n\n        if new_datasets:\n            for dataset in new_datasets:\n                self.datasets.append(dataset)\n        else:\n            # raise LoadError\n            self.logger.warning(\"Could not load\")\n            self.logger.warning(str(cellpy_file))\n\n        self.number_of_datasets = len(self.datasets)\n        self.status_datasets = self._validate_datasets()\n        self._invent_a_name(cellpy_file)\n        return self"}, {"instruction": "def _push_condition(predicate):\n    \"\"\"As we enter new conditions, this pushes them on the predicate stack.\"\"\"\n", "input": "", "output": "    global _depth\n    _check_under_condition()\n    _depth += 1\n    if predicate is not otherwise and len(predicate) > 1:\n        raise PyrtlError('all predicates for conditional assignments must wirevectors of len 1')\n    _conditions_list_stack[-1].append(predicate)\n    _conditions_list_stack.append([])"}, {"instruction": "def add_files(self, *filenames, **kw):\n        \"\"\"\n        Include added and/or removed files in the working tree in the next commit.\n\n        :param filenames: The filenames of the files to include in the next\n                          commit (zero or more strings). If no arguments are\n                          given all untracked files are added.\n        :param kw: Keyword arguments are ignored (instead of raising\n                   :exc:`~exceptions.TypeError`) to enable backwards\n                   compatibility with older versions of `vcs-repo-mgr`\n                   where the keyword argument `all` was used.\n        \"\"\"\n", "input": "", "output": "        # Make sure the local repository exists and supports a working tree.\n        self.create()\n        self.ensure_working_tree()\n        # Include added and/or removed files in the next commit.\n        logger.info(\"Staging changes to be committed in %s ..\", format_path(self.local))\n        self.context.execute(*self.get_add_files_command(*filenames))"}, {"instruction": "def _send_command_wrapper(self, cmd):\n        \"\"\"\n        Send command to the remote device with a caching feature to avoid sending the same command\n        twice based on the SSH_MAPPER_BASE dict cmd key.\n\n        Parameters\n        ----------\n        cmd : str\n            The command to send to the remote device after checking cache.\n\n        Returns\n        -------\n        response : str\n            The response from the remote device.\n        \"\"\"\n", "input": "", "output": "        cached_results = self._results_cache.get(cmd)\n        if not cached_results:\n            response = self._send_command(cmd)\n            self._results_cache[cmd] = response\n            return response\n        else:\n            return cached_results"}, {"instruction": "def reset(self):\n    \"\"\"Reset simulated and real environments.\"\"\"\n", "input": "", "output": "    self._frame_counter = 0\n    ob_real = self.real_env.reset()\n    # Initialize simulated environment with frames from real one.\n    self.sim_env.add_to_initial_stack(ob_real)\n    for _ in range(3):\n      ob_real, _, _, _ = self.real_env.step(self.name_to_action_num[\"NOOP\"])\n      self.sim_env.add_to_initial_stack(ob_real)\n    ob_sim = self.sim_env.reset()\n    assert np.all(ob_real == ob_sim)\n    self._last_step_tuples = self._pack_step_tuples((ob_real, 0, False, {}),\n                                                    (ob_sim, 0, False, {}))\n    self.set_zero_cumulative_rewards()\n    ob, _, _, _ = self._player_step_tuple(self._last_step_tuples)\n    return ob"}, {"instruction": "def clean(self):\n        \"\"\"\n        Checks for the identification and password.\n\n        If the combination can't be found will raise an invalid sign in error.\n\n        \"\"\"\n", "input": "", "output": "        identification = self.cleaned_data.get('identification')\n        password = self.cleaned_data.get('password')\n\n        if identification and password:\n            self.user_cache = authenticate(identification=identification, \n                                password=password)\n            if self.user_cache is None:\n                raise forms.ValidationError(_(u\"Please enter a correct \"\n                        \"username or email address and password. \"\n                        \"Note that both fields are case-sensitive.\"))\n        return self.cleaned_data"}, {"instruction": "def right(self, expand=None):\n        \"\"\" Returns a new Region right of the current region with a width of ``expand`` pixels.\n\n        Does not include the current region. If range is omitted, it reaches to the right border\n        of the screen. The new region has the same height and y-position as the current region.\n        \"\"\"\n", "input": "", "output": "        if expand == None:\n            x = self.x+self.w\n            y = self.y\n            w = self.getScreen().getBounds()[2] - x\n            h = self.h\n        else:\n            x = self.x+self.w\n            y = self.y\n            w = expand\n            h = self.h\n        return Region(x, y, w, h).clipRegionToScreen()"}, {"instruction": "def mapred(self, transport, inputs, query, timeout):\n        \"\"\"\n        mapred(inputs, query, timeout)\n\n        Executes a MapReduce query.\n\n        .. note:: This request is automatically retried :attr:`retries`\n           times if it fails due to network error.\n\n        :param inputs: the input list/structure\n        :type inputs: list, dict\n        :param query: the list of query phases\n        :type query: list\n        :param timeout: the query timeout\n        :type timeout: integer, None\n        :rtype: mixed\n        \"\"\"\n", "input": "", "output": "        _validate_timeout(timeout)\n        return transport.mapred(inputs, query, timeout)"}, {"instruction": "def update_handler(feeds):\n\t\t'''Update all cross-referencing filters results for feeds and others, related to them.\n\t\t\tIntended to be called from non-Feed update hooks (like new Post saving).'''\n", "input": "", "output": "\t\t# Check if this call is a result of actions initiated from\n\t\t#  one of the hooks in a higher frame (resulting in recursion).\n\t\tif Feed._filters_update_handler_lock: return\n\t\treturn Feed._filters_update_handler(Feed, feeds, force=True)"}, {"instruction": "def _checkObjectsToLearn(self, objects):\n    \"\"\"\n    Checks that objects have the correct format before being sent to the\n    experiment.\n    \"\"\"\n", "input": "", "output": "    for objectName, sensationList in objects.iteritems():\n      if objectName not in self.objects:\n        raise ValueError(\n          \"Invalid object name \\\"{}\\\" sent to experiment\".format(objectName)\n        )\n\n      for sensations in sensationList:\n        if set(sensations.keys()) != set(range(self.numColumns)):\n          raise ValueError(\n            \"Invalid number of cortical column sensations sent to experiment\"\n          )\n        for pair in sensations.values():\n          if not isinstance(pair, tuple) or len(pair) != 2 or \\\n                  not isinstance(pair[0], set) or \\\n                  not isinstance(pair[1], set):\n            raise ValueError(\"Invalid SDR's sent to experiment\")"}, {"instruction": "def keywords(s, top=10, **kwargs):\n    \"\"\" Returns a sorted list of keywords in the given string.\n    \"\"\"\n", "input": "", "output": "    return parser.find_keywords(s, top=top, frequency=parser.frequency)"}, {"instruction": "def get_polling_override(self):\n        \"\"\"Get the current polling override value in milliseconds. \n\n        See :meth:`set_polling_override` for more information. \n\n        Returns:\n            None on error, otherwise the current override period in milliseconds \n            (0 = disabled). \n        \"\"\"\n", "input": "", "output": "        polling_override = self.get_characteristic_handle_from_uuid(UUID_POLLING_OVERRIDE)\n        if polling_override is None:\n            logger.warn('Failed to find handle for polling override')\n            return None\n        override_ms = self.dongle._read_attribute(self.conn_handle, polling_override, True)\n        return None if override_ms is None else ord(override_ms)"}, {"instruction": "def diam_floc_vel_term(ConcAl, ConcClay, coag, material,\n                       DIM_FRACTAL, VelTerm, Temp):\n    \"\"\"Calculate floc diamter as a function of terminal velocity.\"\"\"\n", "input": "", "output": "    WaterDensity = pc.density_water(Temp).magnitude\n    return (material.Diameter * (((18 * VelTerm * PHI_FLOC\n                          * pc.viscosity_kinematic(Temp).magnitude\n                          )\n                         / (pc.gravity.magnitude * material.Diameter**2)\n                         )\n                         * (WaterDensity\n                            / (dens_floc_init(ConcAl, ConcClay, coag,\n                                              material).magnitude\n                               - WaterDensity\n                               )\n                            )\n                        ) ** (1 / (DIM_FRACTAL - 1))\n            )"}, {"instruction": "def op_decanonicalize(op_name, canonical_op):\n    \"\"\"\n    Get the current representation of a parsed operation's data, given the canonical representation\n    Meant for backwards-compatibility\n    \"\"\"\n", "input": "", "output": "    global DECANONICALIZE_METHODS\n\n    if op_name not in DECANONICALIZE_METHODS:\n        # no decanonicalization needed\n        return canonical_op\n    else:\n        return DECANONICALIZE_METHODS[op_name](canonical_op)"}, {"instruction": "def collaborations(self, key, value):\n    \"\"\"Populate the ``collaborations`` key.\"\"\"\n", "input": "", "output": "    result = []\n\n    for g_value in force_list(value.get('g')):\n        collaborations = normalize_collaboration(g_value)\n        if len(collaborations) == 1:\n            result.append({\n                'record': get_record_ref(maybe_int(value.get('0')), 'experiments'),\n                'value': collaborations[0],\n            })\n        else:\n            result.extend({'value': collaboration} for collaboration in collaborations)\n\n    return result"}, {"instruction": "def getRunningBatchJobIDs(self):\n        \"\"\"\n        Retrieve running job IDs from local and batch scheduler.\n\n        Respects statePollingWait and will return cached results if not within\n        time period to talk with the scheduler.\n        \"\"\"\n", "input": "", "output": "        if (self._getRunningBatchJobIDsTimestamp and (\n                datetime.now() -\n                self._getRunningBatchJobIDsTimestamp).total_seconds() <\n                self.config.statePollingWait):\n            batchIds = self._getRunningBatchJobIDsCache\n        else:\n            batchIds = with_retries(self.worker.getRunningJobIDs)\n            self._getRunningBatchJobIDsCache = batchIds\n            self._getRunningBatchJobIDsTimestamp = datetime.now()\n        batchIds.update(self.getRunningLocalJobIDs())\n        return batchIds"}, {"instruction": "def course_feature(catalog, soup):\n    \"\"\"Parses all the courses (AKA, the most important part).\n    \"\"\"\n", "input": "", "output": "    courses = {}\n    course_crns = {}\n    for course in soup.findAll('course'):\n        c = Course.from_soup_tag(course)\n        courses[str(c)] = c\n    catalog.courses = courses\n    catalog.courses\n    logger.info('Catalog has %d courses' % len(courses))"}, {"instruction": "def get_refinement_options(self):\n        \"\"\" Returns possible specializations for the upper values in the taxonomy \"\"\"\n", "input": "", "output": "        domain = self.get_domain()\n        for upper_value in self.upper:\n            for suc in domain.successors(upper_value):\n                yield suc"}, {"instruction": "def _parse_contract(self, player_info):\n        \"\"\"\n        Parse the player's contract.\n\n        Depending on the player's contract status, a contract table is located\n        at the bottom of the stats page and includes player wages by season. If\n        found, create a dictionary housing the wages by season.\n\n        Parameters\n        ----------\n        player_info : PyQuery object\n            A PyQuery object containing the HTML from the player's stats page.\n        \"\"\"\n", "input": "", "output": "        contract = {}\n\n        salary_table = player_info('table#br-salaries')\n        for row in salary_table('tbody tr').items():\n            if 'class=\"spacer partial_table\"' in str(row):\n                continue\n            year = row('th[data-stat=\"year_ID\"]').text()\n            if year.strip() == '':\n                continue\n            age = row('td[data-stat=\"age\"]').text()\n            team = self._parse_team_name(str(row('td[data-stat=\"team_name\"]')))\n            salary = row('td[data-stat=\"Salary\"]').text()\n            contract[year] = {\n                'age': age,\n                'team': team,\n                'salary': salary\n            }\n        setattr(self, '_contract', contract)"}, {"instruction": "def _check_bounds(self, v):\n        \"\"\"Check which values are out of bounds.\n\n        Raises\n        ------\n        ValueError:\n\n        \"\"\"\n", "input": "", "output": "        below_bounds = v < self._x[0]\n        above_bounds = v > self._x[-1]\n\n        if self.bounds_error and below_bounds.any():\n            raise ValueError(\"A value in x_new is below the interpolation \"\n                \"range.\")\n        if self.bounds_error and above_bounds.any():\n            raise ValueError(\"A value in x_new is above the interpolation \"\n                \"range.\")\n\n        return below_bounds, above_bounds"}, {"instruction": "def runGetVariant(self, id_):\n        \"\"\"\n        Returns a variant with the given id\n        \"\"\"\n", "input": "", "output": "        compoundId = datamodel.VariantCompoundId.parse(id_)\n        dataset = self.getDataRepository().getDataset(compoundId.dataset_id)\n        variantSet = dataset.getVariantSet(compoundId.variant_set_id)\n        gaVariant = variantSet.getVariant(compoundId)\n        # TODO variant is a special case here, as it's returning a\n        # protocol element rather than a datamodel object. We should\n        # fix this for consistency.\n        jsonString = protocol.toJson(gaVariant)\n        return jsonString"}, {"instruction": "def retreive_sigma_mu_data(self):\n        \"\"\"\n        For the general form of the GMPE this retrieves the sigma mu\n        values from the hdf5 file using the \"general\" model, i.e. sigma mu\n        factors that are independent of the choice of region or depth\n        \"\"\"\n", "input": "", "output": "        fle = h5py.File(os.path.join(BASE_PATH,\n                                     \"KothaEtAl2019_SigmaMu_Fixed.hdf5\"), \"r\")\n        self.mags = fle[\"M\"][:]\n        self.dists = fle[\"R\"][:]\n        self.periods = fle[\"T\"][:]\n        self.pga = fle[\"PGA\"][:]\n        self.pgv = fle[\"PGV\"][:]\n        self.s_a = fle[\"SA\"][:]\n        fle.close()"}, {"instruction": "def col(s, c, bg=0, no_reset=0):\n    \"\"\"\n    print col('foo', 124) -> red 'foo' on the terminal\n    c = color, s the value to colorize \"\"\"\n", "input": "", "output": "    reset = reset_col\n    if no_reset:\n        reset = ''\n    for _strt, _end, _col in ((code_start, code_end, H2),\n                              (stng_start, stng_end, H2),\n                              (emph_start, emph_end, H3)):\n        if _strt in s:\n            # inline code:\n            s = s.replace(_strt, col('', _col, bg=background, no_reset=1))\n            s = s.replace(_end , col('', c, no_reset=1))\n\n    s =  '\\033[38;5;%sm%s%s' % (c, s, reset)\n    if bg:\n        pass\n        #s = col_bg(bg) + s\n    return s"}, {"instruction": "def get_inters_direct(r, L, R_cut):\n    '''\n    Return points within a given cut-off of each other,\n    in a periodic system.\n\n    Uses a direct algorithm, which may be very slow for large numbers of\n    points.\n\n    Parameters\n    ----------\n    r: array, shape (n, d) where d is one of (2, 3).\n        A set of n point coordinates.\n        Coordinates are assumed to lie in [-L / 2, L / 2].\n    L: float.\n        Bounds of the system.\n    R_cut: float.\n        The maximum distance within which to consider points to lie\n        near each other.\n\n    Returns\n    -------\n    inters: array, shape (n, n)\n        Indices of the nearby points.\n        For each particle indexed by the first axis,\n        the second axis lists each index of a nearby point.\n    intersi: array, shape (n,)\n        Total number of nearby points.\n        This array should be used to index `inters`, as for point `i`,\n        elements in `inters[i]` beyond `intersi[i]` have no well-defined value.\n    '''\n", "input": "", "output": "    _cell_list.cell_list_direct.make_inters(r.T, L, R_cut)\n    return _parse_inters()"}, {"instruction": "def kunc_dPdV(v, v0, k0, k0p, order=5, precision=1.e-5):\n    \"\"\"\n    calculate dP/dV for numerical calculation of bulk modulus\n    according to test this differs from analytical result by 1.e-5\n\n    :param v: unit-cell volume in A^3\n    :param v0: unit-cell volume in A^3 at 1 bar\n    :param k0: bulk modulus at reference conditions\n    :param k0p: pressure derivative of bulk modulus at reference conditions\n    :param precision: precision for numerical calc (default = 1.e-5 * v0)\n    :return: dP/dV\n    \"\"\"\n", "input": "", "output": "    def f_scalar(v, v0, k0, k0p, order=order, precision=1.e-5):\n        return derivative(kunc_p, v, args=(v0, k0, k0p, order),\n                          dx=v0 * precision)\n    f_v = np.vectorize(f_scalar, excluded=[1, 2, 3, 4, 5])\n    return f_v(v, v0, k0, k0p, order=order, precision=precision)"}, {"instruction": "def camelcase_to_slash(name):\n    \"\"\" Converts CamelCase to camel/case\n\n    code ripped from http://stackoverflow.com/questions/1175208/does-the-python-standard-library-have-function-to-convert-camelcase-to-camel-cas\n    \"\"\"\n", "input": "", "output": "\n    s1 = re.sub('(.)([A-Z][a-z]+)', r'\\1/\\2', name)\n    return re.sub('([a-z0-9])([A-Z])', r'\\1/\\2', s1).lower()"}, {"instruction": "def _parse(cls, data, key=None):\n        \"\"\"\n        Parse a set of data to extract entity-only data.\n\n        Use classmethod `parse` if available, otherwise use the `endpoint`\n        class variable to extract data from a data blob.\n        \"\"\"\n", "input": "", "output": "        parse = cls.parse if cls.parse is not None else cls.get_endpoint()\n\n        if callable(parse):\n            data = parse(data)\n        elif isinstance(parse, str):\n            data = data[key]\n        else:\n            raise Exception('\"parse\" should be a callable or string got, {0}'\n                            .format(parse))\n        return data"}, {"instruction": "def consume(self, chars, min=0, max=-1):\n        \"\"\"\n        Consume chars until min/max is satisfied is valid.\n        \"\"\"\n", "input": "", "output": "        return self._src.consume(chars=chars, min=min, max=max)"}, {"instruction": "def get_qemu_info(path, backing_chain=False, fail_on_error=True):\n    \"\"\"\n    Get info on a given qemu disk\n\n    Args:\n        path(str): Path to the required disk\n        backing_chain(boo): if true, include also info about\n        the image predecessors.\n    Return:\n        object: if backing_chain == True then a list of dicts else a dict\n    \"\"\"\n", "input": "", "output": "\n    cmd = ['qemu-img', 'info', '--output=json', path]\n\n    if backing_chain:\n        cmd.insert(-1, '--backing-chain')\n\n    result = run_command_with_validation(\n        cmd, fail_on_error, msg='Failed to get info for {}'.format(path)\n    )\n\n    return json.loads(result.out)"}, {"instruction": "def parent(self):\n        \"\"\"\n        Select the direct child(ren) from the UI element(s) given by the query expression, see ``QueryCondition`` for\n        more details about the selectors.\n\n        Warnings:\n            Experimental method, may not be available for all drivers.\n\n        Returns:\n            :py:class:`UIObjectProxy <poco.proxy.UIObjectProxy>`: a new UI proxy object representing the direct parent\n            of the first UI element.\n        \"\"\"\n", "input": "", "output": "\n        sub_query = build_query(None)  # as placeholder\n        query = ('^', (self.query, sub_query))\n        obj = UIObjectProxy(self.poco)\n        obj.query = query\n        return obj"}, {"instruction": "def _init_prtfmt(self, key=\"fmta\"):\n        \"\"\"Return print format for Grouper, which includes hdr1usr01 and num_usrgos.\"\"\"\n", "input": "", "output": "        prtfmt = self.gosubdag.prt_attr[key]\n        return prtfmt.replace(\"{NS}\", \"{NS} {hdr1usr01:2} {num_usrgos:>4} uGOs\")"}, {"instruction": "def update(self, new_email_address, name, access_level, password=None):\n        \"\"\"Updates the details for a person. Password is optional and is only updated if supplied.\"\"\"\n", "input": "", "output": "        params = {\"email\": self.email_address}\n        body = {\n            \"EmailAddress\": new_email_address,\n            \"Name\": name,\n            \"AccessLevel\": access_level,\n            \"Password\": password}\n        response = self._put(\"/clients/%s/people.json\" % self.client_id,\n                             body=json.dumps(body), params=params)\n        # Update self.email_address, so this object can continue to be used\n        # reliably\n        self.email_address = new_email_address"}, {"instruction": "def PROFILE_RAUTIAN(sg0,GamD,Gam0,Shift0,anuVC,eta,sg):\n    \"\"\"\n    # Rautian profile based on HTP.\n    # Input parameters:\n    #      sg0     : Unperturbed line position in cm-1 (Input).\n    #      GamD    : Doppler HWHM in cm-1 (Input)\n    #      Gam0    : Speed-averaged line-width in cm-1 (Input).       \n    #      anuVC   : Velocity-changing frequency in cm-1 (Input).\n    #      Shift0  : Speed-averaged line-shift in cm-1 (Input).\n    #      sg      : Current WaveNumber of the Computation in cm-1 (Input).\n    \"\"\"\n", "input": "", "output": "    return pcqsdhc(sg0,GamD,Gam0,cZero,Shift0,cZero,anuVC,cZero,sg)"}, {"instruction": "def get_stp_mst_detail_output_msti_port_rx_bpdu_count(self, **kwargs):\n        \"\"\"Auto Generated Code\n        \"\"\"\n", "input": "", "output": "        config = ET.Element(\"config\")\n        get_stp_mst_detail = ET.Element(\"get_stp_mst_detail\")\n        config = get_stp_mst_detail\n        output = ET.SubElement(get_stp_mst_detail, \"output\")\n        msti = ET.SubElement(output, \"msti\")\n        instance_id_key = ET.SubElement(msti, \"instance-id\")\n        instance_id_key.text = kwargs.pop('instance_id')\n        port = ET.SubElement(msti, \"port\")\n        rx_bpdu_count = ET.SubElement(port, \"rx-bpdu-count\")\n        rx_bpdu_count.text = kwargs.pop('rx_bpdu_count')\n\n        callback = kwargs.pop('callback', self._callback)\n        return callback(config)"}, {"instruction": "def p_if_statement_2(self, p):\n        \"\"\"if_statement : IF LPAREN expr RPAREN statement ELSE statement\"\"\"\n", "input": "", "output": "        p[0] = self.asttypes.If(\n            predicate=p[3], consequent=p[5], alternative=p[7])\n        p[0].setpos(p)"}, {"instruction": "def geometric_center(coords, periodic):\n    '''Geometric center taking into account periodic boundaries'''\n", "input": "", "output": "    max_vals = periodic\n    theta = 2 * np.pi * (coords / max_vals)\n    eps = np.cos(theta) * max_vals / (2 * np.pi)\n    zeta = np.sin(theta) * max_vals / (2 * np.pi)\n\n    eps_avg = eps.sum(axis=0)\n    zeta_avg = zeta.sum(axis=0)\n    theta_avg = np.arctan2(-zeta_avg, -eps_avg) + np.pi\n\n    return theta_avg * max_vals / (2 * np.pi)"}, {"instruction": "def get_splits(split_bed, gff_file, stype, key):\n    \"\"\"\n    Use intersectBed to find the fused gene => split genes mappings.\n    \"\"\"\n", "input": "", "output": "    bed_file = get_bed_file(gff_file, stype, key)\n    cmd = \"intersectBed -a {0} -b {1} -wao\".format(split_bed, bed_file)\n    cmd += \" | cut -f4,10\"\n    p = popen(cmd)\n    splits = defaultdict(set)\n    for row in p:\n        a, b = row.split()\n        splits[a].add(b)\n\n    return splits"}, {"instruction": "def depsOf_of_mirteFile_module_definition(defs):\n    \"\"\" Returns a function that returns the dependencies of a module\n        definition by its name, where defs is a dictionary of module\n        definitions from a mirteFile \"\"\"\n", "input": "", "output": "    return lambda x: (list(filter(lambda z: z is not None and z in defs,\n                             map(lambda y: y[1].get('type'),\n                                 six.iteritems(defs[x]['settings'])\n                                 if 'settings' in defs[x] else [])))) + \\\n        (list(defs[x]['inherits']) if 'inherits' in defs[x] else [])"}, {"instruction": "async def pause_writing(self):\n        \"\"\"Pause writing.\"\"\"\n", "input": "", "output": "        self._restart_writer = False\n        if self._writer_task:\n            self._writer_task.remove_done_callback(self.restart_writing)\n            self._writer_task.cancel()\n            await self._writer_task\n            await asyncio.sleep(0, loop=self._loop)"}, {"instruction": "def get_complex_and_node_state(self, hosts, services):\n        \"\"\"Get state , handle AND aggregation ::\n\n           * Get the worst state. 2 or max of sons (3 <=> UNKNOWN < CRITICAL <=> 2)\n           * Revert if it's a not node\n\n        :param hosts: host objects\n        :param services: service objects\n        :return: 0, 1 or 2\n        :rtype: int\n        \"\"\"\n", "input": "", "output": "        # First we get the state of all our sons\n        states = [s.get_state(hosts, services) for s in self.sons]\n        # Next we calculate the worst state\n        if 2 in states:\n            worst_state = 2\n        else:\n            worst_state = max(states)\n        # Then we handle eventual not value\n        if self.not_value:\n            return self.get_reverse_state(worst_state)\n        return worst_state"}, {"instruction": "def urlEncodeAndJoin(self, seq, sepr=','):\n        '''sepr.join(urlencode(seq))\n        Args:\n            seq: string list to be urlencoded\n            sepr: join seq with sepr\n        Returns:\n            str\n        '''\n", "input": "", "output": "        try:\n            from urllib.parse import quote_plus as encode\n            return sepr.join([encode(x, encoding=CHARSET_UTF8) for x in seq])\n        except ImportError:\n            from urllib import quote as encode\n            return sepr.join([i for i in map(lambda x: encode(x), seq)])"}, {"instruction": "def addprojecthook(self, project_id, url, push=False, issues=False, merge_requests=False, tag_push=False):\n        \"\"\"\n        add a hook to a project\n\n        :param project_id: project id\n        :param url: url of the hook\n        :return: True if success\n        \"\"\"\n", "input": "", "output": "        data = {\n            'id': project_id,\n            'url': url,\n            'push_events': int(bool(push)),\n            'issues_events': int(bool(issues)),\n            'merge_requests_events': int(bool(merge_requests)),\n            'tag_push_events': int(bool(tag_push)),\n        }\n\n        request = requests.post(\n            '{0}/{1}/hooks'.format(self.projects_url, project_id),\n            headers=self.headers, data=data, verify=self.verify_ssl, auth=self.auth, timeout=self.timeout)\n\n        if request.status_code == 201:\n            return request.json()\n        else:\n            return False"}, {"instruction": "def get_collection(cls):\n        \"\"\"Return a reference to the database collection for the class\"\"\"\n", "input": "", "output": "\n        # By default the collection returned will be the published collection,\n        # however if the `draft` flag has been set against the global context\n        # (e.g `g`) then the collection returned will contain draft documents.\n\n        if g.get('draft'):\n            return getattr(\n                cls.get_db(),\n                '{collection}_draft'.format(collection=cls._collection)\n                )\n\n        return getattr(cls.get_db(), cls._collection)"}, {"instruction": "def get_cmdclass():\n    \"\"\" DEPRICATE \"\"\"\n", "input": "", "output": "    try:\n        from Cython.Distutils import build_ext\n        cmdclass = {'build_ext': build_ext}\n        return cmdclass\n    except Exception as ex:\n        print(ex)\n        print('WARNING: Cython is not installed. This is only a problem if you are building C extensions')\n        return {}"}, {"instruction": "def setupDock(self):\n        \"\"\"Setup empty Dock at startup. \"\"\"\n", "input": "", "output": "        self.dock = QtWidgets.QDockWidget(\"Classes\", self)\n        self.dock.setWidget(self.tree)\n        self.dock.setFeatures(QtWidgets.QDockWidget.NoDockWidgetFeatures)\n        self.addDockWidget(QtCore.Qt.LeftDockWidgetArea, self.dock)"}, {"instruction": "def _parse_json_with_fieldnames(self):\n        \"\"\" Parse the raw JSON with all attributes/methods defined in the class, except for the\n            ones defined starting with '_' or flagged in cls._TO_EXCLUDE.\n\n            The final result is stored in self.json\n        \"\"\"\n", "input": "", "output": "        for key in dir(self):\n            if not key.startswith('_') and key not in self._TO_EXCLUDE:\n                self.fieldnames.append(key)\n                value = getattr(self, key)\n                if value:\n                    self.json[key] = value\n        # Add OK attribute even if value is \"False\"\n        self.json['ok'] = self.ok"}, {"instruction": "def mode(self):\n        \"\"\"\n        Reading returns the currently selected mode. Writing sets the mode.\n        Generally speaking when the mode changes any sensor or motor devices\n        associated with the port will be removed new ones loaded, however this\n        this will depend on the individual driver implementing this class.\n        \"\"\"\n", "input": "", "output": "        self._mode, value = self.get_attr_string(self._mode, 'mode')\n        return value"}, {"instruction": "def _datasets_line(args):\n  \"\"\"Implements the BigQuery datasets magic used to display datasets in a project.\n\n   The supported syntax is:\n\n       %bigquery datasets [-f <filter>] [-p|--project <project_id>]\n\n  Args:\n    args: the arguments following '%bigquery datasets'.\n  Returns:\n    The HTML rendering for the table of datasets.\n  \"\"\"\n", "input": "", "output": "  filter_ = args['filter'] if args['filter'] else '*'\n  return _render_list([str(dataset) for dataset in datalab.bigquery.Datasets(args['project'])\n                       if fnmatch.fnmatch(str(dataset), filter_)])"}, {"instruction": "def collapse_all(self):\n        \"\"\"Collapse all items.\"\"\"\n", "input": "", "output": "\n        def aux(item):\n            self.item(item, open=False)\n            children = self.get_children(item)\n            for c in children:\n                aux(c)\n\n        children = self.get_children(\"\")\n        for c in children:\n            aux(c)"}, {"instruction": "def delete_minion_cachedir(minion_id, provider, opts, base=None):\n    '''\n    Deletes a minion's entry from the cloud cachedir. It will search through\n    all cachedirs to find the minion's cache file.\n    Needs `update_cachedir` set to True.\n    '''\n", "input": "", "output": "    if isinstance(opts, dict):\n        __opts__.update(opts)\n\n    if __opts__.get('update_cachedir', False) is False:\n        return\n\n    if base is None:\n        base = __opts__['cachedir']\n\n    driver = next(six.iterkeys(__opts__['providers'][provider]))\n    fname = '{0}.p'.format(minion_id)\n    for cachedir in 'requested', 'active':\n        path = os.path.join(base, cachedir, driver, provider, fname)\n        log.debug('path: %s', path)\n        if os.path.exists(path):\n            os.remove(path)"}, {"instruction": "def SerializeExclusiveData(self, writer):\n        \"\"\"\n        Serialize object.\n\n        Args:\n            writer (neo.IO.BinaryWriter):\n        \"\"\"\n", "input": "", "output": "        writer.WriteVarBytes(self.Script)\n        if self.Version >= 1:\n            writer.WriteFixed8(self.Gas)"}, {"instruction": "def error_map_source(self, kwargs_source, x_grid, y_grid, cov_param):\n        \"\"\"\n        variance of the linear source reconstruction in the source plane coordinates,\n        computed by the diagonal elements of the covariance matrix of the source reconstruction as a sum of the errors\n        of the basis set.\n\n        :param kwargs_source: keyword arguments of source model\n        :param x_grid: x-axis of positions to compute error map\n        :param y_grid: y-axis of positions to compute error map\n        :param cov_param: covariance matrix of liner inversion parameters\n        :return: diagonal covariance errors at the positions (x_grid, y_grid)\n        \"\"\"\n", "input": "", "output": "\n        error_map = np.zeros_like(x_grid)\n        basis_functions, n_source = self.SourceModel.functions_split(x_grid, y_grid, kwargs_source)\n        basis_functions = np.array(basis_functions)\n\n        if cov_param is not None:\n            for i in range(len(error_map)):\n                error_map[i] = basis_functions[:, i].T.dot(cov_param[:n_source, :n_source]).dot(basis_functions[:, i])\n        return error_map"}, {"instruction": "def get_dates(self):\n        \"\"\" Returns a list of acquisition times from tile info data\n\n        :return: List of acquisition times in the order returned by WFS service.\n        :rtype: list(datetime.datetime)\n        \"\"\"\n", "input": "", "output": "        return [datetime.datetime.strptime('{}T{}'.format(tile_info['properties']['date'],\n                                                          tile_info['properties']['time'].split('.')[0]),\n                                           '%Y-%m-%dT%H:%M:%S') for tile_info in self]"}, {"instruction": "def _find_datastream(self, name):\n        \"\"\"Find and return if a datastream exists, by name.\"\"\"\n", "input": "", "output": "        for stream in self.data_streams: #search to see if this is a new datastream or a known one\n            if stream.name == name:\n                return stream\n        return None"}, {"instruction": "def get_topic_list(num=10, top_items=False):\n    \"\"\"\n    Returns a list of top recent topics, excluding less valuable forums.\n    Default is 10 topics.\n    Can be sorted for most active topics by votes and post count.\n    Usage:\n    {% get_topic_list 5 as topics %}\n    {% get_topic_list 7 top_items=True as topics %}\n    \"\"\"\n", "input": "", "output": "    excluded_forum_ids = [3, 7, 10, 12, 15, 16, 17, 18, 19, 23]\n    topics = Topic.objects.exclude(forum_id__in=excluded_forum_ids).order_by('-id')[0:num]\n    if top_items:\n        topics = sorted(list(topics), key=lambda t: (t.forum_id, -t.votes, -t.post_count))\n    return topics"}, {"instruction": "def read_networks(folder):\n    \"\"\"\n    Read perseus network collection folder format\n    \n    >>> network_table, networks = read_networks(folder)\n    \n    :param folder: Path to network collection\n    :returns: Network table and dictionary with 'name', 'edge_table', and 'node_table' keys.\n    \"\"\"\n", "input": "", "output": "    network_table = read_perseus(path.join(folder, \"networks.txt\"))\n    networks = {}\n    for name, guid in network_table[['Name', 'GUID']].values:\n        networks[guid] = {\n                'name': name,\n                'guid': guid,\n                'node_table': read_perseus(path.join(folder, \"{}_nodes.txt\".format(guid))),\n                'edge_table': read_perseus(path.join(folder, \"{}_edges.txt\".format(guid)))\n                }\n    return network_table, networks"}, {"instruction": "def from_bytes(cls, data):\n        \"\"\"\n        I am so sorry.\n        \"\"\"\n", "input": "", "output": "        len_username = int.from_bytes(data[0:2], byteorder=\"big\")\n        offset_username = 2 + len_username\n        username = data[2:offset_username].decode(\"UTF-8\")\n        offset_password = 2 + offset_username\n        len_password = int.from_bytes(\n            data[offset_username:offset_password], byteorder=\"big\"\n        )\n        pass_begin = offset_password\n        pass_end = offset_password + len_password\n        password = data[pass_begin:pass_end].decode(\"UTF-8\")\n\n        return cls(username, password)"}, {"instruction": "def by_median_home_value(self,\n                             lower=-1,\n                             upper=2 ** 31,\n                             zipcode_type=ZipcodeType.Standard,\n                             sort_by=SimpleZipcode.median_home_value.name,\n                             ascending=False,\n                             returns=DEFAULT_LIMIT):\n        \"\"\"\n        Search zipcode information by median home value.\n        \"\"\"\n", "input": "", "output": "        return self.query(\n            median_home_value_lower=lower,\n            median_home_value_upper=upper,\n            sort_by=sort_by, zipcode_type=zipcode_type,\n            ascending=ascending, returns=returns,\n        )"}, {"instruction": "def _add_chrome_proxy_extension(\n        chrome_options, proxy_string, proxy_user, proxy_pass):\n    \"\"\" Implementation of https://stackoverflow.com/a/35293284 for\n        https://stackoverflow.com/questions/12848327/\n        (Run Selenium on a proxy server that requires authentication.) \"\"\"\n", "input": "", "output": "    if not \"\".join(sys.argv) == \"-c\":\n        # Single-threaded\n        proxy_helper.create_proxy_zip(proxy_string, proxy_user, proxy_pass)\n    else:\n        # Pytest multi-threaded test\n        lock = threading.Lock()\n        with lock:\n            time.sleep(random.uniform(0.02, 0.15))\n            if not os.path.exists(PROXY_ZIP_PATH):\n                proxy_helper.create_proxy_zip(\n                    proxy_string, proxy_user, proxy_pass)\n            time.sleep(random.uniform(0.1, 0.2))\n    proxy_zip = PROXY_ZIP_PATH\n    if not os.path.exists(PROXY_ZIP_PATH):\n        # Handle \"Permission denied\" on the default proxy.zip path\n        proxy_zip = PROXY_ZIP_PATH_2\n    chrome_options.add_extension(proxy_zip)\n    return chrome_options"}, {"instruction": "async def track_event(event, state, service_name):\n    \"\"\"\n    Store state of events in memory\n    :param event: Event object\n    :param state: EventState object\n    :param service_name: Name of service name\n    \"\"\"\n", "input": "", "output": "    redis = await aioredis.create_redis(\n        (EVENT_TRACKING_HOST, 6379), loop=loop)\n\n    now = datetime.utcnow()\n    event_id = event.event_id\n\n    tracking_data = json.dumps({\n        \"event_id\": event_id,\n        \"timestamp\": str(now),\n        \"state\": state\n    })\n    await redis.rpush(service_name, tracking_data)\n    redis.close()\n    await redis.wait_closed()"}, {"instruction": "def resource_schema(raml_resource):\n    \"\"\" Get schema properties of RAML resource :raml_resource:.\n\n    Must be called with RAML resource that defines body schema. First\n    body that defines schema is used. Schema is converted on return using\n    'convert_schema'.\n\n    :param raml_resource: Instance of ramlfications.raml.ResourceNode of\n        POST method.\n    \"\"\"\n", "input": "", "output": "    # NOTE: Must be called with resource that defines body schema\n    log.info('Searching for model schema')\n    if not raml_resource.body:\n        raise ValueError('RAML resource has no body to setup database '\n                         'schema from')\n\n    for body in raml_resource.body:\n        if body.schema:\n            return convert_schema(body.schema, body.mime_type)\n    log.debug('No model schema found.')"}, {"instruction": "def _cache_translation_needs_fallback(instance, language_code, related_name, timeout=cache.default_timeout):\n    \"\"\"\n    Store the fact that a translation doesn't exist, and the fallback should be used.\n    \"\"\"\n", "input": "", "output": "    if not appsettings.PARLER_ENABLE_CACHING or not instance.pk or instance._state.adding:\n        return\n\n    tr_model = instance._parler_meta.get_model_by_related_name(related_name)\n    key = get_translation_cache_key(tr_model, instance.pk, language_code)\n    cache.set(key, {'__FALLBACK__': True}, timeout=timeout)"}, {"instruction": "def bind_sockets(address, port):\n    ''' Bind a socket to a port on an address.\n\n    Args:\n        address (str) :\n            An address to bind a port on, e.g. ``\"localhost\"``\n\n        port (int) :\n            A port number to bind.\n\n            Pass 0 to have the OS automatically choose a free port.\n\n    This function returns a 2-tuple with the new socket as the first element,\n    and the port that was bound as the second. (Useful when passing 0 as a port\n    number to bind any free port.)\n\n    Returns:\n        (socket, port)\n\n    '''\n", "input": "", "output": "    ss = netutil.bind_sockets(port=port or 0, address=address)\n    assert len(ss)\n    ports = {s.getsockname()[1] for s in ss}\n    assert len(ports) == 1, \"Multiple ports assigned??\"\n    actual_port = ports.pop()\n    if port:\n        assert actual_port == port\n    return ss, actual_port"}, {"instruction": "def guard_retract(analysis):\n    \"\"\" Return whether the transition \"retract\" can be performed or not\n    \"\"\"\n", "input": "", "output": "    # Cannot retract if there are dependents that cannot be retracted\n    if not is_transition_allowed(analysis.getDependents(), \"retract\"):\n        return False\n\n    dependencies = analysis.getDependencies()\n    if not dependencies:\n        return True\n\n    # Cannot retract if all dependencies have been verified\n    if all(map(lambda an: IVerified.providedBy(an), dependencies)):\n        return False\n\n    return True"}, {"instruction": "def schema(self):\n        \"\"\"List[google.cloud.bigquery.schema.SchemaField]: Schema of the\n        destination table.\n\n        See\n        https://cloud.google.com/bigquery/docs/reference/rest/v2/jobs#configuration.load.schema\n        \"\"\"\n", "input": "", "output": "        schema = _helpers._get_sub_prop(self._properties, [\"load\", \"schema\", \"fields\"])\n        if schema is None:\n            return\n        return [SchemaField.from_api_repr(field) for field in schema]"}, {"instruction": "def dump(self, raw=False):\n        ''' Dump all output currently in the arm's output queue. '''\n", "input": "", "output": "        raw_out = self.ser.read(self.ser.in_waiting)\n        if raw:\n            return raw_out\n        return raw_out.decode(OUTPUT_ENCODING)"}, {"instruction": "def AsIter(arg):\n  \"\"\"Encapsulates an argument in a tuple, if it's not already iterable.\"\"\"\n", "input": "", "output": "  if isinstance(arg, string_types):\n    rslt = [arg]\n  elif isinstance(arg, collections.Iterable):\n    rslt = arg\n  elif not arg:\n    rslt = []\n  else:\n    rslt = [arg]\n  return tuple(rslt)"}, {"instruction": "def _initialize_repo_cache():\n    \"\"\"Initialize the repository cache used for scraping.\n\n    Retrieves a list of repositories with their provider and last scraping time\n    from Elasticsearch.\n    This list can be used to check which repos need to be scraped (e.g. after\n    a specific amount of time).\n    \"\"\"\n", "input": "", "output": "    LOGGER.info(\"Initializing repository cache\")\n    # Initialize Repo Cache\n    repo_cache = {}\n\n    # Get all repos from Elasticsearch\n    for hit in GitRepo.search().query(\"match_all\").scan():\n        # TODO (fschmidt): Maybe we can use this list as cache for the whole\n        # scraper-webhook part.\n        # This way, we could reduce the amount of operations needed for GitHub\n        # and ElasticSearch\n        repo_cache[hit.repo_name] = hit.to_dict(skip_empty=False)\n\n    return repo_cache"}, {"instruction": "def as_rgb(self):\n        \"\"\"\n        Return a color palette with RGB values instead of hex codes.\n        \"\"\"\n", "input": "", "output": "        rgb = [mpl.colors.colorConverter.to_rgb(hex) for hex in self]\n        return ColorPalette(rgb)"}, {"instruction": "def short_hash(*buffers):\n    \"\"\"\n    :param buffer: a binary buffer (e.g. serialized blob)\n    :return: the first 8 characters of base64 ASCII rendition SHA-1\n    \"\"\"\n", "input": "", "output": "    hashed = hashlib.sha1()\n    for buffer in buffers:\n        hashed.update(buffer)\n    return to_ascii(hashed.digest())[:8]"}, {"instruction": "def get_odoo_args(self, ctx):\n        \"\"\"Return a list of Odoo command line arguments from the Click context.\"\"\"\n", "input": "", "output": "        config = ctx.params.get(\"config\")\n        addons_path = ctx.params.get(\"addons_path\")\n        database = ctx.params.get(\"database\")\n        log_level = ctx.params.get(\"log_level\")\n        logfile = ctx.params.get(\"logfile\")\n\n        odoo_args = []\n\n        if config:\n            odoo_args.extend([\"--config\", config])\n        if addons_path:\n            odoo_args.extend([\"--addons-path\", addons_path])\n        if database:\n            odoo_args.extend([\"--database\", database])\n        if log_level:\n            odoo_args.extend([\"--log-level\", log_level])\n        if logfile:\n            odoo_args.extend([\"--logfile\", logfile])\n\n        return odoo_args"}, {"instruction": "def request_generic(self, act, coro, perform, complete):\r\n        \"\"\"\r\n        Performs an overlapped request (via `perform` callable) and saves\r\n        the token and the (`overlapped`, `perform`, `complete`) trio.\r\n        \"\"\"\n", "input": "", "output": "        overlapped = OVERLAPPED()\r\n        overlapped.object = act\r\n        self.add_token(act, coro, (overlapped, perform, complete))\r\n\r\n        rc, nbytes = perform(act, overlapped)\r\n        completion_key = c_long(0)\r\n        if rc == 0:\r\n            # ah geez, it didn't got in the iocp, we have a result!\r\n            pass\r\n\r\n\r\n            # ok this is weird, apparently this doesn't need to be requeued\r\n            #  - need to investigate why (TODO)\r\n            #~ PostQueuedCompletionStatus(\r\n                #~ self.iocp, # HANDLE CompletionPort\r\n                #~ nbytes, # DWORD dwNumberOfBytesTransferred\r\n                #~ byref(completion_key), # ULONG_PTR dwCompletionKey\r\n                #~ overlapped # LPOVERLAPPED lpOverlapped\r\n            #~ )\r\n        elif rc != WSA_IO_PENDING:\r\n            self.remove_token(act)\r\n            raise SocketError(rc, \"%s on %r\" % (ctypes.FormatError(rc), act))"}, {"instruction": "def is_multisig_script(script, blockchain='bitcoin', **blockchain_opts):\n    \"\"\"\n    Is the given script a multisig script?\n    \"\"\"\n", "input": "", "output": "    if blockchain == 'bitcoin':\n        return btc_is_multisig_script(script, **blockchain_opts)\n    else:\n        raise ValueError('Unknown blockchain \"{}\"'.format(blockchain))"}, {"instruction": "def issue_date(self):\n        \"\"\"Date when the DOI was issued (:class:`datetime.datetime.Datetime`).\n        \"\"\"\n", "input": "", "output": "        dates = _pluralize(self._r['dates'], 'date')\n        for date in dates:\n            if date['@dateType'] == 'Issued':\n                return datetime.datetime.strptime(date['#text'], '%Y-%m-%d')"}, {"instruction": "def build_params(self, params, i):\n        \"\"\"\n        Populates a dictionary with the name/value pairs necessary\n        to identify this Tag in a request.\n        \"\"\"\n", "input": "", "output": "        prefix = 'Tags.member.%d.' % i\n        params[prefix+'ResourceId'] = self.resource_id\n        params[prefix+'ResourceType'] = self.resource_type\n        params[prefix+'Key'] = self.key\n        params[prefix+'Value'] = self.value\n        if self.propagate_at_launch:\n            params[prefix+'PropagateAtLaunch'] = 'true'\n        else:\n            params[prefix+'PropagateAtLaunch'] = 'false'"}, {"instruction": "def has_gap_in_elf_shndx(self):\n        \"\"\"Return the has gap in elf shndx attribute of the BFD file being\n        processed.\n        \"\"\"\n", "input": "", "output": "        if not self._ptr:\n            raise BfdException(\"BFD not initialized\")\n\n        return _bfd.get_bfd_attribute(\n            self._ptr, BfdAttributes.HAS_GAP_IN_ELF_SHNDX)"}, {"instruction": "def contains_rigid(self):\n        \"\"\"Returns True if the Compound contains rigid bodies\n\n        If the Compound contains any particle with a rigid_id != None\n        then contains_rigid will return True. If the Compound has no\n        children (i.e. the Compound resides at the bottom of the containment\n        hierarchy) then contains_rigid will return False.\n\n        Returns\n        -------\n        bool\n            True if the Compound contains any particle with a rigid_id != None\n\n        Notes\n        -----\n        The private variable '_check_if_contains_rigid_bodies' is used to help\n        cache the status of 'contains_rigid'. If '_check_if_contains_rigid_bodies'\n        is False, then the rigid body containment of the Compound has not changed,\n        and the particle tree is not traversed, boosting performance.\n\n        \"\"\"\n", "input": "", "output": "        if self._check_if_contains_rigid_bodies:\n            self._check_if_contains_rigid_bodies = False\n            if any(particle.rigid_id is not None for particle in self._particles()):\n                self._contains_rigid = True\n            else:\n                self._contains_rigid = False\n        return self._contains_rigid"}, {"instruction": "def add(self, tool):\n        \"\"\"\n        Adds a Tool to the list, logs the reference and TODO\n        \"\"\"\n", "input": "", "output": "        self.lstTools.append(tool)\n        self.lg.record_process(self._get_tool_str(tool))"}, {"instruction": "def cancel_port_forward(self, address, port):\n        \"\"\"\n        Ask the server to cancel a previous port-forwarding request.  No more\n        connections to the given address & port will be forwarded across this\n        ssh connection.\n\n        :param str address: the address to stop forwarding\n        :param int port: the port to stop forwarding\n        \"\"\"\n", "input": "", "output": "        if not self.active:\n            return\n        self._tcp_handler = None\n        self.global_request(\"cancel-tcpip-forward\", (address, port), wait=True)"}, {"instruction": "def reply(self, connection, reply, orig_req):\n        \"\"\"Send an asynchronous reply to an earlier request.\n\n        Parameters\n        ----------\n        connection : ClientConnection object\n            The client to send the reply to.\n        reply : Message object\n            The reply message to send.\n        orig_req : Message object\n            The request message being replied to. The reply message's\n            id is overridden with the id from orig_req before the\n            reply is sent.\n\n        \"\"\"\n", "input": "", "output": "        if isinstance(connection, ClientRequestConnection):\n            self._logger.warn(\n                'Deprecation warning: do not use self.reply() '\n                'within a reply handler context -- use req.reply(*msg_args)\\n'\n                'or req.reply_with_message(msg) Traceback:\\n %s',\n                \"\".join(traceback.format_stack()))\n            # Get the underlying ClientConnection instance\n            connection = connection.client_connection\n        connection.reply(reply, orig_req)"}, {"instruction": "def set_iam_policy(self, policy):\n        \"\"\"Sets the access control policy on an instance resource. Replaces any\n        existing policy.\n\n        For more information about policy, please see documentation of\n        class `google.cloud.bigtable.policy.Policy`\n\n        For example:\n\n        .. literalinclude:: snippets.py\n            :start-after: [START bigtable_set_iam_policy]\n            :end-before: [END bigtable_set_iam_policy]\n\n        :type policy: :class:`google.cloud.bigtable.policy.Policy`\n        :param policy: A new IAM policy to replace the current IAM policy\n                       of this instance\n\n        :rtype: :class:`google.cloud.bigtable.policy.Policy`\n        :returns: The current IAM policy of this instance.\n        \"\"\"\n", "input": "", "output": "        instance_admin_client = self._client.instance_admin_client\n        resp = instance_admin_client.set_iam_policy(\n            resource=self.name, policy=policy.to_pb()\n        )\n        return Policy.from_pb(resp)"}, {"instruction": "def get_default_query_from_module(module):\n  \"\"\" Given a %%sql module return the default (last) query for the module.\n\n  Args:\n    module: the %%sql module.\n\n  Returns:\n    The default query associated with this module.\n  \"\"\"\n", "input": "", "output": "  if isinstance(module, types.ModuleType):\n    return module.__dict__.get(_SQL_MODULE_LAST, None)\n  return None"}, {"instruction": "def supervisor(self):\n        \"\"\"Return an authenticated connection for use, open new if required.\n\n        Returns:\n            SupervisorWebService: New or existing session with the Five9\n            Statistics API.\n        \"\"\"\n", "input": "", "output": "        supervisor = self._cached_client('supervisor')\n        if not self._api_supervisor_session:\n            self._api_supervisor_session = self.__create_supervisor_session(\n                supervisor,\n            )\n        return supervisor"}, {"instruction": "def post_login(self, came_from=lurl('/')):\n        \"\"\"\n        Redirect the user to the initially requested page on successful\n        authentication or redirect her back to the login page if login failed.\n\n        \"\"\"\n", "input": "", "output": "        if not request.identity:\n            login_counter = request.environ.get('repoze.who.logins', 0) + 1\n            redirect('/login',\n                params=dict(came_from=came_from, __logins=login_counter))\n        userid = request.identity['repoze.who.userid']\n        flash(_('Welcome back, %s!') % userid)\n        redirect(came_from)"}, {"instruction": "def extract_files_from_dict(d):\n    \"\"\"Return any file objects from the provided dict.\n\n    >>> extract_files_from_dict({\n    ... 'oauth_token': 'foo',\n    ... 'track': {\n    ...   'title': 'bar',\n    ...   'asset_data': open('setup.py', 'rb')\n    ...  }})  # doctest:+ELLIPSIS\n    {'track': {'asset_data': <...}}\n    \"\"\"\n", "input": "", "output": "    files = {}\n    for key, value in six.iteritems(d):\n        if isinstance(value, dict):\n            files[key] = extract_files_from_dict(value)\n        elif is_file_like(value):\n            files[key] = value\n    return files"}, {"instruction": "def _docf(self, tag, val):\n        \"\"\"\n        Callback used as the handler argument to process_docs(). This converts\n        Stone doc references to Sphinx-friendly annotations.\n        \"\"\"\n", "input": "", "output": "        if tag == 'type':\n            return ':class:`{}`'.format(val)\n        elif tag == 'route':\n            if self.args.route_method:\n                return ':meth:`%s`' % self.args.route_method.format(\n                    ns=self.cur_namespace.name, route=fmt_func(val))\n            else:\n                return val\n        elif tag == 'link':\n            anchor, link = val.rsplit(' ', 1)\n            return '`{} <{}>`_'.format(anchor, link)\n        elif tag == 'val':\n            if val == 'null':\n                return 'None'\n            elif val == 'true' or val == 'false':\n                return '``{}``'.format(val.capitalize())\n            else:\n                return val\n        elif tag == 'field':\n            return '``{}``'.format(val)\n        else:\n            raise RuntimeError('Unknown doc ref tag %r' % tag)"}, {"instruction": "def format_rst(self):\n        \"\"\"\n        return table in RST format\n        \"\"\"\n", "input": "", "output": "        res = ''\n        num_cols = len(self.header)\n        col_width = 25\n        for _ in range(num_cols):\n            res += ''.join(['=' for _ in range(col_width - 1)]) + ' ' \n        res += '\\n'\n        for c in self.header:\n            res += c.ljust(col_width) \n        res += '\\n'\n        for _ in range(num_cols):\n            res += ''.join(['=' for _ in range(col_width - 1)]) + ' ' \n        res += '\\n'\n        for row in self.arr:\n            for c in row:\n                res += self.force_to_string(c).ljust(col_width)\n            res += '\\n' \n        for _ in range(num_cols):\n            res += ''.join(['=' for _ in range(col_width - 1)]) + ' ' \n        res += '\\n'\n        return res"}, {"instruction": "def proofMethods():\n    \"\"\"\n    Run the full protocol including proof generation and verification.\n    \"\"\"\n", "input": "", "output": "    r, x = blind(m)\n    y,kw,tTilde = eval(w,t,x,msk,s)\n\n    # Proof in Gt/Gt\n    pi = proveGt(x, tTilde, kw, y)\n    verifyGt(x, tTilde, y, pi, errorOnFail=True)\n\n    # Proof in G1/Gt\n    pi = proveG1(x, tTilde, kw, y)\n    verifyG1(x, tTilde, y, pi, errorOnFail=True)\n\n    z = deblind(r, y)"}, {"instruction": "def acquire_lock(self, name):\n        \"\"\"\n        Wait for a lock with name.\n        This will prevent other processes from acquiring the lock with\n        the name while it is held. Thus they will wait in the position\n        where they are acquiring the lock until the process that has it\n        releases it.\n        \"\"\"\n", "input": "", "output": "        if self._remotelib:\n            try:\n                while not self._remotelib.run_keyword('acquire_lock',\n                                                      [name, self._my_id], {}):\n                    time.sleep(0.1)\n                    logger.debug('waiting for lock to release')\n                return True\n            except RuntimeError:\n                logger.warn('no connection')\n                self.__remotelib = None\n        return _PabotLib.acquire_lock(self, name, self._my_id)"}, {"instruction": "async def _wait(self):\n        '''\n        Wait on the other editatoms who are constructing nodes my new nodes refer to\n        '''\n", "input": "", "output": "        for buid in self.otherbldgbuids:\n            nodeevnt = self.allbldgbuids.get(buid)\n            if nodeevnt is None:\n                continue\n            await nodeevnt[1].wait()"}, {"instruction": "def get_vnetwork_dvpgs_input_vcenter(self, **kwargs):\n        \"\"\"Auto Generated Code\n        \"\"\"\n", "input": "", "output": "        config = ET.Element(\"config\")\n        get_vnetwork_dvpgs = ET.Element(\"get_vnetwork_dvpgs\")\n        config = get_vnetwork_dvpgs\n        input = ET.SubElement(get_vnetwork_dvpgs, \"input\")\n        vcenter = ET.SubElement(input, \"vcenter\")\n        vcenter.text = kwargs.pop('vcenter')\n\n        callback = kwargs.pop('callback', self._callback)\n        return callback(config)"}, {"instruction": "def memoize(func):\n    '''\n    Memoize aka cache the return output of a function\n    given a specific set of arguments\n\n    .. versionedited:: 2016.3.4\n\n    Added **kwargs support.\n    '''\n", "input": "", "output": "    cache = {}\n\n    @wraps(func)\n    def _memoize(*args, **kwargs):\n        str_args = []\n        for arg in args:\n            if not isinstance(arg, six.string_types):\n                str_args.append(six.text_type(arg))\n            else:\n                str_args.append(arg)\n\n        args_ = ','.join(list(str_args) + ['{0}={1}'.format(k, kwargs[k]) for k in sorted(kwargs)])\n        if args_ not in cache:\n            cache[args_] = func(*args, **kwargs)\n        return cache[args_]\n\n    return _memoize"}, {"instruction": "def show_version(a_device):\n    \"\"\"Execute show version command using Netmiko.\"\"\"\n", "input": "", "output": "    remote_conn = ConnectHandler(**a_device)\n    print()\n    print(\"#\" * 80)\n    print(remote_conn.send_command_expect(\"show version\"))\n    print(\"#\" * 80)\n    print()"}, {"instruction": "def _pairwise_chisq(self):\n        \"\"\"Pairwise comparisons (Chi-Square) along axis, as numpy.ndarray.\n\n        Returns a list of square and symmetric matrices of test statistics for the null\n        hypothesis that each vector along *axis* is equal to each other.\n        \"\"\"\n", "input": "", "output": "        return [\n            self._chi_squared(\n                mr_subvar_proportions,\n                self._margin[idx],\n                self._opposite_axis_margin[idx]\n                / np.sum(self._opposite_axis_margin[idx]),\n            )\n            for (idx, mr_subvar_proportions) in enumerate(self._proportions)\n        ]"}, {"instruction": "def get_full_import_name(import_from, name):\n    \"\"\"Get the full path of a name from a ``from x import y`` statement.\n\n    :param import_from: The astroid node to resolve the name of.\n    :type import_from: astroid.nodes.ImportFrom\n    :param name:\n    :type name: str\n\n    :returns: The full import path of the name.\n    :rtype: str\n    \"\"\"\n", "input": "", "output": "    partial_basename = resolve_import_alias(name, import_from.names)\n\n    module_name = import_from.modname\n    if import_from.level:\n        module = import_from.root()\n        assert isinstance(module, astroid.nodes.Module)\n        module_name = module.relative_to_absolute_name(\n            import_from.modname, level=import_from.level\n        )\n\n    return \"{}.{}\".format(module_name, partial_basename)"}, {"instruction": "def close(self, code=None):\n        '''return a `close` :class:`Frame`.\n        '''\n", "input": "", "output": "        code = code or 1000\n        body = pack('!H', code)\n        body += self._close_codes.get(code, '').encode('utf-8')\n        return self.encode(body, opcode=0x8)"}, {"instruction": "def is_whitelisted(self, addrinfo):\n        \"\"\"\n        Returns if a result of ``socket.getaddrinfo`` is in the socket address\n        whitelist.\n        \"\"\"\n", "input": "", "output": "        # For details about the ``getaddrinfo`` struct, see the Python docs:\n        # http://docs.python.org/library/socket.html#socket.getaddrinfo\n        family, socktype, proto, canonname, sockaddr = addrinfo\n        address, port = sockaddr[:2]\n        return address in self.socket_address_whitelist"}, {"instruction": "def selection(self):\n        \"\"\"Returns items in selection as a QItemSelection object\"\"\"\n", "input": "", "output": "        sel = QtGui.QItemSelection()\n        for index in self.selectedIndexes():\n            sel.select(index, index)\n        return sel"}, {"instruction": "def get_content_type_charset(self, default='UTF-8'):\n\t\t\"\"\"\n\t\tInspect the Content-Type header to retrieve the charset that the client\n\t\thas specified.\n\n\t\t:param str default: The default charset to return if none exists.\n\t\t:return: The charset of the request.\n\t\t:rtype: str\n\t\t\"\"\"\n", "input": "", "output": "\t\tencoding = default\n\t\theader = self.headers.get('Content-Type', '')\n\t\tidx = header.find('charset=')\n\t\tif idx > 0:\n\t\t\tencoding = (header[idx + 8:].split(' ', 1)[0] or encoding)\n\t\treturn encoding"}, {"instruction": "def delete_refs(repo, refs, dry_run=False):\n    \"\"\"Note that only the ref to a tag can be explicitly removed.  The tag\n    object will leave on until it's gargabe collected.\"\"\"\n", "input": "", "output": "\n    assert isinstance(repo, github.Repository.Repository), type(repo)\n\n    debug(\"removing {n} refs from {repo}\".format(\n        n=len(refs),\n        repo=repo.full_name)\n    )\n\n    for r in refs:\n        debug(\"  deleting {ref}\".format(ref=r.ref))\n        if dry_run:\n            debug('    (noop)')\n            continue\n\n        r.delete()"}, {"instruction": "def render_GET(self, request):\n    \"\"\"Renders a GET request, by showing this nodes stats and children.\"\"\"\n", "input": "", "output": "    fullPath = request.path.split('/')\n    if not fullPath[-1]:\n      fullPath = fullPath[:-1]\n    parts = fullPath[2:]\n    statDict = util.lookup(scales.getStats(), parts)\n\n    if statDict is None:\n      request.setResponseCode(404)\n      return \"Path not found.\"\n\n    if 'query' in request.args:\n      query = request.args['query'][0]\n    else:\n      query = None\n\n    if 'format' in request.args and request.args['format'][0] == 'json':\n      request.headers['content-type'] = 'text/javascript; charset=UTF-8'\n      formats.jsonFormat(request, statDict, query)\n    elif 'format' in request.args and request.args['format'][0] == 'prettyjson':\n      request.headers['content-type'] = 'text/javascript; charset=UTF-8'\n      formats.jsonFormat(request, statDict, query, pretty=True)\n    else:\n      formats.htmlHeader(request, '/' + '/'.join(parts), self.serverName, query)\n      formats.htmlFormat(request, tuple(parts), statDict, query)\n\n    return ''"}, {"instruction": "async def async_init(self):\n        \"\"\"\n        Handle here the asynchronous part of the init.\n        \"\"\"\n", "input": "", "output": "\n        self.pool = await aioredis.create_pool(\n            (self.host, self.port),\n            db=self.db_id,\n            minsize=self.min_pool_size,\n            maxsize=self.max_pool_size,\n            loop=asyncio.get_event_loop(),\n        )"}, {"instruction": "def _get_netengine_arguments(self, required=False):\n        \"\"\"\n        returns list of available config params\n        returns list of required config params if required is True\n        for internal use only\n        \"\"\"\n", "input": "", "output": "        # inspect netengine class\n        backend_class = self._get_netengine_backend()\n        argspec = inspect.getargspec(backend_class.__init__)\n        # store args\n        args = argspec.args\n        # remove known arguments\n        for argument_name in ['self', 'host', 'port']:\n            args.remove(argument_name)\n\n        if required:\n            # list of default values\n            default_values = list(argspec.defaults)\n            # always remove last default value, which is port number\n            default_values = default_values[0:-1]\n\n            # remove an amount of arguments equals to number of default values, starting from right\n            args = args[0:len(args)-len(default_values)]\n\n        return args"}, {"instruction": "def get_flat_models_from_models(models: Sequence[Type['main.BaseModel']]) -> Set[Type['main.BaseModel']]:\n    \"\"\"\n    Take a list of ``models`` and generate a set with them and all their sub-models in their trees. I.e. if you pass\n    a list of two models, ``Foo`` and ``Bar``, both subclasses of Pydantic ``BaseModel`` as models, and ``Bar`` has\n    a field of type ``Baz`` (also subclass of ``BaseModel``), the return value will be ``set([Foo, Bar, Baz])``.\n    \"\"\"\n", "input": "", "output": "    flat_models: Set[Type['main.BaseModel']] = set()\n    for model in models:\n        flat_models |= get_flat_models_from_model(model)\n    return flat_models"}, {"instruction": "def chown(self, path, uid, gid):\n        \"\"\"\n        Change the owner (``uid``) and group (``gid``) of a file.  As with\n        Python's `os.chown` function, you must pass both arguments, so if you\n        only want to change one, use `stat` first to retrieve the current\n        owner and group.\n\n        :param str path: path of the file to change the owner and group of\n        :param int uid: new owner's uid\n        :param int gid: new group id\n        \"\"\"\n", "input": "", "output": "        path = self._adjust_cwd(path)\n        self._log(DEBUG, \"chown({!r}, {!r}, {!r})\".format(path, uid, gid))\n        attr = SFTPAttributes()\n        attr.st_uid, attr.st_gid = uid, gid\n        self._request(CMD_SETSTAT, path, attr)"}, {"instruction": "def entityId(self, partial, channel=None):\n        '''Get an entity's full id provided a partial one.\n\n        Raises EntityNotFound if partial cannot be resolved.\n        @param partial The partial id (e.g. mysql, precise/mysql).\n        @param channel Optional channel name.\n        '''\n", "input": "", "output": "        url = '{}/{}/meta/any'.format(self.url, _get_path(partial))\n        data = self._get(_add_channel(url, channel))\n        return data.json()['Id']"}, {"instruction": "def mutation(self, config=None, info=None, save_dir=None):\n        \"\"\"\n        Parameters\n        ----------\n        config : str\n        info : str\n        save_dir : str\n        \"\"\"\n", "input": "", "output": "        self.result = None\n        self.config = config\n        self.restore_dir = self.save_dir\n        self.save_dir = save_dir\n        self.info = info"}, {"instruction": "def initialize_segment_register_x64(self, state, concrete_target):\n        \"\"\"\n        Set the fs register in the angr to the value of the fs register in the concrete process\n\n        :param state:               state which will be modified\n        :param concrete_target:     concrete target that will be used to read the fs register\n        :return: None\n        \"\"\"\n", "input": "", "output": "        _l.debug(\"Synchronizing fs segment register\")\n        state.regs.fs = self._read_fs_register_x64(concrete_target)"}, {"instruction": "def _aix_memdata():\n    '''\n    Return the memory information for AIX systems\n    '''\n", "input": "", "output": "    grains = {'mem_total': 0, 'swap_total': 0}\n    prtconf = salt.utils.path.which('prtconf')\n    if prtconf:\n        for line in __salt__['cmd.run'](prtconf, python_shell=True).splitlines():\n            comps = [x for x in line.strip().split(' ') if x]\n            if len(comps) > 2 and 'Memory' in comps[0] and 'Size' in comps[1]:\n                grains['mem_total'] = int(comps[2])\n                break\n    else:\n        log.error('The \\'prtconf\\' binary was not found in $PATH.')\n\n    swap_cmd = salt.utils.path.which('swap')\n    if swap_cmd:\n        swap_data = __salt__['cmd.run']('{0} -s'.format(swap_cmd)).split()\n        try:\n            swap_total = (int(swap_data[-2]) + int(swap_data[-6])) * 4\n        except ValueError:\n            swap_total = None\n        grains['swap_total'] = swap_total\n    else:\n        log.error('The \\'swap\\' binary was not found in $PATH.')\n    return grains"}, {"instruction": "def Axn(mt, x, n):\n    \"\"\" (A^1)x:n : Returns the EPV (net single premium) of a term insurance. \"\"\"\n", "input": "", "output": "    return (mt.Mx[x] - mt.Mx[x + n]) / mt.Dx[x]"}, {"instruction": "def set_volume(self, volume):\n        \"\"\"Set a new volume level.\"\"\"\n", "input": "", "output": "        if volume > 100 or volume < 0:\n            raise Exception('Bad request to volume control. '\n                            'Must be between 0 and 100')\n        params = ('<InstanceID>0</InstanceID><Channel>Master</Channel>'\n                  '<DesiredVolume>{}</DesiredVolume>').format(volume)\n        self.soap_request(URL_CONTROL_DMR, URN_RENDERING_CONTROL,\n                          'SetVolume', params)"}, {"instruction": "def incidental (self):\n        \"\"\" Returns incidental properties.\n        \"\"\"\n", "input": "", "output": "        result = [p for p in self.lazy_properties if p.feature.incidental]\n        result.extend(self.incidental_)\n        return result"}, {"instruction": "def to_protobuf(self):\n        \"\"\"Convert the current object to protobuf.\n\n        :rtype: :class:`google.type.latlng_pb2.LatLng`.\n        :returns: The current point as a protobuf.\n        \"\"\"\n", "input": "", "output": "        return latlng_pb2.LatLng(latitude=self.latitude, longitude=self.longitude)"}, {"instruction": "def log_request(self, code='-', size='-'):\n        \"\"\"Logs the current request.\"\"\"\n", "input": "", "output": "        print_size = getattr(thread_local, 'size', -1)\n        if size != '-':\n            size_str = ' (%s)' % size\n        elif print_size >= 0:\n            size_str = self.log_size_string(print_size) + ' '\n        else:\n            size_str = ''\n        if not self.server.suppress_noise or (code != 200 and code != 304):\n            self.log_message(\n                '%s\"%s\" %s', size_str, self.requestline, str(code))\n        if print_size >= 0:\n            thread_local.size = -1"}, {"instruction": "def indexTupleFromItem(self, treeItem): # TODO: move to BaseTreeItem?\n        \"\"\" Return (first column model index, last column model index) tuple for a configTreeItem\n        \"\"\"\n", "input": "", "output": "        if not treeItem:\n            return (QtCore.QModelIndex(), QtCore.QModelIndex())\n\n        if not treeItem.parentItem: # TODO: only necessary because of childNumber?\n            return (QtCore.QModelIndex(), QtCore.QModelIndex())\n\n        # Is there a bug in Qt in QStandardItemModel::indexFromItem?\n        # It passes the parent in createIndex. TODO: investigate\n\n        row =  treeItem.childNumber()\n        return (self.createIndex(row, 0, treeItem),\n                self.createIndex(row, self.columnCount() - 1, treeItem))"}, {"instruction": "def _field_sort_name(cls, name):\n        \"\"\"Get a sort key for a field name that determines the order\n        fields should be written in.\n\n        Fields names are kept unchanged, unless they are instances of\n        :class:`DateItemField`, in which case `year`, `month`, and `day`\n        are replaced by `date0`, `date1`, and `date2`, respectively, to\n        make them appear in that order.\n        \"\"\"\n", "input": "", "output": "        if isinstance(cls.__dict__[name], DateItemField):\n            name = re.sub('year',  'date0', name)\n            name = re.sub('month', 'date1', name)\n            name = re.sub('day',   'date2', name)\n        return name"}, {"instruction": "def freeze_subjects(self):\n        \"\"\"Converts variable data into numpy arrays.\n\n        This is required after all subjects have been added via the\n        add_subject function, since we don't know ahead of time who is\n        participating in the analysis due to various filtering possibilities.\n        \"\"\"\n", "input": "", "output": "        self.phenotype_data = numpy.array(self.phenotype_data)\n        self.covariate_data = numpy.array(self.covariate_data)"}, {"instruction": "def get_expanded_schema(self, schema_name):\n        \"\"\"\n        Return a schema file with all $ref properties expanded\n        \"\"\"\n", "input": "", "output": "        if schema_name not in self.expanded_schemas:\n            fn = self.get_schema_file(schema_name)\n            schemas_folder = self.get_schemas_folder()\n            base_uri = self.get_schema_path(schemas_folder)\n\n            with open(fn) as f:\n                jsn_schema = jsonref.load(f, base_uri=base_uri)\n\n                # cache the schema for future use\n                self.expanded_schemas[schema_name] = jsn_schema\n        else:\n            jsn_schema = self.expanded_schemas[schema_name]\n\n        return jsn_schema"}, {"instruction": "def update_firmware(self,\n                        hardware_id,\n                        ipmi=True,\n                        raid_controller=True,\n                        bios=True,\n                        hard_drive=True):\n        \"\"\"Update hardware firmware.\n\n        This will cause the server to be unavailable for ~20 minutes.\n\n        :param int hardware_id: The ID of the hardware to have its firmware\n                                updated.\n        :param bool ipmi: Update the ipmi firmware.\n        :param bool raid_controller: Update the raid controller firmware.\n        :param bool bios: Update the bios firmware.\n        :param bool hard_drive: Update the hard drive firmware.\n\n        Example::\n\n            # Check the servers active transactions to see progress\n            result = mgr.update_firmware(hardware_id=1234)\n        \"\"\"\n", "input": "", "output": "\n        return self.hardware.createFirmwareUpdateTransaction(\n            bool(ipmi), bool(raid_controller), bool(bios), bool(hard_drive), id=hardware_id)"}, {"instruction": "def links(self, r_server=None, mask=None):\n        \"\"\"\n        Get LINKS information.\n        Optional arguments:\n        * r_server=None - Forward the query to this server.\n        * mask=None - Match mask servers.\n        \"\"\"\n", "input": "", "output": "        with self.lock:\n            if not r_server:\n                self.send('LINKS')\n            elif not mask and r_server:\n                self.send('LINKS %s' % r_server)\n            else:\n                self.send('LINKS %s %s' % (r_server, mask))\n            links = {}\n            while self.readable():\n                msg = self._recv(expected_replies=('364', '365'))\n                segments = msg[2].split()\n                if msg[0] == '364':\n                    server = segments[0]\n                    desc = ' '.join(segments[3:])\n                    links[server] = desc\n                elif msg[0] == '365':\n                    break\n            return links"}, {"instruction": "def get_instance(self, payload):\n        \"\"\"\n        Build an instance of AllTimeInstance\n\n        :param dict payload: Payload response from the API\n\n        :returns: twilio.rest.api.v2010.account.usage.record.all_time.AllTimeInstance\n        :rtype: twilio.rest.api.v2010.account.usage.record.all_time.AllTimeInstance\n        \"\"\"\n", "input": "", "output": "        return AllTimeInstance(self._version, payload, account_sid=self._solution['account_sid'], )"}, {"instruction": "def iter(self, name):\n        '''\n        Iterate through values added with add() from each scope frame.\n        '''\n", "input": "", "output": "        for frame in self.frames:\n            vals = frame.get(name)\n            if vals is None:\n                continue\n            for valu in vals:\n                yield valu"}, {"instruction": "def add_slices(self, dashboard_id):\n        \"\"\"Add and save slices to a dashboard\"\"\"\n", "input": "", "output": "        data = json.loads(request.form.get('data'))\n        session = db.session()\n        Slice = models.Slice  # noqa\n        dash = (\n            session.query(models.Dashboard).filter_by(id=dashboard_id).first())\n        check_ownership(dash, raise_if_false=True)\n        new_slices = session.query(Slice).filter(\n            Slice.id.in_(data['slice_ids']))\n        dash.slices += new_slices\n        session.merge(dash)\n        session.commit()\n        session.close()\n        return 'SLICES ADDED'"}, {"instruction": "def set(self, name, value):\n        \"\"\"\n        Sets the value of the field `name` to `value`, which is `True` or\n        `False`.\n        \"\"\"\n", "input": "", "output": "        flag = self.flags[name]\n        self._value = (self.value | flag) if value else (self.value & ~flag)"}, {"instruction": "def filter(self, record):\n        \"\"\"Add contextual information to the log record\n\n        :param record: the log record\n        :type record: :class:`logging.LogRecord`\n        :returns: True, if log should get sent\n        :rtype: :class:`bool`\n        :raises: None\n        \"\"\"\n", "input": "", "output": "        record.sitename = self.sitename\n        record.platform = self.platform\n        record.jobid = self.jobid\n        record.submitter = self.logname\n        record.jobname = self.jobname\n        record.queue = self.queue\n        record.fqdn = self.fqdn\n        return True"}, {"instruction": "def get_config_env() -> Dict[str, Any]:\n    \"\"\"\n    Returns the environment map that will be used for config checking when variables aren't set.\n    \"\"\"\n", "input": "", "output": "    if 'PULUMI_CONFIG' in os.environ:\n        env_config = os.environ['PULUMI_CONFIG']\n        return json.loads(env_config)\n    return dict()"}, {"instruction": "def i18n_system_locale():\n    \"\"\"\n    Return the system locale\n    :return: the system locale (as a string)\n    \"\"\"\n", "input": "", "output": "    log.debug('i18n_system_locale() called')\n    lc, encoding = locale.getlocale()\n    log.debug('locale.getlocale() = (lc=\"{lc}\", encoding=\"{encoding}).'.format(lc=lc, encoding=encoding))\n    if lc is None:\n        lc, encoding = locale.getdefaultlocale()\n        log.debug('locale.getdefaultlocale() = (lc=\"{lc}\", encoding=\"{encoding}).'.format(lc=lc, encoding=encoding))\n    return lc"}, {"instruction": "def folderitems(self, full_objects=False, classic=True):\n        \"\"\"Sort by Categories\n        \"\"\"\n", "input": "", "output": "        bsc = getToolByName(self.context, \"bika_setup_catalog\")\n        self.an_cats = bsc(\n            portal_type=\"AnalysisCategory\",\n            sort_on=\"sortable_title\")\n        self.an_cats_order = dict([\n            (b.Title, \"{:04}\".format(a))\n            for a, b in enumerate(self.an_cats)])\n        items = super(AnalysisServicesView, self).folderitems()\n        if self.do_cats:\n            self.categories = map(lambda x: x[0],\n                                  sorted(self.categories, key=lambda x: x[1]))\n        else:\n            self.categories.sort()\n        return items"}, {"instruction": "def _int(ctx, number):\n    \"\"\"\n    Rounds a number down to the nearest integer\n    \"\"\"\n", "input": "", "output": "    return conversions.to_integer(conversions.to_decimal(number, ctx).to_integral_value(ROUND_FLOOR), ctx)"}, {"instruction": "def getSubtotal(self):\n        \"\"\" Compute Subtotal \"\"\"\n", "input": "", "output": "        if self.supplyorder_lineitems:\n            return sum(\n                [(Decimal(obj['Quantity']) * Decimal(obj['Price'])) for obj in self.supplyorder_lineitems])\n        return 0"}, {"instruction": "def decode_hparams(overrides=\"\"):\n  \"\"\"Hparams for decoding.\"\"\"\n", "input": "", "output": "  hparams = decoding.decode_hparams()\n  # Number of interpolations between [0.0, 1.0].\n  hparams.add_hparam(\"num_interp\", 11)\n  # Which level(s) to interpolate.\n  hparams.add_hparam(\"level_interp\", [0, 1, 2])\n  # \"all\" or \"ranked\", interpolate all channels or a \"ranked\".\n  hparams.add_hparam(\"channel_interp\", \"all\")\n  # interpolate channels ranked according to squared L2 norm.\n  hparams.add_hparam(\"rank_interp\", 1)\n  # Whether on not to save frames as summaries\n  hparams.add_hparam(\"save_frames\", True)\n  hparams.parse(overrides)\n  return hparams"}, {"instruction": "def is_parent_of(page1, page2):\n    \"\"\"\n    Determines whether a given page is the parent of another page\n\n    Example::\n\n        {% if page|is_parent_of:feincms_page %} ... {% endif %}\n    \"\"\"\n", "input": "", "output": "\n    try:\n        return page1.tree_id == page2.tree_id and page1.lft < page2.lft and page1.rght > page2.rght\n    except AttributeError:\n        return False"}, {"instruction": "def incr(self, key, delta=1):\n        \"\"\"Increments the specified key value by the specified value.\n       \n        :param str|unicode key:\n    \n        :param int delta:\n\n        :rtype: bool\n        \"\"\"\n", "input": "", "output": "        return uwsgi.cache_inc(key, delta, self.timeout, self.name)"}, {"instruction": "def initialize_media_descriptor(self) -> None:\n        \"\"\"\n        Returns the media descriptor for the first media descriptor where\n        the file can be found.\n        \"\"\"\n", "input": "", "output": "\n        for md in self.media_descriptors:\n            media_path = self.get_media_path(md)\n            if media_path.is_file():\n                self.media_descriptor = md\n                return\n\n        raise FileNotFoundError(\n            "}, {"instruction": "def update(self, cardconnection, ccevent):\n        '''CardConnectionObserver callback.'''\n", "input": "", "output": "\n        apduline = \"\"\n        if 'connect' == ccevent.type:\n            apduline += 'connecting to ' + cardconnection.getReader()\n\n        elif 'disconnect' == ccevent.type:\n            apduline += 'disconnecting from ' + cardconnection.getReader()\n\n        elif 'command' == ccevent.type:\n            apduline += '> ' + toHexString(ccevent.args[0])\n\n        elif 'response' == ccevent.type:\n            if [] == ccevent.args[0]:\n                apduline += \"< %-2X %-2X\" % tuple(ccevent.args[-2:])\n            else:\n                apduline += \"< \" + toHexString(ccevent.args[0]) + \\\n                            \"%-2X %-2X\" % tuple(ccevent.args[-2:])\n\n        self.apdutextctrl.AppendText(apduline + \"\\n\")"}, {"instruction": "def evaluate(self, x, y, flux, x_0, y_0, sigma):\n        \"\"\"Model function Gaussian PSF model.\"\"\"\n", "input": "", "output": "\n        return (flux / 4 *\n                ((self._erf((x - x_0 + 0.5) / (np.sqrt(2) * sigma)) -\n                  self._erf((x - x_0 - 0.5) / (np.sqrt(2) * sigma))) *\n                 (self._erf((y - y_0 + 0.5) / (np.sqrt(2) * sigma)) -\n                  self._erf((y - y_0 - 0.5) / (np.sqrt(2) * sigma)))))"}, {"instruction": "def execute(self, **kwargs):\n    \"\"\" commit the current statements from add()\n    \"\"\"\n", "input": "", "output": "    assert self.resp is None, \"Transaction already committed\"\n    try:\n      self.resp = self.db.tx(list(self.edn_iter), **kwargs)\n    except Exception:\n      self.resp = False\n      raise\n    else:\n      self.resolve()\n      self.adds = None\n      self.tmpents = None\n    return self.resp"}, {"instruction": "def _restart_target(self):\n        \"\"\"\n        Restart our Target.\n        \"\"\"\n", "input": "", "output": "        if self._server:\n            if self._server.returncode is None:\n                self._server.kill()\n                time.sleep(0.2)\n        self._server = subprocess.Popen(\"python session_server.py\", stdout=subprocess.PIPE, stderr=subprocess.PIPE, shell=True)\n        time.sleep(0.2)"}, {"instruction": "def html_page_context(app, pagename, templatename, context, doctree):\n    ''' Collect page names for the sitemap as HTML pages are built.\n\n    '''\n", "input": "", "output": "    site = context['SITEMAP_BASE_URL']\n    version = context['version']\n    app.sitemap_links.add(site + version + '/' + pagename + \".html\")"}, {"instruction": "def _findLocation(self, reference_name, start, end):\n        \"\"\"\n        return a location key form the locationMap\n        \"\"\"\n", "input": "", "output": "        try:\n            # TODO - sequence_annotations does not have build?\n            return self._locationMap['hg19'][reference_name][start][end]\n        except:\n            return None"}, {"instruction": "def send(self, data):\n        \"\"\"\n        Sends data to the server.\n        \"\"\"\n", "input": "", "output": "        self.logger.debug('Send data: {}'.format(data))\n\n        if not self.connected:\n            self.logger.warning('Connection not established. Return...')\n            return\n\n        self.websocket.send(json.dumps(data))"}, {"instruction": "def truncate(self, percentage):\n        \"\"\"\n        Truncate ``percentage`` / 2 [%] of whole time from first and last time.\n\n        :param float percentage: Percentage of truncate.\n\n        :Sample Code:\n            .. code:: python\n\n                from datetimerange import DateTimeRange\n                time_range = DateTimeRange(\n                    \"2015-03-22T10:00:00+0900\", \"2015-03-22T10:10:00+0900\")\n                time_range.is_output_elapse = True\n                print(time_range)\n                time_range.truncate(10)\n                print(time_range)\n        :Output:\n            .. parsed-literal::\n\n                2015-03-22T10:00:00+0900 - 2015-03-22T10:10:00+0900 (0:10:00)\n                2015-03-22T10:00:30+0900 - 2015-03-22T10:09:30+0900 (0:09:00)\n        \"\"\"\n", "input": "", "output": "\n        self.validate_time_inversion()\n\n        if percentage < 0:\n            raise ValueError(\"discard_percent must be greater or equal to zero: \" + str(percentage))\n\n        if percentage == 0:\n            return\n\n        discard_time = self.timedelta // int(100) * int(percentage / 2)\n\n        self.__start_datetime += discard_time\n        self.__end_datetime -= discard_time"}, {"instruction": "def copy(self):\n        \"\"\"\n        Returns a copy of the datamat.\n        \"\"\"\n", "input": "", "output": "        return self.filter(np.ones(self._num_fix).astype(bool))"}, {"instruction": "def to_np(*args):\n    \"\"\" convert GPU arras to numpy and return them\"\"\"\n", "input": "", "output": "    if len(args) > 1:\n        return (cp.asnumpy(x) for x in args)\n    else:\n        return cp.asnumpy(args[0])"}, {"instruction": "def _NTU_from_P_solver(P1, R1, NTU_min, NTU_max, function, **kwargs):\n    '''Private function to solve the P-NTU method backwards, given the\n    function to use, the upper and lower NTU bounds for consideration,\n    and the desired P1 and R1 values.\n    '''\n", "input": "", "output": "    P1_max = _NTU_from_P_objective(NTU_max, R1, 0, function, **kwargs)\n    P1_min = _NTU_from_P_objective(NTU_min, R1, 0, function, **kwargs)\n    if P1 > P1_max:\n        raise ValueError('No solution possible gives such a high P1; maximum P1=%f at NTU1=%f' %(P1_max, NTU_max))\n    if P1 < P1_min:\n        raise ValueError('No solution possible gives such a low P1; minimum P1=%f at NTU1=%f' %(P1_min, NTU_min))\n    # Construct the function as a lambda expression as solvers don't support kwargs\n    to_solve = lambda NTU1: _NTU_from_P_objective(NTU1, R1, P1, function, **kwargs)\n    return ridder(to_solve, NTU_min, NTU_max)"}, {"instruction": "def _handle_clear(self, load):\n        '''\n        Process a cleartext command\n\n        :param dict load: Cleartext payload\n        :return: The result of passing the load to a function in ClearFuncs corresponding to\n                 the command specified in the load's 'cmd' key.\n        '''\n", "input": "", "output": "        log.trace('Clear payload received with command %s', load['cmd'])\n        cmd = load['cmd']\n        if cmd.startswith('__'):\n            return False\n        if self.opts['master_stats']:\n            start = time.time()\n        ret = getattr(self.clear_funcs, cmd)(load), {'fun': 'send_clear'}\n        if self.opts['master_stats']:\n            stats = salt.utils.event.update_stats(self.stats, start, load)\n            self._post_stats(stats)\n        return ret"}, {"instruction": "def compute_rewards(self, scores):\n        \"\"\"Compute the velocity of the best scores\n\n        The velocities are the k distances between the k+1 best scores.\n        \"\"\"\n", "input": "", "output": "        k = self.k\n        m = max(len(scores) - k, 0)\n        best_scores = sorted(scores)[-k - 1:]\n        velocities = np.diff(best_scores)\n        nans = np.full(m, np.nan)\n        return list(velocities) + list(nans)"}, {"instruction": "def _GetFileSystemCacheIdentifier(self, path_spec):\n    \"\"\"Determines the file system cache identifier for the path specification.\n\n    Args:\n      path_spec (PathSpec): path specification.\n\n    Returns:\n      str: identifier of the VFS object.\n    \"\"\"\n", "input": "", "output": "    string_parts = []\n\n    string_parts.append(getattr(path_spec.parent, 'comparable', ''))\n    string_parts.append('type: {0:s}'.format(path_spec.type_indicator))\n\n    return ''.join(string_parts)"}, {"instruction": "def nominal_step(x=None):\n    \"\"\"Return nominal step\"\"\"\n", "input": "", "output": "    if x is None:\n        return 1.0\n    return np.log1p(np.abs(x)).clip(min=1.0)"}, {"instruction": "def _find_tpls(self, name):\n        \"\"\"\n        Return plain, html templates for NAME\n\n        Arguments:\n        - `name`: str\n\n        Return: tuple\n        Exceptions: None\n        \"\"\"\n", "input": "", "output": "        return self._find_tpl(name, extension='.txt'), self._find_tpl(name, extension='.html')"}, {"instruction": "def submit(self, func, *args, **kwargs):\n        \"\"\"Submit a function for serialized execution on sqs\n        \"\"\"\n", "input": "", "output": "        self.op_sequence += 1\n        self.sqs.send_message(\n            QueueUrl=self.map_queue,\n            MessageBody=utils.dumps({'args': args, 'kwargs': kwargs}),\n            MessageAttributes={\n                'sequence_id': {\n                    'StringValue': str(self.op_sequence),\n                    'DataType': 'Number'},\n                'op': {\n                    'StringValue': named(func),\n                    'DataType': 'String',\n                },\n                'ser': {\n                    'StringValue': 'json',\n                    'DataType': 'String'}}\n        )\n\n        self.futures[self.op_sequence] = f = SQSFuture(\n            self.op_sequence)\n        return f"}, {"instruction": "def path_pieces(vault_path):\n    \"\"\"Will return a two part tuple comprising of the vault path\n    and the key with in the stored object\"\"\"\n", "input": "", "output": "    path_bits = vault_path.split('/')\n    path = '/'.join(path_bits[0:len(path_bits) - 1])\n    key = path_bits[len(path_bits) - 1]\n    return path, key"}, {"instruction": "def projection_box(self, min_x, min_y, max_x, max_y):\n        \"\"\"Add a bounding box in projected (native) coordinates to the query.\n\n        This adds a request for a spatial bounding box, bounded by (`min_x`, `max_x`) for\n        x direction and (`min_y`, `max_y`) for the y direction. This modifies the query\n        in-place, but returns ``self`` so that multiple queries can be chained together\n        on one line.\n\n        This replaces any existing spatial queries that have been set.\n\n        Parameters\n        ----------\n        min_x : float\n            The left edge of the bounding box\n        min_y : float\n            The bottom edge of the bounding box\n        max_x : float\n            The right edge of the bounding box\n        max_y: float\n            The top edge of the bounding box\n\n        Returns\n        -------\n        self : NCSSQuery\n            Returns self for chaining calls\n\n        \"\"\"\n", "input": "", "output": "        self._set_query(self.spatial_query, minx=min_x, miny=min_y,\n                        maxx=max_x, maxy=max_y)\n        return self"}, {"instruction": "def next(self):\n        \"\"\"Returns the next batch of data.\"\"\"\n", "input": "", "output": "        if not self.iter_next():\n            raise StopIteration\n        data = self.getdata()\n        label = self.getlabel()\n        # iter should stop when last batch is not complete\n        if data[0].shape[0] != self.batch_size:\n        # in this case, cache it for next epoch\n            self._cache_data = data\n            self._cache_label = label\n            raise StopIteration\n        return DataBatch(data=data, label=label, \\\n            pad=self.getpad(), index=None)"}, {"instruction": "def line_break(self):\n        \"\"\"insert as many line breaks as the insert_line_break variable says\n        \"\"\"\n", "input": "", "output": "        for i in range(self.slide.insert_line_break):\n            # needs to be inside text:p\n            if not self._in_tag(ns(\"text\", \"p\")):\n                # we can just add a text:p and no line-break\n                # Create paragraph style first\n                self.add_node(ns(\"text\", \"p\"))\n            self.add_node(ns(\"text\", \"line-break\"))\n            self.pop_node()\n            if self.cur_node.tag == ns(\"text\", \"p\"):\n                return\n\n            if self.cur_node.getparent().tag != ns(\"text\", \"p\"):\n                self.pop_node()\n        self.slide.insert_line_break = 0"}, {"instruction": "async def create_scene(self, room_id, name, color_id=0, icon_id=0):\n        \"\"\"Creates am empty scene.\n\n        Scenemembers need to be added after the scene has been created.\n\n        :returns: A json object including scene id.\n        \"\"\"\n", "input": "", "output": "        name = unicode_to_base64(name)\n        _data = {\n            \"scene\": {\n                ATTR_ROOM_ID: room_id,\n                ATTR_NAME: name,\n                ATTR_COLOR_ID: color_id,\n                ATTR_ICON_ID: icon_id,\n            }\n        }\n        _response = await self.request.post(self._base_path, data=_data)\n        return _response"}, {"instruction": "def experiments_fmri_upsert_property(self, experiment_id, properties):\n        \"\"\"Upsert property of fMRI data object associated with given experiment.\n\n        Raises ValueError if given property dictionary results in an illegal\n        operation.\n\n        Parameters\n        ----------\n        experiment_id : string\n            Unique experiment identifier\n        properties : Dictionary()\n            Dictionary of property names and their new values.\n\n        Returns\n        -------\n        FMRIDataHandle\n            Handle for updated object of None if object doesn't exist\n        \"\"\"\n", "input": "", "output": "        # Get experiment fMRI to ensure that it exists. Needed to get fMRI\n        # data object identifier for given experiment identifier\n        fmri = self.experiments_fmri_get(experiment_id)\n        if fmri is None:\n            return None\n        # Update properties for fMRI object using the object identifier\n        return self.funcdata.upsert_object_property(fmri.identifier, properties)"}, {"instruction": "def view(self, filename=None, directory=None, cleanup=False):\n        \"\"\"Save the source to file, open the rendered result in a viewer.\n\n        Args:\n            filename: Filename for saving the source (defaults to ``name`` + ``'.gv'``)\n            directory: (Sub)directory for source saving and rendering.\n            cleanup (bool): Delete the source file after rendering.\n        Returns:\n            The (possibly relative) path of the rendered file.\n        Raises:\n            graphviz.ExecutableNotFound: If the Graphviz executable is not found.\n            subprocess.CalledProcessError: If the exit status is non-zero.\n            RuntimeError: If opening the viewer is not supported.\n\n        Short-cut method for calling :meth:`.render` with ``view=True``.\n        \"\"\"\n", "input": "", "output": "        return self.render(filename=filename, directory=directory, view=True,\n                           cleanup=cleanup)"}, {"instruction": "def install_language(language):\n    \"\"\"Install translation service routines into default namespace.\"\"\"\n", "input": "", "output": "    translator = get_translator(default_domain, default_directory,\n        languages=[get_lang(language)], fallback=True)\n    do_unicode = True\n    translator.install(do_unicode)"}, {"instruction": "def fetch( hash ):\n    \"\"\"\n    Fetches CallDescriptor from the local database given a hash key representing the call. If it doesn't exist returns None.\n\n    :param str hash: The sha1 hexdigest to look the CallDescriptor up by.\n\n    :rtype: CallDescriptor corresponding to the hash passed or None if it wasn't found.\n    \"\"\"\n", "input": "", "output": "    res = select_io( hash )\n\n    if res:\n      p = { 'methodname': '', 'returnval': '', 'args': '', 'stack': '' }\n      for packet in res:\n        hash, stack, methodname, returnval, args, packet_num = packet\n        p['methodname'] = p['methodname'] + methodname\n        p['returnval']  = p['returnval'] + returnval\n        p['args']       = p['args'] + args\n        p['stack']      = p['stack'] + stack\n                             \n      return CallDescriptor( hash = hash,\n                             stack = p['stack'],\n                             method = p['methodname'],\n                             returnval = pickle.loads( str( p['returnval'] ) ),\n                             args = pickle.loads( str( p['args'] ) ) )\n    return None"}, {"instruction": "def every_other(x, name=None):\n  \"\"\"Drops every other value from the tensor and returns a 1D tensor.\n\n  This is useful if you are running multiple inputs through a model tower\n  before splitting them and you want to line it up with some other data.\n\n  Args:\n    x: the target tensor.\n    name: the name for this op, defaults to every_other\n  Returns:\n    A tensorflow op.\n  \"\"\"\n", "input": "", "output": "  with tf.name_scope(name, 'every_other', [x]) as scope:\n    x = tf.convert_to_tensor(x, name='x')\n    return tf.reshape(\n        tf.slice(\n            tf.reshape(x, [-1, 2]), [0, 0], [-1, 1]),\n        [-1],\n        name=scope)"}, {"instruction": "def add(self, *constraints: Tuple[Bool]) -> None:\n        \"\"\"Adds the constraints to this solver.\n\n        :param constraints: constraints to add\n        \"\"\"\n", "input": "", "output": "        raw_constraints = [\n            c.raw for c in cast(Tuple[Bool], constraints)\n        ]  # type: List[z3.BoolRef]\n        self.constraints.extend(raw_constraints)"}, {"instruction": "def list_courses(args):\n    \"\"\"\n    List enrolled courses.\n\n    @param args: Command-line arguments.\n    @type args: namedtuple\n    \"\"\"\n", "input": "", "output": "    session = get_session()\n    login(session, args.username, args.password)\n    extractor = CourseraExtractor(session)\n    courses = extractor.list_courses()\n    logging.info('Found %d courses', len(courses))\n    for course in courses:\n        logging.info(course)"}, {"instruction": "def get_users_in_organization(self, organization_id, start=0, limit=50):\n        \"\"\"\n        Get all the users of a specified organization\n\n        :param organization_id: str\n        :param start: OPTIONAL: int\n        :param limit: OPTIONAL: int\n        :return: Users list in organization\n        \"\"\"\n", "input": "", "output": "        url = 'rest/servicedeskapi/organization/{}/user'.format(organization_id)\n        params = {}\n        if start is not None:\n            params['start'] = int(start)\n        if limit is not None:\n            params['limit'] = int(limit)\n\n        return self.get(url, headers=self.experimental_headers, params=params)"}, {"instruction": "def device(self, idx):\n        \"\"\"Get a specific GPU device\n\n        Args:\n            idx: index of device\n\n        Returns:\n            NvidiaDevice: single GPU device\n        \"\"\"\n", "input": "", "output": "\n        class GpuDevice(Structure):\n            pass\n\n        c_nvmlDevice_t = POINTER(GpuDevice)\n\n        c_index = c_uint(idx)\n        device = c_nvmlDevice_t()\n        _check_return(_NVML.get_function(\n            \"nvmlDeviceGetHandleByIndex_v2\")(c_index, byref(device)))\n        return NvidiaDevice(device)"}, {"instruction": "def contains(value: Union[str, 'Type']) -> bool:\n        \"\"\" Checks if a type is defined \"\"\"\n", "input": "", "output": "        if isinstance(value, str):\n            return any(value.lower() == i.value for i in Type)\n\n        return any(value == i for i in Type)"}, {"instruction": "def set_value(self, label, value, takeable=False):\n        \"\"\"\n        Quickly set single value at passed label. If label is not contained, a\n        new object is created with the label placed at the end of the result\n        index\n\n        .. deprecated:: 0.21.0\n\n        Please use .at[] or .iat[] accessors.\n\n        Parameters\n        ----------\n        label : object\n            Partial indexing with MultiIndex not allowed\n        value : object\n            Scalar value\n        takeable : interpret the index as indexers, default False\n\n        Notes\n        -----\n        This method *always* returns a new object. It is not particularly\n        efficient but is provided for API compatibility with Series\n\n        Returns\n        -------\n        series : SparseSeries\n        \"\"\"\n", "input": "", "output": "        warnings.warn(\"set_value is deprecated and will be removed \"\n                      \"in a future release. Please use \"\n                      \".at[] or .iat[] accessors instead\", FutureWarning,\n                      stacklevel=2)\n        return self._set_value(label, value, takeable=takeable)"}, {"instruction": "def _insertDateIndex(date, l):\r\n    '''\r\n    returns the index to insert the given date in a list\r\n    where each items first value is a date\r\n    '''\n", "input": "", "output": "    return next((i for i, n in enumerate(l) if n[0] < date), len(l))"}, {"instruction": "def init():\n    \"\"\" Initialize the lib\n\n    Function-design use is to be able to test language settings changes\n    \"\"\"\n", "input": "", "output": "\n    if settings.USE_L10N:\n        locale = settings.LANGUAGE_CODE.replace('-', '_')\n        try:\n            humanize.i18n.activate(locale)\n        except FileNotFoundError:\n            pass  # Just let it to the default locale\n\n    HUMANIZE_FUNC_LIST = [\n        'naturalday',\n        'naturaltime',\n        'ordinal',\n        'intword',\n        'naturaldelta',\n        'intcomma',\n        'apnumber',\n        'fractional',\n        'naturalsize',\n        'naturaldate'\n    ]\n\n    # registers all humanize functions as template tags\n    for funcname in HUMANIZE_FUNC_LIST:\n        func = getattr(humanize, funcname)\n        register.filter(funcname, func, is_safe=True)"}, {"instruction": "def get_grade_system_gradebook_assignment_session(self, proxy):\n        \"\"\"Gets the session for assigning grade system to gradebook mappings.\n\n        arg:    proxy (osid.proxy.Proxy): a proxy\n        return: (osid.grading.GradeSystemGradebookSession) - a\n                ``GradeSystemGradebookAssignmentSession``\n        raise:  NullArgument - ``proxy`` is ``null``\n        raise:  OperationFailed - unable to complete request\n        raise:  Unimplemented -\n                ``supports_grade_system_gradebook_assignment()`` is\n                ``false``\n        *compliance: optional -- This method must be implemented if\n        ``supports_grade_system_gradebook_assignment()`` is ``true``.*\n\n        \"\"\"\n", "input": "", "output": "        if not self.supports_grade_system_gradebook_assignment():\n            raise errors.Unimplemented()\n        # pylint: disable=no-member\n        return sessions.GradeSystemGradebookAssignmentSession(proxy=proxy, runtime=self._runtime)"}, {"instruction": "def bartlett(timeseries, segmentlength, noverlap=None, window=None, plan=None):\n    # pylint: disable=unused-argument\n    \"\"\"Calculate an PSD of this `TimeSeries` using Bartlett's method\n\n    Parameters\n    ----------\n    timeseries : `~gwpy.timeseries.TimeSeries`\n        input `TimeSeries` data.\n\n    segmentlength : `int`\n        number of samples in single average.\n\n    noverlap : `int`\n        number of samples to overlap between segments, defaults to 50%.\n\n    window : `tuple`, `str`, optional\n        window parameters to apply to timeseries prior to FFT\n\n    plan : `REAL8FFTPlan`, optional\n        LAL FFT plan to use when generating average spectrum\n\n    Returns\n    -------\n    spectrum : `~gwpy.frequencyseries.FrequencySeries`\n        average power `FrequencySeries`\n\n    See also\n    --------\n    lal.REAL8AverageSpectrumWelch\n    \"\"\"\n", "input": "", "output": "    return _lal_spectrum(timeseries, segmentlength, noverlap=0,\n                         method='welch', window=window, plan=plan)"}, {"instruction": "def instance_path_for(name, identifier_type, identifier_key=None):\n    \"\"\"\n    Get a path for thing.\n\n    \"\"\"\n", "input": "", "output": "    return \"/{}/<{}:{}>\".format(\n        name_for(name),\n        identifier_type,\n        identifier_key or \"{}_id\".format(name_for(name)),\n    )"}, {"instruction": "def defaultMachine(use_rpm_default=True):\n    \"\"\" Return the canonicalized machine name. \"\"\"\n", "input": "", "output": "\n    if use_rpm_default:\n        try:\n            # This should be the most reliable way to get the default arch\n            rmachine = subprocess.check_output(['rpm', '--eval=%_target_cpu'], shell=False).rstrip()\n            rmachine = SCons.Util.to_str(rmachine)\n        except Exception as e:\n            # Something went wrong, try again by looking up platform.machine()\n            return defaultMachine(False)\n    else:\n        rmachine = platform.machine()\n\n        # Try to lookup the string in the canon table\n        if rmachine in arch_canon:\n            rmachine = arch_canon[rmachine][0]\n\n    return rmachine"}, {"instruction": "def _time_to_datetime(value):\n  \"\"\"Convert a time to a datetime for Cloud Datastore storage.\n\n  Args:\n    value: A datetime.time object.\n\n  Returns:\n    A datetime object with date set to 1970-01-01.\n  \"\"\"\n", "input": "", "output": "  if not isinstance(value, datetime.time):\n    raise TypeError('Cannot convert to datetime expected time value; '\n                    'received %s' % value)\n  return datetime.datetime(1970, 1, 1,\n                           value.hour, value.minute, value.second,\n                           value.microsecond)"}, {"instruction": "def ReadCronJobs(self, cronjob_ids=None, cursor=None):\n    \"\"\"Reads all cronjobs from the database.\"\"\"\n", "input": "", "output": "    query = (\"SELECT job, UNIX_TIMESTAMP(create_time), enabled, \"\n             \"forced_run_requested, last_run_status, \"\n             \"UNIX_TIMESTAMP(last_run_time), current_run_id, state, \"\n             \"UNIX_TIMESTAMP(leased_until), leased_by \"\n             \"FROM cron_jobs\")\n    if cronjob_ids is None:\n      cursor.execute(query)\n      return [self._CronJobFromRow(row) for row in cursor.fetchall()]\n\n    query += \" WHERE job_id IN (%s)\" % \", \".join([\"%s\"] * len(cronjob_ids))\n    cursor.execute(query, cronjob_ids)\n    res = []\n    for row in cursor.fetchall():\n      res.append(self._CronJobFromRow(row))\n\n    if len(res) != len(cronjob_ids):\n      missing = set(cronjob_ids) - set([c.cron_job_id for c in res])\n      raise db.UnknownCronJobError(\"CronJob(s) with id(s) %s not found.\" %\n                                   missing)\n    return res"}, {"instruction": "def calculate_cardinality(\n        angle,\n        earthquake_hazard=None,\n        place_exposure=None\n):\n    \"\"\"Simple postprocessor where we compute the cardinality of an angle.\n\n    :param angle: Bearing angle.\n    :type angle: float\n\n    :param earthquake_hazard: The hazard to use.\n    :type earthquake_hazard: str\n\n    :param place_exposure: The exposure to use.\n    :type place_exposure: str\n\n    :return: Cardinality text.\n    :rtype: str\n    \"\"\"\n", "input": "", "output": "    # this method could still be improved later, since the acquisition interval\n    # is a bit strange, i.e the input angle of 22.499\u00b0 will return `N` even\n    # though 22.5\u00b0 is the direction for `NNE`\n    _ = earthquake_hazard, place_exposure  # NOQA\n\n    direction_list = tr(\n        'N,NNE,NE,ENE,E,ESE,SE,SSE,S,SSW,SW,WSW,W,WNW,NW,NNW'\n    ).split(',')\n\n    bearing = float(angle)\n    direction_count = len(direction_list)\n    direction_interval = 360. / direction_count\n    index = int(floor(bearing / direction_interval))\n    index %= direction_count\n    return direction_list[index]"}, {"instruction": "def trace_integrations(integrations, tracer=None):\n    \"\"\"Enable tracing on the selected integrations.\n    :type integrations: list\n    :param integrations: The integrations to be traced.\n    \"\"\"\n", "input": "", "output": "    integrated = []\n\n    for item in integrations:\n        module_name = 'opencensus.ext.{}.trace'.format(item)\n        try:\n            module = importlib.import_module(module_name)\n            module.trace_integration(tracer=tracer)\n            integrated.append(item)\n        except Exception as e:\n            log.warning('Failed to integrate module: {}'.format(module_name))\n            log.warning('{}'.format(e))\n\n    return integrated"}, {"instruction": "def assertFileSizeAlmostEqual(\n            self, filename, size, places=None, msg=None, delta=None):\n        '''Fail if ``filename`` does not have the given ``size`` as\n        determined by their difference rounded to the given number of\n        decimal ``places`` (default 7) and comparing to zero, or if\n        their difference is greater than a given ``delta``.\n\n        Parameters\n        ----------\n        filename : str, bytes, file-like\n        size : int, float\n        places : int\n        msg : str\n            If not provided, the :mod:`marbles.mixins` or\n            :mod:`unittest` standard message will be used.\n        delta : int, float\n\n        Raises\n        ------\n        TypeError\n            If ``filename`` is not a str or bytes object and is not\n            file-like.\n        '''\n", "input": "", "output": "        fsize = self._get_file_size(filename)\n        self.assertAlmostEqual(\n                fsize, size, places=places, msg=msg, delta=delta)"}, {"instruction": "def get_report(session, vehicle_index):\n    \"\"\"Get vehicle health report summary.\"\"\"\n", "input": "", "output": "    vhr = get_vehicle_health_report(session, vehicle_index)\n    if 'reportCard' not in vhr:\n        raise MoparError(\"no vhr found\")\n    return _traverse_report(vhr['reportCard'])"}, {"instruction": "def SetQualifier(self, *args, **kwargs):\n        \"\"\"Create or modify a qualifier type in the local repository of this\n        class.\n\n        For a description of the parameters, see\n        :meth:`pywbem.WBEMConnection.SetQualifier`.\n        \"\"\"\n", "input": "", "output": "\n        qual = args[0] if args else kwargs['QualifierDeclaration']\n        try:\n            self.qualifiers[self.default_namespace][qual.name] = qual\n        except KeyError:\n            self.qualifiers[self.default_namespace] = \\\n                NocaseDict({qual.name: qual})"}, {"instruction": "def logger(self):\n        ''' Lazy logger '''\n", "input": "", "output": "        if self.__logger is None:\n            self.__logger = logging.getLogger(self.__name)\n        return self.__logger"}, {"instruction": "def id(self, value):\n        \"\"\"Split into server_and_prefix and identifier.\"\"\"\n", "input": "", "output": "        i = value.rfind('/')\n        if (i > 0):\n            self.server_and_prefix = value[:i]\n            self.identifier = value[(i + 1):]\n        elif (i == 0):\n            self.server_and_prefix = ''\n            self.identifier = value[(i + 1):]\n        else:\n            self.server_and_prefix = ''\n            self.identifier = value"}, {"instruction": "def draw_random(obj, **kwds):\n    \"\"\"Draw random variates from obj.random method.\n\n    If the object has parents whose value must be updated, use\n    parent_name=trace_generator_function.\n\n    Ex:\n    R = draw_random(theta, beta=pymc.utils.trace_generator(beta.trace))\n    R.next()\n    \"\"\"\n", "input": "", "output": "    while True:\n        for k, v in six.iteritems(kwds):\n            obj.parents[k] = v.next()\n        yield obj.random()"}, {"instruction": "def _pydevd_log(level, msg, *args):\n    '''\n    Levels are:\n\n    0 most serious warnings/errors (always printed)\n    1 warnings/significant events\n    2 informational trace\n    3 verbose mode\n    '''\n", "input": "", "output": "    if level <= DebugInfoHolder.DEBUG_TRACE_LEVEL:\n        # yes, we can have errors printing if the console of the program has been finished (and we're still trying to print something)\n        try:\n            try:\n                if args:\n                    msg = msg % args\n            except:\n                msg = '%s - %s' % (msg, args)\n            DebugInfoHolder.DEBUG_STREAM.write('%s\\n' % (msg,))\n            DebugInfoHolder.DEBUG_STREAM.flush()\n        except:\n            pass\n        return True"}, {"instruction": "def walk_dependencies(root, visitor):\n    \"\"\"\n    Call visitor on root and all dependencies reachable from it in breadth\n    first order.\n\n    Args:\n        root (component): component function or class\n        visitor (function): signature is `func(component, parent)`.  The\n            call on root is `visitor(root, None)`.\n    \"\"\"\n", "input": "", "output": "    def visit(parent, visitor):\n        for d in get_dependencies(parent):\n            visitor(d, parent)\n            visit(d, visitor)\n\n    visitor(root, None)\n    visit(root, visitor)"}, {"instruction": "def apply_log(a: tuple, func: Callable[[Any], Tuple[Any, Log]]) -> Tuple[Any, Log]:\n        \"\"\"Apply a function to a value with a log.\n\n        Helper function to apply a function to a value with a log tuple.\n        \"\"\"\n", "input": "", "output": "        value, log = a\n        new, entry = func(value)\n        return new, log + entry"}, {"instruction": "def get_behaviors(brain_or_object):\n    \"\"\"Iterate over all behaviors that are assigned to the object\n\n    :param brain_or_object: A single catalog brain or content object\n    :type brain_or_object: ATContentType/DexterityContentType/CatalogBrain\n    :returns: Behaviors\n    :rtype: list\n    \"\"\"\n", "input": "", "output": "    obj = get_object(brain_or_object)\n    if not is_dexterity_content(obj):\n        fail(400, \"Only Dexterity contents can have assigned behaviors\")\n    assignable = IBehaviorAssignable(obj, None)\n    if not assignable:\n        return {}\n    out = {}\n    for behavior in assignable.enumerateBehaviors():\n        for name, field in getFields(behavior.interface).items():\n            out[name] = field\n    return out"}, {"instruction": "def op_gate_of_type(op: raw_types.Operation,\n                    gate_type: Type[TV]) -> Optional[TV]:\n    \"\"\"Returns the gate of given type, if the op has that gate otherwise None.\n    \"\"\"\n", "input": "", "output": "    if isinstance(op, GateOperation) and isinstance(op.gate, gate_type):\n        return op.gate\n    return None"}, {"instruction": "def contour(c, subsample=1, size=10, color='g'):\n        \"\"\" Draws a contour on the current plot by scattering points.\n\n        Parameters\n        ----------\n        c : :obj:`autolab_core.Contour`\n            contour to draw\n        subsample : int\n            subsample rate for boundary pixels\n        size : int\n            size of scattered points\n        color : :obj:`str`\n            color of box\n        \"\"\"\n", "input": "", "output": "        if not isinstance(c, Contour):\n            raise ValueError('Input must be of type Contour')\n            \n        for i in range(c.num_pixels)[0::subsample]:\n            plt.scatter(c.boundary_pixels[i,1], c.boundary_pixels[i,0], s=size, c=color)"}, {"instruction": "def list_devices(self):\n        \"\"\"List devices in the ALDB.\"\"\"\n", "input": "", "output": "        if self.plm.devices:\n            for addr in self.plm.devices:\n                device = self.plm.devices[addr]\n                if device.address.is_x10:\n                    _LOGGING.info('Device: %s %s', device.address.human,\n                                  device.description)\n                else:\n                    _LOGGING.info('Device: %s cat: 0x%02x subcat: 0x%02x '\n                                  'desc: %s, model: %s',\n                                  device.address.human, device.cat,\n                                  device.subcat, device.description,\n                                  device.model)\n        else:\n            _LOGGING.info('No devices found')\n            if not self.plm.transport:\n                _LOGGING.info('IM connection has not been made.')\n                _LOGGING.info('Use `connect [device]` to open the connection')"}, {"instruction": "def negotiate_safe(self, name, params):\n        \"\"\"\n        `name` and `params` are sent in the HTTP request by the client. Check\n        if the extension name is supported by this extension, and validate the\n        parameters. Returns a dict with accepted parameters, or None if not\n        accepted.\n        \"\"\"\n", "input": "", "output": "        for param in params.iterkeys():\n            if param not in self.defaults:\n                return\n\n        try:\n            return dict(self.negotiate(name, params))\n        except (KeyError, ValueError, AssertionError):\n            pass"}, {"instruction": "def set_gid(self):\n        \"\"\"Change the group of the running process\"\"\"\n", "input": "", "output": "        if self.group:\n            gid = getgrnam(self.group).gr_gid\n            try:\n                os.setgid(gid)\n            except Exception:\n                message = (\"Unable to switch ownership to {0}:{1}. \" +\n                           \"Did you start the daemon as root?\")\n                print(message.format(self.user, self.group))\n                sys.exit(1)"}, {"instruction": "def aggregate(self, variable, components=None, append=False):\n        \"\"\"Compute the aggregate of timeseries components or sub-categories\n\n        Parameters\n        ----------\n        variable: str\n            variable for which the aggregate should be computed\n        components: list of str, default None\n            list of variables, defaults to all sub-categories of `variable`\n        append: bool, default False\n            append the aggregate timeseries to `data` and return None,\n            else return aggregate timeseries\n        \"\"\"\n", "input": "", "output": "        # default components to all variables one level below `variable`\n        components = components or self._variable_components(variable)\n\n        if not len(components):\n            msg = 'cannot aggregate variable `{}` because it has no components'\n            logger().info(msg.format(variable))\n\n            return\n\n        rows = self._apply_filters(variable=components)\n        _data = _aggregate(self.data[rows], 'variable')\n\n        if append is True:\n            self.append(_data, variable=variable, inplace=True)\n        else:\n            return _data"}, {"instruction": "def _check_pillar_exact_minions(self, expr, delimiter, greedy):\n        '''\n        Return the minions found by looking via pillar\n        '''\n", "input": "", "output": "        return self._check_cache_minions(expr,\n                                         delimiter,\n                                         greedy,\n                                         'pillar',\n                                         exact_match=True)"}, {"instruction": "def remove_tag(self, tag):\n        \"\"\"Remove a user's tag from this object.\"\"\"\n", "input": "", "output": "\n        if isinstance(tag, Tag):\n            tag = tag.get_name()\n\n        params = self._get_params()\n        params[\"tag\"] = tag\n\n        self._request(self.ws_prefix + \".removeTag\", False, params)"}, {"instruction": "def _audio_item(self, stream_url=None, offset=0, push_buffer=True, opaque_token=None):\n        \"\"\"Builds an AudioPlayer Directive's audioItem and updates current_stream\"\"\"\n", "input": "", "output": "        audio_item = {'stream': {}}\n        stream = audio_item['stream']\n\n        # existing stream\n        if not stream_url:\n            # stream.update(current_stream.__dict__)\n            stream['url'] = current_stream.url\n            stream['token'] = current_stream.token\n            stream['offsetInMilliseconds'] = current_stream.offsetInMilliseconds\n\n        # new stream\n        else:\n            stream['url'] = stream_url\n            stream['token'] = opaque_token or str(uuid.uuid4())\n            stream['offsetInMilliseconds'] = offset\n\n        if push_buffer:  # prevents enqueued streams from becoming current_stream\n            push_stream(stream_cache, context['System']['user']['userId'], stream)\n        return audio_item"}, {"instruction": "def create(path, docker_compose):\n    '''\n    Create and validate a docker-compose file into a directory\n\n    path\n        Path where the docker-compose file will be stored on the server\n\n    docker_compose\n        docker_compose file\n\n    CLI Example:\n\n    .. code-block:: bash\n\n        salt myminion dockercompose.create /path/where/docker-compose/stored content\n    '''\n", "input": "", "output": "    if docker_compose:\n        ret = __write_docker_compose(path,\n                                     docker_compose,\n                                     already_existed=False)\n        if isinstance(ret, dict):\n            return ret\n    else:\n        return __standardize_result(False,\n                                    'Creating a docker-compose project failed, you must send a valid docker-compose file',\n                                    None, None)\n    return __standardize_result(True,\n                                'Successfully created the docker-compose file',\n                                {'compose.base_dir': path},\n                                None)"}, {"instruction": "def commit(self):\n        \"\"\":return: Commit object the tag ref points to\n        \n        :raise ValueError: if the tag points to a tree or blob\"\"\"\n", "input": "", "output": "        obj = self.object\n        while obj.type != 'commit':\n            if obj.type == \"tag\":\n                # it is a tag object which carries the commit as an object - we can point to anything\n                obj = obj.object\n            else:\n                raise ValueError((\"Cannot resolve commit as tag %s points to a %s object - \" +\n                                  \"use the `.object` property instead to access it\") % (self, obj.type))\n        return obj"}, {"instruction": "def dkim_sign(message, dkim_domain=None, dkim_key=None, dkim_selector=None, dkim_headers=None):\n    \"\"\"Return signed email message if dkim package and settings are available.\"\"\"\n", "input": "", "output": "    try:\n        import dkim\n    except ImportError:\n        pass\n    else:\n        if dkim_domain and dkim_key:\n            sig = dkim.sign(message,\n                            dkim_selector,\n                            dkim_domain,\n                            dkim_key,\n                            include_headers=dkim_headers)\n            message = sig + message\n    return message"}, {"instruction": "def get_tunnel_info_output_tunnel_has_conflicts(self, **kwargs):\n        \"\"\"Auto Generated Code\n        \"\"\"\n", "input": "", "output": "        config = ET.Element(\"config\")\n        get_tunnel_info = ET.Element(\"get_tunnel_info\")\n        config = get_tunnel_info\n        output = ET.SubElement(get_tunnel_info, \"output\")\n        tunnel = ET.SubElement(output, \"tunnel\")\n        has_conflicts = ET.SubElement(tunnel, \"has-conflicts\")\n\n        callback = kwargs.pop('callback', self._callback)\n        return callback(config)"}, {"instruction": "def task(ft):\n\t\"\"\"\n\tto create loading progress bar\n\t\"\"\"\n", "input": "", "output": "\tft.pack(expand = True,  fill = BOTH,  side = TOP)\n\tpb_hD = ttk.Progressbar(ft, orient = 'horizontal', mode = 'indeterminate')\n\tpb_hD.pack(expand = True, fill = BOTH, side = TOP)\n\tpb_hD.start(50)\n\tft.mainloop()"}, {"instruction": "def get_header(self, header_name, default=None):\n        \"\"\"Retrieve ``header_name`` from this request headers.\n        \"\"\"\n", "input": "", "output": "        return self.headers.get(\n            header_name, self.unredirected_headers.get(header_name, default))"}, {"instruction": "def into_buffer(self, buf, offset):\n    \"\"\"Produce a framed/packed SBP message into the provided buffer and offset.\n\n    \"\"\"\n", "input": "", "output": "    self.payload = containerize(exclude_fields(self))\n    self.parser = MsgEphemerisGPSDepF._parser\n    self.stream_payload.reset(buf, offset)\n    return self.pack_into(buf, offset, self._build_payload)"}, {"instruction": "def parse_event_out(self, node):\n        \"\"\"\n        Parses <EventOut>\n\n        @param node: Node containing the <EventOut> element\n        @type node: xml.etree.Element\n        \"\"\"\n", "input": "", "output": "\n        try:\n            port = node.lattrib['port']\n        except:\n            self.raise_error('<EventOut> must be specify a port.')\n\n        action = EventOut(port)\n\n        self.current_event_handler.add_action(action)"}, {"instruction": "def column_reflection_fallback(self):\n        \"\"\"If we can't reflect the table, use a query to at least get column names.\"\"\"\n", "input": "", "output": "        sql = sa.select([sa.text(\"*\")]).select_from(self._table)\n        col_names = self.engine.execute(sql).keys()\n        col_dict = [{'name': col_name} for col_name in col_names]\n        return col_dict"}, {"instruction": "def merge_unscoped_hparams(scopes_and_hparams):\n  \"\"\"Merge multiple HParams into one with scopes.\"\"\"\n", "input": "", "output": "  merged_values = {}\n  for (scope, hparams) in scopes_and_hparams:\n    for key, value in six.iteritems(hparams.values()):\n      scoped_key = \"%s.%s\" % (scope, key)\n      merged_values[scoped_key] = value\n\n  return hparam.HParams(**merged_values)"}, {"instruction": "def print_stream(file, name):\n    \"\"\"Print stream from file to logger.\"\"\"\n", "input": "", "output": "    logger = logging.getLogger('xenon.{}'.format(name))\n    for line in file:\n        logger.info('[{}] {}'.format(name, line.strip()))"}, {"instruction": "def close(self, cancelled=False):\n        \"\"\"\n        Close this temporary pop-up.\n\n        :param cancelled: Whether the pop-up was cancelled (e.g. by pressing Esc).\n        \"\"\"\n", "input": "", "output": "        self._on_close(cancelled)\n        self._scene.remove_effect(self)"}, {"instruction": "def val(self, strictkey):\n        \"\"\"\n        Return a chunk referencing a value in a mapping with the key 'key'.\n        \"\"\"\n", "input": "", "output": "        ruamelkey = self.ruamelindex(strictkey)\n        return self._select(self._pointer.val(ruamelkey, strictkey))"}, {"instruction": "def _read_imu(self):\n        \"\"\"\n        Internal. Tries to read the IMU sensor three times before giving up\n        \"\"\"\n", "input": "", "output": "\n        self._init_imu()  # Ensure imu is initialised\n\n        attempts = 0\n        success = False\n\n        while not success and attempts < 3:\n            success = self._imu.IMURead()\n            attempts += 1\n            time.sleep(self._imu_poll_interval)\n\n        return success"}, {"instruction": "def get_log_events(awsclient, log_group_name, log_stream_name, start_ts=None):\n    \"\"\"Get log events for the specified log group and stream.\n    this is used in tenkai output instance diagnostics\n\n    :param log_group_name: log group name\n    :param log_stream_name: log stream name\n    :param start_ts: timestamp\n    :return:\n    \"\"\"\n", "input": "", "output": "    client_logs = awsclient.get_client('logs')\n\n    request = {\n        'logGroupName': log_group_name,\n        'logStreamName': log_stream_name\n    }\n    if start_ts:\n        request['startTime'] = start_ts\n\n    # TODO exhaust the events!\n    # TODO use all_pages !\n    response = client_logs.get_log_events(**request)\n\n    if 'events' in response and response['events']:\n        return [{'timestamp': e['timestamp'], 'message': e['message']}\n                for e in response['events']]"}, {"instruction": "def ws010(self, value=None):\n        \"\"\"  Corresponds to IDD Field `ws010`\n        Wind speed corresponding to 1.0% annual cumulative frequency of occurrence\n\n        Args:\n            value (float): value for IDD Field `ws010`\n                Unit: m/s\n                if `value` is None it will not be checked against the\n                specification and is assumed to be a missing value\n\n        Raises:\n            ValueError: if `value` is not a valid value\n        \"\"\"\n", "input": "", "output": "        if value is not None:\n            try:\n                value = float(value)\n            except ValueError:\n                raise ValueError('value {} need to be of type float '\n                                 'for field `ws010`'.format(value))\n\n        self._ws010 = value"}, {"instruction": "def _minimize_constraints_fun_summation(x):\n    '''\n    Minimize constraints fun summation\n    '''\n", "input": "", "output": "    summation = sum([x[i] for i in CONSTRAINT_PARAMS_IDX])\n    return CONSTRAINT_UPPERBOUND >= summation >= CONSTRAINT_LOWERBOUND"}, {"instruction": "def _make_ntgrid(grid):\n    \"\"\"make a named tuple grid\n\n    [[\"\",  \"a b\", \"b c\", \"c d\"],\n     [\"x y\", 1,     2,     3 ],\n     [\"y z\", 4,     5,     6 ],\n     [\"z z\", 7,     8,     9 ],]\n    will return\n    ntcol(x_y=ntrow(a_b=1, b_c=2, c_d=3),\n          y_z=ntrow(a_b=4, b_c=5, c_d=6),\n          z_z=ntrow(a_b=7, b_c=8, c_d=9))\"\"\"\n", "input": "", "output": "    hnames = [_nospace(n) for n in grid[0][1:]]\n    vnames = [_nospace(row[0]) for row in grid[1:]]\n    vnames_s = \" \".join(vnames)\n    hnames_s = \" \".join(hnames)\n    ntcol = collections.namedtuple('ntcol', vnames_s)\n    ntrow = collections.namedtuple('ntrow', hnames_s)\n    rdict = [dict(list(zip(hnames, row[1:]))) for row in grid[1:]]\n    ntrows = [ntrow(**rdict[i]) for i, name in enumerate(vnames)]\n    ntcols = ntcol(**dict(list(zip(vnames, ntrows))))\n    return ntcols"}, {"instruction": "def guest_deploy(self, userid, image_name, transportfiles=None,\n                     remotehost=None, vdev=None, hostname=None):\n        \"\"\" Deploy the image to vm.\n\n        :param userid: (str) the user id of the vm\n        :param image_name: (str) the name of image that used to deploy the vm\n        :param transportfiles: (str) the files that used to customize the vm\n        :param remotehost: the server where the transportfiles located, the\n               format is username@IP, eg nova@192.168.99.1\n        :param vdev: (str) the device that image will be deploy to\n        :param hostname: (str) the hostname of the vm. This parameter will be\n               ignored if transportfiles present.\n        \"\"\"\n", "input": "", "output": "        action = (\"deploy image '%(img)s' to guest '%(vm)s'\" %\n                  {'img': image_name, 'vm': userid})\n        with zvmutils.log_and_reraise_sdkbase_error(action):\n            self._vmops.guest_deploy(userid, image_name, transportfiles,\n                                     remotehost, vdev, hostname)"}, {"instruction": "def parse(self, requires_cfg=True):\n        \"\"\"Parse the configuration sources into `Bison`.\n\n        Args:\n            requires_cfg (bool): Specify whether or not parsing should fail\n                if a config file is not found. (default: True)\n        \"\"\"\n", "input": "", "output": "        self._parse_default()\n        self._parse_config(requires_cfg)\n        self._parse_env()"}, {"instruction": "def find_undeclared_variables(ast):\n    \"\"\"Returns a set of all variables in the AST that will be looked up from\n    the context at runtime.  Because at compile time it's not known which\n    variables will be used depending on the path the execution takes at\n    runtime, all variables are returned.\n\n    >>> from jinja2 import Environment, meta\n    >>> env = Environment()\n    >>> ast = env.parse('{% set foo = 42 %}{{ bar + foo }}')\n    >>> meta.find_undeclared_variables(ast) == set(['bar'])\n    True\n\n    .. admonition:: Implementation\n\n       Internally the code generator is used for finding undeclared variables.\n       This is good to know because the code generator might raise a\n       :exc:`TemplateAssertionError` during compilation and as a matter of\n       fact this function can currently raise that exception as well.\n    \"\"\"\n", "input": "", "output": "    codegen = TrackingCodeGenerator(ast.environment)\n    codegen.visit(ast)\n    return codegen.undeclared_identifiers"}, {"instruction": "def backward_delete_char(self, e): # (Rubout)\r\n        u\"\"\"Delete the character behind the cursor. A numeric argument means\r\n        to kill the characters instead of deleting them.\"\"\"\n", "input": "", "output": "        self.l_buffer.backward_delete_char(self.argument_reset)\r\n        self.finalize()"}, {"instruction": "def _log_to_stderr(self, record):\n    \"\"\"Emits the record to stderr.\n\n    This temporarily sets the handler stream to stderr, calls\n    StreamHandler.emit, then reverts the stream back.\n\n    Args:\n      record: logging.LogRecord, the record to log.\n    \"\"\"\n", "input": "", "output": "    # emit() is protected by a lock in logging.Handler, so we don't need to\n    # protect here again.\n    old_stream = self.stream\n    self.stream = sys.stderr\n    try:\n      super(PythonHandler, self).emit(record)\n    finally:\n      self.stream = old_stream"}, {"instruction": "def send(self, s):\n        '''Send data to the subprocess' stdin.\n\n        Returns the number of bytes written.\n        '''\n", "input": "", "output": "        s = self._coerce_send_string(s)\n        self._log(s, 'send')\n\n        b = self._encoder.encode(s, final=False)\n        if PY3:\n            return self.proc.stdin.write(b)\n        else:\n            # On Python 2, .write() returns None, so we return the length of\n            # bytes written ourselves. This assumes they all got written.\n            self.proc.stdin.write(b)\n            return len(b)"}, {"instruction": "def use_value(self, value):\n        \"\"\"Converts value to field type or use original\"\"\"\n", "input": "", "output": "        if self.check_value(value):\n            return value\n        return self.convert_value(value)"}, {"instruction": "def likelihood_table_to_probs(self, lktable):\n        \"\"\"\n        Calculates this formula (1), given the log of the numerator as input\n                     \n                     p_k * f(x_i, a_k)\n        t_k(x_i) = -----------------------\n                    ---K\n                    \\   p_k * f(x_i, a_k)\n                    /__k=1\n        \n        x_i is data point i\n        P_k is cluster k of K\n        t_k is the posterior probability of x_i belonging to P_k\n        p_k is the prior probability of belong to P_k (the proportional size of P_k)\n        f(x, a) is the likelihood of x with parameters a\n        \"\"\"\n", "input": "", "output": "        m = lktable.max(1)  # row max of lktable\n        shifted = lktable-m[:,np.newaxis]  # shift lktable of log-likelihoods to a non-underflowing range\n        expsum = np.exp(shifted).sum(1)  # convert logs to (scaled) normal space, and sum the rows\n        logexpsum = np.log(expsum)+m  # convert back to log space, and undo the scaling\n        return np.exp(lktable - logexpsum[:, np.newaxis])"}, {"instruction": "def get_name_str(self, element):\n        '''get_name_str\n\n        High-level api: Produce a string that represents the name of a node.\n\n        Parameters\n        ----------\n\n        element : `Element`\n            A node in model tree.\n\n        Returns\n        -------\n\n        str\n            A string that represents the name of a node.\n        '''\n", "input": "", "output": "\n        if element.get('diff') == 'added':\n            return self.model2.get_name_str(element)\n        else:\n            return self.model1.get_name_str(element)"}, {"instruction": "def cancel_order(self, order_id, stock):\n        \"\"\"Cancel An Order\n\n        https://starfighter.readme.io/docs/cancel-an-order\n        \"\"\"\n", "input": "", "output": "        url_fragment = 'venues/{venue}/stocks/{stock}/orders/{order_id}'.format(\n            venue=self.venue,\n            stock=stock,\n            order_id=order_id,\n        )\n        url = urljoin(self.base_url, url_fragment)\n        return self.session.delete(url).json()"}, {"instruction": "def filter_nodes(graph: BELGraph, node_predicates: NodePredicates) -> Iterable[BaseEntity]:\n    \"\"\"Apply a set of predicates to the nodes iterator of a BEL graph.\"\"\"\n", "input": "", "output": "    concatenated_predicate = concatenate_node_predicates(node_predicates=node_predicates)\n    for node in graph:\n        if concatenated_predicate(graph, node):\n            yield node"}, {"instruction": "def refresh_datasources(self, refreshAll=True):\n        \"\"\"endpoint that refreshes druid datasources metadata\"\"\"\n", "input": "", "output": "        session = db.session()\n        DruidCluster = ConnectorRegistry.sources['druid'].cluster_class\n        for cluster in session.query(DruidCluster).all():\n            cluster_name = cluster.cluster_name\n            valid_cluster = True\n            try:\n                cluster.refresh_datasources(refreshAll=refreshAll)\n            except Exception as e:\n                valid_cluster = False\n                flash(\n                    \"Error while processing cluster '{}'\\n{}\".format(\n                        cluster_name, utils.error_msg_from_exception(e)),\n                    'danger')\n                logging.exception(e)\n                pass\n            if valid_cluster:\n                cluster.metadata_last_refreshed = datetime.now()\n                flash(\n                    _('Refreshed metadata from cluster [{}]').format(\n                        cluster.cluster_name),\n                    'info')\n        session.commit()\n        return redirect('/druiddatasourcemodelview/list/')"}, {"instruction": "def dict_filter(d, exclude=[]):\n    \"\"\"\n    Exclude specified keys from a nested dict\n    \"\"\"\n", "input": "", "output": "\n    if isinstance(d, list):\n        ret = []\n        for e in d:\n            ret.append(dict_filter(e, exclude))\n        return ret\n    elif isinstance(d, dict):\n        ret = {}\n        for k, v in d.items():\n            if isinstance(k, builtin_str):\n                k = str(k)\n\n            assert isinstance(k, str)\n            if k in exclude:\n                continue\n            ret[k] = dict_filter(v, exclude)\n        return ret\n\n    return d"}, {"instruction": "def is_twss(self, phrase):\n        \"\"\"\n        The magic function- this accepts a phrase and tells you if it\n        classifies as an entendre\n        \"\"\"\n", "input": "", "output": "        featureset = self.extract_features(phrase)\n        return self.classifier.classify(featureset)"}, {"instruction": "def int_out_of_bounds(self, index, axis=0):\n        \"\"\" examples if index is out of local processing bounds\n\n        function is used to perform examples for index of type integer\n        :param index: global index to examples as type int\n        :param axis: current axis to examples\n        :return: return input or raise error\n        \"\"\"\n", "input": "", "output": "        #if index >= self._global_shape[axis]:\n        if index > self._global_shape[axis]:\n            raise IndexError('index is larger than the upper bound')\n\n        # wrap around index if negative like in python\n        if index < 0:\n            index += self._global_shape[axis]\n            #warnings.warn('warp around may occur')\n\n        # check for invalid wrap around\n        if index < 0:\n            raise IndexError('index is smaller than the lower bound')\n\n        return index"}, {"instruction": "def _get_method(self, rdata):\n        \"\"\"\n        Returns jsonrpc request's method value.\n\n        InvalidRequestError will be raised if it's missing or is wrong type.\n        MethodNotFoundError will be raised if a method with given method name\n        does not exist.\n        \"\"\"\n", "input": "", "output": "        if 'method' in rdata:\n            if not isinstance(rdata['method'], basestring):\n                raise InvalidRequestError\n        else:\n            raise InvalidRequestError\n\n        if rdata['method'] not in self.method_data.keys():\n            raise MethodNotFoundError\n\n        return rdata['method']"}, {"instruction": "def args(self, args):\n        '''Set additional arguments to be passed to the fitness function\n\n        Args:\n            args (dict): additional arguments\n        '''\n", "input": "", "output": "        self._args = args\n        self._logger.log('debug', 'Args set to {}'.format(args))"}, {"instruction": "def visit(self, node):\n        '''The main visit function. Visits the passed-in node and calls\n        finalize.\n        '''\n", "input": "", "output": "        for token in self.itervisit(node):\n            pass\n        result = self.finalize()\n        if result is not self:\n            return result"}, {"instruction": "def get_readwrite_instance(cls, working_dir, restore=False, restore_block_height=None):\n        \"\"\"\n        Get a read/write instance to the db, without the singleton check.\n        Used for low-level operations like db restore.\n        Not used in the steady state behavior of the system.\n        \"\"\"\n", "input": "", "output": "        log.warning(\"!!! Getting raw read/write DB instance !!!\")\n\n        import virtualchain_hooks\n        db_path = virtualchain.get_db_filename(virtualchain_hooks, working_dir)\n        db = BlockstackDB(db_path, DISPOSITION_RW, working_dir, get_genesis_block())\n        rc = db.db_setup()\n        if not rc:\n            if restore:\n                # restore from backup instead of bailing out\n                log.debug(\"Restoring from unclean shutdown\")\n                rc = db.db_restore(block_number=restore_block_height)\n                if rc:\n                    return db\n                else:\n                    log.error(\"Failed to restore from unclean shutdown\")\n\n            db.close()\n            raise Exception(\"Failed to set up db\")\n\n        return db"}, {"instruction": "def __send_exc_clear(self, log_if_exc_set=None):\n        \"\"\"Clear send exception and time. If exception was previously was set, optionally log log_if_exc_set at INFO\n        level.\n        \"\"\"\n", "input": "", "output": "        if not (log_if_exc_set is None or self.__send_exc is None):\n            logger.info(log_if_exc_set)\n        self.__send_exc_time = None\n        self.__send_exc = None"}, {"instruction": "def sendmail(self, to, message):\n        \"\"\"Send mail to one or more recipients. The required arguments are a\n        list of RFC 822 to-address strings (a bare string will be treated as a\n        list with 1 address), and a message string.\n\n        \"\"\"\n", "input": "", "output": "\n        # If we were passed a bare string as the To: address, convert it to\n        # a single element list.\n        if isinstance(to, str):\n            to = [ to, ]\n\n        # Send one email with the appropriate recipient list.\n        server = self._smtp_server()\n        server.sendmail(self.get_rfc2822_address(), to, message)\n        server.quit()"}, {"instruction": "def compute_distance_matrix(points):\n    \"\"\"\n    Return a matrix of distance (in meters) between every point in a given list\n    of (lat, lon) location tuples.\n    \"\"\"\n", "input": "", "output": "    n = len(points)\n    return [[1000 * great_circle_distance(points[i], points[j])\n             for j in range(n)] for i in range(n)]"}, {"instruction": "def _split_classes_and_methods(folds):\n    \"\"\"\n    Split out classes and methods into two separate lists.\n\n    Parameters\n    ----------\n    folds : list of :class:`FoldScopeHelper`\n        The result of :func:`_get_fold_levels`.\n\n    Returns\n    -------\n    classes, functions: list of :class:`FoldScopeHelper`\n        Two separate lists of :class:`FoldScopeHelper` objects. The former\n        contains only class definitions while the latter contains only\n        function/method definitions.\n    \"\"\"\n", "input": "", "output": "    classes = []\n    functions = []\n    for fold in folds:\n        if fold.def_type == OED.FUNCTION_TOKEN:\n            functions.append(fold)\n        elif fold.def_type == OED.CLASS_TOKEN:\n            classes.append(fold)\n\n    return classes, functions"}, {"instruction": "def _create_request_map(cls, input_map):\n        \"\"\"Create request map.\"\"\"\n", "input": "", "output": "        mapped = super(Certificate, cls)._create_request_map(input_map)\n        if mapped.get('service') == CertificateType.developer:\n            mapped['service'] = CertificateType.bootstrap\n        return mapped"}, {"instruction": "def shellfilter(value):\n    \"\"\"Replace HTML chars for shell usage.\"\"\"\n", "input": "", "output": "    replacements = {'\\\\': '\\\\\\\\',\n                    '`': '\\\\`',\n                    \"'\": \"\\\\'\",\n                    '\"': '\\\\\"'}\n    for search, repl in replacements.items():\n        value = value.replace(search, repl)\n    return safestring.mark_safe(value)"}, {"instruction": "def next(self):\n        \"\"\"\n        Returns the next sequence of results, given stride and n.\n        \n        \"\"\"\n", "input": "", "output": "        try:\n            results = self._stride_buffer.pop()\n        except (IndexError, AttributeError):\n            self._rebuffer()\n            results = self._stride_buffer.pop()\n        if not results:\n            raise StopIteration\n        return results"}, {"instruction": "def soup(self):\n        \"\"\"Download the page and create the soup\"\"\"\n", "input": "", "output": "        try:\n            return self._soup\n        except AttributeError:\n            url = client.get_url(\"/presentations/%s\" % self.index)\n            content = self.client.fetch_no_cache(url).decode('utf-8')\n            self._soup = bs4.BeautifulSoup(content, \"html.parser\")\n\n            return self._soup"}, {"instruction": "def to_xdr_object(self):\n        \"\"\"Creates an XDR Memo object for a transaction with MEMO_RETURN.\"\"\"\n", "input": "", "output": "        return Xdr.types.Memo(\n            type=Xdr.const.MEMO_RETURN, retHash=self.memo_return)"}, {"instruction": "def hisat2_general_stats_table(self):\n        \"\"\" Take the parsed stats from the HISAT2 report and add it to the\n        basic stats table at the top of the report \"\"\"\n", "input": "", "output": "\n        headers = OrderedDict()\n        headers['overall_alignment_rate'] = {\n            'title': '% Aligned',\n            'description': 'overall alignment rate',\n            'max': 100,\n            'min': 0,\n            'suffix': '%',\n            'scale': 'YlGn'\n        }\n        self.general_stats_addcols(self.hisat2_data, headers)"}, {"instruction": "def to_unicode(text):\n    \"\"\"\n    Return *text* as a unicode string. All text in Python 3 is unicode, so\n    this just returns *text* unchanged.\n    \"\"\"\n", "input": "", "output": "    if not isinstance(text, str):\n        tmpl = 'expected unicode string, got %s value %s'\n        raise TypeError(tmpl % (type(text), text))\n    return text"}, {"instruction": "def locale_export():\n    \"\"\"Exports for dealing with Click-based programs and ASCII/Unicode errors.\n\n    RuntimeError: Click will abort further execution because Python 3 was\n    configured to use ASCII as encoding for the environment.\n    Consult https://click.palletsprojects.com/en/7.x/python3/ for mitigation steps.\n\n    Looks up available locales on the system to find an appropriate one to pick,\n    defaulting to C.UTF-8 which is globally available on newer systems.\n    \"\"\"\n", "input": "", "output": "    locale_to_use = \"C.UTF-8\"\n    try:\n        locales = subprocess.check_output([\"locale\", \"-a\"]).decode(errors=\"ignore\").split(\"\\n\")\n    except subprocess.CalledProcessError:\n        locales = []\n    for locale in locales:\n        if locale.lower().endswith((\"utf-8\", \"utf8\")):\n            locale_to_use = locale\n            break\n    return \"export LC_ALL=%s && export LANG=%s && \" % (locale_to_use, locale_to_use)"}, {"instruction": "def register_schema(self, directory, path):\n        \"\"\"Register a json-schema.\n\n        :param directory: root directory path.\n        :param path: schema path, relative to the root directory.\n        \"\"\"\n", "input": "", "output": "        self.schemas[path] = os.path.abspath(directory)"}, {"instruction": "def get_gpg_home( appname, config_dir=None ):\n    \"\"\"\n    Get the GPG keyring directory for a particular application.\n    Return the path.\n    \"\"\"\n", "input": "", "output": "    assert is_valid_appname(appname)\n    config_dir = get_config_dir( config_dir )\n    path = os.path.join( config_dir, \"gpgkeys\", appname )\n    return path"}, {"instruction": "def abivalidate_inputs(self):\n        \"\"\"\n        Run ABINIT in dry mode to validate all the inputs of the flow.\n\n        Return:\n            (isok, tuples)\n\n            isok is True if all inputs are ok.\n            tuples is List of `namedtuple` objects, one for each task in the flow.\n            Each namedtuple has the following attributes:\n\n                retcode: Return code. 0 if OK.\n                log_file:  log file of the Abinit run, use log_file.read() to access its content.\n                stderr_file: stderr file of the Abinit run. use stderr_file.read() to access its content.\n\n        Raises:\n            `RuntimeError` if executable is not in $PATH.\n        \"\"\"\n", "input": "", "output": "        if not self.allocated:\n            self.allocate()\n\n        isok, tuples = True, []\n        for task in self.iflat_tasks():\n            t = task.input.abivalidate()\n            if t.retcode != 0: isok = False\n            tuples.append(t)\n\n        return isok, tuples"}, {"instruction": "def d2Ibr_dV2(Ybr, V, lam):\n    \"\"\" Computes 2nd derivatives of complex branch current w.r.t. voltage.\n    \"\"\"\n", "input": "", "output": "    nb = len(V)\n    diaginvVm = spdiag(div(matrix(1.0, (nb, 1)), abs(V)))\n\n    Haa = spdiag(mul(-(Ybr.T * lam), V))\n    Hva = -1j * Haa * diaginvVm\n    Hav = Hva\n    Hvv = spmatrix([], [], [], (nb, nb))\n\n    return Haa, Hav, Hva, Hvv"}, {"instruction": "def say(self, event):\n        \"\"\"Chat event handler for incoming events\n        :param event: say-event with incoming chat message\n        \"\"\"\n", "input": "", "output": "\n        try:\n            userid = event.user.uuid\n            recipient = self._get_recipient(event)\n            content = self._get_content(event)\n\n            if self.config.name in content:\n                self.log('I think, someone mentioned me:', content)\n\n        except Exception as e:\n            self.log(\"Error: '%s' %s\" % (e, type(e)), exc=True, lvl=error)"}, {"instruction": "def get_empty_dirs(self, path):\n        \"\"\"Return a list of empty directories in path.\"\"\"\n", "input": "", "output": "        empty_dirs = []\n        for i in os.listdir(path):\n            child_path = os.path.join(path, i)\n            if i == '.git' or os.path.isfile(child_path) or os.path.islink(child_path):  # noqa\n                continue\n            if self.path_only_contains_dirs(child_path):\n                empty_dirs.append(i)\n        return empty_dirs"}, {"instruction": "def prepare_sparse_params(self, param_rowids):\n        '''Prepares the module for processing a data batch by pulling row_sparse\n        parameters from kvstore to all devices based on rowids.\n\n        Parameters\n        ----------\n        param_rowids : dict of str to NDArray of list of NDArrays\n        '''\n", "input": "", "output": "        if not self._kvstore:\n            return\n        assert(isinstance(param_rowids, dict))\n        for param_name, rowids in param_rowids.items():\n            if isinstance(rowids, (tuple, list)):\n                rowids_1d = []\n                for r in rowids:\n                    rowids_1d.append(r.reshape((-1,)).astype(np.int64))\n                rowid = mx.nd.concat(*rowids_1d, dim=0)\n            else:\n                rowid = rowids\n            param_idx = self._exec_group.param_names.index(param_name)\n            param_val = self._exec_group.param_arrays[param_idx]\n            self._kvstore.row_sparse_pull(param_name, param_val, row_ids=rowid,\n                                          priority=-param_idx)"}, {"instruction": "def reorient(self, up, look):\n        '''\n        Reorient the mesh by specifying two vectors.\n\n        up: The foot-to-head direction.\n        look: The direction the body is facing.\n\n        In the result, the up will end up along +y, and look along +z\n        (i.e. facing towards a default OpenGL camera).\n\n        '''\n", "input": "", "output": "        from blmath.geometry.transform import rotation_from_up_and_look\n        from blmath.numerics import as_numeric_array\n\n        up = as_numeric_array(up, (3,))\n        look = as_numeric_array(look, (3,))\n\n        if self.v is not None:\n            self.v = np.dot(rotation_from_up_and_look(up, look), self.v.T).T"}, {"instruction": "def find_compatible_interpreters(pex_python_path=None, compatibility_constraints=None):\n  \"\"\"Find all compatible interpreters on the system within the supplied constraints and use\n     PEX_PYTHON_PATH if it is set. If not, fall back to interpreters on $PATH.\n  \"\"\"\n", "input": "", "output": "  if pex_python_path:\n    interpreters = []\n    for binary in pex_python_path.split(os.pathsep):\n      try:\n        interpreters.append(PythonInterpreter.from_binary(binary))\n      except Executor.ExecutionError:\n        print(\"Python interpreter %s in PEX_PYTHON_PATH failed to load properly.\" % binary,\n          file=sys.stderr)\n    if not interpreters:\n      die('PEX_PYTHON_PATH was defined, but no valid interpreters could be identified. Exiting.')\n  else:\n    # We may have been invoked with a specific interpreter not on the $PATH, make sure our\n    # sys.executable is included as a candidate in this case.\n    interpreters = OrderedSet([PythonInterpreter.get()])\n\n    # Add all qualifying interpreters found in $PATH.\n    interpreters.update(PythonInterpreter.all())\n\n  return list(\n    matched_interpreters(interpreters, compatibility_constraints)\n    if compatibility_constraints\n    else interpreters\n  )"}, {"instruction": "def composition(mol):\n    \"\"\"Molecular composition in dict format\n    (ex. Glucose {'C': 6, 'H': 12, 'O': 6}).\n    \"\"\"\n", "input": "", "output": "    mol.require(\"Valence\")\n    c = Counter()\n    for _, a in mol.atoms_iter():\n        c += a.composition()\n    return c"}, {"instruction": "def migrate_050_to_051(session):\n    \"\"\"Set time_out field of all flagged\n    timesheet entries to Null.\n    \"\"\"\n", "input": "", "output": "    entries_to_update = session.query(Entry).filter(\n            Entry.forgot_sign_out.is_(True)).filter(\n            Entry.time_out.isnot(None))\n\n    for entry in entries_to_update:\n        entry.time_out = None\n        logging.info('Entry updated {}'.format(entry.uuid))\n        logging.debug(entry.uuid)\n        session.add(entry)"}, {"instruction": "def get_approvals(self, issue_id_or_key, start=0, limit=50):\n        \"\"\"\n        Get all approvals on a request, for a given request ID/Key\n\n        :param issue_id_or_key: str\n        :param start: OPTIONAL: int\n        :param limit: OPTIONAL: int\n        :return:\n        \"\"\"\n", "input": "", "output": "        url = 'rest/servicedeskapi/request/{}/approval'.format(issue_id_or_key)\n        params = {}\n        if start is not None:\n            params['start'] = int(start)\n        if limit is not None:\n            params['limit'] = int(limit)\n\n        return self.get(url, headers=self.experimental_headers, params=params).get('values')"}, {"instruction": "def hash160(msg_bytes):\n    '''\n    byte-like -> bytes\n    '''\n", "input": "", "output": "    h = hashlib.new('ripemd160')\n    if 'decred' in riemann.get_current_network_name():\n        h.update(blake256(msg_bytes))\n        return h.digest()\n    h.update(sha256(msg_bytes))\n    return h.digest()"}, {"instruction": "def weld_str_get(array, i):\n    \"\"\"Retrieve character at index i.\n\n    Parameters\n    ----------\n    array : numpy.ndarray or WeldObject\n        Input data.\n    i : int\n        Index of character to retrieve. If greater than length of string, returns None.\n\n    Returns\n    -------\n    WeldObject\n        Representation of this computation.\n\n    \"\"\"\n", "input": "", "output": "    obj_id, weld_obj = create_weld_object(array)\n    index_literal = to_weld_literal(i, WeldLong())\n    missing_literal = default_missing_data_literal(WeldVec(WeldChar()))\n    missing_literal_id = get_weld_obj_id(weld_obj, missing_literal)\n\n    weld_template = "}, {"instruction": "def _get_current_albedo(self):\n        '''Simple step-function albedo based on ice line at temperature Tf.'''\n", "input": "", "output": "        ice = self.subprocess['iceline'].ice\n        # noice = self.subprocess['iceline'].diagnostics['noice']\n        cold_albedo = self.subprocess['cold_albedo'].albedo\n        warm_albedo = self.subprocess['warm_albedo'].albedo\n        albedo = Field(np.where(ice, cold_albedo, warm_albedo), domain=self.domains['Ts'])\n        return albedo"}, {"instruction": "def start(self, version=None, **kwargs):#game_version=None, data_version=None, **kwargs):\n    \"\"\"Launch the game process.\"\"\"\n", "input": "", "output": "    if not version:\n        version = self.mostRecentVersion\n    pysc2Version = lib.Version( # convert to pysc2 Version\n        version.version,\n        version.baseVersion,\n        version.dataHash,\n        version.fixedHash)\n    return sc_process.StarcraftProcess(\n                self,\n                exec_path=self.exec_path(version.baseVersion),\n                version=pysc2Version,\n                **kwargs)"}, {"instruction": "def _searchservices(device, name=None, uuid=None, uuidbad=None):\n    \"\"\"\n    Searches the given IOBluetoothDevice using the specified parameters.\n    Returns an empty list if the device has no services.\n\n    uuid should be IOBluetoothSDPUUID object.\n    \"\"\"\n", "input": "", "output": "    if not isinstance(device, _IOBluetooth.IOBluetoothDevice):\n        raise ValueError(\"device must be IOBluetoothDevice, was %s\" % \\\n            type(device))\n\n    services = []\n    allservices = device.getServices()\n    if uuid:\n        gooduuids = (uuid, )\n    else:\n        gooduuids = ()\n    if uuidbad:\n        baduuids = (uuidbad, )\n    else:\n        baduuids = ()\n\n    if allservices is not None:\n        for s in allservices:\n            if gooduuids and not s.hasServiceFromArray_(gooduuids):\n                continue\n            if baduuids and s.hasServiceFromArray_(baduuids):\n                continue\n            if name is None or s.getServiceName() == name:\n                services.append(s)\n    return services"}, {"instruction": "def get_raw(self, name=None):\n        '''Shortcut for getting a :class:`~statsd.raw.Raw` instance\n\n        :keyword name: See :func:`~statsd.client.Client.get_client`\n        :type name: str\n        '''\n", "input": "", "output": "        return self.get_client(name=name, class_=statsd.Raw)"}, {"instruction": "def get_nameserver_detail_output_show_nameserver_nameserver_porttype(self, **kwargs):\n        \"\"\"Auto Generated Code\n        \"\"\"\n", "input": "", "output": "        config = ET.Element(\"config\")\n        get_nameserver_detail = ET.Element(\"get_nameserver_detail\")\n        config = get_nameserver_detail\n        output = ET.SubElement(get_nameserver_detail, \"output\")\n        show_nameserver = ET.SubElement(output, \"show-nameserver\")\n        nameserver_portid_key = ET.SubElement(show_nameserver, \"nameserver-portid\")\n        nameserver_portid_key.text = kwargs.pop('nameserver_portid')\n        nameserver_porttype = ET.SubElement(show_nameserver, \"nameserver-porttype\")\n        nameserver_porttype.text = kwargs.pop('nameserver_porttype')\n\n        callback = kwargs.pop('callback', self._callback)\n        return callback(config)"}, {"instruction": "def clear_jobs():\n    '''Clear old jobs\n\n    :param days: Jobs for how many days should be kept (default: 10)\n    :type days: integer\n\n    :statuscode 200: no error\n    :statuscode 403: not authorized to delete jobs\n    :statuscode 409: an error occurred\n    '''\n", "input": "", "output": "    if not is_authorized():\n        return json.dumps({'error': 'not authorized'}), 403, headers\n\n    days = flask.request.args.get('days', None)\n    return _clear_jobs(days)"}, {"instruction": "def derivativeX(self,*args):\n        '''\n        Returns the derivative of the function with respect to the X dimension.\n        This is the first input whenever n_dims < 4 and the second input otherwise.\n        '''\n", "input": "", "output": "        if self.n_dims >= 4:\n            j = 1\n        else:\n            j = 0\n        if self.i_dim == j:\n            return np.ones_like(*args[0])\n        else:\n            return np.zeros_like(*args[0])"}, {"instruction": "def retention_load(self, forced=False):\n        \"\"\"Call hook point 'load_retention'.\n        Retention modules will read retention (from file, db etc)\n\n        :param forced: is load forced?\n        :type forced: bool\n        :return: None\n        \"\"\"\n", "input": "", "output": "        # If we set the retention update to 0, we do not want to manage retention\n        # If we are not forced (like at stopping)\n        if self.pushed_conf.retention_update_interval == 0 and not forced:\n            logger.debug(\"Should have loaded retention but it is not enabled\")\n            return\n\n        _t0 = time.time()\n        self.hook_point('load_retention')\n        statsmgr.timer('hook.retention-load', time.time() - _t0)\n\n        self.add(make_monitoring_log('INFO', 'RETENTION LOAD: %s' % self.my_daemon.name))\n        logger.info('Retention data loaded: %.2f seconds', time.time() - _t0)"}, {"instruction": "def size(self, table=None):\n        \"\"\"\n        Return the size, in bytes, of the profile or *table*.\n\n        If *table* is `None`, this function returns the size of the\n        whole profile (i.e. the sum of the table sizes). Otherwise, it\n        returns the size of *table*.\n\n        Note: if the file is gzipped, it returns the compressed size.\n        \"\"\"\n", "input": "", "output": "        size = 0\n        if table is None:\n            for table in self.relations:\n                size += self.size(table)\n        else:\n            try:\n                fn = _table_filename(os.path.join(self.root, table))\n                size += os.stat(fn).st_size\n            except ItsdbError:\n                pass\n        return size"}, {"instruction": "def make_static_url(\n        cls, settings: Dict[str, Any], path: str, include_version: bool = True\n    ) -> str:\n        \"\"\"Constructs a versioned url for the given path.\n\n        This method may be overridden in subclasses (but note that it\n        is a class method rather than an instance method).  Subclasses\n        are only required to implement the signature\n        ``make_static_url(cls, settings, path)``; other keyword\n        arguments may be passed through `~RequestHandler.static_url`\n        but are not standard.\n\n        ``settings`` is the `Application.settings` dictionary.  ``path``\n        is the static path being requested.  The url returned should be\n        relative to the current host.\n\n        ``include_version`` determines whether the generated URL should\n        include the query string containing the version hash of the\n        file corresponding to the given ``path``.\n\n        \"\"\"\n", "input": "", "output": "        url = settings.get(\"static_url_prefix\", \"/static/\") + path\n        if not include_version:\n            return url\n\n        version_hash = cls.get_version(settings, path)\n        if not version_hash:\n            return url\n\n        return \"%s?v=%s\" % (url, version_hash)"}, {"instruction": "def is_path_like(obj, attr=('name', 'is_file', 'is_dir', 'iterdir')):\n    \"\"\"test if object is pathlib.Path like\"\"\"\n", "input": "", "output": "    for a in attr:\n        if not hasattr(obj, a):\n            return False\n    return True"}, {"instruction": "def __get_factory_with_context(self, factory_name):\n        # type: (str) -> Tuple[type, FactoryContext]\n        \"\"\"\n        Retrieves the factory registered with the given and its factory context\n\n        :param factory_name: The name of the factory\n        :return: A (factory, context) tuple\n        :raise TypeError: Unknown factory, or factory not manipulated\n        \"\"\"\n", "input": "", "output": "        factory = self.__factories.get(factory_name)\n        if factory is None:\n            raise TypeError(\"Unknown factory '{0}'\".format(factory_name))\n\n        # Get the factory context\n        factory_context = getattr(\n            factory, constants.IPOPO_FACTORY_CONTEXT, None\n        )\n        if factory_context is None:\n            raise TypeError(\n                \"Factory context missing in '{0}'\".format(factory_name)\n            )\n\n        return factory, factory_context"}, {"instruction": "def get(self, *args, **kwargs):\n        '''\n        /label/s/view\n        '''\n", "input": "", "output": "        url_arr = self.parse_url(args[0])\n\n        if len(url_arr) == 2:\n            if url_arr[0] == 'remove':\n                self.remove_redis_keyword(url_arr[1])\n            else:\n                self.list(url_arr[0], url_arr[1])\n        elif len(url_arr) == 3:\n            self.list(url_arr[0], url_arr[1], url_arr[2])\n        else:\n            return False"}, {"instruction": "def rotation_from_axes(x_axis, y_axis, z_axis):\n        \"\"\"Convert specification of axis in target frame to\n        a rotation matrix from source to target frame.\n\n        Parameters\n        ----------\n        x_axis : :obj:`numpy.ndarray` of float\n            A normalized 3-vector for the target frame's x-axis.\n\n        y_axis : :obj:`numpy.ndarray` of float\n            A normalized 3-vector for the target frame's y-axis.\n\n        z_axis : :obj:`numpy.ndarray` of float\n            A normalized 3-vector for the target frame's z-axis.\n\n        Returns\n        -------\n        :obj:`numpy.ndarray` of float\n            A 3x3 rotation matrix that transforms from a source frame to the\n            given target frame.\n        \"\"\"\n", "input": "", "output": "        return np.hstack((x_axis[:,np.newaxis], y_axis[:,np.newaxis], z_axis[:,np.newaxis]))"}, {"instruction": "def _bell(self):\n        u'''ring the bell if requested.'''\n", "input": "", "output": "        if self.bell_style == u'none':\n            pass\n        elif self.bell_style == u'visible':\n            raise NotImplementedError(u\"Bellstyle visible is not implemented yet.\")\n        elif self.bell_style == u'audible':\n            self.console.bell()\n        else:\n            raise ReadlineError(u\"Bellstyle %s unknown.\"%self.bell_style)"}, {"instruction": "def checksum(path, hashfunc='md5'):\n    \"\"\"\n    Return checksum given by path. Wildcards can be used in check sum. Function is strongly\n    dependent on checksumdir package by 'cakepietoast'.\n\n    :param path:\n    :param hashfunc:\n    :return:\n    \"\"\"\n", "input": "", "output": "    import checksumdir\n    hash_func = checksumdir.HASH_FUNCS.get(hashfunc)\n    if not hash_func:\n        raise NotImplementedError('{} not implemented.'.format(hashfunc))\n\n    if os.path.isdir(path):\n        return checksumdir.dirhash(path, hashfunc=hashfunc)\n\n    hashvalues = []\n    path_list = glob.glob(path)\n    logger.debug(\"path_list \" + str(path_list))\n    for path in path_list:\n        if os.path.isfile(path):\n            hashvalues.append(checksumdir._filehash(path, hashfunc=hash_func))\n    logger.debug(str(hashvalues))\n    hash = checksumdir._reduce_hash(hashvalues, hashfunc=hash_func)\n    return hash"}, {"instruction": "def _detect_available_configs():\n        \"\"\"\n        Returns all currently used channels as well as\n        one other currently unused channel.\n\n        .. note::\n\n            This method will run into problems if thousands of\n            autodetected busses are used at once.\n\n        \"\"\"\n", "input": "", "output": "        with channels_lock:\n            available_channels = list(channels.keys())\n\n        # find a currently unused channel\n        get_extra = lambda: \"channel-{}\".format(randint(0, 9999))\n        extra = get_extra()\n        while extra in available_channels:\n            extra = get_extra()\n\n        available_channels += [extra]\n\n        return [\n            {'interface': 'virtual', 'channel': channel}\n            for channel in available_channels\n        ]"}, {"instruction": "def ignore_whitespace_text_nodes(cls, wrapped_node):\n        \"\"\"\n        Find and delete any text nodes containing nothing but whitespace in\n        in the given node and its descendents.\n\n        This is useful for cleaning up excess low-value text nodes in a\n        document DOM after parsing a pretty-printed XML document.\n        \"\"\"\n", "input": "", "output": "        for child in wrapped_node.children:\n            if child.is_text and child.value.strip() == '':\n                child.delete()\n            else:\n                cls.ignore_whitespace_text_nodes(child)"}, {"instruction": "def ChargeColorMapping(maptype='jet', reverse=False):\n    \"\"\"Maps amino-acid charge at neutral pH to colors. \n    Currently does not use the keyword arguments for *maptype*\n    or *reverse* but accepts these arguments to be consistent\n    with KyteDoolittleColorMapping and MWColorMapping for now.\"\"\"\n", "input": "", "output": "\n    pos_color = '#FF0000'\n    neg_color = '#0000FF'\n    neut_color = '#000000'\n\n    mapping_d = {'A':neut_color,'R':pos_color,'N':neut_color,\\\n                 'D':neg_color,'C':neut_color,'Q':neut_color,\\\n                 'E':neg_color,'G':neut_color,'H':pos_color,\\\n                 'I':neut_color,'L':neut_color,'K':pos_color,\\\n                 'M':neut_color,'F':neut_color,'P':neut_color,\\\n                 'S':neut_color,'T':neut_color,'W':neut_color,\\\n                 'Y':neut_color,'V':neut_color}\n\n    return (None, mapping_d, None)"}, {"instruction": "def match_string(self, stype):\n        \"\"\"Match string type.\"\"\"\n", "input": "", "output": "\n        return not (stype - self.string_types) or bool(stype & self.wild_string_types)"}, {"instruction": "def\tpurge_url(self, host, path):\n\t\t\"\"\"Purge an individual URL.\"\"\"\n", "input": "", "output": "\t\tcontent = self._fetch(path, method=\"PURGE\", headers={ \"Host\": host }) \n\t\treturn FastlyPurge(self, content)"}, {"instruction": "def hasChannelType(self, chan):\n        \"\"\"Returns True if chan is among the supported channel types.\n        \n        @param app: Module name.\n        @return:    Boolean \n        \n        \"\"\"\n", "input": "", "output": "        if self._chantypes is None:\n            self._initChannelTypesList()\n        return chan in self._chantypes"}, {"instruction": "def wavefunction(self) -> pyquil.Wavefunction:\n        \"\"\"\n        Return the wavefunction of a completed program.\n        \"\"\"\n", "input": "", "output": "        assert self.status == 'done'\n        assert self._ket is not None\n        wavefn = state_to_wavefunction(self._ket)\n        return wavefn"}, {"instruction": "def to_pixel(self, wcs, mode='all'):\n        \"\"\"\n        Convert the aperture to a `RectangularAnnulus` object defined in\n        pixel coordinates.\n\n        Parameters\n        ----------\n        wcs : `~astropy.wcs.WCS`\n            The world coordinate system (WCS) transformation to use.\n\n        mode : {'all', 'wcs'}, optional\n            Whether to do the transformation including distortions\n            (``'all'``; default) or only including only the core WCS\n            transformation (``'wcs'``).\n\n        Returns\n        -------\n        aperture : `RectangularAnnulus` object\n            A `RectangularAnnulus` object.\n        \"\"\"\n", "input": "", "output": "\n        pixel_params = self._to_pixel_params(wcs, mode=mode)\n        return RectangularAnnulus(**pixel_params)"}, {"instruction": "def populate(self, obj=None, section=None, parse_types=True):\n        \"\"\"Set attributes in ``obj`` with ``setattr`` from the all values in\n        ``section``.\n\n        \"\"\"\n", "input": "", "output": "        section = self.default_section if section is None else section\n        obj = Settings() if obj is None else obj\n        is_dict = isinstance(obj, dict)\n        for k, v in self.get_options(section).items():\n            if parse_types:\n                if v == 'None':\n                    v = None\n                elif self.FLOAT_REGEXP.match(v):\n                    v = float(v)\n                elif self.INT_REGEXP.match(v):\n                    v = int(v)\n                elif self.BOOL_REGEXP.match(v):\n                    v = v == 'True'\n                else:\n                    m = self.EVAL_REGEXP.match(v)\n                    if m:\n                        evalstr = m.group(1)\n                        v = eval(evalstr)\n            logger.debug('setting {} => {} on {}'.format(k, v, obj))\n            if is_dict:\n                obj[k] = v\n            else:\n                setattr(obj, k, v)\n        return obj"}, {"instruction": "def update_distant_reference(self, ref):\n        \"\"\"Validate and update the reference in Zotero.\n\n        Existing fields not present will be left unmodified.\n        \"\"\"\n", "input": "", "output": "        self.validate_reference_data(ref[\"data\"])\n        self._zotero_lib.update_item(ref)"}, {"instruction": "def mysql_aes_encrypt(val, key):\n    \"\"\"Mysql AES encrypt value with secret key.\n\n    :param val: Plain text value.\n    :param key: The AES key.\n    :returns: The encrypted AES value.\n    \"\"\"\n", "input": "", "output": "    assert isinstance(val, binary_type) or isinstance(val, text_type)\n    assert isinstance(key, binary_type) or isinstance(key, text_type)\n    k = _mysql_aes_key(_to_binary(key))\n    v = _mysql_aes_pad(_to_binary(val))\n    e = _mysql_aes_engine(k).encryptor()\n\n    return e.update(v) + e.finalize()"}, {"instruction": "def run(self, *args):\n        \"\"\"Add an identity to the registry.\"\"\"\n", "input": "", "output": "\n        params = self.parser.parse_args(args)\n\n        code = self.add(params.source, params.email, params.name, params.username,\n                        params.uuid, params.matching, params.interactive)\n\n        return code"}, {"instruction": "def get_element_centroids(self):\n        \"\"\"return the central points of all elements\n\n        Returns\n        -------\n        Nx2 array\n            x/z coordinates for all (N) elements\n        \"\"\"\n", "input": "", "output": "        centroids = np.vstack((\n            np.mean(self.grid['x'], axis=1), np.mean(self.grid['z'], axis=1)\n        )).T\n\n        return centroids"}, {"instruction": "def set_settings(self, releases=None, default_release=None):\n        \"\"\"set path to storage\"\"\"\n", "input": "", "output": "        super(ReplicaSets, self).set_settings(releases, default_release)\n        Servers().set_settings(releases, default_release)"}, {"instruction": "def initialize(self, store):\r\n        \"\"\"Common initialization of handlers happens here. If additional\r\n        initialization is required, this method must either be called with\r\n        ``super`` or the child class must assign the ``store`` attribute and\r\n        register itself with the store.\r\n\r\n        \"\"\"\n", "input": "", "output": "        assert isinstance(store, stores.BaseStore)\r\n        self.messages = Queue()\r\n        self.store = store\r\n        self.store.register(self)"}, {"instruction": "def pprint(obj, *args, **kwargs):\n    \"\"\"Pretty-printing function that can pretty-print OrderedDicts\n    like regular dictionaries. Useful for printing the output of\n    :meth:`marshmallow.Schema.dump`.\n    \"\"\"\n", "input": "", "output": "    if isinstance(obj, collections.OrderedDict):\n        print(json.dumps(obj, *args, **kwargs))\n    else:\n        py_pprint(obj, *args, **kwargs)"}, {"instruction": "def get_athlete_stats(self, athlete_id=None):\n        \"\"\"\n        Returns Statistics for the athlete.\n        athlete_id must be the id of the authenticated athlete or left blank.\n        If it is left blank two requests will be made - first to get the\n        authenticated athlete's id and second to get the Stats.\n\n        http://strava.github.io/api/v3/athlete/#stats\n\n        :return: A model containing the Stats\n        :rtype: :py:class:`stravalib.model.AthleteStats`\n        \"\"\"\n", "input": "", "output": "        if athlete_id is None:\n            athlete_id = self.get_athlete().id\n\n        raw = self.protocol.get('/athletes/{id}/stats', id=athlete_id)\n        # TODO: Better error handling - this will return a 401 if this athlete\n        #       is not the authenticated athlete.\n\n        return model.AthleteStats.deserialize(raw)"}, {"instruction": "def as_blocks(self, copy=True):\n        \"\"\"\n        Convert the frame to a dict of dtype -> Constructor Types that each has\n        a homogeneous dtype.\n\n        .. deprecated:: 0.21.0\n\n        NOTE: the dtypes of the blocks WILL BE PRESERVED HERE (unlike in\n              as_matrix)\n\n        Parameters\n        ----------\n        copy : boolean, default True\n\n        Returns\n        -------\n        values : a dict of dtype -> Constructor Types\n        \"\"\"\n", "input": "", "output": "        warnings.warn(\"as_blocks is deprecated and will \"\n                      \"be removed in a future version\",\n                      FutureWarning, stacklevel=2)\n        return self._to_dict_of_blocks(copy=copy)"}, {"instruction": "def initialize_request(self, request, *args, **kwargs):\n        \"\"\"\n        Returns the initial request object.\n        \"\"\"\n", "input": "", "output": "        parser_context = self.get_parser_context(request)\n\n        return Request(\n            request,\n            parsers=self.get_parsers(),\n            authenticators=self.get_authenticators(),\n            negotiator=self.get_content_negotiator(),\n            parser_context=parser_context\n        )"}, {"instruction": "def setPrefilter(self, edfsignal, prefilter):\n        \"\"\"\n        Sets the prefilter of signal edfsignal (\"HP:0.1Hz\", \"LP:75Hz N:50Hz\", etc.)\n\n        :param edfsignal: int\n        :param prefilter: str\n\n        Notes\n        -----\n        This function is optional for every signal and can be called only after opening a file in writemode and before the first sample write action.\n        \"\"\"\n", "input": "", "output": "        if edfsignal < 0 or edfsignal > self.n_channels:\n            raise ChannelDoesNotExist(edfsignal)\n        self.channels[edfsignal]['prefilter'] = prefilter\n        self.update_header()"}, {"instruction": "def _join_signals(self):\n        r\"\"\"Join N 1D signals into one N-dimensional signal.\"\"\"\n", "input": "", "output": "        joined = dict()\n        for name in self.signals:\n            name_base = name.rsplit('_', 1)[0]\n            names = joined.get(name_base, list())\n            names.append(name)\n            joined[name_base] = names\n        for name_base, names in joined.items():\n            if len(names) > 1:\n                names = sorted(names)  # ensure dim ordering (_0, _1, etc.)\n                signal_nd = np.stack([self.signals[n] for n in names], axis=1)\n                self.signals[name_base] = signal_nd\n                for name in names:\n                    del self.signals[name]"}, {"instruction": "def n_sections(neurites, neurite_type=NeuriteType.all, iterator_type=Tree.ipreorder):\n    '''Number of sections in a collection of neurites'''\n", "input": "", "output": "    return sum(1 for _ in iter_sections(neurites,\n                                        iterator_type=iterator_type,\n                                        neurite_filter=is_type(neurite_type)))"}, {"instruction": "def find_project_file(start_dir, basename):\n    '''Walk up the directory tree until we find a file of the given name.'''\n", "input": "", "output": "    prefix = os.path.abspath(start_dir)\n    while True:\n        candidate = os.path.join(prefix, basename)\n        if os.path.isfile(candidate):\n            return candidate\n        if os.path.exists(candidate):\n            raise PrintableError(\n                \"Found {}, but it's not a file.\".format(candidate))\n        if os.path.dirname(prefix) == prefix:\n            # We've walked all the way to the top. Bail.\n            raise PrintableError(\"Can't find \" + basename)\n        # Not found at this level. We must go...shallower.\n        prefix = os.path.dirname(prefix)"}, {"instruction": "def show_run(command_history_id):\n    \"\"\"\n    Show detailed command history by its ID.\n    \"\"\"\n", "input": "", "output": "    from pprint import pprint\n    from .config import ConfigStore\n    from .database import DataBase\n    db = DataBase(ConfigStore().db_path)\n    with db.connection():\n        for ch_id in command_history_id:\n            crec = db.get_full_command_record(ch_id)\n            pprint(crec.__dict__)\n            print(\"\")"}, {"instruction": "def init_widget(self):\n        \"\"\" Initialize the underlying widget.\n\n        \"\"\"\n", "input": "", "output": "        super(AndroidTextureView, self).__init__(self)\n        w = self.widget\n        w.setSurfaceTextureListener(w.getId())\n        w.onSurfaceTextureAvailable.connect(self.on_surface_texture_available)\n        w.onSurfaceTextureDestroyed.connect(self.on_surface_texture_destroyed)\n        w.onSurfaceTextureChanged.connect(self.on_surface_texture_changed)\n        w.onSurfaceTextureUpdated.connect(self.on_surface_texture_updated)"}, {"instruction": "def rgb(color,default=(0,0,0)):\n    \"\"\" return rgb tuple for named color in rgb.txt or a hex color \"\"\"\n", "input": "", "output": "    c = color.lower()\n    if c[0:1] == '#' and len(c)==7:\n        r,g,b = c[1:3], c[3:5], c[5:]\n        r,g,b = [int(n, 16) for n in (r, g, b)]\n        return (r,g,b)\n\n    if c.find(' ')>-1:    c = c.replace(' ','')\n    if c.find('gray')>-1: c = c.replace('gray','grey')\n    if c in x11_colors.keys():  return x11_colors[c]\n    return default"}, {"instruction": "def get_mapping_variable(variable_name, variables_mapping):\n    \"\"\" get variable from variables_mapping.\n\n    Args:\n        variable_name (str): variable name\n        variables_mapping (dict): variables mapping\n\n    Returns:\n        mapping variable value.\n\n    Raises:\n        exceptions.VariableNotFound: variable is not found.\n\n    \"\"\"\n", "input": "", "output": "    try:\n        return variables_mapping[variable_name]\n    except KeyError:\n        raise exceptions.VariableNotFound(\"{} is not found.\".format(variable_name))"}, {"instruction": "def request_type(self):\n        \"\"\"Retrieve the type of the request, by fetching it from\n        `xenon.proto.xenon_pb2`.\"\"\"\n", "input": "", "output": "        if self.static and not self.uses_request:\n            return getattr(xenon_pb2, 'Empty')\n\n        if not self.uses_request:\n            return None\n\n        return getattr(xenon_pb2, self.request_name)"}, {"instruction": "def _full_diff(merge_result, key, context_lines=3):\n    \"\"\"Generate a full diff based on a Weave merge result\"\"\"\n", "input": "", "output": "    header_printed = False\n    for group in _split_diff(merge_result, context_lines=context_lines):\n        if not header_printed:\n            header_printed = True\n            yield color.Header('diff a/%s b/%s' % (key, key))\n            yield color.DeletedHeader('--- %s' % key)\n            yield color.AddedHeader('+++ %s' % key)\n\n        for l in _diff_group(group):\n            yield l"}, {"instruction": "def sky2vec(cls, sky):\n        \"\"\"\n        Convert sky positions in to 3d-vectors on the unit sphere.\n\n        Parameters\n        ----------\n        sky : numpy.array\n            Sky coordinates as an array of (ra,dec)\n\n        Returns\n        -------\n        vec : numpy.array\n            Unit vectors as an array of (x,y,z)\n\n        See Also\n        --------\n        :func:`AegeanTools.regions.Region.vec2sky`\n        \"\"\"\n", "input": "", "output": "        theta_phi = cls.sky2ang(sky)\n        theta, phi = map(np.array, list(zip(*theta_phi)))\n        vec = hp.ang2vec(theta, phi)\n        return vec"}, {"instruction": "def get_debug_option(packagename):\n    \"\"\" Determines if the build is in debug mode.\n\n    Returns\n    -------\n    debug : bool\n        True if the current build was started with the debug option, False\n        otherwise.\n\n    \"\"\"\n", "input": "", "output": "\n    try:\n        current_debug = get_pkg_version_module(packagename,\n                                               fromlist=['debug'])[0]\n    except (ImportError, AttributeError):\n        current_debug = None\n\n    # Only modify the debug flag if one of the build commands was explicitly\n    # run (i.e. not as a sub-command of something else)\n    dist = get_dummy_distribution()\n    if any(cmd in dist.commands for cmd in ['build', 'build_ext']):\n        debug = bool(get_distutils_build_option('debug'))\n    else:\n        debug = bool(current_debug)\n\n    if current_debug is not None and current_debug != debug:\n        build_ext_cmd = dist.get_command_class('build_ext')\n        build_ext_cmd._force_rebuild = True\n\n    return debug"}, {"instruction": "def disassemble_string(self, lpAddress, code):\n        \"\"\"\n        Disassemble instructions from a block of binary code.\n\n        @type  lpAddress: int\n        @param lpAddress: Memory address where the code was read from.\n\n        @type  code: str\n        @param code: Binary code to disassemble.\n\n        @rtype:  list of tuple( long, int, str, str )\n        @return: List of tuples. Each tuple represents an assembly instruction\n            and contains:\n             - Memory address of instruction.\n             - Size of instruction in bytes.\n             - Disassembly line of instruction.\n             - Hexadecimal dump of instruction.\n        \"\"\"\n", "input": "", "output": "        aProcess = self.get_process()\n        return aProcess.disassemble_string(lpAddress, code)"}, {"instruction": "def mask(args):\n    \"\"\"\n    %prog mask fastafile\n\n    Mask the contaminants. By default, this will compare against UniVec_Core and\n    Ecoli.fasta. Merge the contaminant results, and use `maskFastaFromBed`. Can\n    perform FASTA tidy if requested.\n    \"\"\"\n", "input": "", "output": "    p = OptionParser(mask.__doc__)\n    p.add_option(\"--db\",\n                 help=\"Contaminant db other than Ecoli K12 [default: %default]\")\n    opts, args = p.parse_args(args)\n\n    if len(args) != 1:\n        sys.exit(not p.print_help())\n\n    fastafile, = args\n    assert op.exists(fastafile)\n\n    outfastafile = fastafile.rsplit(\".\", 1)[0] + \".masked.fasta\"\n    vecbedfile = blast([fastafile])\n    ecoliurl = \\\n    \"ftp://ftp.ncbi.nih.gov/genomes/Bacteria/Escherichia_coli_K_12_substr__DH10B_uid58979/NC_010473.fna\"\n    ecolifile = opts.db or download(ecoliurl, filename=\"Ecoli.fasta\")\n    assert op.exists(ecolifile)\n    ecolibedfile = blast([fastafile, \"--db={0}\".format(ecolifile)])\n\n    cmd = \"cat {0} {1}\".format(vecbedfile, ecolibedfile)\n    cmd += \" | mergeBed -nms -d 100 -i stdin\"\n    cmd += \" | maskFastaFromBed -fi {0} -bed stdin -fo {1}\".\\\n            format(fastafile, outfastafile)\n    sh(cmd)\n\n    return tidy([outfastafile])"}, {"instruction": "def find_all_commands(management_dir):\n    \"\"\"\n    Find all valid commands in a directory\n    management_dir : directory path\n    return - List of commands\n    \"\"\"\n", "input": "", "output": "    try:\n        #Find all commands in the directory that are not __init__.py and end in .py.  Then, remove the trailing .py\n        return [f[:-3] for f in os.listdir(management_dir) if f.endswith('.py') and not f.startswith(\"__\")]\n    except OSError:\n        #If nothing is found, return empty\n        return []"}, {"instruction": "def switch_to_output(self, value=False, **kwargs):\n        \"\"\"Switch the pin state to a digital output with the provided starting\n        value (True/False for high or low, default is False/low).\n        \"\"\"\n", "input": "", "output": "        self.direction = digitalio.Direction.OUTPUT\n        self.value = value"}, {"instruction": "def get_versioned_delete_collector_class():\n    \"\"\"\n    Gets the class to use for deletion collection.\n\n    :return: class\n    \"\"\"\n", "input": "", "output": "    key = 'VERSIONED_DELETE_COLLECTOR'\n    try:\n        cls = _cache[key]\n    except KeyError:\n        collector_class_string = getattr(settings, key)\n        cls = import_from_string(collector_class_string, key)\n        _cache[key] = cls\n    return cls"}, {"instruction": "def parse_cgmlst_alleles(cgmlst_fasta):\n    \"\"\"Parse cgMLST alleles from fasta file\n    cgMLST FASTA file must have a header format of \">{marker name}|{allele name}\"\n\n    Args:\n        cgmlst_fasta (str): cgMLST fasta file path\n\n    Returns:\n        dict of list: Marker name to list of allele sequences\n    \"\"\"\n", "input": "", "output": "    out = defaultdict(list)\n    for header, seq in parse_fasta(cgmlst_fasta):\n        if not '|' in header:\n            raise Exception('Unexpected format for cgMLST fasta file header. No \"|\" (pipe) delimiter present! Header=\"{}\"'.format(header))\n        marker_name, allele_name = header.split('|')\n        out[marker_name].append(seq)\n    return out"}, {"instruction": "def list_licenses(service_instance=None):\n    '''\n    Lists all licenses on a vCenter.\n\n    service_instance\n        Service instance (vim.ServiceInstance) of the vCenter/ESXi host.\n        Default is None.\n\n    .. code-block:: bash\n\n        salt '*' vsphere.list_licenses\n    '''\n", "input": "", "output": "    log.trace('Retrieving all licenses')\n    licenses = salt.utils.vmware.get_licenses(service_instance)\n    ret_dict = [{'key': l.licenseKey,\n                 'name': l.name,\n                 'description': l.labels[0].value if l.labels else None,\n                 # VMware handles unlimited capacity as 0\n                 'capacity': l.total if l.total > 0 else sys.maxsize,\n                 'used': l.used if l.used else 0}\n                 for l in licenses]\n    return ret_dict"}, {"instruction": "def job_info(self, **kwargs):\n        \"\"\"\n        Get the information about the jobs returned by a particular search.\n        See the [GetJobs][] documentation for more info.\n\n        [GetJobs]: http://casjobs.sdss.org/casjobs/services/jobs.asmx?op=GetJobs\n\n        \"\"\"\n", "input": "", "output": "        search = \";\".join([\"%s : %s\"%(k, str(kwargs[k])) for k in kwargs])\n        params = {\"owner_wsid\": self.userid, \"owner_pw\": self.password,\n                \"conditions\": search, \"includeSystem\": False}\n        r = self._send_request(\"GetJobs\", params=params)\n        results = []\n        for n in minidom.parseString(r.text).getElementsByTagName(\"CJJob\"):\n            results.append({})\n            for e in n.childNodes:\n                if e.nodeType != e.TEXT_NODE:\n                    results[-1][e.tagName] = e.firstChild.data\n        return results"}, {"instruction": "def process_settings(pelicanobj):\n    \"\"\"Sets user specified settings (see README for more details)\"\"\"\n", "input": "", "output": "\n    # Default settings\n    inline_settings = {}\n    inline_settings['config'] = {'[]':('', 'pelican-inline')}\n\n    # Get the user specified settings\n    try:\n        settings = pelicanobj.settings['MD_INLINE']\n    except:\n        settings = None\n\n    # If settings have been specified, add them to the config\n    if isinstance(settings, dict):\n        inline_settings['config'].update(settings)\n\n    return inline_settings"}, {"instruction": "def _handle_tag_text(self, text):\n        \"\"\"Handle regular *text* inside of an HTML open tag.\"\"\"\n", "input": "", "output": "        next = self._read(1)\n        if not self._can_recurse() or text not in self.MARKERS:\n            self._emit_text(text)\n        elif text == next == \"{\":\n            self._parse_template_or_argument()\n        elif text == next == \"[\":\n            self._parse_wikilink()\n        elif text == \"<\":\n            self._parse_tag()\n        else:\n            self._emit_text(text)"}, {"instruction": "def ValidateLanguageCode(lang, column_name=None, problems=None):\n  \"\"\"\n  Validates a non-required language code value using the pybcp47 module:\n    - if invalid adds InvalidValue error (if problems accumulator is provided)\n    - distinguishes between 'not well-formed' and 'not valid' and adds error\n      reasons accordingly\n    - an empty language code is regarded as valid! Otherwise we might end up\n      with many duplicate errors because of the required field checks.\n    - returns true if the language is valid, false if not well-formed or\n      invalid.\n  \"\"\"\n", "input": "", "output": "  if util.IsEmpty(lang):\n    return True\n  bcp47_obj = parser.ParseLanguage(str(lang.lower()))\n  if not bcp47_obj.wellformed:\n    if problems:\n      problems.InvalidValue(column_name, lang,\n                            'language code \"%s\" is not well-formed' %\n                            lang, type=problems_class.TYPE_ERROR)\n    return False\n  if not bcp47_obj.valid:\n    if problems:\n      problems.InvalidValue(column_name, lang,\n                            'language code \"%s\" is not valid, parses as: %s' %\n                            (lang, bcp47_obj), type=problems_class.TYPE_WARNING)\n    return False\n  return True"}, {"instruction": "def addAdminResource(self, pluginSubPath: bytes, resource: BasicResource) -> None:\n        \"\"\" Add Site Resource\n\n        Add a cusotom implementation of a served http resource.\n\n        :param pluginSubPath: The resource path where you want to serve this resource.\n        :param resource: The resource to serve.\n        :return: None\n\n        \"\"\"\n", "input": "", "output": "        pluginSubPath = pluginSubPath.strip(b'/')\n        self.__rootAdminResource.putChild(pluginSubPath, resource)"}, {"instruction": "def post_optimization_step(self, batch_info, device, model, rollout):\n        \"\"\" Steps to take after optimization has been done\"\"\"\n", "input": "", "output": "        if batch_info.aggregate_batch_number % self.target_update_frequency == 0:\n            self.target_model.load_state_dict(model.state_dict())\n            self.target_model.eval()"}, {"instruction": "def _count_dollars_before_index(self, s, i):\n        \"\"\"Returns the number of '$' characters right in front of s[i].\"\"\"\n", "input": "", "output": "        dollar_count = 0\n        dollar_index = i - 1\n        while dollar_index > 0 and s[dollar_index] == '$':\n            dollar_count += 1\n            dollar_index -= 1\n        return dollar_count"}, {"instruction": "def _construct_regex(cls, fmt):\n        \"\"\"Given a format string, construct the regex with class attributes.\"\"\"\n", "input": "", "output": "        return re.compile(fmt.format(**vars(cls)), flags=re.U)"}, {"instruction": "def __add_options(parser):\n    \"\"\"\n    Add the `Configure` options to a option-parser instance or a\n    option group.\n    \"\"\"\n", "input": "", "output": "    parser.add_option('--upx-dir', default=None,\n                      help='Directory containing UPX.')\n    parser.add_option('-C', '--configfile',\n                      default=DEFAULT_CONFIGFILE,\n                      dest='configfilename',\n                      help='Name of generated configfile (default: %default)')"}, {"instruction": "def node_changed(self, node):\n        \"\"\"\n        Calls :meth:`QAbstractItemModel.dataChanged` with given Node index.\n\n        :param node: Node.\n        :type node: AbstractCompositeNode or GraphModelNode\n        :return: Method success.\n        :rtype: bool\n        \"\"\"\n", "input": "", "output": "\n        index = self.get_node_index(node)\n        if index is not None:\n            self.dataChanged.emit(index, index)\n            return True\n        else:\n            return False"}, {"instruction": "def _iter_names(self):\n        \"\"\"\n        Generate a key/value pair for each name in this table. The key is a\n        (platform_id, name_id) 2-tuple and the value is the unicode text\n        corresponding to that key.\n        \"\"\"\n", "input": "", "output": "        table_format, count, strings_offset = self._table_header\n        table_bytes = self._table_bytes\n\n        for idx in range(count):\n            platform_id, name_id, name = self._read_name(\n                table_bytes, idx, strings_offset\n            )\n            if name is None:\n                continue\n            yield ((platform_id, name_id), name)"}, {"instruction": "def findall(dir=os.curdir):\n    \"\"\"\n    Find all files under 'dir' and return the list of full filenames.\n    Unless dir is '.', return full filenames with dir prepended.\n    \"\"\"\n", "input": "", "output": "    files = _find_all_simple(dir)\n    if dir == os.curdir:\n        make_rel = functools.partial(os.path.relpath, start=dir)\n        files = map(make_rel, files)\n    return list(files)"}, {"instruction": "def data_filler_simple_registration(self, number_of_rows, conn):\n        '''creates and fills the table with simple regis. information\n        '''\n", "input": "", "output": "        cursor = conn.cursor()\n\n        cursor.execute("}, {"instruction": "def finalize(self, **kwargs):\n        \"\"\"\n        Finalize executes any subclass-specific axes finalization steps.\n        The user calls poof and poof calls finalize.\n\n        Parameters\n        ----------\n        kwargs: generic keyword arguments.\n\n        \"\"\"\n", "input": "", "output": "        # Set the title and add the legend\n        self.set_title('ROC Curves for {}'.format(self.name))\n        self.ax.legend(loc='lower right', frameon=True)\n\n        # Set the limits for the ROC/AUC (always between 0 and 1)\n        self.ax.set_xlim([0.0, 1.0])\n        self.ax.set_ylim([0.0, 1.0])\n\n        # Set x and y axis labels\n        self.ax.set_ylabel('True Postive Rate')\n        self.ax.set_xlabel('False Positive Rate')"}, {"instruction": "def load(self, page = None, verbose=False):\n        \"\"\"\n        call to execute the collection loading\n        :param page: integer of the page to load\n        :param verbose: boolean to print to console\n        :returns response\n        :raises the SalesKingException\n        \"\"\"\n", "input": "", "output": "        url = self._build_query_url(page, verbose)\n        response = self._load(url, verbose)\n        response = self._post_load(response, verbose)\n        return response"}, {"instruction": "def to_match(self):\n        \"\"\"Return a unicode object with the MATCH representation of this GlobalContextField.\"\"\"\n", "input": "", "output": "        self.validate()\n\n        mark_name, field_name = self.location.get_location_name()\n        validate_safe_string(mark_name)\n        validate_safe_string(field_name)\n\n        return u'%s.%s' % (mark_name, field_name)"}, {"instruction": "def findnode(obj, path=''):\n    \"\"\"Returns a Node pointing to obj.\n    \n    If obj is a ctypes-derived class, an UnboundNode is returned.  If obj is\n    an instance of such a class, then a BoundNode will be returned.\n    \n    If the optional path is provided, it is a string to look up searching\n    down the original source node, such as '.overhead.window[2].page'\n    \"\"\"\n", "input": "", "output": "    if isclass(obj):\n        node = _createunbound(obj)\n    else:\n        node = _createbound(obj)\n    \n    # And walk it down.\n    pathparts = re.split(r'\\]?(?:[[.]|$)', path)\n    for part in pathparts:\n        if not part:    continue\n        try:\n            idx = int(part)\n            node = node[idx]\n        except ValueError:\n            node = node[part]\n    return node"}, {"instruction": "def all_inspections(obj):\n    \"\"\"\n    Generator to iterate all current Jishaku inspections.\n    \"\"\"\n", "input": "", "output": "\n    for name, callback in INSPECTIONS:\n        result = callback(obj)\n        if result:\n            yield name, result"}, {"instruction": "def cache(self):\n      \"\"\"Caches the result of loader(filename) to cachename.\"\"\"\n", "input": "", "output": "      msg = 'Saving updates from more recent \"%s\" to \"%s\"'\n      log.info(msg, self.filename, self.cachename)\n      with open(self.cachename, 'wb') as output:\n          cPickle.dump(self._dict, output, -1)"}, {"instruction": "def _make_proxy(self, varname, parent=None, constructor=MlabObjectProxy):\n        \"\"\"Creates a proxy for a variable.\n\n        XXX create and cache nested proxies also here.\n        \"\"\"\n", "input": "", "output": "        # FIXME why not just use gensym here?\n        proxy_val_name = \"PROXY_VAL%d__\" % self._proxy_count\n        self._proxy_count += 1\n        mlabraw.eval(self._session, \"%s = %s;\" % (proxy_val_name, varname))\n        res = constructor(self, proxy_val_name, parent)\n        self._proxies[proxy_val_name] = res\n        return res"}, {"instruction": "def _disbatch_runner_async(self, chunk):\n        '''\n        Disbatch runner client_async commands\n        '''\n", "input": "", "output": "        pub_data = self.saltclients['runner'](chunk)\n        raise tornado.gen.Return(pub_data)"}, {"instruction": "def create(context, name):\n    \"\"\"create(context, name)\n\n    Create a tag.\n\n    >>> dcictl tag-create [OPTIONS]\n\n    :param string name: Name of the tag [required]\n    \"\"\"\n", "input": "", "output": "\n    result = tag.create(context, name=name)\n    utils.format_output(result, context.format)"}, {"instruction": "def replace(self, child, *nodes):\n        r\"\"\"Replace provided node with node(s).\n\n        :param TexNode child: Child node to replace\n        :param TexNode nodes: List of nodes to subtitute in\n\n        >>> from TexSoup import TexSoup\n        >>> soup = TexSoup(r'''\n        ... \\begin{itemize}\n        ...     \\item Hello\n        ...     \\item Bye\n        ... \\end{itemize}''')\n        >>> items = list(soup.find_all('item'))\n        >>> bye = items[1]\n        >>> soup.itemize.replace(soup.item, bye)\n        >>> soup.itemize\n        \\begin{itemize}\n            \\item Bye\n        \\item Bye\n        \\end{itemize}\n        \"\"\"\n", "input": "", "output": "        self.expr.insert(\n            self.expr.remove(child.expr),\n            *nodes)"}, {"instruction": "def from_yaml(data):\n    \"\"\"\n    Interpolate the provided data and return a dict.\n\n    Currently, this is used to reinterpolate the `molecule.yml` inside an\n    Ansible playbook.  If there were any interpolation errors, they would\n    have been found and raised earlier.\n\n    :return: dict\n    \"\"\"\n", "input": "", "output": "    molecule_env_file = os.environ['MOLECULE_ENV_FILE']\n\n    env = os.environ.copy()\n    env = config.set_env_from_file(env, molecule_env_file)\n\n    i = interpolation.Interpolator(interpolation.TemplateWithDefaults, env)\n    interpolated_data = i.interpolate(data)\n\n    return util.safe_load(interpolated_data)"}, {"instruction": "def get_bucket_type_props(self, bucket_type):\n        \"\"\"\n        Fetch bucket-type properties\n        \"\"\"\n", "input": "", "output": "        self._check_bucket_types(bucket_type)\n        msg_code = riak.pb.messages.MSG_CODE_GET_BUCKET_TYPE_REQ\n        codec = self._get_codec(msg_code)\n        msg = codec.encode_get_bucket_type_props(bucket_type)\n        resp_code, resp = self._request(msg, codec)\n        return codec.decode_bucket_props(resp.props)"}, {"instruction": "def u_edit(*args):\n    \"\"\"\n    Edits given paths into Umbra.\n\n    :param \\*args: Arguments.\n    :type \\*args: \\*\n    :return: Definition success.\n    :rtype: bool\n    \"\"\"\n", "input": "", "output": "\n    paths = []\n    for path in args:\n        if not os.path.exists(path):\n            continue\n\n        paths.append(os.path.abspath(path))\n\n    if not paths:\n        return\n\n    connection = socket.socket(socket.AF_INET, socket.SOCK_STREAM)\n    connection.connect((socket.gethostbyname(socket.gethostname()), 16384))\n    connection.send(\"{0}<!RE>\".format(\"\\n\".join(COMMAND_TEMPLATE).format(paths)))\n    connection.close()\n    return True"}, {"instruction": "def matrix(self, x=(0, 0), y=(0, 0) , z=(0, 0)):\r\n        \"\"\"\r\n        Copy the ``pyny.Polyhedron`` along a 3D matrix given by the \r\n        three tuples x, y, z:        \r\n\r\n        :param x: Number of copies and distance between them in this\r\n            direction.\r\n        :type x: tuple (len=2)\r\n        :returns: list of ``pyny.Polyhedron``\r\n        \"\"\"\n", "input": "", "output": "        polygon = np.array([[0,0], [0,1], [1,1]])\r\n        space = Space(Place(polygon, polyhedra=self))\r\n        space = space.matrix(x, y, z, inplace=False)\r\n        return [place.polyhedra[0] for place in space]"}, {"instruction": "def keys_by_alg_and_usage(self, issuer, alg, usage):\n        \"\"\"\n        Find all keys that can be used for a specific crypto algorithm and\n        usage by key owner.\n\n        :param issuer: Key owner\n        :param alg: a crypto algorithm\n        :param usage: What the key should be used for\n        :return: A possibly empty list of keys\n        \"\"\"\n", "input": "", "output": "        if usage in [\"sig\", \"ver\"]:\n            ktype = jws_alg2keytype(alg)\n        else:\n            ktype = jwe_alg2keytype(alg)\n\n        return self.get(usage, ktype, issuer)"}, {"instruction": "def _add_temporary_results(self, results, label):\n        \"\"\"Adds `results` to a temporary table with `label`.\n\n        :param results: results file\n        :type results: `File`\n        :param label: label to be associated with results\n        :type label: `str`\n\n        \"\"\"\n", "input": "", "output": "        NGRAM, SIZE, NAME, SIGLUM, COUNT, LABEL = constants.QUERY_FIELDNAMES\n        reader = csv.DictReader(results)\n        data = [(row[NGRAM], row[SIZE], row[NAME], row[SIGLUM], row[COUNT],\n                 label) for row in reader]\n        self._conn.executemany(constants.INSERT_TEMPORARY_RESULTS_SQL, data)"}, {"instruction": "def lazy_property(fn):\n    \"\"\"\n    Decorator that makes a property lazy-evaluated whilst preserving\n    docstrings.\n\n    Args:\n        fn (function): the property in question\n\n    Returns:\n        evaluated version of the property.\n    \"\"\"\n", "input": "", "output": "    attr_name = '_lazy_' + fn.__name__\n\n    @property\n    @wraps(fn)\n    def _lazy_property(self):\n        if not hasattr(self, attr_name):\n            setattr(self, attr_name, fn(self))\n        return getattr(self, attr_name)\n    return _lazy_property"}, {"instruction": "def to_dict(obj):\n    \"\"\"\n    Create a filtered dict from the given object.\n\n    Note: This function is currently specific to the FailureLine model.\n    \"\"\"\n", "input": "", "output": "    if not isinstance(obj.test, str):\n        # TODO: can we handle this in the DB?\n        # Reftests used to use tuple indicies, which we can't support.\n        # This is fixed upstream, but we also need to handle it here to allow\n        # for older branches.\n        return\n\n    keys = [\n        'id',\n        'job_guid',\n        'test',\n        'subtest',\n        'status',\n        'expected',\n        'message',\n        'best_classification',\n        'best_is_verified',\n    ]\n\n    all_fields = obj.to_dict()\n    return {k: v for k, v in all_fields.items() if k in keys}"}, {"instruction": "def replay_scope(self, sess):\n    \"\"\"Enters a replay scope that unsets it at the end.\"\"\"\n", "input": "", "output": "    current_replay = self.replay(sess)\n    try:\n      self.set_replay(sess, True)\n      yield\n    finally:\n      self.set_replay(sess, current_replay)"}, {"instruction": "def change_speed(body, speed=1):\n    \"\"\"Change the voice speed of the wave body.\"\"\"\n", "input": "", "output": "    if speed == 1:\n        return body\n\n    length = int(len(body) * speed)\n    rv = bytearray(length)\n\n    step = 0\n    for v in body:\n        i = int(step)\n        while i < int(step + speed) and i < length:\n            rv[i] = v\n            i += 1\n        step += speed\n    return rv"}, {"instruction": "def qos_rcv_queue_multicast_threshold_traffic_class6(self, **kwargs):\n        \"\"\"Auto Generated Code\n        \"\"\"\n", "input": "", "output": "        config = ET.Element(\"config\")\n        qos = ET.SubElement(config, \"qos\", xmlns=\"urn:brocade.com:mgmt:brocade-qos\")\n        rcv_queue = ET.SubElement(qos, \"rcv-queue\")\n        multicast = ET.SubElement(rcv_queue, \"multicast\")\n        threshold = ET.SubElement(multicast, \"threshold\")\n        traffic_class6 = ET.SubElement(threshold, \"traffic-class6\")\n        traffic_class6.text = kwargs.pop('traffic_class6')\n\n        callback = kwargs.pop('callback', self._callback)\n        return callback(config)"}, {"instruction": "def decode_feedback(binary_tuples):\n  \"\"\" Returns a list of tuples in (datetime, token_str) format \n  \n        binary_tuples   the binary-encoded feedback tuples\n  \"\"\"\n", "input": "", "output": "  \n  fmt = '!lh32s'\n  size = struct.calcsize(fmt)\n  with StringIO(binary_tuples) as f:\n    return [(datetime.datetime.fromtimestamp(ts), binascii.hexlify(tok))\n            for ts, toklen, tok in (struct.unpack(fmt, tup) \n                              for tup in iter(lambda: f.read(size), ''))]"}, {"instruction": "def recherche(self, pattern, entete):\n        \"\"\"Performs a search field by field, using functions defined in formats.\n        Matchs are marked with info[`font`]\n\n        :param pattern: String to look for\n        :param entete: Fields to look into\n        :return: Nothing. The collection is changed in place\n        \"\"\"\n", "input": "", "output": "\n        new_liste = []\n        sub_patterns = pattern.split(\" \")\n        for p in self:\n            d_font = {att: False for att in entete}\n            row_valid = True\n            for sub_pattern in sub_patterns:\n                found = False\n                for att in entete:\n                    fonction_recherche = formats.ASSOCIATION[att][1]\n                    attr_found = bool(fonction_recherche(p[att], sub_pattern))\n                    if attr_found:\n                        found = True\n                        d_font[att] = True\n                if not found:\n                    row_valid = False\n                    break\n            if row_valid:\n                new_liste.append(p)\n                info = dict(self.get_info(Id=p.Id),font=d_font)\n                self.infos[p.Id] = info\n\n        list.__init__(self, new_liste)"}, {"instruction": "async def try_trigger_before_first_request_functions(self) -> None:\n        \"\"\"Trigger the before first request methods.\"\"\"\n", "input": "", "output": "        if self._got_first_request:\n            return\n\n        # Reverse the teardown functions, so as to match the expected usage\n        self.teardown_appcontext_funcs = list(reversed(self.teardown_appcontext_funcs))\n        for key, value in self.teardown_request_funcs.items():\n            self.teardown_request_funcs[key] = list(reversed(value))\n        for key, value in self.teardown_websocket_funcs.items():\n            self.teardown_websocket_funcs[key] = list(reversed(value))\n\n        async with self._first_request_lock:\n            if self._got_first_request:\n                return\n            for function in self.before_first_request_funcs:\n                await function()\n            self._got_first_request = True"}, {"instruction": "def is_cnpjcpf(numero, estrito=False):\n    \"\"\"Uma vers\u00e3o conveniente para usar em testes condicionais. Apenas retorna\n    verdadeiro ou falso, conforme o argumento \u00e9 validado.\n\n    :param bool estrito: Padr\u00e3o ``False``, indica se apenas os d\u00edgitos do\n        n\u00famero dever\u00e3o ser considerados. Se verdadeiro, potenciais caracteres\n        que formam a m\u00e1scara ser\u00e3o removidos antes da valida\u00e7\u00e3o ser realizada.\n\n    \"\"\"\n", "input": "", "output": "    _numero = digitos(numero) if not estrito else numero\n    try:\n        cnpj(_numero)\n        return True\n    except NumeroCNPJError:\n        try:\n            cpf(_numero)\n            return True\n        except NumeroCPFError:\n            pass\n    return False"}, {"instruction": "def uf(sigla):\n    \"\"\"\n    Valida a sigla da Unidade Federativa. Se n\u00e3o for uma sigla de UF v\u00e1lida,\n    ser\u00e1 lan\u00e7ada a exce\u00e7\u00e3o :exc:`UnidadeFederativaError`.\n    \"\"\"\n", "input": "", "output": "    if not sigla in [s for s, i, n, r in UNIDADES_FEDERACAO]:\n        raise UnidadeFederativaError('Estado (sigla) UF \"%s\" '\n                'inexistente' % sigla)"}, {"instruction": "def search(**criteria):\n    \"\"\"\n    Search registered *component* classes matching the given criteria.\n\n    :param criteria: search criteria of the form: ``a='1', b='x'``\n    :return: parts registered with the given criteria\n    :rtype: :class:`set`\n\n    Will return an empty :class:`set` if nothing is found.\n\n    ::\n\n        from cqparts.search import search\n        import cqparts_motors  # example of a 3rd party lib\n\n        # Get all DC motor classes\n        dc_motors = search(type='motor', current_class='dc')\n\n        # For more complex queries:\n        air_cooled = search(cooling='air')\n        non_aircooled_dcmotors = dc_motors - air_cooled\n        # will be all DC motors that aren't air-cooled\n    \"\"\"\n", "input": "", "output": "    # Find all parts that match the given criteria\n    results = copy(class_list)  # start with full list\n    for (category, value) in criteria.items():\n        results &= index[category][value]\n\n    return results"}, {"instruction": "def sleep_if_necessary(cls, user, token, endpoint='search', msg=''):\n        \"\"\"Sleep a little if hit github recently to honor rate limit.\n        \"\"\"\n", "input": "", "output": "        my_kw = {'auth': (user, token)} if user else {}\n        info = requests.get('https://api.github.com/rate_limit', **my_kw)\n        info_dict = info.json()\n        remaining = info_dict['resources'][endpoint]['remaining']\n        logging.debug('Search remaining on github is at %s', remaining)\n\n        if remaining <= 5:\n            sleep_time = 120\n        else:\n            sleep_time = 0\n        if sleep_time:\n            logging.warning('Sleep %i since github requests remaining  = %i%s',\n                            sleep_time, remaining, msg)\n            time.sleep(sleep_time)\n            return True\n\n        return False"}, {"instruction": "def start(self, reloading=False):\n        \"\"\"Called when the module is loaded.\n\n        If the load is due to a reload of the module, then the 'reloading'\n        argument will be set to True. By default, this method calls the\n        controller's listen() for each event in the self.event_handlers dict.\n        \"\"\"\n", "input": "", "output": "        for event in self.event_handlers:\n            self.controller.listen(event)"}, {"instruction": "def copy_dataset_files(self, ds, incver=False, cb=None, **kwargs):\n        \"\"\"\n        Copy only files and configs into the database.\n        :param ds: The source dataset to copy\n        :param cb: A progress callback, taking two parameters: cb(message, num_records)\n        :return:\n        \"\"\"\n", "input": "", "output": "        from ambry.orm import File\n\n        tables = [File]\n\n        return self._copy_dataset_copy(ds, tables, incver, cb, **kwargs)"}, {"instruction": "def connect(self):\n        \"\"\"\n        Connects to Redis\n        \"\"\"\n", "input": "", "output": "        logger.info(\"Connecting to Redis on {host}:{port}...\".format(\n            host=self.host, port=self.port))\n\n        super(RedisSubscriber, self).connect()\n        logger.info(\"Successfully connected to Redis\")\n\n        # Subscribe to channel\n        self.pubsub = self.client.pubsub()\n        self.pubsub.subscribe(self.channel)\n\n        logger.info(\"Subscribed to [{channel}] Redis channel\".format(\n            channel=self.channel))\n\n        # Start listening\n        t = Thread(target=self.listen)\n        t.setDaemon(True)\n        t.start()"}, {"instruction": "def SetSchema(self, reader):\n        \"\"\"Use XSD Schema to validate the document as it is processed.\n          Activation is only possible before the first Read(). if\n          @schema is None, then Schema validation is desactivated. @\n          The @schema should not be freed until the reader is\n           deallocated or its use has been deactivated. \"\"\"\n", "input": "", "output": "        if reader is None: reader__o = None\n        else: reader__o = reader._o\n        ret = libxml2mod.xmlTextReaderSetSchema(reader__o, self._o)\n        return ret"}, {"instruction": "def send(self, data, **kws):\r\n        \"\"\"Send data to the socket. The socket must be connected to a remote\r\n        socket. Ammount sent may be less than the data provided.\"\"\"\n", "input": "", "output": "        return yield_(Send(self, data, timeout=self._timeout, **kws))"}, {"instruction": "def kunc_v(p, v0, k0, k0p, order=5, min_strain=0.01):\n    \"\"\"\n    find volume at given pressure using brenth in scipy.optimize\n\n    :param p: pressure in GPa\n    :param v0: unit-cell volume in A^3 at 1 bar\n    :param k0: bulk modulus at reference conditions\n    :param k0p: pressure derivative of bulk modulus at reference conditions\n    :param order: order of Kunc function\n    :param min_strain: defining minimum v/v0 value to search volume for\n    :return: unit-cell volume at high pressure in GPa\n    :note: a wrapper function vectorizing kunc_v_single\n    \"\"\"\n", "input": "", "output": "    if isuncertainties([p, v0, k0, k0p]):\n        f_u = np.vectorize(uct.wrap(kunc_v_single), excluded=[1, 2, 3, 4, 5])\n        return f_u(p, v0, k0, k0p, order=order, min_strain=min_strain)\n    else:\n        f_v = np.vectorize(kunc_v_single, excluded=[1, 2, 3, 4, 5])\n        return f_v(p, v0, k0, k0p, order=order, min_strain=min_strain)"}, {"instruction": "def _create_table(self, table_name):\n        ''' create sqlite's table for storing simple dictionaries\n        '''\n", "input": "", "output": "        if self.fieldnames:\n            sql_fields = []\n            for field in self._fields:\n                if field != '_id':\n                    if 'dblite' in self._fields[field]:\n                        sql_fields.append(' '.join([field, self._fields[field]['dblite']]))\n                    else:\n                        sql_fields.append(field)\n            sql_fields = ','.join(sql_fields)\n            SQL = 'CREATE TABLE IF NOT EXISTS %s (%s);' % (table_name, sql_fields)\n            try:\n                self._cursor.execute(SQL)\n            except sqlite3.OperationalError, err:\n                raise RuntimeError('Create table error, %s, SQL: %s' % (err, SQL))"}, {"instruction": "def delete_nic(access_token, subscription_id, resource_group, nic_name):\n    '''Delete a network interface.\n\n    Args:\n        access_token (str): A valid Azure authentication token.\n        subscription_id (str): Azure subscription id.\n        resource_group (str): Azure resource group name.\n        nic_name (str): Name of the NIC.\n\n    Returns:\n        HTTP response.\n    '''\n", "input": "", "output": "    endpoint = ''.join([get_rm_endpoint(),\n                        '/subscriptions/', subscription_id,\n                        '/resourceGroups/', resource_group,\n                        '/providers/Microsoft.Network/networkInterfaces/', nic_name,\n                        '?api-version=', NETWORK_API])\n    return do_delete(endpoint, access_token)"}, {"instruction": "def add_bits_to_path(path_, filename_prefix=None, extension=None):\r\n    \"\"\"\r\n    Adds prefix/suffix to filename\r\n\r\n    Arguments:\r\n        path_ -- path to file\r\n        filename_prefix -- prefix to be added to file name\r\n        extension -- extension to be added to file name. The dot is automatically added, such as\r\n            \"ext\" and \".ext\" will have the same effect\r\n\r\n    Examples:\r\n        > add_bits_to_path(\"/home/user/file\", \"prefix-\")\r\n        /home/user/prefix-file\r\n\r\n        > add_bits_to_path(\"/home/user/file\", None, \".ext\")\r\n        /home/user/file.ext\r\n\r\n        > add_bits_to_path(\"/home/user/file\", None, \"ext\")  # dot in extension is optional\r\n        /home/user/file.ext\r\n\r\n        > add_bits_to_path(\"/home/user/\", None, \".ext\")\r\n        /home/user/.ext\r\n    \"\"\"\n", "input": "", "output": "\r\n    dir_, basename = os.path.split(path_)\r\n\r\n    if filename_prefix:\r\n        basename = filename_prefix+basename\r\n    if extension:\r\n        if not extension.startswith(\".\"):\r\n            extension = \".\"+extension\r\n        basename = basename+extension\r\n\r\n    return os.path.join(dir_, basename)"}, {"instruction": "def db_for_write(self, model, **hints):\n        \"\"\"\n        Prevent write actions on read-only tables.\n\n        Raises:\n            WriteNotSupportedError: If models.sf_access is ``read_only``.\n\n        \"\"\"\n", "input": "", "output": "        try:\n            if model.sf_access == READ_ONLY:\n                raise WriteNotSupportedError(\"%r is a read-only model.\" % model)\n        except AttributeError:\n            pass\n        return None"}, {"instruction": "def add_lock(packages, root=None, **kwargs):  # pylint: disable=unused-argument\n    '''\n    Add a package lock. Specify packages to lock by exact name.\n\n    root\n        operate on a different root directory.\n\n    CLI Example:\n\n    .. code-block:: bash\n\n        salt '*' pkg.add_lock <package name>\n        salt '*' pkg.add_lock <package1>,<package2>,<package3>\n        salt '*' pkg.add_lock pkgs='[\"foo\", \"bar\"]'\n    '''\n", "input": "", "output": "    salt.utils.versions.warn_until('Sodium', 'This function is deprecated. Please use hold() instead.')\n    locks = list_locks(root)\n    added = []\n    try:\n        packages = list(__salt__['pkg_resource.parse_targets'](packages)[0].keys())\n    except MinionError as exc:\n        raise CommandExecutionError(exc)\n\n    for pkg in packages:\n        if not locks.get(pkg):\n            added.append(pkg)\n\n    if added:\n        __zypper__(root=root).call('al', *added)\n\n    return {'added': len(added), 'packages': added}"}, {"instruction": "def parse_args():\n  \"\"\"Parses command line arguments.\"\"\"\n", "input": "", "output": "  parser = ArgumentParser(description=\"ModelBase builder\")\n  subparsers = parser.add_subparsers()\n\n  sql_parser = subparsers.add_parser(\n    \"get-query\",\n    description=\"Usage: e.g. psql -c \\\"copy ($(python3 lib/generate_models.py get-query)) to \" +\n                \"stdout with csv header\\\" DB_NAME postgres\")\n  sql_parser.set_defaults(func=print_sql_query)\n\n  gen_parser = subparsers.add_parser(\"generate\")\n  gen_parser.add_argument(\"filename\", nargs=\"?\", help=\"Read this file for input, or STDIN if not \" \\\n                                                      \"given\")\n  gen_parser.add_argument(\"-i\", \"--indent\", default=\"  \")\n  gen_parser.add_argument(\"-c\", \"--created-at-col-name\", default=\"created_at\")\n  gen_parser.add_argument(\"-u\", \"--updated-at-col-name\", default=\"updated_at\")\n  gen_parser.set_defaults(func=generate_models)\n\n  args = parser.parse_args()\n\n  if hasattr(args, \"func\"):\n    return args\n  else:\n    arg_parser.print_help()\n    sys.exit(1)"}, {"instruction": "def get_attributes(self):\n        \"\"\"\n        Used by the uni_form_tags to get helper attributes\n        \"\"\"\n", "input": "", "output": "        items = {}\n        items['form_method'] = self.form_method.strip()\n        items['form_tag'] = self.form_tag\n        items['form_style'] = self.form_style.strip()\n        \n        if self.form_action:\n            items['form_action'] = self.form_action.strip()\n        if self.form_id:\n            items['id'] = self.form_id.strip()\n        if self.form_class:\n            items['class'] = self.form_class.strip()\n        if self.inputs:\n            items['inputs'] = self.inputs\n        if self.form_error_title:\n            items['form_error_title'] = self.form_error_title.strip()\n        if self.formset_error_title:\n            items['formset_error_title'] = self.formset_error_title.strip()\n        return items"}, {"instruction": "def followers(self):\n        \"\"\" :class:`Feed <pypump.models.feed.Feed>` with all\n        :class:`Person <pypump.models.person.Person>` objects following the person.\n\n        Example:\n            >>> alice = pump.Person('alice@example.org')\n            >>> for follower in alice.followers[:2]:\n            ...     print(follower.id)\n            ...\n            acct:bob@example.org\n            acct:carol@example.org\n        \"\"\"\n", "input": "", "output": "        if self._followers is None:\n            self._followers = Followers(self.links['followers'], pypump=self._pump)\n        return self._followers"}, {"instruction": "def get_foreign_keys_in_altered_table(self, diff):\n        \"\"\"\n        :param diff: The table diff\n        :type diff: eloquent.dbal.table_diff.TableDiff\n\n        :rtype: list\n        \"\"\"\n", "input": "", "output": "        foreign_keys = diff.from_table.get_foreign_keys()\n        column_names = self.get_column_names_in_altered_table(diff)\n\n        for key, constraint in foreign_keys.items():\n            changed = False\n            local_columns = []\n            for column_name in constraint.get_local_columns():\n                normalized_column_name = column_name.lower()\n                if normalized_column_name not in column_names:\n                    del foreign_keys[key]\n                    break\n                else:\n                    local_columns.append(column_names[normalized_column_name])\n                    if column_name != column_names[normalized_column_name]:\n                        changed = True\n\n            if changed:\n                pass\n\n        return foreign_keys"}, {"instruction": "def receive_ping(self, ping: Ping):\n        \"\"\" Handle a Ping message by answering with a Pong. \"\"\"\n", "input": "", "output": "\n        self.log_healthcheck.debug(\n            'Ping received',\n            message_id=ping.nonce,\n            message=ping,\n            sender=pex(ping.sender),\n        )\n\n        pong = Pong(nonce=ping.nonce)\n        self.raiden.sign(pong)\n\n        try:\n            self.maybe_send(ping.sender, pong)\n        except (InvalidAddress, UnknownAddress) as e:\n            self.log.debug(\"Couldn't send the `Delivered` message\", e=e)"}, {"instruction": "async def unformat(self):\n        \"\"\"Unformat this block device.\"\"\"\n", "input": "", "output": "        self._data = await self._handler.unformat(\n            system_id=self.node.system_id, id=self.id)"}, {"instruction": "def security_iter(nodearr):\n        \"\"\" provide a security data iterator by returning a tuple of (Element, SecurityError) which are mutually exclusive \"\"\"\n", "input": "", "output": "        assert nodearr.name() == 'securityData' and nodearr.isArray()\n        for i in range(nodearr.numValues()):\n            node = nodearr.getValue(i)\n            err = XmlHelper.get_security_error(node)\n            result = (None, err) if err else (node, None)\n            yield result"}, {"instruction": "def prepare_request(self, **kwargs):\n        \"\"\"\n        Configure all things to make real network request.\n        This method is called before doing real request via\n        transport extension.\n        \"\"\"\n", "input": "", "output": "\n        if self.transport is None:\n            self.setup_transport(self.transport_param)\n        self.reset()\n        self.request_counter = next(REQUEST_COUNTER)\n        if kwargs:\n            self.setup(**kwargs)\n        if self.proxylist.size() and self.config['proxy_auto_change']:\n            self.change_proxy()\n        self.request_method = self.detect_request_method()\n        self.transport.process_config(self)"}, {"instruction": "def CountClientPlatformsByLabel(self, day_buckets):\n    \"\"\"Computes client-activity stats for all client platforms in the DB.\"\"\"\n", "input": "", "output": "\n    def ExtractPlatform(client_info):\n      return client_info.last_snapshot.knowledge_base.os\n\n    return self._CountClientStatisticByLabel(day_buckets, ExtractPlatform)"}, {"instruction": "def _object_to_json(obj):\n        \"\"\"Convert objects that cannot be natively serialized into JSON\n        into their string representation\n\n        For datetime based objects convert them into their ISO formatted\n        string as specified by :meth:`datetime.datetime.isoformat`.\n\n        :param obj: object to convert into a JSON via getting its string\n            representation.\n        :type obj: object\n\n        :return: String value representing the given object ready to be\n            encoded into a JSON.\n        :rtype: str\n        \"\"\"\n", "input": "", "output": "        if isinstance(obj, datetime.datetime):\n            return obj.isoformat()\n        return repr(obj)"}, {"instruction": "def _copy(self, other, copy_func):\n        \"\"\"\n        Copies the contents of another ParsableOctetString object to itself\n\n        :param object:\n            Another instance of the same class\n\n        :param copy_func:\n            An reference of copy.copy() or copy.deepcopy() to use when copying\n            lists, dicts and objects\n        \"\"\"\n", "input": "", "output": "\n        super(ParsableOctetString, self)._copy(other, copy_func)\n        self._bytes = other._bytes\n        self._parsed = copy_func(other._parsed)"}, {"instruction": "def unwrap(self, message, signature):\n        \"\"\"\n        NTLM GSSUnwrap()\n        :param message: The message to be decrypted\n        :return: The decrypted message\n        \"\"\"\n", "input": "", "output": "        plain_text = _Ntlm2Session.decrypt(self, message)\n        _Ntlm2Session.verify(self, plain_text, signature)\n        return plain_text"}, {"instruction": "def dir_tails(self, rr_id: str) -> str:\n        \"\"\"\n        Return path to the correct directory for the tails file on input revocation registry identifier.\n\n        :param rr_id: revocation registry identifier of interest\n        :return: path to tails dir for input revocation registry identifier\n        \"\"\"\n", "input": "", "output": "\n        return Tails.dir(self._dir_tails, rr_id)"}, {"instruction": "def _get_content_type_queryset(models_list):\n    \"\"\" Get list of services content types \"\"\"\n", "input": "", "output": "    content_type_ids = {c.id for c in ContentType.objects.get_for_models(*models_list).values()}\n    return ContentType.objects.filter(id__in=content_type_ids)"}, {"instruction": "def _prepare_sample_data(self, submission_type):\n    \"\"\"Prepares sample data for the submission.\n\n    Args:\n      submission_type: type of the submission.\n    \"\"\"\n", "input": "", "output": "    # write images\n    images = np.random.randint(0, 256,\n                               size=[BATCH_SIZE, 299, 299, 3], dtype=np.uint8)\n    for i in range(BATCH_SIZE):\n      Image.fromarray(images[i, :, :, :]).save(\n          os.path.join(self._sample_input_dir, IMAGE_NAME_PATTERN.format(i)))\n    # write target class for targeted attacks\n    if submission_type == 'targeted_attack':\n      target_classes = np.random.randint(1, 1001, size=[BATCH_SIZE])\n      target_class_filename = os.path.join(self._sample_input_dir,\n                                           'target_class.csv')\n      with open(target_class_filename, 'w') as f:\n        for i in range(BATCH_SIZE):\n          f.write((IMAGE_NAME_PATTERN + ',{1}\\n').format(i, target_classes[i]))"}, {"instruction": "def bbox_vert_aligned_right(box1, box2):\n    \"\"\"\n    Returns true if the right boundary of both boxes is within 2 pts\n    \"\"\"\n", "input": "", "output": "    if not (box1 and box2):\n        return False\n    return abs(box1.right - box2.right) <= 2"}, {"instruction": "def chk_goids(goids, msg=None, raise_except=True):\n    \"\"\"check that all GO IDs have the proper format.\"\"\"\n", "input": "", "output": "    for goid in goids:\n        if not goid_is_valid(goid):\n            if raise_except:\n                raise RuntimeError(\"BAD GO({GO}): {MSG}\".format(GO=goid, MSG=msg))\n            else:\n                return goid"}, {"instruction": "def instance(host=None, port=None):\n        \"\"\"\n        Singleton to return only one instance of Server.\n\n        :returns: instance of Server\n        \"\"\"\n", "input": "", "output": "\n        if not hasattr(WebServer, \"_instance\") or WebServer._instance is None:\n            assert host is not None\n            assert port is not None\n            WebServer._instance = WebServer(host, port)\n        return WebServer._instance"}, {"instruction": "def callback_oauth2(self, request):\n        \"\"\"\n            Process for oAuth 2\n            :param request: contains the current session\n            :return:\n        \"\"\"\n", "input": "", "output": "        callback_url = self.callback_url(request)\n\n        oauth = OAuth2Session(client_id=self.consumer_key, redirect_uri=callback_url, scope=self.scope)\n        request_token = oauth.fetch_token(self.REQ_TOKEN,\n                                          code=request.GET.get('code', ''),\n                                          authorization_response=callback_url,\n                                          client_secret=self.consumer_secret,\n                                          scope=self.scope,\n                                          verify=False)\n        return request_token.get('access_token')"}, {"instruction": "def _construct_key(self, rule_id: str, spacy_rule_id:int) -> int:\n        \"\"\"\n        Use a mapping to store the information about rule_id for each matches, create the mapping key here\n        Args:\n            rule_id: str\n            spacy_rule_id:int\n\n        Returns: int\n        \"\"\"\n", "input": "", "output": "\n        hash_key = (rule_id, spacy_rule_id)\n        hash_v = hash(hash_key) + sys.maxsize + 1\n        self._hash_map[hash_v] = hash_key\n        return hash_v"}, {"instruction": "def toString(self):\n        \"\"\" Returns time as string. \"\"\"\n", "input": "", "output": "        slist = self.toList()\n        string = angle.slistStr(slist)\n        return string if slist[0] == '-' else string[1:]"}, {"instruction": "def is_volatile(self):\n        \"\"\"\n        True if combination of field access properties result in a field that\n        should be interpreted as volatile.\n        (Any hardware-writable field is inherently volatile)\n        \"\"\"\n", "input": "", "output": "\n        hw = self.get_property('hw')\n        return (\n            (hw in (rdltypes.AccessType.rw, rdltypes.AccessType.rw1,\n                    rdltypes.AccessType.w, rdltypes.AccessType.w1))\n            or self.get_property('counter')\n            or (self.get_property('next') is not None)\n            or self.get_property('hwset')\n            or self.get_property('hwclr')\n        )"}, {"instruction": "def mkdir(self, name=None, folder_id='0'):\n\t\t'''Create a folder with a specified \"name\" attribute.\n\t\t\tfolder_id allows to specify a parent folder.'''\n", "input": "", "output": "\t\treturn self( 'folders', method='post', encode='json',\n\t\t\tdata=dict(name=name, parent=dict(id=folder_id)) )"}, {"instruction": "def approx_equals(self, other, atol):\n        \"\"\"Return ``True`` in case of approximate equality.\n\n        Returns\n        -------\n        approx_eq : bool\n            ``True`` if ``other`` is a `RectPartition` instance with\n            ``self.set == other.set`` up to ``atol`` and\n            ``self.grid == other.other`` up to ``atol``, ``False`` otherwise.\n        \"\"\"\n", "input": "", "output": "        if other is self:\n            return True\n        elif not isinstance(other, RectPartition):\n            return False\n        else:\n            return (self.set.approx_equals(other.set, atol=atol) and\n                    self.grid.approx_equals(other.grid, atol=atol))"}, {"instruction": "def bic(self) -> str:\n        \"\"\"Generate random ``BIC`` (Bank ID Code).\n\n        :return: BIC.\n\n        :Example:\n            044025575.\n        \"\"\"\n", "input": "", "output": "        country_code = '04'\n        code = '{:02}'.format(self.random.randint(1, 10))\n        bank_number = '{:02}'.format(self.random.randint(0, 99))\n        bank_office = '{:03}'.format(self.random.randint(50, 999))\n        bic = country_code + code + bank_number + bank_office\n        return bic"}, {"instruction": "def fingerprint(self):\n    \"\"\"A memoized sha1 hexdigest hashing the contents of this PayloadField\n\n    The fingerprint returns either a string or None.  If the return is None, consumers of the\n    fingerprint may choose to elide this PayloadField from their combined hash computation.\n\n    :API: public\n    \"\"\"\n", "input": "", "output": "    if self._fingerprint_memo is None:\n      self._fingerprint_memo = self._compute_fingerprint()\n    return self._fingerprint_memo"}, {"instruction": "def _cryptography_cipher(key, iv):\n    \"\"\"Build a cryptography TripleDES Cipher object.\n\n    :param bytes key: Encryption key\n    :param bytesiv iv: Initialization vector\n    :returns: TripleDES Cipher instance\n    :rtype: cryptography.hazmat.primitives.ciphers.Cipher\n    \"\"\"\n", "input": "", "output": "    return Cipher(\n        algorithm=algorithms.TripleDES(key),\n        mode=modes.CBC(iv),\n        backend=default_backend()\n    )"}, {"instruction": "def execute_ssh_command(self, client, command):\n        \"\"\"Execute the provided command and log output.\"\"\"\n", "input": "", "output": "        try:\n            out = ipa_utils.execute_ssh_command(client, command)\n        except Exception as error:\n            raise IpaCloudException(\n                'Command: \"{0}\", failed execution: {1}.'.format(\n                    command, error\n                )\n            )\n        else:\n            self._write_to_log(out)"}, {"instruction": "def anonymous_login(self):\n        \"\"\"Login as anonymous user\n\n        :return: logon result, see `CMsgClientLogonResponse.eresult <https://github.com/ValvePython/steam/blob/513c68ca081dc9409df932ad86c66100164380a6/protobufs/steammessages_clientserver.proto#L95-L118>`_\n        :rtype: :class:`.EResult`\n        \"\"\"\n", "input": "", "output": "        self._LOG.debug(\"Attempting Anonymous login\")\n\n        self._pre_login()\n\n        self.username = None\n        self.login_key = None\n\n        message = MsgProto(EMsg.ClientLogon)\n        message.header.steamid = SteamID(type='AnonUser', universe='Public')\n        message.body.protocol_version = 65579\n        self.send(message)\n\n        resp = self.wait_msg(EMsg.ClientLogOnResponse, timeout=30)\n        return EResult(resp.body.eresult) if resp else EResult.Fail"}, {"instruction": "def scan(self, folder, sub=None, next_=None):\n        \"\"\" Request immediate rescan of a folder, or a specific path within a\n        folder.\n\n            Args:\n                folder (str): Folder ID.\n                sub (str): Path relative to the folder root. If sub is omitted\n                    the entire folder is scanned for changes, otherwise only\n                    the given path children are scanned.\n                next_ (int): Delays Syncthing's automated rescan interval for\n                    a given amount of seconds.\n\n            Returns:\n                str\n        \"\"\"\n", "input": "", "output": "        if not sub:\n            sub = ''\n        assert isinstance(sub, string_types)\n        assert isinstance(next_, int) or next_ is None\n        return self.post('scan', params={'folder': folder,\n                                         'sub': sub,\n                                         'next': next_})"}, {"instruction": "def show_state_usage(queue=False, **kwargs):\n    '''\n    Retrieve the highstate data from the salt master to analyse used and unused states\n\n    Custom Pillar data can be passed with the ``pillar`` kwarg.\n\n    CLI Example:\n\n    .. code-block:: bash\n\n        salt '*' state.show_state_usage\n    '''\n", "input": "", "output": "    conflict = _check_queue(queue, kwargs)\n    if conflict is not None:\n        return conflict\n    pillar = kwargs.get('pillar')\n    pillar_enc = kwargs.get('pillar_enc')\n    if pillar_enc is None \\\n            and pillar is not None \\\n            and not isinstance(pillar, dict):\n        raise SaltInvocationError(\n            'Pillar data must be formatted as a dictionary, unless pillar_enc '\n            'is specified.'\n        )\n\n    st_ = salt.state.HighState(__opts__, pillar, pillar_enc=pillar_enc)\n    st_.push_active()\n\n    try:\n        ret = st_.compile_state_usage()\n    finally:\n        st_.pop_active()\n    _set_retcode(ret)\n    return ret"}, {"instruction": "def times(A, b, offset=0):\n    \"\"\"\n    Times the view of A with b in place (!).\n    Returns modified A\n    Broadcasting is allowed, thus b can be scalar.\n\n    if offset is not zero, make sure b is of right shape!\n\n    :param ndarray A: 2 dimensional array\n    :param ndarray-like b: either one dimensional or scalar\n    :param int offset: same as in view.\n    :rtype: view of A, which is adjusted inplace\n    \"\"\"\n", "input": "", "output": "    return _diag_ufunc(A, b, offset, np.multiply)"}, {"instruction": "def raw_repr(obj):\n    '''Produce a representation using the default repr() regardless of\n    whether the object provides an implementation of its own.'''\n", "input": "", "output": "    if isproxy(obj):\n        return '<%s with prime_id=%d>' % (obj.__class__.__name__, obj.prime_id)\n    else:\n        return repr(obj)"}, {"instruction": "def disable_cors(self):\n        \"\"\"\n        Switches CORS off.\n\n        :returns: CORS status in JSON format\n        \"\"\"\n", "input": "", "output": "        return self.update_cors_configuration(\n            enable_cors=False,\n            allow_credentials=False,\n            origins=[],\n            overwrite_origins=True\n        )"}, {"instruction": "def triangle(self, x1, y1, x2, y2, x3, y3, color):\n        \"\"\"\n        See the Processing function triangle():\n        https://processing.org/reference/triangle_.html\n        \"\"\"\n", "input": "", "output": "        self.context.set_source_rgb(*color)\n        self.context.move_to(self.tx(x1), self.ty(y1))\n        self.context.line_to(self.tx(x2), self.ty(y2))\n        self.context.line_to(self.tx(x3), self.ty(y3))\n        self.context.line_to(self.tx(x1), self.ty(y1))\n        self.context.fill()"}, {"instruction": "def write(content, filename='cache'):\n\t\"\"\" write data to cache file\n\tparameters:\n\t\tcache_path - path to cache file\n\t\tcontent - a data structure to save into cache file\"\"\"\n", "input": "", "output": "\tcache_path = get_cache_path(filename)\n\twith open(cache_path, 'w') as file:\n\t\tif content is not None:\n\t\t\tjson.dump(content, file, indent=3, sort_keys=True)"}, {"instruction": "def get_ip(request):\n    \"\"\"Return the IP address inside the HTTP_X_FORWARDED_FOR var inside\n    the `request` object.\n\n    The return of this function can be overrided by the\n    `LOCAL_GEOLOCATION_IP` variable in the `conf` module.\n\n    This function will skip local IPs (starting with 10. and equals to\n    127.0.0.1).\n    \"\"\"\n", "input": "", "output": "    if getsetting('LOCAL_GEOLOCATION_IP'):\n        return getsetting('LOCAL_GEOLOCATION_IP')\n\n    forwarded_for = request.META.get('HTTP_X_FORWARDED_FOR')\n\n    if not forwarded_for:\n        return UNKNOWN_IP\n\n    for ip in forwarded_for.split(','):\n        ip = ip.strip()\n        if not ip.startswith('10.') and not ip == '127.0.0.1':\n            return ip\n\n    return UNKNOWN_IP"}, {"instruction": "def compute_stats(array, stats, weights):\n    \"\"\"\n    :param array:\n        an array of R elements (which can be arrays)\n    :param stats:\n        a sequence of S statistic functions\n    :param weights:\n        a list of R weights\n    :returns:\n        an array of S elements (which can be arrays)\n    \"\"\"\n", "input": "", "output": "    result = numpy.zeros((len(stats),) + array.shape[1:], array.dtype)\n    for i, func in enumerate(stats):\n        result[i] = apply_stat(func, array, weights)\n    return result"}, {"instruction": "def buffer_iter(self, block_size=1024):\n        \"\"\"\n        Iterate through chunks of the vertices, and indices buffers seamlessly.\n\n        .. note::\n\n            To see a usage example, look at the :class:`ShapeBuffer` description.\n        \"\"\"\n", "input": "", "output": "        streams = (\n            self.vert_data,\n            self.idx_data,\n        )\n\n        # Chain streams seamlessly\n        for stream in streams:\n            stream.seek(0)\n            while True:\n                chunk = stream.read(block_size)\n                if chunk:\n                    yield chunk\n                else:\n                    break"}, {"instruction": "def linear_insert(self, item, priority):\n    \"\"\"Linear search. Performance is O(n^2).\"\"\"\n", "input": "", "output": "\n    with self.lock:\n        self_data = self.data\n        rotate = self_data.rotate\n        maxlen = self._maxlen\n        length = len(self_data)\n        count = length\n\n        # in practice, this is better than doing a rotate(-1) every\n        # loop and getting self.data[0] each time only because deque\n        # implements a very efficient iterator in C\n        for i in self_data:\n            if priority > i[1]:\n                break\n            count -= 1\n\n        rotate(-count)\n        self_data.appendleft((item, priority))\n        rotate(length-count)\n\n        try:\n            self.items[item] += 1\n        except TypeError:\n            self.items[repr(item)] += 1\n\n        if maxlen is not None and maxlen < len(self_data):\n                self._poplast()"}, {"instruction": "def get(feature, obj, **kwargs):\n    '''Obtain a feature from a set of morphology objects\n\n    Parameters:\n        feature(string): feature to extract\n        obj: a neuron, population or neurite tree\n        **kwargs: parameters to forward to underlying worker functions\n\n    Returns:\n        features as a 1D or 2D numpy array.\n\n    '''\n", "input": "", "output": "\n    feature = (NEURITEFEATURES[feature] if feature in NEURITEFEATURES\n               else NEURONFEATURES[feature])\n\n    return _np.array(list(feature(obj, **kwargs)))"}, {"instruction": "def head(self, n=10):\n        \"\"\"\n        Display the top of the file.\n\n        Args:\n            n (int): Number of lines to display\n        \"\"\"\n", "input": "", "output": "        r = self.__repr__().split('\\n')\n        print('\\n'.join(r[:n]), end=' ')"}, {"instruction": "def delete_unique_identity(db, uuid):\n    \"\"\"Remove a unique identity from the registry.\n\n    Function that removes from the registry, the unique identity\n    that matches with uuid. Data related to this identity will be\n    also removed.\n\n    It checks first whether the unique identity is already on the registry.\n    When it is found, the unique identity is removed. Otherwise, it will\n    raise a 'NotFoundError' exception.\n\n    :param db: database manager\n    :param uuid: unique identifier assigned to the unique identity set\n        for being removed\n\n    :raises NotFoundError: raised when the unique identity does not exist\n        in the registry.\n    \"\"\"\n", "input": "", "output": "    with db.connect() as session:\n        uidentity = find_unique_identity(session, uuid)\n\n        if not uidentity:\n            raise NotFoundError(entity=uuid)\n\n        delete_unique_identity_db(session, uidentity)"}, {"instruction": "def _wrap_parse(code, filename):\n        \"\"\"\n        async wrapper is required to avoid await calls raising a SyntaxError\n        \"\"\"\n", "input": "", "output": "        code = 'async def wrapper():\\n' + indent(code, ' ')\n        return ast.parse(code, filename=filename).body[0].body[0].value"}, {"instruction": "def sonTraceRootPath():\n    \"\"\"\n    function for finding external location\n    \"\"\"\n", "input": "", "output": "    import sonLib.bioio\n    i = os.path.abspath(sonLib.bioio.__file__)\n    return os.path.split(os.path.split(os.path.split(i)[0])[0])[0]"}, {"instruction": "def expire_queues(self):\n        '''\n        Expires old queue_dict keys that have not been used in a long time.\n        Prevents slow memory build up when crawling lots of different domains\n        '''\n", "input": "", "output": "        curr_time = time.time()\n        for key in list(self.queue_dict):\n            diff = curr_time - self.queue_dict[key][1]\n            if diff > self.queue_timeout:\n                self.logger.debug(\"Expiring domain queue key \" + key)\n                del self.queue_dict[key]\n                if key in self.queue_keys:\n                    self.queue_keys.remove(key)"}, {"instruction": "def run(self):\n        \"\"\"\n        1. count the words for each of the :py:meth:`~.InputText.output` targets created by :py:class:`~.InputText`\n        2. write the count into the :py:meth:`~.WordCount.output` target\n        \"\"\"\n", "input": "", "output": "        count = {}\n\n        # NOTE: self.input() actually returns an element for the InputText.output() target\n        for f in self.input():  # The input() method is a wrapper around requires() that returns Target objects\n            for line in f.open('r'):  # Target objects are a file system/format abstraction and this will return a file stream object\n                for word in line.strip().split():\n                    count[word] = count.get(word, 0) + 1\n\n        # output data\n        f = self.output().open('w')\n        for word, count in six.iteritems(count):\n            f.write(\"%s\\t%d\\n\" % (word, count))\n        f.close()"}, {"instruction": "def get_automation(self, automation_id, refresh=False):\n        \"\"\"Get a single automation.\"\"\"\n", "input": "", "output": "        if self._automations is None:\n            self.get_automations()\n            refresh = False\n\n        automation = self._automations.get(str(automation_id))\n\n        if automation and refresh:\n            automation.refresh()\n\n        return automation"}, {"instruction": "def match_repository_configuration(url, page_size=10, page_index=0, sort=\"\"):\n    \"\"\"\n    Search for Repository Configurations based on internal or external url with exact match\n    \"\"\"\n", "input": "", "output": "    content = match_repository_configuration_raw(url, page_size, page_index, sort)\n    if content:\n        return utils.format_json_list(content)"}, {"instruction": "def from_fqdn(cls, fqdn):\n        \"\"\"Retrieve domain id associated to a FQDN.\"\"\"\n", "input": "", "output": "        result = cls.list({'fqdn': fqdn})\n        if len(result) > 0:\n            return result[0]['id']"}, {"instruction": "def get_dataframe(self, *args, **kwargs):\n        \"\"\"\n        Retrieve data as a Pandas dataframe.\n\n        Args:\n            search: (dict) Search query like {\"categ_A\": \"val_A\", \"categ_B\": \"val_B\"},\n                documented at https://developer.mpds.io/#Categories\n            phases: (list) Phase IDs, according to the MPDS distinct phases concept\n            fields: (dict) Data of interest for C-, S-, and P-entries,\n                e.g. for phase diagrams: {'C': ['naxes', 'arity', 'shapes']},\n                documented at https://developer.mpds.io/#JSON-schemata\n            columns: (list) Column names for Pandas dataframe\n\n        Returns: (object) Pandas dataframe object containing the results\n        \"\"\"\n", "input": "", "output": "        columns = kwargs.get('columns')\n        if columns:\n            del kwargs['columns']\n        else:\n            columns = self.default_titles\n\n        return pd.DataFrame(self.get_data(*args, **kwargs), columns=columns)"}, {"instruction": "def dump(self, output, close_after_write=True):\n        \"\"\"Write a worksheet to the current workbook.\n\n        Args:\n            output (str):\n                Path to the workbook file to write.\n            close_after_write (bool, optional):\n                Close the workbook after write.\n                Defaults to |True|.\n        \"\"\"\n", "input": "", "output": "\n        self.open(output)\n        try:\n            self.make_worksheet(self.table_name)\n            self.write_table()\n        finally:\n            if close_after_write:\n                self.close()"}, {"instruction": "def make_k8s_lando_router(config, obj, queue_name):\n        \"\"\"\n        Makes MessageRouter which can listen to queue_name sending messages to the k8s version of lando.\n        :param config: WorkerConfig/ServerConfig: settings for connecting to the queue\n        :param obj: object: implements lando specific methods\n        :param queue_name: str: name of the queue we will listen on.\n        :return MessageRouter\n        \"\"\"\n", "input": "", "output": "        return MessageRouter(config, obj, queue_name, K8S_LANDO_INCOMING_MESSAGES,\n                             processor_constructor=WorkQueueProcessor)"}, {"instruction": "def max_len(iterable, minimum=0):\n    \"\"\"Return the len() of the longest item in ``iterable`` or ``minimum``.\n\n    >>> max_len(['spam', 'ham'])\n    4\n\n    >>> max_len([])\n    0\n\n    >>> max_len(['ham'], 4)\n    4\n    \"\"\"\n", "input": "", "output": "    try:\n        result = max(map(len, iterable))\n    except ValueError:\n        result = minimum\n    return minimum if result < minimum else result"}, {"instruction": "def set_size(self, size):\n        \"\"\"Set figure size\"\"\"\n", "input": "", "output": "        w, h = size\n        self.root.set('width', w)\n        self.root.set('height', h)"}, {"instruction": "def send_request(endpoint, **kwargs):\n    \"\"\"Return the response to a query as JSON from the NewsAPI web service.\n\n    The basic API is limited to 100 results which is chosen unless explicitly\n    given as an argument. Beyond that, paging is supported through the \"page\"\n    argument, if needed.\n\n    Parameters\n    ----------\n    endpoint : str\n        Endpoint to query, e.g. \"everything\" or \"top-headlines\"\n\n    kwargs : dict\n        A list of keyword arguments passed as parameters with the query.\n        The basic ones are \"q\" which is the search query, \"from\" is a start\n        date formatted as for instance 2018-06-10 and \"to\" is an end date\n        with the same format.\n\n    Returns\n    -------\n    res_json : dict\n        The response from the web service as a JSON dict.\n    \"\"\"\n", "input": "", "output": "    if api_key is None:\n        logger.error('NewsAPI cannot be used without an API key')\n        return None\n    url = '%s/%s' % (newsapi_url, endpoint)\n    if 'apiKey' not in kwargs:\n        kwargs['apiKey'] = api_key\n    if 'pageSize' not in kwargs:\n        kwargs['pageSize'] = 100\n    res = requests.get(url, params=kwargs)\n    res.raise_for_status()\n    res_json = res.json() \n    return res_json"}, {"instruction": "def ecdsa_private_key(privkey_str=None, compressed=None):\n    \"\"\"\n    Make a private key, but enforce the following rule:\n    * unless the key's hex encoding specifically ends in '01', treat it as uncompressed.\n    \"\"\"\n", "input": "", "output": "    if compressed is None:\n        compressed = False\n        if privkey_str is not None:\n            if len(privkey_str) == 66 and privkey_str[-2:] == '01':\n                compressed = True\n\n    return _ECPrivateKey(privkey_str, compressed=compressed)"}, {"instruction": "def _load_data():\n    \"\"\"Load the word and character mapping data into a dictionary.\n\n    In the data files, each line is formatted like this:\n        HANZI   PINYIN_READING/PINYIN_READING\n\n    So, lines need to be split by '\\t' and then the Pinyin readings need to be\n    split by '/'.\n\n    \"\"\"\n", "input": "", "output": "    data = {}\n    for name, file_name in (('words', 'hanzi_pinyin_words.tsv'),\n                            ('characters', 'hanzi_pinyin_characters.tsv')):\n        # Split the lines by tabs: [[hanzi, pinyin]...].\n        lines = [line.split('\\t') for line in\n                 dragonmapper.data.load_data_file(file_name)]\n        # Make a dictionary: {hanzi: [pinyin, pinyin]...}.\n        data[name] = {hanzi: pinyin.split('/') for hanzi, pinyin in lines}\n    return data"}, {"instruction": "def clean(self):\n        \"\"\"\n        Automatically construct the suggestion input and weight by taking all\n        possible permutation of Person's name as ``input`` and taking their\n        popularity as ``weight``.\n        \"\"\"\n", "input": "", "output": "        self.suggest = {\n            'input': [' '.join(p) for p in permutations(self.name.split())],\n            'weight': self.popularity\n        }"}, {"instruction": "def _visit(self, element, operation):\n        \"\"\"\n        Visit and execute a operation in element and descendants.\n\n        :param element: The element.\n        :type element: hatemile.util.html.htmldomelement.HTMLDOMElement\n        :param operation: The operation to be executed.\n        :type operation: function\n        \"\"\"\n", "input": "", "output": "\n        if self._is_valid_inherit_element(element):\n            if element.has_children_elements():\n                children = element.get_children_elements()\n                for child in children:\n                    self._visit(child, operation)\n            elif self._is_valid_element(element):\n                operation(element)"}, {"instruction": "def upsert(self, table: str, record: dict, create_cols: bool=False,\n               dtypes: list=None, pks=[\"id\"], namefields=[\"id\"]):\n        \"\"\"\n        Upsert a record in a table\n        \"\"\"\n", "input": "", "output": "        try:\n            self.db[table].upsert(record, pks, create_cols, dtypes)\n        except Exception as e:\n            self.err(e, \"Can not upsert data\")\n            return\n        names = \"\"\n        for el in namefields:\n            names += \" \" + record[el]\n        self.ok(\"Upserted record\"+names)"}, {"instruction": "def get_prep_value(self, value):\n        \"\"\"Returns field's value prepared for saving into a database.\"\"\"\n", "input": "", "output": "\n        if isinstance(value, LocalizedValue):\n            prep_value = LocalizedValue()\n            for k, v in value.__dict__.items():\n                if v is None:\n                    prep_value.set(k, '')\n                else:\n                    # Need to convert File objects provided via a form to\n                    # unicode for database insertion\n                    prep_value.set(k, six.text_type(v))\n            return super().get_prep_value(prep_value)\n        return super().get_prep_value(value)"}, {"instruction": "def chunks(l, n):\n    \"\"\"Yield successive n-sized chunks from l.\"\"\"\n", "input": "", "output": "    if n:\n        for i in xrange(0, len(l), n):\n            yield l[i:i + n]"}, {"instruction": "def leaves(self, nodes=None, unique=True):\n        \"\"\"Get the leaves of the tree starting at this root.\n\n        Args:\n            nodes (iterable): limit leaves for these node names\n            unique: only include individual leaf nodes once\n\n        Returns:\n            list of leaf nodes\n\n        \"\"\"\n", "input": "", "output": "        if nodes is None:\n            return super(DependencyTree, self).leaves(unique=unique)\n\n        res = list()\n        for child_id in nodes:\n            for sub_child in self._all_nodes[child_id].leaves(unique=unique):\n                if not unique or sub_child not in res:\n                    res.append(sub_child)\n        return res"}, {"instruction": "def create_hparams():\n  \"\"\"Create hparams.\"\"\"\n", "input": "", "output": "  if FLAGS.use_tpu and \"tpu\" not in FLAGS.hparams_set:\n    tf.logging.warn(\"Not all hyperparameter sets work on TPU. \"\n                    \"Prefer hparams_sets with a '_tpu' suffix, \"\n                    \"e.g. transformer_tpu, if available for your model.\")\n  hparams_path = os.path.join(FLAGS.output_dir, \"hparams.json\")\n  return trainer_lib.create_hparams(FLAGS.hparams_set, FLAGS.hparams,\n                                    hparams_path=hparams_path)"}, {"instruction": "def windowing(self, windowing):\n        \"\"\"Sets the windowing of this ChartSettings.\n\n        For the tabular view, whether to use the full time window for the query or the last X minutes  # noqa: E501\n\n        :param windowing: The windowing of this ChartSettings.  # noqa: E501\n        :type: str\n        \"\"\"\n", "input": "", "output": "        allowed_values = [\"full\", \"last\"]  # noqa: E501\n        if windowing not in allowed_values:\n            raise ValueError(\n                \"Invalid value for `windowing` ({0}), must be one of {1}\"  # noqa: E501\n                .format(windowing, allowed_values)\n            )\n\n        self._windowing = windowing"}, {"instruction": "def parse_obj(obj):\n    \"\"\"\n    >>> parse_obj('bucket/key')\n    ('bucket', 'key')\n    >>> parse_obj('my-bucket/path/to/file.txt')\n    ('my-bucket', 'path/to/file.txt')\n    >>> parse_obj('s3://this_bucket/some/path.txt')\n    ('this_bucket', 'some/path.txt')\n    >>> parse_obj('https://s3.amazonaws.com/bucket/file.txt')\n    ('bucket', 'file.txt')\n    >>> parse_obj('http://the-bucket.s3.amazonaws.com/the/file.txt')\n    ('the-bucket', 'the/file.txt')\n    \"\"\"\n", "input": "", "output": "    obj = obj.lstrip('s3://')\n    if obj.startswith('http'):\n        url = urlparse.urlparse(obj)\n        if url.netloc == 's3.amazonaws.com':\n            path = url.path[1:]  # remove leading slash\n            bucket, key = path.split('/', 1)\n        else:\n            # bucket.s3.amazonaws.com form\n            bucket = url.netloc.split('.', 1)[0]\n            key = url.path[1:]\n    else:\n        bucket, key = obj.split('/', 1)\n    return bucket, key"}, {"instruction": "def make_request(parameters):\n    \"\"\"Submit a getfeature request to DataBC WFS and return features\n    \"\"\"\n", "input": "", "output": "    r = requests.get(bcdata.WFS_URL, params=parameters)\n    return r.json()[\"features\"]"}, {"instruction": "def modify(self, **kwargs):\n        \"\"\"We need to implement the custom exclusive parameter check.\"\"\"\n", "input": "", "output": "        self._check_exclusive_parameters(**kwargs)\n        return super(Rule, self)._modify(**kwargs)"}, {"instruction": "def btc_is_p2sh_script( script_hex ):\n    \"\"\"\n    Is the given scriptpubkey a p2sh script?\n    \"\"\"\n", "input": "", "output": "    if script_hex.startswith(\"a914\") and script_hex.endswith(\"87\") and len(script_hex) == 46:\n        return True\n    else:\n        return False"}, {"instruction": "def _remove_unicode_encoding(xml_file):\n    '''\n    attempts to remove the \"encoding='unicode'\" from an xml file\n    as lxml does not support that on a windows node currently\n    see issue #38100\n    '''\n", "input": "", "output": "    with salt.utils.files.fopen(xml_file, 'rb') as f:\n        xml_content = f.read()\n    modified_xml = re.sub(r' encoding=[\\'\"]+unicode[\\'\"]+', '', xml_content.decode('utf-16'), count=1)\n    xmltree = lxml.etree.parse(six.StringIO(modified_xml))\n    return xmltree"}, {"instruction": "def Split(g, *, maxbuffer=10, tuple_len=None):\n    \"\"\"\n    Split a tuple generator into individual generators.\n\n    Parameters\n    ----------\n    g: tohu generator\n        The generator to be split. The items produced by `g` must be tuples.\n    maxbuffer: integer\n        Maximum number of items produced by `g` that will be buffered.\n    \"\"\"\n", "input": "", "output": "    if tuple_len is None:\n        try:\n            tuple_len = g.tuple_len\n        except AttributeError:\n            raise ValueError(\"Argument 'tuple_len' must be given since generator is not of type TupleGenerator.\")\n\n    g_buffered = BufferedTuple(g, maxbuffer=maxbuffer, tuple_len=tuple_len)\n\n    return tuple(NthElementBuffered(g_buffered, i) for i in range(tuple_len))"}, {"instruction": "def to_bytes(value):\n    \"\"\" str to bytes (py3k) \"\"\"\n", "input": "", "output": "    vtype = type(value)\n\n    if vtype == bytes or vtype == type(None):\n        return value\n\n    try:\n        return vtype.encode(value)\n    except UnicodeEncodeError:\n        pass\n    return value"}, {"instruction": "def add_timescale(self, timescale):\n        \"\"\"\n        Add a time scale to the plot.\n\n        :param timescale: the timescale to be added\n        :type  timescale: :class:`~aeneas.plotter.PlotTimeScale`\n        :raises: TypeError: if ``timescale`` is not an instance of :class:`~aeneas.plotter.PlotTimeScale`\n        \"\"\"\n", "input": "", "output": "        if not isinstance(timescale, PlotTimeScale):\n            self.log_exc(u\"timescale must be an instance of PlotTimeScale\", None, True, TypeError)\n        self.timescale = timescale\n        self.log(u\"Added timescale\")"}, {"instruction": "def _compute_distance_scaling(self, rup, dists, C):\n        \"\"\"\n        Compute distance-scaling term, equations (3) and (4), pag 107.\n        \"\"\"\n", "input": "", "output": "        Mref = 4.5\n        Rref = 1.0\n        R = np.sqrt(dists.rjb ** 2 + C['h'] ** 2)\n        return (C['c1'] + C['c2'] * (rup.mag - Mref)) * np.log(R / Rref) + \\\n            C['c3'] * (R - Rref)"}, {"instruction": "def start_worker(self):\n        \"\"\"Trigger new process as a RQ worker.\"\"\"\n", "input": "", "output": "        if not self.include_rq:\n            return None\n\n        worker = Worker(queues=self.queues,\n                        connection=self.connection)\n        worker_pid_path = current_app.config.get(\n            \"{}_WORKER_PID\".format(self.config_prefix), 'rl_worker.pid'\n        )\n\n        try:\n            worker_pid_file = open(worker_pid_path, 'r')\n            worker_pid = int(worker_pid_file.read())\n            print(\"Worker already started with PID=%d\" % worker_pid)\n            worker_pid_file.close()\n\n            return worker_pid\n\n        except (IOError, TypeError):\n            self.worker_process = Process(target=worker_wrapper, kwargs={\n                'worker_instance': worker,\n                'pid_path': worker_pid_path\n            })\n            self.worker_process.start()\n            worker_pid_file = open(worker_pid_path, 'w')\n            worker_pid_file.write(\"%d\" % self.worker_process.pid)\n            worker_pid_file.close()\n\n            print(\"Start a worker process with PID=%d\" %\n                  self.worker_process.pid)\n\n            return self.worker_process.pid"}, {"instruction": "def get_gateway_socket(gateway):\r\n    \"\"\"Takes a gateway address string and returns a non-blocking UDP\r\n       socket to communicate with its NAT-PMP implementation on\r\n       NATPMP_PORT.\r\n       \r\n       e.g. addr = get_gateway_socket('10.0.1.1')\r\n    \"\"\"\n", "input": "", "output": "    if not gateway:\r\n        raise NATPMPNetworkError(NATPMP_GATEWAY_NO_VALID_GATEWAY,\r\n                                 error_str(NATPMP_GATEWAY_NO_VALID_GATEWAY))\r\n    response_socket = socket.socket(socket.AF_INET, socket.SOCK_DGRAM)\r\n    response_socket.setblocking(0)\r\n    response_socket.connect((gateway, NATPMP_PORT))\r\n    return response_socket"}, {"instruction": "def rosmsg(self):\n        \"\"\":obj:`sensor_msgs.Image` : ROS Image\n        \"\"\"\n", "input": "", "output": "        from cv_bridge import CvBridge, CvBridgeError\n        cv_bridge = CvBridge()\n        try:\n            return cv_bridge.cv2_to_imgmsg(self._data, encoding=self._encoding)\n        except CvBridgeError as cv_bridge_exception:\n            logging.error('%s' % (str(cv_bridge_exception)))"}, {"instruction": "def byteify(input_object):\n    \"\"\"Recursive function to transform an object to byte.\n\n    :param input_object: A python object such as unicode, dictionary or list.\n    :type: unicode, list, dict\n\n    :return: The object with byte only.\n    \"\"\"\n", "input": "", "output": "    if isinstance(input_object, dict):\n        return {byteify(key): byteify(value)\n                for key, value in list(input_object.items())}\n    elif isinstance(input_object, list):\n        return [byteify(element) for element in input_object]\n    elif isinstance(input_object, str):\n        return input_object.encode('utf-8')\n    else:\n        return input_object"}, {"instruction": "def save_graph(cn_topo, filename, showintfs=False, showaddrs=False):\n    '''\n    Save the topology to an image file \n    '''\n", "input": "", "output": "    __do_draw(cn_topo, showintfs=showintfs, showaddrs=showaddrs)\n    pyp.savefig(filename)"}, {"instruction": "def branches(self):\n        \"\"\"\n        Returns a data frame of all branches in origin.  The DataFrame will have the columns:\n\n         * repository\n         * local\n         * branch\n\n        :returns: DataFrame\n        \"\"\"\n", "input": "", "output": "\n        df = pd.DataFrame(columns=['repository', 'local', 'branch'])\n\n        if _has_joblib:\n            ds = Parallel(n_jobs=-1, backend='threading', verbose=0)(\n                delayed(_branches_func)\n                (x) for x in self.repos\n            )\n            for d in ds:\n                df = df.append(d)\n        else:\n            for repo in self.repos:\n                try:\n                    df = df.append(_branches_func(repo))\n                except GitCommandError:\n                    print('Warning! Repo: %s couldn\\'t be inspected' % (repo, ))\n\n        df.reset_index()\n\n        return df"}, {"instruction": "def get_all_repos(self):\n        \"\"\"Gets user repos\n        :return: List of all user repositories (public, orgs and private)\n        \"\"\"\n", "input": "", "output": "        url = \"https://api.github.com/user/repos\"\n        params = {\n            \"access_token\": GITHUB_TOKEN\n        }  # add auth params\n        url = add_params_to_url(url, params)\n        return self._get_repos(url)"}, {"instruction": "def auth_approle(self, role_id, secret_id=None, mount_point='approle', use_token=True):\n        \"\"\"POST /auth/<mount_point>/login\n\n        :param role_id:\n        :type role_id:\n        :param secret_id:\n        :type secret_id:\n        :param mount_point:\n        :type mount_point:\n        :param use_token:\n        :type use_token:\n        :return:\n        :rtype:\n        \"\"\"\n", "input": "", "output": "        params = {\n            'role_id': role_id\n        }\n        if secret_id is not None:\n            params['secret_id'] = secret_id\n\n        return self.login('/v1/auth/{0}/login'.format(mount_point), json=params, use_token=use_token)"}, {"instruction": "def mobility(sdat, tstart=None, tend=None):\n    \"\"\"Plates mobility.\n\n    Compute the ratio vsurf / vrms.\n\n    Args:\n        sdat (:class:`~stagpy.stagyydata.StagyyData`): a StagyyData instance.\n        tstart (float): time at which the computation should start. Use the\n            beginning of the time series data if set to None.\n        tend (float): time at which the computation should end. Use the\n            end of the time series data if set to None.\n    Returns:\n        tuple of :class:`numpy.array`: mobility and time arrays.\n    \"\"\"\n", "input": "", "output": "    tseries = sdat.tseries_between(tstart, tend)\n    steps = sdat.steps[tseries.index[0]:tseries.index[-1]]\n    time = []\n    mob = []\n    for step in steps.filter(rprof=True):\n        time.append(step.timeinfo['t'])\n        mob.append(step.rprof.iloc[-1].loc['vrms'] / step.timeinfo['vrms'])\n    return np.array(mob), np.array(time)"}, {"instruction": "def get_user_info(apikey, username, password):\n    \"\"\"\n    blogger.getUserInfo(api_key, username, password)\n    => user structure\n    \"\"\"\n", "input": "", "output": "    user = authenticate(username, password)\n    site = Site.objects.get_current()\n    return user_structure(user, site)"}, {"instruction": "def _fetch_partition_info(self, topic_id, partition_id):\n        \"\"\"Fetch partition info for given topic-partition.\"\"\"\n", "input": "", "output": "        info_path = \"/brokers/topics/{topic_id}/partitions/{p_id}\"\n        try:\n            _, partition_info = self.get(\n                info_path.format(topic_id=topic_id, p_id=partition_id),\n            )\n            return partition_info\n        except NoNodeError:\n            return {}"}, {"instruction": "def _get_line(self) -> str:\n        \"\"\"Returns the current line from the file while incrementing the index.\"\"\"\n", "input": "", "output": "        line = self.in_lines[self.index]\n        self.index += 1\n        return line"}, {"instruction": "def from_tuples(cls, tups):\n        \"\"\"\n        Create a new IntervalTree from an iterable of 2- or 3-tuples,\n         where the tuple lists begin, end, and optionally data.\n        \"\"\"\n", "input": "", "output": "        ivs = [Interval(*t) for t in tups]\n        return IntervalTree(ivs)"}, {"instruction": "def from_parmed(cls, path, *args, **kwargs):\n        \"\"\"\n        Try to load a file automatically with ParmEd. Not guaranteed to work, but\n        might be useful if it succeeds.\n\n        Arguments\n        ---------\n        path : str\n            Path to file that ParmEd can load\n        \"\"\"\n", "input": "", "output": "        st = parmed.load_file(path, structure=True, *args, **kwargs)\n        box = kwargs.pop('box', getattr(st, 'box', None))\n        velocities = kwargs.pop('velocities', getattr(st, 'velocities', None))\n        positions = kwargs.pop('positions', getattr(st, 'positions', None))\n        return cls(master=st, topology=st.topology, positions=positions, box=box,\n                   velocities=velocities, path=path, **kwargs)"}, {"instruction": "def path_helper(self, operations, view, app=None, **kwargs):\n        \"\"\"Path helper that allows passing a Flask view function.\"\"\"\n", "input": "", "output": "        rule = self._rule_for_view(view, app=app)\n        operations.update(yaml_utils.load_operations_from_docstring(view.__doc__))\n        if hasattr(view, 'view_class') and issubclass(view.view_class, MethodView):\n            for method in view.methods:\n                if method in rule.methods:\n                    method_name = method.lower()\n                    method = getattr(view.view_class, method_name)\n                    operations[method_name] = yaml_utils.load_yaml_from_docstring(method.__doc__)\n        return self.flaskpath2openapi(rule.rule)"}, {"instruction": "def cartesian_to_spherical(vectors):\n    \"\"\"\n    Return the spherical coordinates for coordinates in Cartesian space.\n\n    This function does an opposite to :func:`spherical_to_cartesian`.\n\n    :param vectors:\n        Array of 3d vectors in Cartesian space of shape (..., 3)\n    :returns:\n        Tuple of three arrays of the same shape as ``vectors`` representing\n        longitude (decimal degrees), latitude (decimal degrees) and depth (km)\n        in specified order.\n    \"\"\"\n", "input": "", "output": "    rr = numpy.sqrt(numpy.sum(vectors * vectors, axis=-1))\n    xx, yy, zz = vectors.T\n    lats = numpy.degrees(numpy.arcsin((zz / rr).clip(-1., 1.)))\n    lons = numpy.degrees(numpy.arctan2(yy, xx))\n    depths = EARTH_RADIUS - rr\n    return lons.T, lats.T, depths"}, {"instruction": "def make_transparent(image):\n    \"\"\"Turn all black pixels in an image into transparent ones\"\"\"\n", "input": "", "output": "    data = image.copy().getdata()\n    modified = []\n    for item in data:\n        if _check_pixel(item) is True:\n            modified.append((255, 255, 255, 255))  # White transparent pixel\n            continue\n        modified.append(item)\n    image.putdata(modified)\n    return image"}, {"instruction": "def _writeConnectivity(self, links, fileObject):\n        \"\"\"\n        Write Connectivity Lines to File Method\n        \"\"\"\n", "input": "", "output": "        for link in links:\n            linkNum = link.linkNumber\n            downLink = link.downstreamLinkID\n            numUpLinks = link.numUpstreamLinks\n            upLinks = ''\n            for upLink in link.upstreamLinks:\n                upLinks = '{}{:>5}'.format(upLinks, str(upLink.upstreamLinkID))\n\n            line = 'CONNECT{:>5}{:>5}{:>5}{}\\n'.format(linkNum, downLink, numUpLinks, upLinks)\n            fileObject.write(line)\n        fileObject.write('\\n')"}, {"instruction": "def parse_kv(args):\n    ''' convert a string of key/value items to a dict '''\n", "input": "", "output": "\n    options = {}\n    if args is not None:\n        # attempting to split a unicode here does bad things\n        vargs = shlex.split(str(args), posix=True)\n        for x in vargs:\n            if x.find(\"=\") != -1:\n                k, v = x.split(\"=\",1)\n                options[k]=v\n    return options"}, {"instruction": "def _output_ret(self, ret, out, retcode=0):\n        '''\n        Print the output from a single return to the terminal\n        '''\n", "input": "", "output": "        import salt.output\n        # Handle special case commands\n        if self.config['fun'] == 'sys.doc' and not isinstance(ret, Exception):\n            self._print_docs(ret)\n        else:\n            # Determine the proper output method and run it\n            salt.output.display_output(ret,\n                                       out=out,\n                                       opts=self.config,\n                                       _retcode=retcode)\n        if not ret:\n            sys.stderr.write('ERROR: No return received\\n')\n            sys.exit(2)"}, {"instruction": "def timestamp_file():\n    \"\"\"Opens a file for tracking the time of the last version check\"\"\"\n", "input": "", "output": "    config_dir = os.path.join(\n        os.path.expanduser(\"~\"), BaseGlobalConfig.config_local_dir\n    )\n\n    if not os.path.exists(config_dir):\n        os.mkdir(config_dir)\n\n    timestamp_file = os.path.join(config_dir, \"cumulus_timestamp\")\n\n    try:\n        with open(timestamp_file, \"r+\") as f:\n            yield f\n    except IOError:  # file does not exist\n        with open(timestamp_file, \"w+\") as f:\n            yield f"}, {"instruction": "def update_jsonb(uid, extinfo):\n        '''\n        Update the json.\n        '''\n", "input": "", "output": "        cur_extinfo = MPost.get_by_uid(uid).extinfo\n        for key in extinfo:\n            cur_extinfo[key] = extinfo[key]\n        entry = TabPost.update(\n            extinfo=cur_extinfo,\n        ).where(TabPost.uid == uid)\n        entry.execute()\n        return uid"}, {"instruction": "def cancelMarketDepth(self, contracts=None):\n        \"\"\"\n        Cancel streaming market data for contract\n        https://www.interactivebrokers.com/en/software/api/apiguide/java/cancelmktdepth.htm\n        \"\"\"\n", "input": "", "output": "        if contracts == None:\n            contracts = list(self.contracts.values())\n        elif not isinstance(contracts, list):\n            contracts = [contracts]\n\n        for contract in contracts:\n            tickerId = self.tickerId(self.contractString(contract))\n            self.ibConn.cancelMktDepth(tickerId=tickerId)"}, {"instruction": "def update_metadata(self, scaling_group, metadata):\n        \"\"\"\n        Adds the given metadata dict to the existing metadata for the scaling\n        group.\n        \"\"\"\n", "input": "", "output": "        if not isinstance(scaling_group, ScalingGroup):\n            scaling_group = self.get(scaling_group)\n        curr_meta = scaling_group.metadata\n        curr_meta.update(metadata)\n        return self.update(scaling_group, metadata=curr_meta)"}, {"instruction": "def delete(self, object_id):\n        \"\"\"\n        Delete an object by its id\n\n        :param object_id: the objects id.\n        :return: the deleted object\n        :raises: :class: NoResultFound when the object could not be found\n        \"\"\"\n", "input": "", "output": "        obj = self.session.query(self.cls).filter_by(id=object_id).one()\n        self.session.delete(obj)\n        return obj"}, {"instruction": "def set_unavailable(self):\n        \"\"\"Sets the agent availability to False.\"\"\"\n", "input": "", "output": "        show = PresenceShow.NONE\n        self.set_presence(PresenceState(available=False, show=show))"}, {"instruction": "def _get_s3_key():\n    '''\n    Get AWS keys from pillar or config\n    '''\n", "input": "", "output": "\n    key = __opts__['s3.key'] if 's3.key' in __opts__ else None\n    keyid = __opts__['s3.keyid'] if 's3.keyid' in __opts__ else None\n    service_url = __opts__['s3.service_url'] \\\n        if 's3.service_url' in __opts__ \\\n        else None\n    verify_ssl = __opts__['s3.verify_ssl'] \\\n        if 's3.verify_ssl' in __opts__ \\\n        else None\n    kms_keyid = __opts__['aws.kmw.keyid'] if 'aws.kms.keyid' in __opts__ else None\n    location = __opts__['s3.location'] \\\n        if 's3.location' in __opts__ \\\n        else None\n    path_style = __opts__['s3.path_style'] \\\n        if 's3.path_style' in __opts__ \\\n        else None\n    https_enable = __opts__['s3.https_enable'] \\\n        if 's3.https_enable' in __opts__ \\\n        else None\n\n    return key, keyid, service_url, verify_ssl, kms_keyid, location, path_style, https_enable"}, {"instruction": "def get_content_from_url(self, base_url):\n        \"\"\"\n        Sections can have SectionPage and ArticlePage child objects.\n        These have different fields, and thus have to be treated\n        differently.\n        \"\"\"\n", "input": "", "output": "        # assemble url\n        base_url = base_url.rstrip(\"/\")\n        url = base_url + API_PAGES_ENDPOINT + \"?type=\" + self._content_type + \\\n            \"&fields=\" + \",\".join(self._fields) + \\\n            \"&order=latest_revision_created_at\"\n\n        # make request\n        try:\n            response = requests.get(url)\n            self._base_url = base_url\n            self._content = response.json()\n            self._content = self._content[\"items\"]\n            return self._content\n        except requests.exceptions.ConnectionError:\n            return \"No content could be found from {}. \" \\\n                \"Are you sure this is the correct URL?\".format(base_url)\n        except requests.exceptions.RequestException:\n            return \"Content could not be imported at this time. \" \\\n                   \"Please try again later.\""}, {"instruction": "def _load_config(self):\n        \"\"\"\n        Loads the YAML configuration file and sets python dictionary and raw contents\n        as instance attrs.\n        \"\"\"\n", "input": "", "output": "        if not os.path.exists(self._path):\n            sys.exit(\"Config path %s does not exist\" % self._path)\n        # create empty config object\n        self._config_dict = {}\n        # read file and marshal yaml\n        with open(self._path, 'r') as f:\n            self._raw = f.read()\n            self._config_dict = yaml.load(self._raw)"}, {"instruction": "def package(self):\n        \"\"\"Copy Flatbuffers' artifacts to package folder\n        \"\"\"\n", "input": "", "output": "        cmake = self.configure_cmake()\n        cmake.install()\n        self.copy(pattern=\"LICENSE.txt\", dst=\"licenses\")\n        self.copy(pattern=\"FindFlatBuffers.cmake\", dst=os.path.join(\"lib\", \"cmake\", \"flatbuffers\"), src=\"CMake\")\n        self.copy(pattern=\"flathash*\", dst=\"bin\", src=\"bin\")\n        self.copy(pattern=\"flatc*\", dst=\"bin\", src=\"bin\")\n        if self.settings.os == \"Windows\" and self.options.shared:\n            if self.settings.compiler == \"Visual Studio\":\n                shutil.move(os.path.join(self.package_folder, \"lib\", \"%s.dll\" % self.name),\n                            os.path.join(self.package_folder, \"bin\", \"%s.dll\" % self.name))\n            elif self.settings.compiler == \"gcc\":\n                shutil.move(os.path.join(self.package_folder, \"lib\", \"lib%s.dll\" % self.name),\n                            os.path.join(self.package_folder, \"bin\", \"lib%s.dll\" % self.name))"}, {"instruction": "def clear_context(self, app=None):\n        \"\"\"Clear the component's context.\n\n        Keyword Args:\n            app (flask.Flask, optional): The app to clear this component's\n                context for. If omitted, the value from ``Component.app`` is\n                used.\n        \"\"\"\n", "input": "", "output": "        if (app is None and self._context is _CONTEXT_MISSING\n                and not in_app_context()):\n            raise RuntimeError(\"Attempted to clear component context without\"\n                               \" a bound app context or eager app set! Please\"\n                               \" pass the related app you want to update the\"\n                               \" context for!\")\n\n        if self._context is not _CONTEXT_MISSING:\n            self._context = DEFAULT_DICT\n        else:\n            key = self._get_context_name(app=app)\n            setattr(_CONTEXT_LOCALS, key, DEFAULT_DICT)"}, {"instruction": "def _Rzderiv(self,R,z,phi=0.,t=0.):\n        \"\"\"\n        NAME:\n           _Rzderiv\n        PURPOSE:\n           evaluate the mixed radial, vertical derivative for this potential\n        INPUT:\n           R - Galactocentric cylindrical radius\n           z - vertical height\n           phi - azimuth\n           t - time\n        OUTPUT:\n           the mixed radial, vertical derivative\n        \"\"\"\n", "input": "", "output": "        if not self.isNonAxi:\n            phi= 0.\n        x,y,z= self._compute_xyz(R,phi,z,t)\n        phixza= self._2ndderiv_xyz(x,y,z,0,2)\n        phiyza= self._2ndderiv_xyz(x,y,z,1,2)\n        ang = self._omegab*t + self._pa\n        c, s = np.cos(ang), np.sin(ang)\n        phixz = c*phixza + s*phiyza\n        phiyz = -s*phixza + c*phiyza\n        return np.cos(phi)*phixz + np.sin(phi)*phiyz"}, {"instruction": "async def power(source, exponent):\n    \"\"\"Raise the elements of an asynchronous sequence to the given power.\"\"\"\n", "input": "", "output": "    async with streamcontext(source) as streamer:\n        async for item in streamer:\n            yield item ** exponent"}, {"instruction": "def _prune_invalid_time_reductions(spec):\n    \"\"\"Prune time reductions of spec with no time dimension.\"\"\"\n", "input": "", "output": "    valid_reductions = []\n    if not spec['var'].def_time and spec['dtype_out_time'] is not None:\n        for reduction in spec['dtype_out_time']:\n            if reduction not in _TIME_DEFINED_REDUCTIONS:\n                valid_reductions.append(reduction)\n            else:\n                msg = (\"Var {0} has no time dimension \"\n                       \"for the given time reduction \"\n                       \"{1} so this calculation will \"\n                       \"be skipped\".format(spec['var'].name, reduction))\n                logging.info(msg)\n    else:\n        valid_reductions = spec['dtype_out_time']\n    return valid_reductions"}]